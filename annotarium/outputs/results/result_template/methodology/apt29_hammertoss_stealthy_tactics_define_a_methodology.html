<article class="wiki-page">
<header><h1>Annotarium Methodology: APT1: Exposing One of China's Cyber Espionage Units</h1><div class="wiki-meta">Generated at 2026-02-22T21:11:52Z</div></header>
<nav class="wiki-toc"><h2>Contents</h2><ol>
<li><a href="#sec-introduction">Introduction and Epistemic Framing</a></li><li><a href="#sec-scope_units">Scope, Units of Analysis, and Output Semantics</a></li><li><a href="#sec-data_ingestion">Data Ingestion and Corpus Handling</a></li><li><a href="#sec-pdf_to_markdown">PDF-to-Markdown Conversion</a></li><li><a href="#sec-structural_parsing">Structural Parsing of Text, Tables, and Figures</a></li><li><a href="#sec-artifact_extraction">Artifact Extraction and Technical Object Normalization</a></li><li><a href="#sec-reference_parsing">Footnote and Reference Parsing</a></li><li><a href="#sec-institution_inference">Institution Inference and Source Typology</a></li><li><a href="#sec-claim_evidence_graph">Claim-Evidence Graph Construction</a></li><li><a href="#sec-scoring_overview">Scoring Framework Overview</a></li><li><a href="#sec-chain_of_custody">Chain of Custody Axis</a></li><li><a href="#sec-credibility_corroboration">Credibility Axis with Corroboration Subcomponent</a></li><li><a href="#sec-clarity_axis">Clarity Axis and State Responsibility Pathways</a></li><li><a href="#sec-aggregation_calibration">Aggregation, Calibration, and Uncertainty</a></li><li><a href="#sec-validation_quality_assurance">Validation and Quality Assurance</a></li><li><a href="#sec-limitations_governance">Limitations, Governance, and Future Refinement</a></li>
</ol></nav>
<section class="wiki-section" id="sec-introduction"><section aria-labelledby="intro"><h2 id="intro">Introduction and Epistemic Framing</h2><p>This methodology is presented as an evidentiary framework for cyber attribution under contestation. Its epistemic posture is explicitly jurisprudential: the analyst treats attribution as a contestable evidentiary claim rather than as a mere declarative observation. The framework is burden-sensitive in that it distinguishes between the raw presence of material and the probative force that material may carry in adversarial review. Accordingly, the method privileges transparent chains linking proposition to source and insists that structured extraction precede legal inference so that inferential leaps remain auditable and reversible.</p><p>Framing the work in this manner recognizes two realities. First, cyber-attribution dossiers are heterogeneous collections of artifacts, formal citations, and analytic narration; their evidentiary import can only be assessed when each unit is represented in a schema that preserves anchors, identifiers, and provenance. Second, courts and international fora adjudicating state responsibility conventionally apply differential weighting to heterogeneous materials; translating those judicially legible practices into reproducible processes requires a disciplined, document-level approach to extraction and scoring rather than reliance on holistic impression.</p></section><section aria-labelledby="data"><h2 id="data">Data Processing and Extraction</h2><p>Data processing proceeds through deterministic, auditable stages to mitigate parser convenience and to sustain persistence of intermediate records. Primary transcription from PDF to markdown is effected by the configured provider-backed OCR process (process_pdf_mistral_ocr.py) with a documented offline fallback using PyMuPDF4LLM when provider conversion fails or times out. The pipeline emits a strict evidentiary record: metadata, a source inventory, artifact indices, claim-level analytic blocks, and anchors for tables and figures. Structural parsing isolates paragraphs, captions, footnotes, and tabular cells and assigns stable identifiers so that downstream integrity checks can detect collisions or missing anchors.</p><p>Artifact extraction implements schema-constrained extraction of observable indicators (hashes, timestamps, network artifacts, file names, capture provenance) and normalizes temporal anchors and versioning metadata. Extraction logic emphasizes preservation over inference: fields are populated only when explicitly present in the transcribed text or in anchored table cells. Deterministic transformations are preferred where practicable and any model-assisted extraction is constrained by schema validators and post-run verification to ensure reproducibility.</p></section><section aria-labelledby="refs"><h2 id="refs">Reference Parsing and Institution Inference</h2><p>Reference parsing identifies citations and footnote-like references and links them to a source registry. The registry records bibliographic metadata, retrieval URIs, and anchor loci within the document. Parsing follows the pipeline methods disclosed in the methodological record and enforces admissibility-style integrity checks: unresolved references, ambiguous anchors, or citation pathways that point to non-existent registry entries are flagged and cause run-level failures until resolved.</p><p>Institution inference operates as a constrained, auditable augmentation rather than a free-form attribution. The configured inference module (infer_source_institutions.py using gpt-5-mini with an optional web fallback) proposes institutional identities and confidence markers, but each inferred institution is recorded with its provenance and with the textual cues that supported the inference. This permits later contestation of inferred institutional identifiers without altering the primary evidentiary record.</p></section><section aria-labelledby="scoring"><h2 id="scoring">Scoring Framework</h2><p>The scoring framework is motivated by an ICJ-inspired evidentiary logic. Item-level weight is assessed along bounded dimensions—independence, authentication/provenance, methodological soundness, procedural testing, and contemporaneity—and is combined multiplicatively so that serious deficiency in any one dimension constrains overall item probative force. Corroboration is modeled as constrained convergence: evidence is clustered by origin, aggregated with diminishing returns, and barred from deriving independence through downstream repetition of a single upstream source.</p><p>Top-level outputs are organized around the core 3Cs: Chain of Custody, Credibility, and Clarity. Chain of Custody is computed from normalized indicators (provenance markers, integrity markers, temporal anchors, artifact identifiers, and update lineage) and bounded within a unit interval to preserve auditable interpretability. Credibility integrates source-quality measures and corroborative convergence, is calibrated by a document-level credibility_coverage_factor tied to high-quality source support, and explicitly penalizes single-source dependence. Clarity assesses whether attribution propositions are articulated with the specificity required for legal intelligibility, and whether a legal pathway—state organ conduct, control/direction of non-state actors, or omission/due diligence failure—is coherently signposted in the text.</p></section><section aria-labelledby="validation"><h2 id="validation">Validation and Quality Assurance</h2><p>Validation consists of structural integrity checks, deterministic replayability, and explicit failure modes. Integrity controls mirror admissibility discipline: runs fail when identifier collisions occur, anchors are missing, citation pathways cannot be traced, or references resolve to nonexistent entities. Persistence policies require that intermediate artifacts be written to disk so that any score can be reconstructed and contested from the underlying record. These controls are intended to reduce pseudo-precision and to ensure that numerical outputs remain tethered to inspectable evidence.</p><p>Quality assurance includes post-extraction verification of institution inferences, cross-checks between extracted artifacts and cited sources, and spot audits of multiplicative item-level scoring to ensure no single indicator dominates improperly. All validation actions, flags, and resolutions are recorded in the evidentiary registry to preserve an audit trail for subsequent adversarial review. The approach is expressly conservative and burden-sensitive: it provides an evidentiary scaffold that supports legal inference without substituting technical scoring for adjudicative judgment.</p></section></section>
<section class="wiki-section" id="sec-scope_units"><p>Introduction: This methodological statement defines units of analysis and the semantic content of quantitative outputs produced by an automated cyber‑attribution scoring pipeline. The intent is to set out principled definitions and interpretive boundaries so that downstream consumers can understand what a given numeric score does—and crucially, what it does not—imply. The exposition that follows is grounded in the document metadata and processing counts made available for the sample input document (for example, a vendor report identified in metadata as "APT1: Exposing One of China's Cyber Espionage Units," with provenance indicators captured in source_locator and inferred publication_date), but it deliberately treats those items only as illustrative background for methodological description rather than as substantive findings about the document itself.</p>

<p>Data processing and extraction: Source materials enter the pipeline as documents and are subject to staged transformation: ingestion of file‑level metadata, conversion to a structured text representation, and structural parsing to identify discrete units such as pages, sections, tables, figures, and candidate artifact strings. In the present sample the pipeline registered a small instrumented input (pages: 1) with upstream counts of claims and artifacts (claims: 3, artifacts: 6). Extraction proceeds from coarse to fine granularity: document‑level metadata (title, authoring_entity, publication_date and source_locator) anchors provenance; structural parsing produces candidate document segments; and artifact_extraction isolates artifacts (for example, file names, hashes, network indicators) that become attributable evidence items. At each step provenance and custody metadata are attached to the derived object so that subsequent scoring can condition on processing history and source locality.</p>

<p>References and institution inference: Reference parsing attempts to canonicalize cited sources and identifiers, when present, and to link those references to external institutional entities. Institution inference is an interpretive layer that maps textual mentions, headers, and metadata to plausible issuing institutions while tracking uncertainty. Where explicit citations are absent or limited (the sample shows citation_coverage_sources_0_1: 0.0 and citations_total: 0), institution inference relies on internal metadata signals and documented heuristics but preserves that uncertainty in downstream scores rather than collapsing it into deterministic assertions. All inferred institution attributions are represented as probabilistic associations tied to the underlying evidence items and to the document-level provenance record.</p>

<p>Scoring framework and output semantics: The pipeline produces measured scores at multiple hierarchical units of analysis: the claim, the source, the artifact, the evidence item, and the document-level aggregate. A claim score encodes the calibrated degree to which the available evidence supports a discrete propositional assertion; a source score represents assessed reliability of an identified reporting or originating actor; an artifact score conveys the assessed integrity and traceability of a specific technical object (for example, a binary hash or log record); an evidence item score reflects the combined grounding and custody attributes of a single extracted unit; and a document-level score aggregates these component measures into summary axes such as grounding, custody, credibility, corroboration, clarity, and belief_weighted. These axes are explicitly descriptive: a higher custody_avg_0_100 indicates that the artifacts and evidence items retained stronger provenance and handling metadata in the pipeline records, whereas a higher grounding_avg_0_100 indicates denser linkage between claims and concrete extracted evidence. Scores are not probabilistic proofs of factual truth, do not substitute for independent legal verification, and do not by themselves identify actors with legal certainty. For example metrics in the sample (belief_weighted_0_100 approximately 0.05 and custody_avg_0_100 approximately 61.4) are presented to illustrate scale relationships between dimensions, not to assert substantive conclusions about the underlying entity.</p>

<p>Validation and quality assurance: Quality assurance combines automated checks, calibration against held‑out benchmarks, and human audit. Bootstrap confidence intervals around aggregate metrics (as represented in the sample bootstrap_95ci fields) quantify sampling and processing uncertainty. Validation processes include consistency checks between claim-level grounding scores and the evidence_support fractions, custody trace audits that verify presence of source_locator and processing timestamps, and targeted manual review of items with extreme or contradictory axis scores. Calibration is maintained by periodic re‑weighting of scoring subcomponents based on observed agreement between human adjudicators and automated scores; any adjustments are logged and versioned so that document-level provenance retains a record of the scoring regime. Together these procedures are intended to make scores transparent and defensible for downstream policy, legal, or intelligence uses while maintaining clear limits on inferential claims.</p></section>
<section class="wiki-section" id="sec-data_ingestion"><section>
  <h2>Introduction</h2>
  <p>This methodology describes the ingestion and corpus handling procedures used to convert source documents into intermediate artifacts that support downstream attribution scoring. The account emphasizes deterministic operations and reproducibility guarantees across the pipeline from raw PDF inputs through markup, structural parsing, artifact extraction, reference linking, and institution inference. Where appropriate, the description cites concrete artifacts and storage locations within the processing environment to demonstrate how the input corpus and its derivatives are tracked and preserved.</p>

  <h2>Scope and Units of Analysis</h2>
  <p>The input corpus for the procedures described herein consists of discrete source files and their machine-generated derivatives. For a given report this corpus includes the original PDF (preserved as an immutable file), the pipeline extraction output, validation and scoring artifacts, and the ancillary markdown and JSON artifacts generated throughout the workflow. The primary unit of analysis is the single-report document and its associated artifact bundle as represented by file paths such as /home/pantera/projects/TEIA/annotarium/Reports/apt29-hammertoss-stealthy-tactics-define-a.pdf and the companion JSON outputs. Analysis operates on deterministic file names and structured indices so that every derived object is unambiguously mappable back to the originating input corpus entry.</p>

  <h2>Data Ingestion and Deterministic File Handling</h2>
  <p>Ingestion begins with the immutable retention of the original PDF in the designated repository location. The pipeline records the provenance of that file through a stable report identifier and timestamp metadata, for example the report identifier apt29_hammertoss_stealthy_tactics_define_a_report and the generation timestamp 2026-02-21T23:32:41.069Z. Deterministic handling is achieved by enforcing a canonical naming convention for derived artifacts, writing outputs to predetermined paths (for example the extraction artifact at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/apt29_hammertoss_stealthy_tactics_define_a/apt29_hammertoss_stealthy_tactics_define_a.output.json), and retaining file-size and existence metadata (e.g., the extraction JSON reported as size_bytes 212191). This combination of stable identifiers, recorded file locations, and artifact metadata permits reproducibility of the ingestion stage: given the same input corpus and environment, the pipeline is designed to regenerate the same set of artifact files with the same paths and structure.</p>

  <h2>PDF-to-Markdown Conversion and Fallbacks</h2>
  <p>The primary conversion step employs the provider-backed process_pdf_mistral_ocr.py to convert the PDF to a structured markdown representation, capturing text, tables, figures, and anchors. A documented offline fallback conversion (PyMuPDF4LLM) is used when provider conversion is unavailable or times out. Both conversion routes produce a markdown artifact (for example apt29_hammertoss_stealthy_tactics_define_a.md) and a corresponding extraction JSON intended to be semantically equivalent in structure. The pipeline records which conversion method executed and preserves both the markdown and the original PDF so that the conversion step can be re-run deterministically against an identical environment and input file.</p>

  <h2>Structural Parsing and Document Modeling</h2>
  <p>Structured parsing operates on the markdown or extraction JSON to identify hierarchical document elements: headings, paragraphs, tables, figure captions, and anchored images. This stage emits a structural model that indexes each element by a stable anchor token and its offset in the source markdown. The approach intentionally models structure rather than raw glyph positions so that subsequent extraction stages reference named anchors rather than fragile page coordinates. Structural artifacts are persisted in the pipeline output directory alongside the markdown and extraction JSON to maintain a traceable chain from source PDF to parsed representation.</p>

  <h2>Artifact Extraction and Schema Emission</h2>
  <p>Artifact extraction applies deterministic parsing rules and schema definitions to convert structural elements into typed artifacts: indicators, IOCs, table rows, code listings, and figure metadata. The extraction stage emits an artifact index that records artifact type, source anchor, and positional context within the document model. The schema-driven approach enables stable artifact identifiers across repeated runs: identical structural inputs produce the same artifact index entries, supporting reproducibility and deterministic traceability of each extracted item back to its source anchor in the markdown and ultimately the original PDF.</p>

  <h2>Reference Parsing and Citation Linking</h2>
  <p>Reference parsing uses specialized heuristics to detect citations, footnotes, and inline reference markers, producing a linked registry of source citations tied to document anchors. The registry is emitted as part of the extraction artifacts and is structured so that each in-text reference resolves to a canonical bibliographic entry or external source identifier. Because reference linking is applied deterministically to the parsed structure, the same input produces the same reference resolution outputs, enabling consistent cross-artifact citation resolution and support for automated provenance queries.</p>

  <h2>Institution Inference</h2>
  <p>Institution inference is performed by a reproducible inference module (infer_source_institutions.py) that applies a documented model (noting the used model version such as gpt-5-mini where applicable) and optional web fallback queries. The inference stage consumes extracted artifacts and references and emits candidate source institution attributions with confidence metadata and explanatory anchors. To maintain reproducibility, the pipeline records the model version, the deterministic prompt templates, and the invocation parameters; where web lookups are enabled the pipeline records snapshot contexts or cache artifacts so that the same results can be reproduced or audited given the same external conditions.</p>

  <h2>Claim–Evidence Graph Construction</h2>
  <p>The claim–evidence graph formalizes relations between asserted claims and the extracted artifacts that support them. Nodes represent claims, artifacts, references, and inferred institutions; edges represent supporting, contradicting, or contextual relations annotated with provenance anchors and extraction confidence. The graph is instantiated deterministically from the artifact index and reference registry so that equivalent inputs yield structurally equivalent graphs, facilitating reproducible downstream scoring and explanation generation.</p>

  <h2>Scoring Overview and Aggregation</h2>
  <p>Scoring consumes the claim–evidence graph and a set of calibrated scoring rules to produce composite attribution scores. The scoring pipeline reads explicit inputs preserved in score input artifacts (for example apt29_hammertoss_stealthy_tactics_define_a.score_input_v3.json) and writes full score reports (for example apt29_hammertoss_stealthy_tactics_define_a.icj_score_report.json and a v3 variant). All scoring computations are parameterized, versioned, and recorded so that scores can be recomputed from the same score input and the same scoring engine version, thereby preserving reproducibility.</p>

  <h2>Chain of Custody and Provenance Recording</h2>
  <p>Throughout ingestion, each artifact is labeled with provenance metadata including source path, generation timestamp, processing module identifier, and environment metadata. The environment record captures Python runtime and relevant package versions (for example python 3.10.12 and packages such as</section>
<section class="wiki-section" id="sec-pdf_to_markdown"><h2>Introduction</h2>
<p>This methodology chapter describes the processes, transformations, and assurance controls used to convert, parse, and evaluate source material for cyber-attribution scoring. The exposition is intentionally methodological and normative: it explains the end-to-end pipeline, the rationale for chosen techniques, and the controls used to preserve evidentiary fidelity and analytical transparency. The discussion references artifacts and pipeline components present in the project workspace to ground methodological claims in the operational environment, without drawing or restating case-specific analytic conclusions from the source materials.</p>

<h2>Scope and Units of Analysis</h2>
<p>The unit of ingestion for the pipeline is definable at document, page, claim, artifact, and citation levels. Operational counts for this run are recorded in the ingestion metadata and illustrate how the pipeline distributes effort across units: for example, the material processed for this run comprised one document page, three identified claim-level excerpts, one listed source, six extracted artifacts, zero parsed formal citations, two tables, and six figures. These counts are cited only to illustrate pipeline behavior and to indicate granularity at which downstream modules operate, not to summarize substantive content.</p>

<h2>Data Ingestion and Preprocessing</h2>
<p>The pipeline begins by registering the original document and its checksums into a provenance ledger and then progresses to text extraction. The canonical PDF artifact is preserved at the documented path for chain-of-custody purposes. In the current operational workspace the original PDF is located at the preserved filesystem location identified in the ingestion metadata, and associated extraction and report JSON files are also persisted to the workspace. Each file is recorded with size and existence metadata so that downstream validation can confirm the presence and immutability of primary artifacts prior to transformation.</p>

<h2>PDF-to-Markdown Conversion</h2>
<p>The conversion of PDF to markdown is implemented with a primary, provider-backed optical character recognition and layout inference stage that uses a Mistral-backed processor. This primary stage is invoked by the process_pdf_mistral_ocr.py component and emits a first-pass markdown representation that preserves paragraph structure, inline formatting, anchored tables, and figure/image anchors. The use of a Mistral-based model in the primary path is a deliberate operational choice that balances extraction fidelity, layout preservation, and throughput.</p>

<p>To preserve operational resilience, the pipeline maintains an offline fallback posture: when the Mistral provider-backed conversion fails, times out, or is judged to produce low-confidence outputs, the engine automatically transitions to an offline conversion implemented with a PyMuPDF4LLM utility. This offline fallback is described in the pipeline methods as an explicit resilience mechanism rather than a conceptual change to the conversion semantics. The two-stage posture—provider-backed Mistral first, PyMuPDF4LLM offline fallback second—ensures that a markdown artifact is produced even in degraded network or provider conditions, while enabling auditors to compare both outputs and document divergence as part of quality assurance.</p>

<h2>Structural Parsing</h2>
<p>After markdown generation, a structural parser consumes the markdown representation and produces a first-stage abstract syntax tree of document structure. The stage emits table and figure anchors that reference the original page coordinates and embedded image artifacts, enabling reconciliations between extracted text, tables, and figures. The parser also annotates paragraph-level confidence scores derived from OCR-layout alignment heuristics and model confidence outputs where available. These structural annotations feed downstream artifact extraction and claim linking with explicit provenance pointers to the originating markdown fragment and to the original PDF file preserved in the workspace.</p>

<h2>Artifact Extraction</h2>
<p>Artifact extraction operates on the structural parse to identify discrete evidence elements: code snippets, IOCs, configuration fragments, table rows, and labeled figures. The extraction schema emits an artifact index and metadata describing artifact type, bounding coordinates, and extraction confidence. Where tables are present, the parser preserves cell boundaries and header inference to allow tabular cells to be treated as discrete artifacts. Extracted artifacts are cataloged with links back to the markdown anchors and to the underlying PDF coordinates to maintain bidirectional traceability between evidence and source.</p>

<h2>Reference Parsing</h2>
<p>References, citations, and footnote-like constructs are parsed and normalized in a separate stage. The parser identifies structured reference candidates within text and tables and assigns them to a local source registry. Where conventional citation fields are absent, the parser still records in-text referents as informal citations with positional provenance. The reference parsing stage is designed to be conservative: ambiguous tokens are flagged for human review, and the registry records both parsed entries and parsing confidence to support later provenance validation.</p>

<h2>Institution Inference</h2>
<p>Institutional inference is a downstream enrichment that attempts to associate extracted references and artifacts with likely originating institutions. The methodology relies on a model-assisted inference component (identified in the pipeline as infer_source_institutions.py) that primarily uses a gpt-5-mini model for contextual inference and, where required, an optional web-backed fallback to corroborate institutional identifiers when model confidence is insufficient. The design separates inference from attribution: institution inference produces candidate institution labels with confidence scores and explicit provenance, and it is subject to adjudication and human review rather than being treated as a definitive assignment.</p>

<h2>Claim–Evidence Graph Construction</h2>
<p>Claims identified during extraction are represented as nodes in an evidence graph and are linked to artifacts, references, and inferred institutions through labeled edges. Each edge carries provenance metadata, a confidence score derived from upstream extraction and inference stages, and a pointer to the originating markdown slice and original PDF location. The graph formalizes the relationship between textual claims and the evidence that supports them, enabling analysts to reason</section>
<section class="wiki-section" id="sec-structural_parsing"><h2>Introduction</h2>
<p>This methodology chapter describes a reproducible, anchor-preserving approach to extracting and structurally parsing text, tables, and figures from forensic vendor reports for the purpose of cyber-attribution scoring. The procedures presented are grounded in the supplied document metadata and pipeline diagnostics and are intended to support auditability, traceability, and defensible chain-of-custody of extracted evidence. The description below references the provided extraction context (for example, the document metadata that lists a title and authoring entity and the pipeline counts indicating pages, tables, and figures) to motivate design choices without making substantive assertions about the underlying report content.</p>

<h2>Data Processing and Extraction</h2>
<p>The extraction pipeline begins with ingestion of the source artifact identified in the metadata. In the supplied context the source locator provides a file path and an input format labeled as markdown, and the document metadata records fields such as title, authoring_entity, publication_date (including an anchor describing the provenance of the date inference), version, and document_type. These metadata elements are captured early to establish provenance. The pipeline records quantitative diagnostics (for example, a pages count, tables count, and figures count) to guide downstream processing decisions such as resource allocation for optical character recognition or table recognition modules. All processing steps generate structured outputs (for example, JSON payloads and intermediate markdown) that are retained with explicit references to the originating source file paths to maintain a verifiable chain of custody.</p>

<p>Textual extraction preserves the original document layout by segmenting content into discrete text blocks with associated location coordinates, page indices, and stable anchor identifiers. Anchor-preserving segmentation attaches to each text block an anchor id and extraction method (for example, manual_description or automated_ocr) together with the verbatim text. This enables auditors to map every derived analytical assertion back to a specific, locatable fragment of the source file. When the input source contains embedded objects, such as tables or images, each object is assigned an object identifier and an anchor that records its bounding region, page index, and extraction provenance.</p>

<h2>References and Institution Inference</h2>
<p>References and citations are parsed into structured bibliographic records that include anchor metadata linking the citation text to its location in the source. The parser distinguishes between inline references, footnotes, and bibliographic entries and records the exact extracted verbatim text alongside its anchor. Institution inference proceeds from named-entity recognition outputs augmented with contextual metadata: entities that appear in headers, bylines, or source_locator fields (for example, authoring_entity and source file path) are weighted more heavily for institutional attribution. Importantly, inferred institutional associations are stored with confidence scores and the anchors of the supporting text fragments so that any inference can be audited back to the underlying anchor-preserved content.</p>

<h2>Scoring Framework</h2>
<p>The scoring framework operates on an evidence graph that links claims to supporting artifacts via anchors. Each extracted claim node is associated with one or more evidence anchors drawn from text blocks, tables, or figures. Tables and figures are handled as first-class objects: table extraction yields structured rows, columns, and cell anchors, and figures yield image object identifiers with any accompanying captions preserved as anchored text. The scoring model aggregates indicators along dimensions such as provenance strength, anchor specificity, and corroboration across distinct anchors. Anchor specificity—measured by the granularity of the anchor (for example, exact page and bounding box versus whole-document)—is explicitly incorporated into the score computation so that more precisely anchored evidence carries greater weight, thereby enhancing auditability.</p>

<h2>Validation and Quality Assurance</h2>
<p>Quality assurance includes automated validation of structural consistency (for example, ensuring every extracted table cell references a parent table anchor and that figures have associated captions and anchors) and manual spot checks that reconcile extracted anchors against the original source file. The pipeline persists validation artifacts such as a validation report and full scoring outputs to designated payload paths to enable independent re-running and verification. Metrics collected during validation include extraction completeness (fraction of declared tables and figures successfully parsed), anchor integrity (ratio of anchors that resolve to a unique location), and provenance fidelity (consistency between declared metadata such as publication_date_anchor and extracted anchor records). Records of these metrics, together with the saved intermediate payloads, support traceable, auditable review of the attribution scoring workflow and its limitations.</p></section>
<section class="wiki-section" id="sec-artifact_extraction"><h2>Introduction</h2>
<p>This methodology sets forth a structured approach for artifact extraction and the normalization of technical objects within a cyber-attribution scoring pipeline. The account that follows is grounded in the raw ingestion manifest and pipeline descriptions supplied with the source dataset. It addresses processes across modalities, the transformation of heterogeneous artefactual forms into normalized technical objects, and the consequent implications for provenance and custody evaluation. The objective is to explicate method and rationale rather than to adjudicate or recount case-specific findings.</p>

<h3>Scope and Units of Analysis</h3>
<p>The unit of analysis comprises discrete artifacts as enumerated in the source manifest and the higher-order objects derived through normalization. Raw artifact classes include domains, emails, file names, cryptographic hashes, and URLs. In the provided preview, for example, the artifact_type_counts identify nine domain entries, seven URLs, one email, one file name, and single MD5 and SHA-1 hashes. These raw items serve as the primary tokens from which normalized technical objects are constructed and subsequently linked into evidentiary graphs.</p>

<h3>Data Ingestion</h3>
<p>Ingestion is governed by a staged pipeline that preserves provenance metadata at each transformation step. The pipeline manifest documents primary and fallback conversion mechanisms, table and image extraction points, artifact indexing stages, and reference parsing. Preservation of source offsets, extraction timestamps, and processor identifiers is required so that subsequent custody metrics may quantify the number and nature of operations applied to each artifact.</p>

<h3>PDF-to-Markdown Conversion</h3>
<p>Document conversion is the first substantive transformation for many inputs. The configured primary conversion utilises a provider-backed Mistral OCR process (process_pdf_mistral_ocr.py) with an offline fallback implemented through PyMuPDF4LLM when provider conversion is unavailable or times out. The conversion step emits a markdown-like representation, including anchors for figures and tables, which serves as the substrate for structural parsing. Conversion outputs carry conversion provenance tokens to indicate which engine and version produced the tokens.</p>

<h3>Structural Parsing</h3>
<p>Structural parsing decomposes the markdown representation into semantic blocks: headings, paragraphs, tables, figures, and captions. This stage annotates the offsets of tables and images and emits anchors that the artifact extraction stage consumes. Structural parsing is designed to be idempotent and deterministic to the degree possible, so that repeated parsing of the same converted text yields stable anchor identifiers and mitigates provenance ambiguity introduced by non-deterministic OCR artifacts.</p>

<h3>Artifact Extraction</h3>
<p>Artifact extraction operates across modalities. Textual modalities (body text, captions, table cells, footnotes) and non-textual modalities (embedded images with OCR-extracted text, table images) are normalized into structured artifact records. The schema extraction stage emits indexed artifact lists linking each artifact to its originating block, offset, and modality. The raw_artifacts_preview provides representative examples for each class—domains such as www.twitter.com and twitter.com, URLs numbering seven in the preview, and single instances of hashes and file names—that illustrate the heterogeneity the extraction stage must reconcile.</p>

<h3>Reference Parsing</h3>
<p>Reference parsing identifies citation patterns and footnote-like structures and links them to a source registry. It standardises bibliographic fragments and attempts to resolve them to persistent identifiers where available. The process records degrees of confidence on each resolution attempt so that downstream scoring can weight assertions by the resolvability and the provenance confidence of referenced sources.</p>

<h3>Institution Inference</h3>
<p>Institution inference maps artifact origins and reference metadata to inferred source institutions using a model-assisted approach (infer_source_institutions.py, augmented by gpt-5-mini and an optional web fallback). The inference engine outputs candidate institutions with associated confidence scores and provenance tokens indicating whether inference used internal heuristics, model reasoning, or external resolution. These metadata feed into institution-level credibility and corroboration axes without substituting for human validation.</p>

<h3>Claim–Evidence Graph Construction</h3>
<p>Normalized technical objects are nodes in a claim–evidence graph; edges express relationships such as derivation, citation, co-occurrence, and asserted control channels. The graph formalises provenance by preserving the chain of transformations from raw modality to normalized node, enabling traceability from a high-level claim to the originating artifact offsets and conversion steps.</p>

<h3>Scoring Framework Overview</h3>
<p>The scoring framework computes orthogonal axes—custody, grounding, credibility, corroboration, and clarity—each aggregating lower-level signals that include modality provenance, conversion fidelity, and reference resolvability. Document-level metrics present in the supplied document_scores_v4 illustrate aggregate behavior: for this dataset the custody_avg_0_100 is reported as 61.43 and grounding_avg_0_100 as 58.65, while belief_weighted_0_100 is low. Such figures are used here only to demonstrate how normalized artifact provenance and extraction metadata inform axis calibration rather than to substantively evaluate particular assertions.</p>

<h3>Chain of Custody and Provenance</h3>
<p>Chain-of-custody is operationalised by immutable provenance stamps attached to each normalized technical object. These stamps record ingestion source, conversion engine, parsing stage, and any manual interventions. The custody axis aggregates the quantity and quality of these stamps, penalising objects with ambiguous or multi-step, lossy transformations. For example, objects reconstructed from image OCR are treated with lower transformation confidence than objects extracted from structured table text unless corroborated by multiple independent modalities.</p>

<h3>Credibility and Corroboration Axes</h3>
<p>Credibility scoring separates institutional-source credibility (inferred from reference parsing and institution inference) from content corroboration (pattern matches across independent artifacts and external resolvers). Corroboration is computed by detecting independent modality concurrence and independent source linkage; credibility is modulated by the resolvability and provenance confidence of references. The supplied scores indicate cases where credibility and corroboration may be underpopulated, and the methodology prescribes specific follow-up enumeration and human review where these axes are score-deficient.</p>

<h3>Clarity Axis</h3>
<p>Clarity measures the semantic parseability and disambiguation success for each normalized object. Low clarity arises from ambiguous filenames, truncated URLs, or inconsistent domain normalisation (for example, presence or absence of leading www). The normalization process standardises domains, canonicalises URLs, and associates cryptographic hashes with asserted filenames to reduce ambiguity and improve downstream matching.</p>

<h3>Aggregation and Calibration</h3>
<p>Aggregation rules specify how object-level scores roll up to claim- and document-level summaries. Calibration is performed against holdout sets and uses bootstrap confidence intervals to characterise measurement uncertainty. The document-level bootstrap outputs in the source manifest, including a custody bootstrap 95% CI, exemplify the intended practice of reporting both point estimates and interval uncertainty for aggregated axes.</p>

<h3>Validation and Quality Assurance</h3>
<p>Validation combines automated checks, such as schema conformance</section>
<section class="wiki-section" id="sec-reference_parsing"><h2>Introduction</h2>
<p>This methodology describes the systematic approach used to parse footnote-like references and rhetorical citations and to resolve them into an analyzable source graph for cyber-attribution scoring. The description emphasizes methods and rationale rather than substantive findings. It is grounded in the pipeline artifacts and metadata produced during ingestion and preprocessing, and it treats reference parsing as an explicit transformation from rhetorical citation forms encountered in primary documents into structured, verifiable links in a provenance-aware source graph.</p>

<h2>Scope and Units</h2>
<p>The unit of analysis for the reference-parsing subsystem is the document-level element (page, paragraph, figure caption, table footnote, inline parenthetical) and the derived artifact (extracted image, table cell, binary artifact descriptor). For the previewed raw data this includes one identified source (SRC0001), one internal_document_section, and the pipeline counts produced during ingestion: one page, three flagged claims, one listed source, six extracted artifacts, zero explicit citation registry entries, two tables, and six figures. These units guide the granularity of parsing and the construction of the source graph, and they inform downstream scoring dimensions without asserting content-level conclusions about any particular source.</p>

<h2>Data Ingestion</h2>
<p>All source material enters the system via an ingestion routine that records provenance metadata and the transformation chain. In the present pipeline the ingestion layer produced a raw sources preview that lists source identifiers and minimal bibliographic metadata (for example, SRC0001 identified as an internal_document_section attributed to Mandiant, year 2013). The ingestion step is explicitly responsible for capturing original filenames, timestamps, checksums, and the mapping between physical pages and logical fragments so that all later parsing steps can reprovision exact inputs for verification or reprocessing. The ingestion metadata populates fields in the source registry that downstream reference-parsing consults when resolving ambiguous citations.</p>

<h2>PDF-to-Markdown Conversion</h2>
<p>Primary document conversion into a structured, line-oriented representation is performed by a provider-backed routine identified in the pipeline methods as process_pdf_mistral_ocr.py. A documented offline fallback conversion uses an alternative engine (PyMuPDF4LLM) when the primary conversion fails or times out. Both conversion paths produce a markdown-like intermediate that preserves layout anchors for tables and figures and retains inline tokens used in conventional footnote and citation markers. The conversion layer is instrumented to emit diagnostics and confidence metadata for OCRed text, which are later used to weight uncertain citation linkages in the scoring model.</p>

<h2>Structural Parsing</h2>
<p>Following conversion, a structural parse segments the markdown-like output into semantic blocks: headings, paragraphs, figure captions, table captions, footnotes, and inline citation tokens. The stage1 markdown parse explicitly emits anchors for tables and images, and these anchors are appended to the artifact index so that references in text to “Table X” or “Figure Y” can be tied to extracted artifact records. Structural parsing also normalizes common patterns of rhetorical citation, such as parenthetical author–year strings, bracketed numeric tokens, and endnote/footnote markers, by annotating their character spans and token types while preserving original text for manual review.</p>

<h2>Artifact Extraction</h2>
<p>An artifact extraction stage converts structural elements into typed artifact records. The pipeline’s artifact_extraction component produces artifact indices from text, tables, and images and records attributes such as artifact type, positional anchors, and any linked media (e.g., embedded images). Artifact records are assigned stable internal identifiers and are cross-referenced with the structural parse so that citations which reference artifact anchors can be programmatically resolved. Artifacts are also fingerprinted and, when practical, associated with external identifiers (hashes, URL candidates) to support later corroboration and chain-of-custody checks.</p>

<h2>Reference and Footnote Parsing</h2>
<p>The reference-parsing process is tasked with extracting rhetorical citations and footnote markers and converting them into links against the source registry. The parser identifies candidate citation tokens from the structural parse, classifies token types (numeric reference, author–date, named report, or implicit footnote), and attempts to match each candidate against the available source metadata. When a textual citation does not directly match an entry in the source registry, the process produces a nonfatal unresolved marker that carries the original span, surrounding context, and a ranked set of candidate matches. Where no explicit bibliographic entry exists in the registry—as indicated by the pipeline_counts field showing zero pre-registered citations—the parser relies on expansion heuristics that consider nearby headings, bibliographic sections, and typical report metadata patterns to hypothesize linkages while recording confidence scores for each hypothesis.</p>

<h2>Institution Inference</h2>
<p>Institution inference transforms publisher or entity strings found in metadata and in-document mentions into normalized institution identifiers and institutional attributes. The pipeline documents an inference tool (infer_source_institutions.py) that uses a model-assisted approach (noted in the pipeline methods as using gpt-5-mini with an optional web fallback) to disambiguate publisher names, map internal document sections to organizational parents, and append institution-level metadata used in scoring. Institution inference produces both a canonical name and a provenance log describing the inference steps and any web lookups performed; the provenance log is retained to support auditability and potential human correction.</p>

<h2>Claim–Evidence Graph Construction</h2>
<p>All parsed citations, artifacts, and inferred institutions are materialized as nodes in a directed graph whose edges encode citation linkage, artifact provenance, and institutional attribution relationships. The</section>
<section class="wiki-section" id="sec-institution_inference"><section>
  <h2>Introduction</h2>
  <p>This methodology chapter describes the systematic approach used for institution inference and source typology within a cyber‑attribution scoring framework. The exposition is methodological and normative: it explains how source signals are transformed into institutional classifications and how those classifications inform credibility weighting and corroboration eligibility. The procedures are grounded in the supplied raw data artifacts and pipeline metadata and are deliberately framed to be replicable, auditable, and amenable to independent validation. The following sections set out the scope units, data ingestion and structural parsing steps, the institution inference method and source typology mapping, the construction of claim–evidence relationships, the scoring overview pertinent to institutional considerations, chain‑of‑custody and corroboration criteria, quality assurance mechanisms, and a short statement on limitations and governance.</p>

  <h2>Scope Units</h2>
  <p>The unit of analysis is the discrete source artifact as represented in the source registry. In the supplied preview, an example artifact is SRC0001 titled "APT1 Executive Summary and Key Findings," classified under the pipeline as an internal_document_section. Source_type_counts in the raw data indicate one instance of this class (internal_document_section: 1), which illustrates the necessity of treating document sections, whole reports, and other granular extracts as separate scope units. Each scope unit carries structural metadata (title, entity_name, publication_or_venue, year, and url_or_identifier) that anchors downstream provenance assertions without invoking substantive case content.</p>

  <h2>Data Ingestion</h2>
  <p>Data ingestion encompasses the controlled conversion of source files into machine‑readable artifacts, the extraction of structural metadata, and the registration of identifiers into the source registry. The pipeline methods named in the raw data—pdf_to_markdown_primary via process_pdf_mistral_ocr.py, with a fallback to PyMuPDF4LLM—are used to maximize fidelity of textual extraction from binary inputs. The runtime environment and installed libraries (for example Python 3.10.12 and packages such as openai 2.9.0, pypdf 6.0.0, and pymupdf 1.26.7) are recorded to provide technical context for reproducibility and to document potential sources of systematic extraction variation.</p>

  <h2>PDF to Markdown and Structural Parsing</h2>
  <p>The pdf_to_markdown stage produces an initial tokenized representation that preserves headings, paragraphs, tables, figures, and image anchors. Structural parsing then segments the document into hierarchically labeled components (sections, subsections, figure captions, and table bodies) and emits these segments as separately addressable objects in the artifact index. The pipeline explicitly records table_and_image_extraction behavior: stage1 markdown parsing emits table and figure anchors which enable later artifact-level indexing without conflating visual artifacts with narrative text.</p>

  <h2>Artifact Extraction</h2>
  <p>Artifact extraction transforms structural segments into typed artifacts suitable for evidence linking. The pipeline's artifact_extraction stage creates indices for textual spans, tabular records, and embedded images. These indices become the nodes referenced by the reference_parsing stage and by downstream claim–evidence graph algorithms. Maintaining separate artifact identifiers allows institutional attributions to be attached at the appropriate granularity (for instance, attaching a provenance assessment to a report section rather than to an entire corpus).</p>

  <h2>Reference Parsing</h2>
  <p>Reference parsing identifies citations, footnotes, and other cross‑references within and across artifacts and links them to entries in the source registry. The pipeline metadata indicates that citations and footnote‑like references are parsed and linked to the source registry, which enables tracing of claims to supporting or antecedent sources. The parsed reference graph is used as an input to institution inference and to corroboration eligibility determinations, as cross‑referencing behavior is often informative of editorial provenance and chain of custody.</p>

  <h2>Institution Inference</h2>
  <p>Institution inference combines heuristic signal extraction with a model‑assisted classification step. The raw pipeline references an infer_source_institutions.py component that leverages a model (noted as gpt-5-mini) with an optional web fallback. The method first extracts institutional cues such as entity_name, publication_venue, author metadata, explicit</section>
<section class="wiki-section" id="sec-claim_evidence_graph"><div>
  <h3>Introduction</h3>
  <p>This methodology describes the construction and use of a claim-evidence graph for cyber-attribution scoring in a manner that emphasises reproducible provenance, anchor-level traceability, and anti-circularity safeguards. The design is grounded in the supplied processing outputs and scoring bundle metadata (for example, the provided raw_claims_preview, raw_sources_preview, raw_artifacts_preview and the normalized evidence_items and claims within scoring_bundle). The presentation below treats claims, sources, artifacts, and evidence items as discrete nodes and relations in a directed provenance graph. It focuses on methodological rationale and process, avoiding substantive assertions about any particular operational actor or incident documented in the underlying report.</p>

  <h3>Data processing and artifact extraction</h3>
  <p>Input records are first normalized into canonical node types: claim nodes (identified in the input by claim_id such as C001–C003), source nodes (for example SRC0001), artifact nodes (ART00001–ART00020 in the normalized artifact list), and evidence-item nodes (for example E-0001–E-0006 in scoring_bundle.normalized.evidence_items). Structural parsing converts extracted artifact occurrences (domains, URLs, emails, file names, and cryptographic hashes as enumerated in raw_artifacts_preview) into artifact nodes that carry an extraction provenance record. That extraction provenance record contains the extraction modality (text, OCR, metadata), the block or anchor identifiers (block_id values such as ART00002, ART00003, etc.), the page location, and an extractor confidence score. The normalized artifact objects in the scoring bundle provide the necessary fields (location.page, location.block_id, extracted_from and confidence) to support downstream anchor-level traceability and integrity checking.</p>

  <h3>References and institutional inference</h3>
  <p>Source normalization maps document-level metadata to source nodes and origin clusters. For example, the supplied normalized source SRC0001 includes authoring_org and publisher fields and an origin_signature (e.g., src0001). Institutional inference applies deterministic and heuristic mappings from publisher and authoring_org values to institution-level nodes while recording uncertainty. The method records whether a source is report-derived (report_derived_ratio) and whether multiple citation relationships recover referenced material (recovered_reference_count). These fields are retained and indexed because institutional inference and any subsequent credibility weighting must depend on clearly recorded provenance rather than on opaque assumptions about institutional reputations.</p>

  <h3>Claim-evidence graph construction and safeguards</h3>
  <p>The claim-evidence graph is constructed as a directed graph in which claim nodes point to evidence-item nodes, evidence-item nodes point to artifact nodes and source nodes, and artifact nodes are anchored to document block identifiers. Each evidence-item in the normalized scoring bundle includes explicit anchors (for example block_id entries such as ART00003, ART00004) and an origin_id (for example ORIG:src0001). These anchor fields enable atomic traceability: any path from a claim to an artifact is resolvable to the original document block and extraction operation. To prevent circular inferences—where a claim is used as evidence for itself or where derived summaries are re-ingested as independent evidence—the graph enforces anti-circularity rules. The primary anti-circularity mechanisms are: (1) origin clustering and unique_origin_count tracking to ensure evidence contributions from the same origin are not treated as independent corroboration; (2) a recovered_reference_count and report_derived_ratio diagnostic used to flag evidence that is report-derived rather than primary; (3) explicit origin_id lineage on evidence items so any transitive step that would reuse a claim as evidence triggers an exclusion rule; and (4) nondup and anchor_quality diagnostics (drawn from the chain_provenance_diagnostics fields) that reduce weight for non-independent or derivative anchors. These safeguards are implemented at graph construction time and again during scoring aggregation to ensure a claim-evidence graph reflects true provenance relations rather than circular dependencies.</p>

  <h3>Scoring framework overview</h3>
  <p>Scoring operates over the claim-evidence graph. Individual evidence-item nodes carry modal feature metrics (for example I, A, M, P, T scores present in the evidence_items features) and a computed probative_weight. The scoring pipeline aggregates evidence weights to produce claim-level grounding and evidence_weight aggregates and then applies multiplicative penalties for single-source dependence (the scoring_bundle shows a single_source penalty being applied in claim scoring). Chain-of-custody diagnostics (provenance_quality, anchor_quality, artifact_proximity_tiers) and credibility indicators (source counts, domain independence) are combined in a multi-axis vector (credibility, corroboration, custody, clarity, confidence, and coherence). Aggregation uses shrinkage and calibration factors (referenced in the statistical_calibration_v4 and scores_raw_v3 fields) so that small evidence sets or single-origin collections are appropriately down-weighted in the final claim score while preserving chain-of-custody signals.</p>

  <h3>Validation and quality assurance</h3>
  <p>Validation is performed at three levels: syntactic verification of graph connectivity (every claim edge resolves to evidence</section>
<section class="wiki-section" id="sec-scoring_overview"><h2>Introduction</h2><p>This methodology chapter describes a reproducible architecture for scoring attribution-related claims in technical reports. It is framed to preserve a separation between structured extraction outputs and later inferential weighting, and to enable independent validation. The exposition is grounded in the supplied scoring bundle and document-level metadata (for example, the provided document_scores_v4 and scoring_bundle structures) while avoiding discussion of substantive allegations contained in the underlying report. The design objective is to treat the report as a structured input: discrete claims (three in the supplied bundle), numbered evidence items (six evidence_items in the normalized bundle), and an artifacts registry, then to propagate provenance and quality signals from extraction to inferential layers in a way that is auditable and statistically calibrated.</p><h2>Data processing and extraction</h2><p>The ingestion pipeline begins with provider-backed PDF conversion to a machine-readable intermediate (the pipeline_methods entries indicate a primary converter and a fallback; see "pdf_to_markdown_primary" and the fallback entry). Conversion produces a markdown-like representation that preserves anchors for tables and figures. Structural parsing then identifies textual blocks and anchors; the normalized.artifacts array in the supplied data illustrates the artifact index produced by the extraction stage, including file hashes, URLs, domains and email addresses with per-artifact confidence values. Artifact extraction emits typed artifact records (for example, hash_md5, hash_sha1, url, domain, email) and assigns a location anchor for each extraction. Extracted artifacts feed the evidence identification stage, which binds artifacts to evidence items (the normalized.evidence_items list shows technical_artifact kind and associated anchors). Throughout extraction we capture per-item extraction confidence and anchor locations to support later chain-of-custody scoring.</p><h2>References and institution inference</h2><p>Reference parsing is performed as a distinct structural step to detect citations and other reference-like constructs and to register source records. The normalized.sources array documents canonicalized source records (for example, a single vendor source in the bundle), and citation coverage metrics appear in document_scores_v4 (for example, citation_coverage_sources_0_1 = 0.0 and citations_total = 0 in the supplied document_scores_v4). Institution inference is an ancillary process that attempts to infer or validate the authoring institution for a source record; the pipeline_methods entry names the inference routine used. The design treats inferred institutional attributes as metadata subject to validation and not as hard inputs to claim-level belief without explicit provenance linkage and quality gating.</p><h2>Scoring framework: from claim-level axes to document-level synthesis</h2><p>The scoring architecture separates two conceptual layers. First, the extraction layer produces structured objects: claim records (claim_score_preview_v4 and scoring_bundle.normalized.claims), evidence items (normalized.evidence_items), artifacts (normalized.artifacts), and source records. Second, an inferential weighting layer consumes those objects to generate multi-axis claim-level scores and to synthesize document-level assessments. Claim-level axes in the supplied scoring bundle include grounding, custody, credibility, corroboration, clarity, and confidence; claim_score_preview_v4 entries show example per-claim values (for instance belief_0_100 values near 0.05–0.06 and custody scores in the ~56–64 range). Each claim is scored by (a) aggregating probative weights from associated evidence items (evidence_weight_aggregate entries in the scoring section), (b) applying penalties and multipliers that reflect provenance patterns (for example, a single_source penalty multiplier observed in the scoring_bundle), and (c) computing the six-c vector and core_3c diagnostics (see per-claim core_3c and six_c_vector structures). Document-level synthesis then aggregates claim-level vectors into coverage and headline vectors, computes overall claim score means and geometric aggregates, and evaluates gates (the document-level serious- ness_gate in the supplied bundle demonstrates thresholding logic). Crucially, the methodology maintains a strict separation: extraction outputs are immutable inputs to the inferential weighting stage, and inferential weighting is implemented as a transparent set of arithmetic transformations (e.g., evidence-weight aggregation, single-source penalties, shrinkage toward priors) described in the scoring bundle and statistical calibration fields.</p><h2>Validation and quality assurance</h2><p>Quality assurance combines deterministic checks and statistical calibration. Deterministic QA verifies anchor coverage, artifact-location consistency, and extraction confidence thresholds; these checks are reflected in chain_provenance_diagnostics fields such as anchor_quality and provenance_quality. Statistical QA uses shrinkage, reliability factors, and bootstrap confidence intervals to express uncertainty in aggregated metrics: the supplied full_icj_v4.statistical_calibration_v4 contains shrinkage_lambda and reliability_factor parameters, and document_scores_v4.bootstrap_95ci provides example interval estimates. Validation also includes gate tests (for example, thresholds used in the seriousness_gate) and explicit flags for low-credibility or missing-corroboration cases (the supplied credi- bility and corroboration averages and zero-valued citation coverage illustrate such gating). All QA outputs are logged alongside original extraction artifacts so that any downstream assessor may inspect the provenance trace and recompute scores. The methodology therefore supports both repeatability and independent challenge while preserving the distinction between extraction outputs and inferential weighting.</p></section>
<section class="wiki-section" id="sec-chain_of_custody"><section>
  <h2>Introduction</h2>
  <p>This methodology describes the chain-of-custody axis used in a quantitative cyber-attribution scoring pipeline. Its purpose is to set out the variables, processing steps, and quality controls that determine how evidentiary artifacts extracted from a source document are assembled, assessed, and reduced to custody scores that feed higher-level attribution and belief calculations. The approach is modular: (a) systematic extraction and normalization of artifacts and source metadata, (b) tabulation of explicit custody signals (provenance, integrity, time anchors, artifact identifiers, and versioning), (c) calibrated combination of those signals using statistical shrinkage and penalties, and (d) validation and quality assurance informed by bootstrap and diagnostic vectors. For transparency, the method references the extraction and scoring fields that operationalize each concept (for example, artifact-level confidence and the custody components returned by score_details.custody in the normalized scoring bundle).</p>

  <h2>Data processing and artifact extraction</h2>
  <p>Document ingestion begins with text and structural extraction from the original report, followed by entity and artifact extraction. Extracted artifacts are recorded with a typed artifact_id, artifact_type, value, extraction location, extraction modality, and an extraction confidence field. The raw_artifacts_preview in the input demonstrates the variety of artifact types the pipeline handles (domains, urls, file names, hashes, email addresses) and provides counts and example values that guide downstream validation. Each artifact in the normalized.artifacts table is assigned a confidence (for example confidence=1 for automatically high-confidence text extractions) and explicit location anchors (page and block_id). Those anchors are referenced by evidence items through the anchors array; evidence items synthesize modality features and anchor sets and are the primary units that feed custody computations.</p>

  <p>Structural parsing records provenance metadata at multiple levels: the normalized.sources table provides source_id, source_kind, authoring_org, publisher, and date_published; artifacts record an extracted_from field; evidence items include origin_id and source_ids. These chained identifiers create an auditable lineage that supports provenance assessments. Artifact-level identifiers such as hash values and canonicalized URLs (artifact_identifiers) are preserved and counted, while versioning signals are derived where explicit version indications are present in either artifact values or the source metadata. Extraction confidence and anchor_quality fields are used to implement automated accept/reject gates prior to scoring.</p>

  <h2>References, citation parsing, and institution inference</h2>
  <p>Reference parsing distinguishes explicit citations embedded in the document from implicit provenance cues such as domain names, report headers, and authoring_organization fields. The pipeline records citation_coverage and citation counts (for example document_scores_v4.citations_total) and flags the absence of explicit citations when citation_coverage equals zero. When conventional bibliographic citations are absent, institution inference relies on normalized.source fields (authoring_org, publisher), on artifact domains that are strongly associated with organizational actors, and on context anchors that map passages to the source record. To preserve epistemic humility the system stores a recovered_reference_count and a report_derived_ratio to indicate how much of the evidence corpus depends on prior published sources versus newly presented artifacts.</p>

  <p>Institution inference is explicitly conservative: a candidate institution label is proposed only when multiple independent signals align (for example an authoring_org entry in normalized.sources plus domain evidence and anchor alignment). Where only a single source_id supports a claim, the system preserves the linkage but flags the instance for downstream penalties and human review. The design is reflected in summary fields such as scoring_bundle.full_icj_v4.document_scores.sources_total and the single_source penalties recorded within claim scoring.</p>

  <h2>Scoring framework for the chain-of-custody axis</h2>
  <p>The custody score is decomposed into five conceptual components that are computed from artifact-, evidence-, and source-level data: provenance, integrity, time anchors, artifact identifiers, and versioning. Each component is scored on a bounded scale and then combined into a custody composite. The score_details.custody object in the normalized output demonstrates the operationalization: provenance and time_anchors appear as binary or fractional indicators (for example provenance values of 1.0 or 0.9), integrity is represented as an observed integrity signal (for example integrity values such as 0.3333 or 0.1667), artifact_identifiers reflects the presence of machine-identifying values (hashes, canonicalized filenames) and versioning records whether explicit version metadata could be located.</p>

  <p>Combination rules are calibrated using statistical shrinkage and reliability factors to avoid overconfidence in small or homogeneous evidence sets. The statistical_calibration_v4 block provides reliability_factor, effective_evidence_n, and shrinkage_lambda per axis. These parameters shrink custody component estimates toward prior_scores when evidence is sparse or internally inconsistent. Evidence weight is aggregated across evidence items (evidence_weight_aggregate and evidence_weight_0_100 illustrate the concept) and serves as a multiplier in the final per-claim score calculation. The pipeline also computes chain_provenance_diagnostics (context_completeness, lineage_quality, provenance_quality, anchor_quality) and a six</section>
<section class="wiki-section" id="sec-credibility_corroboration"><h2>Introduction</h2>
<p>This methodology chapter defines the procedures and rationale used to operationalize a Credibility Axis with an embedded Corroboration subcomponent for cyber‑attribution scoring. The purpose of the axis is to provide transparent, reproducible assessments of how much inferential weight to place on claims that connect technical artifacts to actor attributions, emphasizing measurable signals such as provenance, independence, and cross‑source convergence rather than substantive verdicts about particular events. The description below is grounded in the supplied input metadata and scoring artifacts (for example, a single normalized source entry identified as SRC0001, a set of extracted artifacts ART00001–ART00020, and six evidence items E‑0001–E‑0006) and therefore illustrates method behavior using those structural inputs rather than asserting case‑specific factual findings.</p>

<h2>Data processing and evidence extraction</h2>
<p>Ingestion begins with document normalization and artifact extraction. Source metadata and content are converted into a structured intermediate layer that records normalized sources (the provided normalized.sources array), artifact records (the provided scoring_bundle.normalized.artifacts list), anchor references (block and page identifiers), and evidence items that link artifacts and anchors to claim identifiers (the evidence_items collection E‑0001..E‑0006). Structural parsing produces evidence anchors and maps them to claim statements; in the supplied inputs each claim (three total) has multiple anchored evidence markers. The extraction pipeline preserves provenance fields (origin identifiers such as ORIG:src0001), systemic integrity signals (extracted confidence scores), and artifact modality labels (e.g., domain, url, hash) to support downstream custody and modality diversity calculations.</p>

<h2>Reference parsing and institution inference</h2>
<p>Reference parsing identifies explicit citations and infers institutional attributes from normalized source records. For each source we record authoring organization, publisher, declared conflicts, litigation preparation flags, and available identifiers. The supplied normalized source for SRC0001 includes authoring_org and publisher information (authoring_org: "Mandiant", is_litigation_prepared: 0, has_stated_conflict: 0, has_countervailing_detail: 0). Institution inference draws on those fields plus domain tokens (from artifacts such as ART00018: www.fireeye.com) to assign an institutional quality prior. Where explicit citations are absent (citations_total: 0, citation_coverage_sources_0_1: 0.0) recovered_reference_count remains zero and institutional priors are predominantly a function of declared publisher metadata and known domain reputations. All inferences are recorded with a provenance tag so that subsequent reviewers can trace which institutional signals were used to seed credibility priors.</p>

<h2>Scoring framework: source hierarchy, independence, corroboration, claim coverage, and exclusion criteria</h2>
<p>We implement a layered source hierarchy that groups inputs into ordered classes (for example: primary technical reporting with provenance, vendor reports, peer‑reviewed academic work, government disclosures, mainstream media, and low‑value aggregators/social feeds). Each class is associated with a prior quality distribution used as a starting point for credibility. Independence logic quantifies effective independence by clustering source origins and domains (unique_origin_count, unique domain counts) and by computing a domain_independence multiplier; in the supplied scoring bundle the explicit unique_origin_count per claim equals 1 and domain independence signals are correspondingly minimal. Corroboration rules require multi‑origin convergence, modality diversity (presence of independent technical artifacts, third‑party logs, or independent technical write</section>
<section class="wiki-section" id="sec-clarity_axis"><section>
  <h2>Introduction</h2>
  <p>This methodology chapter defines the approach used to score the clarity of attribution in reports that assert links between cyber operations and state actors. The objective is to operationalize how textual and technical signals are translated into quantitative clarity measures for three legal responsibility pathways: actions by state organs, state control over non-state actors, and state responsibility by failure to exercise due diligence. The method is designed to be evidence‑centric, reproducible, and calibrated to the quality and variety of the source material; it does not adjudicate contested facts but rather assesses how clearly the material supports particular modes of state responsibility.</p>

  <h2>Data processing and artifact extraction</h2>
  <p>Source ingestion begins with a normalized document record capturing metadata (publication date, authoring organisation, source kind) and a structured extraction of textual anchors, artifacts, and evidence items. In the supplied data bundle, artifacts were enumerated (for example multiple domains, URLs, file names and hash identifiers) and aggregated into evidence items with modality and feature vectors. Each evidence item in the normalized record carries modality tags (for instance infrastructure) and a feature vector that quantifies inspector judgments about indicators (fields labelled I, A, M, P, T). Evidence items are also linked to anchors in the source text; anchor coverage and evidence_anchor_count are recorded for claims (eight anchors per claim in the example). Probative weights quantify per-item contribution to claims prior to later calibration steps (see evidence_weight_aggregate entries and per-evidence probative_weight in the evidence_items block).</p>

  <p>During structural parsing and artifact extraction the pipeline records chain provenance diagnostics such as provenance, integrity, lineage_quality, and anchor_quality. These diagnostics feed the custody and grounding subscores: grounding reflects anchor coverage and marker strength (for example evidence_anchor_count and evidence_marker_strength), and custody reflects provenance and artifact identifiers. In the example data, grounding_avg_0_100 is 58.65 and custody_avg_0_100 is reported near 61, illustrating a measurable separation between how well elements are anchored in the source and the assessed provenance/integrity of those elements.</p>

  <h2>References, citation parsing and institution inference</h2>
  <p>Reference parsing identifies explicit citations and the distinct sources that support claims. The pipeline records source counts and citation coverage (fields such as sources_total and citations_total) and flags single‑source dependence where appropriate (single_source penalty applied as a factor in per-claim scoring). Institution inference proceeds from authoring_org and publisher fields included in normalized sources; when supporting references are absent, institutional inferences are conservative and reflected in the credibility component rather than in clarity alone. In the provided bundle there is a single normalized source (sources_total = 1) and citations_total = 0. The presence or absence of corroborating institutions is captured in credibility and corroboration subscores and used downstream to apply penalties such as the documented single_source penalty (penalty factor 0.85 in claim scoring). The state‑linking signal is also registered: flags such as state_link_evidence_flag and state_claim_flag capture whether the source text contains evidence framed as linking an operation to a state actor and whether the source explicitly asserts a state attribution, respectively; examples in the record show state_link_evidence_flag = 1 while state_claim_flag = 0 for the claims shown.</p>

  <h2>Scoring framework for clarity and state responsibility pathways</h2>
  <p>The clarity axis decomposes attributional clarity into orthogonal subcomponents: act_specificity, actor_specificity, and link_specificity, each scored for both claim and evidence contexts. These subcomponents are combined to yield a claim-level clarity score and then aggregated across claims to produce document clarity (clarity_avg_0_100). The system additionally quantifies three responsibility pathways: conducted_by_state_organs (organ path), non_state_actors_under_state_control (control path), and state_due_diligence_failure (due diligence path). For each claim the pipeline computes numeric path clarity scores (organ_path_clarity, control_path_clarity, due_diligence_path_clarity) and three diagnostic question scores (attribution_clarity, responsibility_mode_clarity, due_diligence_clarity) that express whether attribution and the particular legal modality are presented clearly enough to satisfy an operational threshold.</p>

  <p>Methodologically, organ_path_clarity places weight on actor specificity and direct evidence tying an action to a state organ; control_path_clarity emphasizes patterns that connect non-state actors to state direction or sustained support; due_diligence_path_clarity focuses on evidence of state knowledge plus a failure to prevent or take reasonable measures. The pipeline records intermediary signals that inform these path scores: a state_actor_signal metric captures text or evidence that indicates a state</section>
<section class="wiki-section" id="sec-aggregation_calibration"><div>
  <h2>Introduction</h2>
  <p>This methodology section describes how individual claims and supporting documents are aggregated into composite attribution scores, how those aggregates are calibrated against statistical priors, and how uncertainty and dispersion are characterised and reported. The description that follows emphasizes methodological rationale and operational mechanics rather than case-specific findings. It uses fields and diagnostic outputs present in the supplied scoring bundle and document score tables to show how aggregation, calibration, and uncertainty are handled in practice.</p>

  <h2>Data processing and evidence extraction</h2>
  <p>At the outset, textual and technical inputs are converted to discrete evidence items and artifacts during structural parsing and artifact extraction. Each evidence item is represented with a small feature vector and an associated probative_weight (for example, the normalized probative_weight values found in the evidence_items records). Anchor references and artifact identifiers (artifact_id and anchor block identifiers) are preserved to support chain-of-custody diagnostics. The system records provenance counts (for example, sources_total and citations_total) and produces per-evidence modality tags (for example, infrastructure) so that modality diversity and anchor coverage can later influence weighting. These extraction and provenance indicators form the first-stage inputs to aggregation rather than being discarded as free text.</p>

  <h2>Reference parsing and institution inference</h2>
  <p>Reference parsing isolates explicit citations and authoring organizations from each source record; institution inference augments explicit metadata by mapping anonymous or incomplete author fields to originating organizations where possible. In the supplied bundle, source-level metadata fields (such as authoring_org, publisher, and origin_signature in the normalized sources array) are used to compute domain independence and to identify single-source conditions (sources_total and citation counts). When explicit citation coverage is absent, the pipeline flags the evidence as report-derived and uses provenance diagnostics (for example, the chain_provenance_diagnostics elements) to indicate limits on independent corroboration. Institution inference therefore supplies the domain-level and organizational-level indicators that feed credibility and corroboration axes while preserving the underlying provenance anchors for traceability.</p>

  <h2>Scoring framework: aggregation, weighting, and calibration</h2>
  <p>Aggregation proceeds in layers. At the claim level, individual evidence items contribute a weighted evidence weight aggregate (for example, evidence_weight_aggregate and per-evidence probative_weight) that is then adjusted by provenance and quality penalties. The scoring bundle records explicit penalty multipliers (for example, the single_source penalty factor of 0.85) which are applied multiplicatively to prevent over-reliance on single-origin evidence clusters. Core axis scores (for example, grounding, custody, credibility, corroboration, confidence, and clarity) are computed from constituent diagnostics such as anchor_coverage, integrity_signal, lineage_quality, and modality_diversity; these form the six-c vectors or core_3c components used in downstream aggregation.</p>

  <p>Once claim-level raw scores are computed, a statistical calibration step shrinks raw estimates toward empirically derived priors to mitigate overfitting when effective sample size is small. The pipeline records shrinkage parameters (for example, shrinkage_lambda values per axis and prior_scores_0_1 in the statistical_calibration_v4 block) that determine the convex combination between raw claim-level estimates and historical priors. Reliability diagnostics such as reliability_factor and effective_evidence_n indicate the degree of shrinkage: stronger shrinkage is applied when effective_evidence_n is small or when reliability_factor is low. Saturation_factors (for example, chain_quantity_score and corroboration_saturation) are applied to limit marginal gains once evidence quantity or modality coverage reaches pragmatically chosen thresholds.</p>

  <p>Belief or headline scores (for example, belief_weighted_0_100) are produced after calibration and reflect both the calibrated core axes and gravity-weight adjustments for allegation severity. Aggregation across claims to a document-level profile preserves per-claim vectors (for example, the headline_vector and coverage_vector_median) and reports both arithmetic and geometric aggregates (for example, overall_claim_score_mean and overall_claim_score_geometric) so users can inspect central tendency choices. Penalties, data contribution multipliers, and recovered_reference_count are retained to ensure transparency of how each claim contributes to the final document profile.</p>

  <h2>Uncertainty quantification and dispersion diagnostics</h2>
  <p>Uncertainty is explicitly represented via bootstrap intervals and by reporting calibrated posterior dispersion. The supplied document-level diagnostics include bootstrap_95ci outputs for key axes (for example, custody_avg_0_100.ci95_low and ci95_high and belief_weighted_0_100.ci95_low/ci95_high) that quantify sampling variability under resampling of the evidence set. CI width is used as a practical dispersion diagnostic: narrow intervals (small ci95_high – ci95_low) indicate stability under resampling, while wide intervals signal sensitivity to individual evidence items or to single-source leverage. Effective_evidence_n and reliability_factor are reported in tandem with bootstrap intervals to help interpret dispersion: a low effective_evidence_n with a wide CI suggests limited evidentiary depth, whereas a low n and narrow CI might indicate tightly clustered but non-independent evidence.</p>

  <h2>Validation and quality assurance</h2>
  <p>Quality assurance combines automated diagnostics and human review. Automated checks validate internal consistency (for example, that claim_support_coverage equals the ratio of evidence anchors to expected anchors, and that anchor_coverage is between 0 and 1), that provenance markers (provenance, integrity, artifact_identifiers) are present, and that statistical calibration parameters (shrinkage_lambda, prior_scores_0_1) are within acceptable ranges. Human reviewers then inspect key traceability items such as anchor block identifiers, artifact_ids, and origin_signature entries to verify chain-of-custody claims. Diagnostic fields such as chain_provenance_diagnostics, core_3c bands, and saturation_factors are used to prioritise human attention: claims with low credibility and high claimed gravity or with large bootstrap dispersion are escalated for additional review.</p>

  <p>Together, these components — explicit evidence weighting, provenance-sensitive penalties, shrinkage-based calibration toward priors, and bootstrap-based uncertainty reporting — form an auditable pipeline for producing aggregated attribution scores. By surfacing diagnostics (for example, single-source penalties, effective evidence n, shrinkage lamb</section>
<section class="wiki-section" id="sec-validation_quality_assurance"><section aria-labelledby="validation_quality_assurance"><h2 id="validation_quality_assurance">Validation and Quality Assurance</h2><p>This Validation and Quality Assurance section describes the automated validation gates and the layered QA protocol used to assure the integrity and traceability of processed materials. The discussion is framed as a methodological exposition and does not present or interpret case-specific substantive findings. The validation system produces a formal validation bundle (see validation report file at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/apt29_hammertoss_stealthy_tactics_define_a/apt29_hammertoss_stealthy_tactics_define_a.validation_report.json) that is used to gate downstream workflow stages. The bundle contains a certification status (PASS) and an aggregate numerical assessment (overall_score: 92.54) as well as component-level scores that function as explicit automated validation gates for schema conformity, data integrity, tabular extraction, citation presence, temporal coverage and artifact extraction.</p><p>Operationally, validation gates are implemented as deterministic checks within the ingestion and processing pipeline. Schema conformity and integrity checks are treated as hard gates that must be satisfied before artifacts are promoted; these are reflected by component scores reported at 100.0 for schema and integrity in the validation bundle. Other gates—such as tables, citations, and corroboration metrics—are scored quantitatively and produce advisory conditions when thresholds are not fully met. The validation report enumerates summary_counts (for example: pages: 1, tables: 2, artifacts: 20, claims: 3, sources: 1) and documents findings such as warnings that indicate duplicate anchors, limited anchor diversity, or components missing non-duplicative support. Hard_failures is an explicit field used to record any gate failures that would block promotion; in the referenced validation bundle this field is empty, indicating no hard-failure level gate trips for the processed package.</p><p>The QA protocol is layered and combines automated gating, agent review, and targeted human review. Agent review is enabled as part of the pipeline orchestration (agent_review_enabled: true) and performs triage, reformatting, and contextual consistency checks that supplement the deterministic validation gates. Parallel to automated processes, a targeted human review is executed on a sampled fraction of outputs to provide an independent quality check. The human_sample_fraction is 0.1 (10%), and the observed error rate within that sampled subset is reported as 0.0, that is, no observed errors were detected in the reviewed sample. The note field in the QA metadata clarifies that human review is targeted and sampled and that reported results correspond to the reviewed sample.</p><p>Quality assurance practices also include explicit escalation criteria and remediation workflows: advisory warnings (for example, limited corroboration or duplicated anchors) trigger either automated re-processing or assignment to agent review for focused correction. All validation artifacts and score outputs are retained to maintain a chain of custody and reproducibility; relevant artifacts include the primary report payload (/home/pantera/projects/TEIA/annotarium/outputs/reports/apt29_hammertoss_stealthy_tactics_define_a_report.json), the raw extraction output (/home/pantera/projects/TEIA/annotarium/outputs/pipeline/apt29_hammertoss_stealthy_tactics_define_a/apt29_hammertoss_stealthy_tactics_define_a.output.json), the scoring reports (/home/pantera/projects/TEIA/annotarium/outputs/scoring/apt29_hammertoss_stealthy_tactics_define_a.icj_score_report.json and /home/pantera/projects/TEIA/annotarium/outputs/scoring/apt29_hammertoss_stealthy_tactics_define_a.icj_score_report_v3.json) and the source PDF (/home/pantera/projects/TEIA/annotarium/Reports/apt29-hammertoss-stealthy-tactics-define-a.pdf). These artifacts permit revalidation, independent audit, and iterative calibration of both validation gates and reviewer guidance.</p><p>Finally, ongoing quality assurance is governed by continuous monitoring of category scores (for example, credibility_grounding and corroboration_grounding) and by periodic re-sampling of human review beyond the baseline 10% fraction when advisory warnings cluster or when score drift is observed. Observed sample performance (no observed errors within the 10% human sample) is used to inform risk tolerances and sampling strategies, but does not obviate the need for escalation triggers tied to automated gate warnings. Together, the deterministic validation gates, enabled agent review, and targeted human review constitute a defensible QA posture designed to balance scalability with human oversight and reproducibility.</section>
<section class="wiki-section" id="sec-limitations_governance"><section id="introduction"><h3>Introduction</h3><p>This methodological addendum explains the limitations, governance controls, and a principled refinement roadmap for a reproducible cyber-attribution scoring pipeline. It situates the discussion within the project’s jurisprudential evidentiary posture: scores are instruments for assessing the structure and quality of argumentation in attribution dossiers rather than definitive factual verdicts. The exposition below avoids operational claim-level description and instead addresses scope, data processing, institutional inference, scoring architecture, and validation controls in sequence, with explicit attention to limitations, governance, and a refinement roadmap for iterative improvement.</p></section>

<section id="scope_units"><h3>Scope and Analytical Units</h3><p>The analytical units are documents, discrete claims within those documents, associated source records, and named artifacts extracted from source text. The pipeline treats each claim as an atomic proposition subject to evidentiary linkage: artifacts and sources must be anchored to a claim with explicit locators. This unitization supports differentiated treatment of provenance, temporal linkage, and source diversity and forms the basis for subsequent chain-of-custody and credibility assessments.</p></section>

<section id="data_ingestion"><h3>Data Ingestion</h3><p>Source reports are ingested as binary PDF files and are retained unchanged on disk as canonical inputs. The ingestion step records provenance metadata for each file including ingestion timestamp, file path, and checksum. The validation bundle provided with a run records whether ingestion and subsequent integrity checks passed; in the supplied validation bundle the certification field reports a run-level PASS, which is an operational indicator that integrity checks were satisfied prior to scoring. Such run-level metadata are critical governance artifacts: they enable auditors to reproduce the exact input corpus used for any numerical output.</p></section>

<section id="pdf_to_markdown"><h3>PDF-to-Markdown Transcription</h3><p>Transcription from PDF to markdown is performed under deterministic rules where possible, with model-assisted recovery used only when structure cannot be parsed deterministically. The procedure logs the transformation result and any heuristics applied. This step preserves evidence anchors by locating page numbers, table indexes, and figure captions in machine-parsable form. The transcript is therefore the first persistent intermediate record in the custody chain and is subject to integrity verification before structural parsing proceeds.</p></section>

<section id="structural_parsing"><h3>Structural Parsing</h3><p>Parsed outputs conform to a constrained schema that separates claims, sources, artifacts, and evidence-link objects. The parser enforces mandatory anchors for each evidence link and reports schema compliance as a categorical score. The methodology reference excerpt emphasizes that strict schema enforcement prevents retrospective reconstruction of support by implicit inference; this constraint intentionally narrows extraction breadth in the service of evidentiary legibility and reproducibility.</p></section>

<section id="artifact_extraction"><h3>Artifact Extraction</h3><p>Artifact extraction isolates discrete forensic items such as logs, technical indicators, and timestamps. Each artifact record is annotated with provenance metadata, technical provenance claims present in the text, and any asserted linkage to infrastructure or actor identities. Artifacts form the nodes of the claim–evidence graph and are the primary objects evaluated by chain-of-custody calculations. Extraction heuristics are conservatively tuned to favor precision over recall to reduce the risk of spurious linkage in downstream scoring.</p></section>

<section id="reference_parsing"><h3>Reference Parsing</h3><p>Reference parsing identifies cited documents, organizational names, and bibliographic anchors. The system classifies references by type (e.g., international institution, peer-reviewed article, government publication) because source-type quality factors into credibility calculations. The validation output includes a count of sources and citations; such counts are used only to illustrate processing coverage and not to substitute for qualitative assessment of source independence.</p></section>

<section id="institution_inference"><h3>Institution Inference</h3><p>Institutional inference maps textual references to canonical organizational identifiers and infers institutional quality attributes used in the credibility model. The mapping process is conservative: ambiguous references are flagged for manual review rather than resolved by automatic substitution. Institution inference draws on classification rules defined in the methodology excerpt which privilege international judicial and peer-reviewed sources as higher-quality anchors. This conservative stance reduces false elevation of credibility through misclassification.</p></section>

<section id="claim_evidence_graph"><h3>Claim–Evidence Graph Construction</h3><p>Claims and evidence items are connected into a directed graph where edges carry explicit anchors indicating the textual locus of the asserted linkage. The graph model supports cluster-based corroboration calculations and anti-circularity checks: origins are identified and aggregated with diminishing returns so that repeated downstream reporting of one upstream source does not produce spurious independent corroboration. The graph also supports traceability requirements demanded by governance controls: any score is reconstructible to the explicit set of nodes and edges that produced it.</p></section>

<section id="scoring_overview"><h3>Scoring Overview</h3><p>The scoring architecture implements the ICJ-inspired evidentiary weighting model set out in the methodology excerpt. Item-level weights are computed along bounded dimensions of independence, authentication/provenance, methodological soundness, procedural testing, and contemporaneity, and these are combined multiplicatively to preserve the requirement that severe weakness in any one dimension meaningfully reduces probative force. Claim-level metrics—Chain of Custody, Credibility, and Clarity—are derived from these item-level assessments and calibrated by document-level coverage factors to prevent over-interpretation of isolated high-quality items.</p></section>

<section id="chain_of_custody"><h3>Chain of Custody</h3><p>The chain-of-custody axis evaluates evidentiary handling quality per claim by aggregating provenance markers, integrity markers, temporal anchors, artifact identifiers, and versioning lineage into a bounded score. This aggregation is linear and auditable, with each subcomponent retained as an explanatory field. The chain-of-custody score functions as a filter on admissibility: records failing minimal custody thresholds are excluded from credibility aggregation to avoid pseudo-precision.</p></section>

<section id="credibility_corroboration"><h3>Credibility and Corroboration</h3><p>Credibility is computed as a composite of source-quality and corroborative convergence subject to an explicit single-source penalty and domain-independence adjustments. Corroboration is measured by convergence across independent origins rather than citation volume; origins are clustered and aggregated with diminishing returns to operationalize anti-circularity. The methodology excerpt documents a coverage calibration whereby document-level credibility is attenuated when high-quality sources cover only a subset of weighted claims, ensuring that high scores reflect breadth of high-quality support rather than concentration on a narrow claim subset.</p></section>

<section id="clarity_axis"><h3>Clarity Axis</h3><p>The clarity axis evaluates the intelligibility of attribution reasoning in legal terms. It records whether the report articulates a clear mode of responsibility consistent with state-responsibility doctrine and evaluates whether act–actor–link specificity is sufficient to support the chosen legal pathway.</section>
</article>