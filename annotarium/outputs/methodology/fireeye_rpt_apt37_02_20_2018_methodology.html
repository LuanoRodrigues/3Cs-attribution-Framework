<article class="wiki-page">
<header><h1>Annotarium Methodology: APT1: Exposing One of China's Cyber Espionage Units</h1><div class="wiki-meta">Generated at 2026-02-22T21:20:40Z</div></header>
<nav class="wiki-toc"><h2>Contents</h2><ol>
<li><a href="#sec-introduction">Introduction and Epistemic Framing</a></li><li><a href="#sec-scope_units">Scope, Units of Analysis, and Output Semantics</a></li><li><a href="#sec-data_ingestion">Data Ingestion and Corpus Handling</a></li><li><a href="#sec-pdf_to_markdown">PDF-to-Markdown Conversion</a></li><li><a href="#sec-structural_parsing">Structural Parsing of Text, Tables, and Figures</a></li><li><a href="#sec-artifact_extraction">Artifact Extraction and Technical Object Normalization</a></li><li><a href="#sec-reference_parsing">Footnote and Reference Parsing</a></li><li><a href="#sec-institution_inference">Institution Inference and Source Typology</a></li><li><a href="#sec-claim_evidence_graph">Claim-Evidence Graph Construction</a></li><li><a href="#sec-scoring_overview">Scoring Framework Overview</a></li><li><a href="#sec-chain_of_custody">Chain of Custody Axis</a></li><li><a href="#sec-credibility_corroboration">Credibility Axis with Corroboration Subcomponent</a></li><li><a href="#sec-clarity_axis">Clarity Axis and State Responsibility Pathways</a></li><li><a href="#sec-aggregation_calibration">Aggregation, Calibration, and Uncertainty</a></li><li><a href="#sec-validation_quality_assurance">Validation and Quality Assurance</a></li><li><a href="#sec-limitations_governance">Limitations, Governance, and Future Refinement</a></li>
</ol></nav>
<section class="wiki-section" id="sec-introduction"><section id="introduction"><h2>Introduction and Epistemic Framing</h2><p>This methodology is presented as an evidentiary framework for cyber attribution where findings are contestable and subject to adversarial scrutiny. Its epistemic posture is jurisprudential: the framework treats attribution claims as premises in structured evidentiary arguments rather than as standalone assertions of confidence. This burden-sensitive orientation requires that analysts make explicit which materials support each proposition, how those materials are connected to the proposition, and what inferential steps are required before legal responsibility might be imputed. The approach therefore separates descriptive extraction from normative legal inference so that upstream uncertainties remain visible to downstream adjudication.</p></section><section id="scope_units"><h2>Scope and Units of Analysis</h2><p>The primary unit of analysis is the evidentiary record derived from a single attribution report, identified in the raw dataset by path and identifier values. For the present implementation these identifiers include the transcribed source path and report identifier reported in the input metadata. Each report is decomposed into constituent claims, discrete artifacts, and explicit source references. Claims are weighted according to declared gravity and evidentiary anchoring; artifacts are cataloged with provenance markers; and source references are registered to permit later institution inference. Aggregation operates only after claim-level scoring to preserve localized uncertainty and to avoid conflating disparate inferential steps.</p></section><section id="data_ingestion"><h2>Data Ingestion and Record Formation</h2><p>Documents are ingested from a corpus of PDF reports and associated metadata records. The pipeline records ingestion provenance and timestamps from the input metadata to ensure persistence and auditability; examples of such metadata include the internal report path and the generated-at timestamp. Deterministic transformations are preferred where possible to reduce parser-induced variance. Ingestion produces an immutable raw-markdown transcription that is the authoritative input for downstream structural parsing and integrity checks.</p></section><section id="pdf_to_markdown"><h2>PDF-to-Markdown Conversion</h2><p>The conversion stage employs a primary provider-backed OCR/transcription method with an explicitly declared fallback process. Primary and fallback methods are recorded in the pipeline metadata to ensure reproducibility of any particular run. During conversion, page-level anchors for tables, figures, and footnotes are emitted into the markdown so that later artifact extraction can reference concrete document offsets rather than paraphrased summaries. This design supports burden-sensitive review because it preserves the original textual context required for legal argumentation.</p></section><section id="structural_parsing"><h2>Structural Parsing and Schema Enforcement</h2><p>Transcribed markdown is parsed into a schema-constrained evidentiary record separating claims, sources, artifacts, and evidentiary links. The schema enforces explicit anchors for each reference and disallows implicit support chains. Structural integrity checks are applied at this stage and runs are failed where anchors are missing, identifier collisions occur, or citation pathways cannot be traced. These admissibility-style controls prevent pseudo-precision by ensuring that numerical outputs reflect inspectable linkages rather than parser convenience.</p></section><section id="artifact_extraction"><h2>Artifact Extraction</h2><p>Artifact extraction indexes items referenced in text, tables, and images into an artifact register. Each artifact entry records provenance markers, temporal anchors, artifact identifiers, and any versioning information recoverable from the record. The extraction method is deliberately conservative: only artifacts explicitly anchored to the source document are admitted to the register. This constraint preserves the evidentiary distinction between what is asserted and what is demonstrably present in the corpus.</p></section><section id="reference_parsing"><h2>Reference Parsing</h2><p>Citation and footnote-like references are parsed into a source registry that preserves original citation text and resolved identifiers when available. The registry records source type categories and a linkage to the artifact indices so that later scoring computations can disaggregate independent origin contributions from downstream repetition. Reference parsing therefore operationalizes the anti-circularity principle by making origin relationships explicit and machine-inspectable.</p></section><section id="institution_inference"><h2>Institution Inference</h2><p>Institution inference maps parsed references to canonical institutional identities using a named-institution resolution process recorded in the pipeline metadata. The method privileges deterministic resolution paths, with an explicit optional web-fallback recorded when lexical matching is insufficient. Inferred institutions are assigned provisional quality markers to reflect uncertainty in resolution; those markers feed directly into the later credibility calculations so that source-identity ambiguity is reflected as diminished probative force rather than silently normalized away.</p></section><section id="claim_evidence_graph"><h2>Claim–Evidence Graph Construction</h2><p>Claims and their supporting items are represented as a directed claim–evidence graph in which nodes denote claims, artifacts, and sources, and edges encode explicit anchor relationships. This graph preserves the chain of evidentiary dependence and enables automated queries about independence, redundancy, and provenance. The graph model underwrites burden-sensitive adjudication because it makes explicit what must be proved for a claim to carry weight and which inferential steps depend on contested links.</p></section><section id="scoring_overview"><h2>Scoring Framework Overview</h2><p>Scoring is performed at the item and claim levels under an ICJ-inspired weighting model recorded in the methodological reference. Item-level evaluations consider bounded dimensions such as independence, authentication/provenance, methodological soundness, procedural testing, and contemporaneity. Item scores combine multiplicatively to ensure that severe deficiencies in any single dimension constrain overall probative force. Claim-level outputs aggregate item-level scores with attention to origin clustering and diminishing returns to guard against artificial inflation by repetitive downstream reporting.</p></section><section id="chain_of_custody"><h2>Chain of Custody Axis</h2><p>The Chain of Custody axis operationalizes evidentiary handling quality for each claim. It is computed from normalized variables extracted from the text: provenance markers, integrity markers, temporal anchors, artifact identifiers, and version/update lineage. These variables are combined in a bounded, auditable function so that custody is interpretable as claim-specific handling quality rather than raw artifact quantity, and so that custody can be independently assessed during adversarial review.</p></section><section id="credibility_corroboration"><h2>Credibility and Corroboration</h2><p>Credibility integrates source-quality assessment and corroborative convergence. Source quality weights are applied according to specified taxonomies that privilege judicial and peer-reviewed materials while penalizing single-source dependence and self-reporting. Corroboration is modeled as constrained convergence: claims receive corroborative strength only from materially independent origins and only to the extent the supporting items are substantively relevant to the claim wording. Corroboration is retained as an auditable subscore and then merged into the top-level credibility axis using prespecified weights to preserve interpretability.</p></section><section id="clarity_axis"><h2>Clarity Axis</h2><p>Clarity measures legal intelligibility: whether the report sets out a clear act–actor–link specification and whether it maps that specification onto recognized modes of state responsibility. The clarity assessment explicitly queries whether attribution reasoning corresponds to direct state conduct, control/direction over non-state actors, or omission/due-diligence failure. Because clarity bears on legal utility rather than technical plausibility alone, it is computed separately</section>
<section class="wiki-section" id="sec-scope_units"><h2>Introduction</h2>
<p>This methodological chapter sets out the units of analysis and the interpretive semantics that govern an attribution scoring pipeline for cyber-incidents. It explains, in legal-academic register, how discrete objects extracted from a document are normalized, related, and scored; and it clarifies what each output score is intended to indicate and expressly not to indicate. The exposition is grounded in the supplied raw data excerpt and pipeline metadata, which serve as exemplar inputs for the described procedures rather than as case-specific findings. References to counts and example metric values that appear in the excerpt are used only to illustrate method behavior and calibration, not to substantiate or repeat substantive allegations contained in any specific report.</p>

<h2>Data processing and extraction</h2>
<p>Ingestion begins with a document and attendant metadata. The supplied excerpt demonstrates standard metadata fields (title, authoring entity, publication date and its anchor, version, and input format) and a filesystem source locator. The first stage performs content normalization and provenance capture: the document-level metadata are extracted verbatim and any inferred anchors (for example a publication date derived from a filename heuristic) are recorded with an explicit provenance tag. Structural parsing then segments the text into machine-readable units; in the exemplar dataset the pipeline recorded one sampled page, six sampled claims, one sampled source, and four sampled artifacts. Each segment is assigned a stable identifier and a custody record that records the transformation history (for example: original file → markdown conversion → structural parse). Artifact extraction isolates technical items such as binaries, network indicators, tables, and figures and associates them with the containing document and the structural location where they appeared.</p>

<h2>References and institution inference</h2>
<p>Reference parsing treats in-text citations, bibliographic entries, and implicit source mentions as structured source objects. The pipeline stores source-level metadata (such as a vendor report label and file path) separately from extracted claims. Where explicit affiliations or authoring entities are present in document metadata, those values are recorded; where affiliation must be inferred (for example from a filename or a byline line that lacks formal affiliation), the inference is recorded with a confidence flag and the heuristic used. The supplied raw_metadata shows an authoring entity field and an inferred publication-date anchor; the methodology requires that such inferences remain auditable by preserving the extraction method and verbatim anchor text. Institution inference therefore proceeds only on the basis of explicit metadata or on transparent heuristics, and always produces structured output that includes the provenance and a cautionary note that institutional association is an interpretive step rather than an evidentiary conclusion.</p>

<h2>Scoring framework: units and semantics</h2>
<p>Units of analysis are defined as follows. A claim is a discrete propositional statement extracted from the text whose truth or probative force the pipeline seeks to characterize. A source is an identifiable origin of information (for example a vendor report, a research article, or a dataset) and is represented separately from claims it contains. An artifact is a physical or digital object described in the document (for example a file hash, binary, network indicator, table, or figure) that may function as an evidence item. An evidence item is a parsed, named unit created from an artifact or a quoted passage that can be linked to one or more claims. Document-level denotes metrics and metadata that apply to the entire document as opposed to individual claims or artifacts.</p>
<p>The scoring model produces multiple axes intended to describe different epistemic properties rather than to deliver a single definitive judgment. A grounding score quantifies the degree to which a claim is anchored in locally supplied evidence items; a custody score records the completeness and traceability of artifacts and transformation steps; a credibility axis expresses the assessed reliability of a source independent of the claim’s internal grounding; a corroboration metric captures whether independent sources provide concordant support; a clarity axis measures the semantic preciseness and extractability of the claim; and a belief or confidence score synthesizes these axes into a calibrated epistemic judgment. These outputs are probabilistic or ordinal summaries and must be read as indicators of evidentiary weight and provenance quality, not as legal determinations or final attribution verdicts. For example, a low credibility score does not by itself prove falsity; a high grounding score does not adjudicate motives or organizational responsibility. In the exemplar document_scores_v4, numeric summaries such as an average grounding value and a custody average are illustrative of how the pipeline aggregates per-claim and per-document measures for calibration and quality monitoring.</p>

<h2>Validation and quality assurance</h2>
<p>Validation comprises automated consistency checks, manual review, and bootstrap estimation of metric stability. Automated checks assert internal invariants (for example that every evidence item references a document-level custody record and that inferred metadata contain provenance tags). Manual adjudication samples parsed claims and artifacts against their source text to measure extraction precision and recall; the pipeline’s provided preview of claim-level scores is used to select stratified samples across the score distribution. Statistical resampling—illustrated in the raw output by the provided bootstrap confidence intervals—is used to estimate the stability of aggregated metrics and to detect overfitting to specific parsing idiosyncrasies. Quality assurance also imposes governance constraints: all inferences about institution or authorship are logged with the heuristic used, and outputs intended for downstream decision-making are accompanied by explicit caveats about what the scores do not establish (for example that they do not constitute legal proof of attribution). Periodic recalibration of the scoring functions is required when the composition of the input corpus changes; the exemplar pipeline_counts and claim_score_preview_v4 provide the kinds of summary statistics used to trigger such recalibrations.</p>

<p>Taken together, these procedures create an auditable pipeline that separates extraction, inference, and scoring, preserves provenance at every transformation step, and frames scores as structured epistemic assessments rather than categorical assertions. Where specific numeric values from the provided excerpt are cited, they serve to illustrate aggregation and validation behaviors and are not invoked as factual findings about any actor or incident.</p></section>
<section class="wiki-section" id="sec-data_ingestion"><h1>Methodology: Data Ingestion and Corpus Handling for Cyber-Attribution Scoring</h1>
<p>Introduction. This methodology describes the deterministic processes and reproducibility guarantees applied to the input corpus from raw PDF reports through intermediate artifacts that feed downstream attribution scoring. It is written to articulate the rationale and operational controls used to transform an input corpus into structured artifacts suitable for automated extraction, institutional inference, and quantitative scoring. The exposition emphasizes process invariants, artifact provenance, and verifiable metadata while avoiding any adjudication of the substantive claims contained in source documents.</p>

<h2>Scope and Units of Analysis</h2>
<p>The input corpus for this pipeline is one or more publisher artifacts in portable document format, together with derived machine-readable representations. Every unit of analysis is a discrete report file instantiated on disk and recorded in the system registry. For the file set treated here, the canonical report identifier and its record in the processing registry are preserved as immutable metadata elements. The working units include the original PDF, a primary JSON report manifest, a structured text extraction JSON, a validation report, and versioned scoring outputs; these units form the minimal reproducible bundle for any subsequent audit or reprocessing.</p>

<h2>Data Ingestion</h2>
<p>Report ingestion begins with the intake of the original PDF identified in the processing registry. The pipeline records absolute file paths, file existence flags, and size-in-bytes as first-order provenance properties. The storage paths used in the project context are recorded as persistent artifact names, for example the canonical PDF path and manifest paths. Deterministic file handling is achieved through strict naming conventions for intermediate artifacts, controlled directory layout, and immutably recorded artifact metadata. For each ingested file the pipeline emits a processing manifest that records the original path, a processing timestamp, the environment snapshot, and the set of produced artifacts. This manifest supports reproducibility by providing a complete mapping from an input corpus item to a set of downstream artifacts without reliance on external state.</p>

<h2>PDF-to-Markdown and Text Extraction</h2>
<p>The textual conversion stage is implemented with a primary provider-backed OCR and conversion routine and a deterministic offline fallback. The primary conversion is executed by a provider-backed script that produces a stage-one markdown artifact; when provider conversion fails the offline fallback executes via an alternative converter with equivalent deterministic settings. All conversion stages are parameterized with fixed options and are executed in a predictable order so that a given input PDF and a fixed runtime environment will yield the same intermediate markdown output. Tables and figures are extracted into anchored representations within the markdown during this stage, and the extraction stage emits a structured output JSON that indexes tables, figure anchors, and block-level text segments to enable later deterministic parsing.</p>

<h2>Structural Parsing and Artifact Extraction</h2>
<p>Structural parsing transforms the stage-one markdown and extraction JSON into typed artifact indices. The extraction schema defines artifact classes such as text span, table, figure, and code-like blocks; each artifact is assigned a stable identifier derived from the source file identity and a positional ordinal. Artifact indices are materialized into a machine-readable JSON that includes artifact offsets, anchor tokens, and contextual snippets. Deterministic behavior is enforced by using canonical tokenization rules, fixed header parsing precedence, and a documented block classification order. Artifact extraction additionally produces an artifacts registry that can be audited to trace every extracted element back to a location in the original PDF and the stage-one markdown.</p>

<h2>Reference Parsing</h2>
<p>References, citations, and footnote-like constructs are parsed using a rule-based parser that links reference tokens to a cross-document source registry. The parser emits a mapping from in-text citation anchors to resolved reference entries in the registry, and it records uncertainty metrics for ambiguous matches. Reference parsing is deterministic in that the same inputs, parser configuration, and environment yield the same linkage decisions. All reference mappings and their confidence metadata are persisted in the extraction artifacts so that downstream processes can re-evaluate or override automatic linkages in follow-up analyses.</p>

<h2>Institution Inference</h2>
<p>Institution inference is implemented as a staged process that first applies deterministic heuristics and gazetteer lookups and then augments those results with a supervised inference model when heuristic resolution is insufficient. The model-based step is performed with a fixed model version and parameters; optional external web lookups are recorded as separate ephemeral annotations so that they may be disabled to preserve strict determinism where required. The institution inference stage produces provenance annotations that reference the originating artifact identifiers and the precise textual evidence used for each inferred institution, enabling reviewers to trace every inferred association back to its textual basis.</p>

<h2>Claim–Evidence Graph Construction</h2>
<p>Extracted artifacts and parsed references are integrated into a claim–evidence graph that encodes provenance edges, textual support spans, and cross-references. Each node in the graph corresponds to either a claim tokenization from the text or to an evidence artifact such as a table cell or figure caption. The graph construction uses fixed graph schema and deterministic ordering rules; node and edge identifiers are stable functions of the artifact registry and the token offsets. This graph provides the canonical linkage layer for scoring and allows reproducibility of any claim-level decision by replaying the deterministic graph construction process from the archived intermediate artifacts.</p>

<h2>Scoring Overview</h2>
<p>Scoring is performed against the claim–evidence graph using a versioned scoring engine. All scoring runs are recorded as distinct output artifacts, and version identifiers are embedded in artifact filenames to preserve the chain of custody between scoring logic and scored outputs. Versioned scoring outputs are persisted alongside the extraction artifacts so that any score can be traced back to the exact extraction and inference inputs used to compute it. Metrics and confidence values are stored as structured JSON that references the exact graph node and edge identifiers that informed each scored item, enabling deterministic recomputation under the same runtime snapshot.</p>

<h2>Chain of Custody and Provenance</h2>
<p>Chain-of-custody is enforced by recording file system metadata and cryptographic checksums for the input PDF and all intermediate artifacts. The processing manifest captures environment details including runtime language version, operating platform, and installed package versions. Artifact existence flags and size-in-bytes are recorded as part of the manifest to support rapid integrity checks. The provenance trail therefore comprises the original file path, the transformation history with timestamps, checksum values, and the set of downstream</section>
<section class="wiki-section" id="sec-pdf_to_markdown"><h2>Introduction</h2>
<p>This methodology chapter sets out the procedural and evidentiary framework used to convert source reports into structured inputs for cyber-attribution scoring. The account that follows is procedural and normative: it explains how original artefacts are treated, how text and tabular structures are extracted, how references and institutional assertions are inferred, and how those outputs feed a transparent scoring procedure. Methodological rationale is prioritized over substantive case findings; the objective is to render the pipeline reproducible and auditable while emphasising safeguards that preserve evidentiary integrity and permit independent validation.</p>

<h2>Data Processing and Extraction</h2>
<p>The pipeline ingests native report files and emits structured representations that form the basis for subsequent analytic stages. Primary inputs for the project instance described here included a portable document format (PDF) located at /home/pantera/projects/TEIA/annotarium/Reports/Fireeye_rpt_APT37(02-20-2018).pdf (size: 2,920,140 bytes) and an associated markdown output at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.md (size: 22,477 bytes). The broader pipeline produces and archives intermediate JSON artifacts, including report-level JSON, raw extraction outputs, validation metadata and scoring inputs; paths and sizes for these objects are recorded (for example, report JSON at /home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json, 520,983 bytes, and raw extraction at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.output.json, 316,434 bytes). These files serve as the auditable trail for provenance and reproducibility checks.</p>

<h2>PDF-to-Markdown Conversion</h2>
<p>The conversion of PDF to markdown constitutes a core resilient stage in the pipeline. The primary conversion path uses a provider-backed Mistral OCR process implemented in process_pdf_mistral_ocr.py. This Mistral-based processing is employed to capture textual content, layout cues and basic structural markers rapidly and with provider-accelerated recognition models. When the provider-backed conversion fails or exceeds operational time limits, the pipeline adopts an offline fallback posture: PyMuPDF4LLM is invoked as an alternative renderer and extractor. Presenting the offline fallback as a posture of resilience, rather than a conceptual departure, ensures that the logical model of extraction—transforming visual page elements into a linear, semantically annotated markdown representation—remains consistent across modes. Both modes emit a primary stage-one markdown that encodes paragraphs, headings, inline artefact anchors, tables and figures. The pipeline archives both the original PDF and the derived markdown to preserve chain-of-custody and to allow reprocessing should extraction heuristics be revised. The conversion design records metrics about pages, tables and figures (in this instance the pipeline registered 1 page, 3 tables and 4 figures) to illustrate handling modes such as table-preservation fidelity and image anchoring, not to adjudicate content.</p>

<h2>Structural Parsing and Artifact Extraction</h2>
<p>After markdown synthesis, a structural parsing stage normalises the markdown into discrete units for downstream analysis. The stage identifies tabular structures, embedded figures and inline artefacts and emits an artefact index of extracted indicators and blobs. The artefact extraction schema produces indexable references for text extracts, table cells and images so that individual evidence items can be referenced by identifier throughout the scoring workflow. This separation of concerns—document-level markdown and item-level artefact indices—supports differential validation, targeted re-extraction and selective manual review while preserving the original markdown as an auditable transformation layer.</p>

<h2>References and Institution Inference</h2>
<p>References, citations and footnote-like linkages are parsed from the markdown output and linked to a source registry. The reference-parsing component associates citation text with provenance anchors and record identifiers, enabling cross-indexing across documents and external registries. Institution inference is implemented in infer_source_institutions.py and leverages a probabilistic language model (gpt-5-mini) to propose candidate source institutions from the extracted text and contextual artefacts; an optional web fallback can be consulted to corroborate institutional metadata. The inference process maintains explicable provenance: candidate matches are recorded together with confidence markers and the textual segments that triggered the inference so that downstream reviewers can evaluate the degree to which an asserted institutional link depends on explicit textual citation versus inference heuristics.</p>

<h2>Claim–Evidence Graph and Scoring Framework</h2>
<p>The pipeline organises extracted claims and evidence into an explicit claim–evidence graph that maps discrete assertions to their supporting artefacts and to the parsed references. The graph formalism supports traceable scoring because each score component is defined as a function over graph features—such as artefact type, reference quality, and cross-source corroboration—rather than as an opaque aggregate. Scoring inputs and intermediate score reports are persisted (for example, score input at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.score_input_v3.json, 21,210 bytes; and score reports at /home/pantera/projects/TEIA/annotarium/outputs/scoring/fireeye_rpt_apt37_02_20_2018.icj_score_report.json, 30,870 bytes). The scoring framework distinguishes evidentiary axes—such as attribution plausibility, artefact integrity and corroboration density—and calibrates weights via validation sets rather than arbitrary heuristics. Rationale for weighting choices is preserved in accompanying metadata files so that the scoring process remains auditable and subject to revision.</p>

<h2>Validation and Quality Assurance</h2>
<p>Quality assurance is operationalised through multiple layers. The pipeline emits a validation report (for this run: /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.validation_report.json, 3,983 bytes) that records extraction success rates, fallback invocations and anomalies in structural parsing. Validation routines compare the provider-backed Mistral output with the offline PyMuPDF4LLM fallback to detect substantive divergences; differences are flagged for human review rather than automatically prioritised, consistent with the design principle that fallback behaviour</section>
<section class="wiki-section" id="sec-structural_parsing"><h2>Introduction</h2>
<p>This methodology chapter sets out the processes and rationale for structural parsing and downstream processing of vendor-origin cyber‑attribution reports within a reproducible scoring pipeline. The objective is to preserve provenance and enable independent auditability while transforming heterogeneous source artifacts (pages, text blocks, tables, figures, and embedded objects) into structured, machine‑readable representations that feed reference inference and scoring modules. The description that follows is procedural and methodological; it explains why certain extraction choices are made and how anchor preservation and artifact linking support chain‑of‑custody and verification, without advancing or interpreting substantive case findings contained in any particular report.</p>

<h2>Data Processing and Extraction</h2>
<p>The ingestion pipeline begins with acquisition of the original source file and associated auxiliary artifacts. Source acquisition is recorded with file system and checksum metadata and stored alongside an extraction manifest (for example, the pipeline stores references to both the original PDF and derived markdown and JSON extractions). Conversion from a paginated document to a structured intermediate is performed so that textual content, typographic structure, and object boundaries are explicitly represented. Structural parsing produces discrete text blocks (paragraphs and headings), table objects with cell‑level coordinates, and figure/image objects with bounding boxes and any embedded captions. When optical character recognition is required, the OCR output is retained together with confidence scores and bounding coordinates to preserve a map between raw pixels and extracted text.</p>

<p>Preserving anchors that reference the original document location for every extracted element is essential for auditability. An anchor is a stable identifier and a locator tuple (e.g., page index, bounding box, object id) that links an extracted item back to the exact position in the source. The pipeline attaches anchor metadata to document metadata fields and every extracted object; an example of such an anchor is a manually noted publication date anchor recorded with an anchor identifier and page location. Anchors enable third parties to re‑inspect the original artifact at the precise locus of extraction, to resolve ambiguity in context, and to validate that subsequent annotations, annotations revisions, or scoring decisions were applied to the intended fragment of the source. Without anchors, provenance claims are unverifiable because textual snippets and tabular values can be displaced or conflated during iterative processing.</p>

<h2>References and Institution Inference</h2>
<p>Reference parsing identifies and normalizes citations, named organizations, and institutionally relevant entities through multi‑stage processing. The first stage performs surface extraction of in‑text citations, footnotes, headers, and contact blocks; the second stage applies normalization heuristics and authority resolution against curated registries and the document’s own metadata. Anchor linkage is maintained for every reference so that the textual span that yielded the reference can be compared back to the original context. Institution inference leverages explicit references (for example, organization names appearing in author or source metadata) and implicit signals (such as recurring domains in command and control lists, or recurring contact patterns in artifacts). Rationale for each inference is recorded as structured evidence items with anchors to the source spans used, a categorical description of the signal type (named entity, domain, artifact header), and the transformation rules or external registries applied during normalization. This design permits forensic review of why a given institution label was suggested or scored, and the anchor trace supports contested corrections.</p>

<h2>Scoring Framework</h2>
<p>The scoring framework consumes structured evidence and applies calibrated rules that weight signal types according to provenance, extraction confidence, and corroboration. Structural elements—text blocks, tables, and figures—are treated distinctly because they vary in evidentiary semantics: narrative text often contains analyst interpretation, tables commonly contain enumerations and structured mappings, and figures may contain visual evidence or captured logs. Anchored evidence items carry both a qualitative tag (the element type) and quantitative attributes (extraction confidence, table cell coordinates, figure OCR confidence). Scores are computed by aggregating weighted signals across anchors, with explicit handling to avoid double‑counting when the same content is present in multiple representations (for example, a table rendered as an image and transcribed as text). The pipeline uses counts of structural elements—such as the number of tables or figures extracted—as operational diagnostics to guide sampling and manual review, not as substantive indicators of attribution. All scoring steps persist the contributing anchor identifiers so that any score can be decomposed back to the specific anchored items that produced it.</p>

<h2>Validation and Quality Assurance</h2>
<p>Quality assurance is implemented through automated validation checks, manual spot audits, and versioned artifacts. Automated checks include anchor integrity verification (ensuring every extracted item references a valid anchor and coordinate), schema validation of table and figure objects, and cross‑consistency tests between different extraction outputs (for example, comparing the markdown extraction to the page‑level JSON extraction). Manual audit trails use the stored anchors and payload paths to permit human reviewers to open the original artifact at the exact locus of the extraction and to confirm or correct parsed values. All validation activities and corrections are recorded in a change log that includes references to the extraction manifest and the file paths used in processing. The pipeline also outputs machine‑readable validation reports and store locations for score artifacts and intermediate files so that independent reprocessing can reproduce the chain of custody. Paths and manifests to primary processing artifacts (for example, raw extraction JSON, markdown conversion, validation reports, and final score outputs) are retained and referenced so that every analytical decision is traceable back to a specific file and anchor. In summary, anchor‑preserving structural extraction, explicit reference provenance, and persistent recording of processing artifacts together constitute the core measures that enable reproducible, auditable, and defensible attribution scoring.</p></section>
<section class="wiki-section" id="sec-artifact_extraction"><h3>Introduction</h3>
<p>This methodology chapter describes procedures for extracting artifacts across modalities and for normalizing technical objects to support provenance and custody evaluations within a cyber-attribution scoring pipeline. The presentation follows a structured roadmap that begins with definitions of scope and units of analysis and proceeds through ingestion, conversion, structural parsing, artifact extraction, reference linking, institutional inference, claim–evidence graph construction, scoring, and validation. The account is written in legal-academic prose and emphasizes methodological rationale rather than adjudicating specific case findings.</p>

<h3>Scope and Units</h3>
<p>The unit of analysis is the discrete technical object or reference as it appears in source materials, hereafter termed a technical object. Technical objects include identifiers such as CVE entries, domains, email addresses, and cryptographic hashes, as well as structural elements such as table rows, figure captions, and inline citations. The provided raw metadata indicates artifact_type_counts that include six CVE entries, five domain strings, one email address, and two MD5 hashes; these counts are used to illustrate how modality coverage affects downstream normalization and provenance reasoning without implying any substantive conclusion about network activity or actor attribution.</p>

<h3>Data Ingestion</h3>
<p>Data ingestion begins with archival retrieval of primary documents and associated artifacts. The ingestion stage enforces file integrity checks, timestamps for receipt, and an initial metadata capture that records the source identifier and any accompanying registry references. This stage is designed to preserve chain-of-custody markers and to annotate modality labels so that downstream processing is aware whether a given technical object originated in main text, a table, an image caption, or a footnote.</p>

<h3>PDF-to-Markdown Conversion</h3>
<p>Conversion of binary source documents to text seeks to maximize fidelity to the original layout because modality and positional cues are material to provenance assessments. The pipeline_methods show a primary conversion via a provider-backed Mistral OCR wrapper (process_pdf_mistral_ocr.py) with an offline fallback using PyMuPDF4LLM when provider conversion fails or times out. The rationale for a dual-path approach is to reduce loss of modality-specific cues (for example, table delimiters and figure anchors) and to provide an auditable conversion log that records which converter and which configuration produced the markdown output.</p>

<h3>Structural Parsing</h3>
<p>Structural parsing operates on the markdown output to recover hierarchical constructs such as headings, tables, figures, captions, and footnotes. The pipeline’s stage1 markdown parse deliberately emits tables and figures with stable anchors to enable later cross-referencing between extracted technical objects and their visual or tabular context. Maintaining these anchors is essential to provenance because the same string token may bear different evidentiary weight depending on whether it appears in the main narrative or within a table footnote.</p>

<h3>Artifact Extraction and Technical Object Normalization</h3>
<p>Artifact extraction is performed by a schema extraction stage that identifies candidate technical objects across modalities using pattern detection and context heuristics. Recognized types in the raw preview include CVE identifiers (e.g., CVE-2018-4878), domain names (for example, daum.net and fireeye.com), an email address (info@fireeye.com), and MD5 hash values. Normalization translates parsed tokens into canonical representations: CVE entries are normalized to the standard CVE registry form, domain names are lowercased and punycode-decoded where applicable, email addresses are canonicalized following mailbox parsing rules, and cryptographic hashes are validated for length and character class. Normalization also records provenance metadata for each normalized technical object, including the originating file, conversion path, markdown anchor, and byte-offset when available. This combination of normalization plus preservation of modality and positional metadata enables comparative analyses across documents while preserving the chain-of-custody context required for rigorous provenance assessment.</p>

<h3>Reference Parsing</h3>
<p>Reference parsing links in-text citations and footnote-like references to a source registry. The pipeline’s reference_parsing stage extracts citation anchors and attempts to reconcile them with recorded sources; where explicit bibliographic identifiers are absent, the parser emits candidate links with confidence scores. The parser records both the matched registry entry and the matching rationale (for example, exact string match versus fuzzy title similarity). These linkages are retained as part of the technical object’s provenance record to support later corroboration analyses.</p>

<h3>Institution Inference</h3>
<p>Institution inference augments explicit source metadata when institutional attributions are necessary for scoring or aggregation. The provided pipeline_methods indicate an infer_source_institutions.py step that leverages a language model (gpt-5-mini) with an optional web fallback. The methodology prescribes conservative heuristics: inferred institutions are accompanied by provenance notes that distinguish explicit self-identification from model-inferred associations and by confidence markers reflecting the inference source. This preserves an auditable trail and avoids conflating model-derived suggestions with primary-source claims.</p>

<h3>Claim–Evidence Graph</h3>
<p>Extracted and normalized technical objects are organized into a claim–evidence graph that represents propositions (claims) and their supporting artifacts. Each graph edge encodes modality, normalized object identifier, source anchor, and a provenance tuple documenting conversion path, parser version, and the inference confidence. The graph design supports queryable provenance chains, enabling assessments that trace a claim back to the precise textual or tabular location where a technical object appeared.</p>

<h3>Scoring Overview</h3>
<p>Scoring operates along multiple axes—custody, credibility, corroboration, clarity, and overall belief weighting—and uses normalized technical objects and their provenance records as inputs. Document-level example metrics in the provided document_scores_v4, such as custody_avg_0_100 and grounding_avg_0_100, illustrate how modality coverage and conversion fidelity can materially influence score components. Methodologically, the normalization and provenance metadata are used to weight contributions from artifacts that originate in high-fidelity modalities (for instance, well-structured tables with preserved anchors) more heavily than from low-fidelity OCR outputs with ambiguous anchors.</p>

<h3>Chain of Custody</h3>
<p>Chain-of-custody procedures embed immutable metadata at ingestion and retention points to record custody transitions. For each technical object the system records the ingest timestamp, the conversion path (including whether the Mistral OCR or the PyMuPDF4LLM fallback was used), and the structural anchor that tied the object to its original location. These records are cryptographically hashed and appended to an audit log to support later validation inquiries and to demonstrate provenance continuity.</p>

<h3>Credibility and Corroboration</h3>
<p>Credibility assessment separates source-level credibility from artifact-level corroboration. Credibility scoring incorporates explicit citations parsed by the reference_parsing stage, institutional provenance from the institution_inference stage, and contextual signals such as modality and extraction confidence. Corroboration measures compare normalized technical objects across sources and modalities; for example, exact canonical matches across independent source anchors</section>
<section class="wiki-section" id="sec-reference_parsing"><section><h2>Introduction</h2><p>This methodology chapter describes procedures for transforming rhetorically framed citations and footnote constructs in source documents into an analyzable source graph suitable for cyber‑attribution scoring. The exposition is methodological and abstains from asserting any case‑specific factual conclusions. It defines the purpose of reference parsing within a broader pipeline that ingests documents, extracts artifacts and claims, and produces structured inputs for downstream scoring and validation. The approach is designed to preserve evidentiary provenance, enable reproducible linkage between claims and supporting materials, and to provide clear audit trails for chain‑of‑custody and quality assessment.</p></section><section><h2>Data processing and extraction</h2><p>Ingestion begins with document acquisition and deterministic conversion to a machine‑readable representation. The available pipeline metadata indicates primary PDF conversion through a provider‑backed OCR pipeline (process_pdf_mistral_ocr.py) with an offline fallback (PyMuPDF4LLM) to guard against provider timeouts. Structural parsing produces pages, tables, figures and embedded artifacts; pipeline counts in the supplied preview show one source with a single page, multiple tables and figures, four extracted artifacts and six textually identified claims. These counts inform resource allocation and sampling strategies for subsequent manual review but are not interpreted as substantive evidence themselves.</p><p>Structured extraction proceeds in stages. The first stage converts visual and typographic elements to a markdown‑like intermediary that retains anchors for tables and figures. A subsequent schema extraction stage enumerates artifacts referenced by caption or inline mention and emits indices tying text spans to artifact identifiers. All conversions retain bounding metadata, original page offsets and the conversion method used to facilitate reproducibility and to support later chain‑of‑custody checks.</p></section><section><h2>References and institution inference</h2><p>Reference parsing converts rhetorical citation tokens—footnote markers, parenthetical citations, numbered endnotes and inline source mentions—into explicit links to registry entries in the source index. The pipeline component described as reference_parsing locates footnote markers in the intermediary representation, extracts adjacent citation text, and attempts canonicalization against the source registry. Canonicalization uses deterministic heuristics (title and year matching, entity name normalization) augmented by an institution_inference stage that applies a language model (infer_source_institutions.py with gpt‑5‑mini and optional web fallback) to resolve ambiguous attributions such as institutional authoring statements embedded in front matter or internal sections. The method records confidence scores, the mapping algorithm used, and any fallback evidence (for example, matches to known publication titles or institutional identifiers) to support later review.</p><p>These steps produce a directed bipartite graph between rhetorical citation nodes (the textual locations of footnote or inline citation) and source registry nodes. The graph explicitly records unresolved citation tokens as orphan nodes with provenance metadata so that missing or ambiguous references are visible to analysts rather than silently dropped. The resulting source graph therefore encodes citation linkage at the granularity of text span → source identifier → inferred institution, permitting queries for all claims that reference a given source or all sources cited by a particular claim.</p></section><section><h2>Scoring framework</h2><p>The scoring framework treats parsed references and inferred institutions as evidence attributes that feed credibility, corroboration and clarity axes. Each claim receives a structured evidence vector containing pointers to artifact indices, the set of linked sources from the source graph, and institutional inference metadata with confidence values. Scoring components operate on these vectors to produce intermediate scores—chain‑of‑custody completeness, citation linkage confidence and corroborative corroboration counts—which are then aggregated with calibrated weights to form composite attribution scores. Aggregation calibration is performed on held‑out examples and synthetic cases to ensure the system responds predictively to changes in citation density, the presence of primary versus secondary sources, and institution inference confidence.</p><p>Methodological rationale for separating citation linkage from institution inference is to avoid conflating surface textual citation patterns with the provenance of origin: a clear footnote may link to a document whose authorship is disputed; institution inference is therefore maintained as a distinct attribute rather than a derived certainty. Confidence propagation rules are explicit and conservative: low confidence in reference resolution limits the downstream weight of that evidence in composite scoring, and the system records the dependency path from a score back to the underlying reference nodes for auditability.</p></section><section><h2>Validation and quality assurance</h2><p>Validation is multi‑tiered. Automated checks verify that every parsed footnote has been assigned either a resolved source identifier or an explicit unresolved tag; statistics on unresolved citations are tracked as quality metrics. Synthetic and seeded ground‑truth documents exercise the reference_parsing and institution_inference components to measure precision and recall of citation linkage and correct institution assignment. Cross‑validation with manual review samples is used to calibrate confidence thresholds and to estimate human‑machine agreement rates. The pipeline records the conversion method (primary OCR or fallback) and uses that metadata to stratify error analyses, since OCR quality materially affects footnote and inline citation recognition.</p><p>Chain‑of‑custody documentation accompanies every output: conversion timestamps, tool identifiers, and reviewer annotations are retained in the provenance record for each claim and each source graph edge. Periodic audits reprocess a random sample of documents end‑to‑end to detect regressions in parsing behavior. Limitations of the approach, including dependence on OCR accuracy, model inference fallibility in institution assignment, and the reduced reliability of citation linkage in poorly formatted or scanned documents, are monitored and surfaced in metadata so that downstream consumers can apply appropriate caution in interpreting attribution scores.</p></section></section>
<section class="wiki-section" id="sec-institution_inference"><section><h2>Introduction</h2><p>This methodology chapter defines procedures for institution inference and source typology within a cyber-attribution scoring pipeline. It sets out the conceptual basis for mapping extracted source artifacts to institutional classes, and for operationalizing those classes into quantitative and qualitative effects on downstream credibility weighting and corroboration eligibility. The treatment below is methodological and refrains from applying attributions to any particular incident; it instead describes how institutional signals are detected, classified, and used in scoring while referencing the pipeline artefacts and processing modalities present in the supplied raw data.</p></section><section><h2>Scope and Units of Analysis</h2><p>The primary units of analysis are source objects as emitted by the ingestion and parsing stages of the pipeline. In the provided raw data this is illustrated by a preview record (source_id: SRC0001, title: "APT1 Executive Summary and Key Findings", source_type: "internal_document_section", entity_name: "Mandiant", year: 2013). The approach treats each source object as a bounded evidentiary unit from which institutional signals, reference links, and artifact indices are extracted. Source typology focuses on origin, provenance, and intended audience, and is operationalized at the granularity of source objects and their referenced artifacts rather than at the level of individual tokens.</p></section><section><h2>Data Ingestion</h2><p>Ingestion comprises the reception of primary documents and their registration in a source registry. As indicated in the pipeline metadata, source_type_counts currently reports a single internal_document_section; this count is used illustratively to show how typology calibration begins with available source classes. The registry preserves metadata fields such as title, entity_name, publication_or_venue, year, and source_type to support subsequent institution inference and chain-of-custody records.</p></section><section><h2>PDF-to-Markdown Conversion</h2><p>The pipeline’s primary conversion step is performed with a provider-backed OCR and conversion routine identified as process_pdf_mistral_ocr.py, with an offline fallback via PyMuPDF4LLM when provider conversion fails or times out. These routines produce machine-readable markdown representations of documents. Methodologically, conversion is assessed against fidelity metrics for textual accuracy and structural preservation; conversion artifacts (e.g., OCR confidence scores, timeout events) are recorded as inputs to credibility weighting.</p></section><section><h2>Structural Parsing</h2><p>Structural parsing decomposes converted markdown into hierarchical elements: sections, headings, paragraphs, tables, and figures. The pipeline’s stage1 markdown parse emits tables and figures/images with anchors, enabling linkage of in-text citations to visual artifacts. Structural metadata—such as section identifiers and anchor IDs—is stored alongside original source identifiers to maintain traceability.</p></section><section><h2>Artifact Extraction</h2><p>Artifact extraction isolates discrete evidentiary items (for example network indicators, configuration snippets, or labeled figures) and assigns them schema-record indices. The pipeline’s artifact_extraction stage emits artifact indices that reference both the structural location and the original source object. Each artifact index includes provenance metadata sufficient for later chain-of-custody reconstruction.</p></section><section><h2>Reference Parsing</h2><p>Reference parsing identifies in-text citations, footnotes, and bibliographic references and links them to the source registry. The reference_parsing stage attempts canonicalization and dereferencing where possible; unresolved references are flagged and their resolution status recorded. These linkage edges form the initial graph topology used to propagate institutional signals across the corpus.</p></section><section><h2>Institution Inference</h2><p>Institution inference assigns institutional attributes to source objects using a hybrid model described in the pipeline metadata: infer_source_institutions.py leveraging a purpose-built LLM (gpt-5-mini) with an optional web fallback for external validation. Institutional attributes include publisher identity, organizational type (e.g., vendor, academic, independent researcher, government), and known relationships (e.g., affiliation with a parent organization). The module consumes structured metadata, textual cues from the parsed content, and, when available, external resolving evidence. All inferences are probabilistic and tagged with confidence scores and provenance rationales to support downstream interpretability.</p></section><section><h2>Claim–Evidence Graph Construction</h2><p>Following institution inference, the system constructs a claim–evidence graph that links extracted claims and artifacts to source nodes and inferred institutions. Graph edges encode relations such as cites, asserts, and corroborates, and carry provenance metadata (source id, section id, artifact id, and parsing confidence). Institutional nodes are annotated with the inference confidence from the previous stage and with metadata taken from raw ingestion records, enabling both structural and institutional reasoning over claims.</p></section><section><h2>Scoring Overview</h2><p>The scoring framework integrates institutional attributes into a multi-axis credibility model. Institutional class is one axis among others (technical provenance, artifact fidelity, and corroboration count). Scores are computed by combining institution-derived priors with artifact-level quality metrics and graph-based corroboration signals. The framework treats institutional signals as soft priors rather than determinative labels, ensuring that empirical artifact quality can override institutional expectations where warranted.</p></section><section><h2>Chain of Custody</h2><p>Chain-of-custody records capture each transformation from original document ingestion through conversion, parsing, and artifact extraction. Each transformation step logs tool identifiers and versions as present in the runtime_libraries metadata (for example, python_version and key packages such as pypdf, pymupdf, and pymupdf4llm), along with timestamps and operator or automated-process identifiers. These records support reproducibility and enable targeted re-evaluation of institutional inferences when upstream transformations are questioned.</p></section><section><h2>Credibility and Corroboration</h2><p>Credibility weighting uses institution inference to modulate both individual source credibility and corroboration eligibility. Institutional class affects the prior assigned to a source: institutional priors adjust the weight of claims in the aggregation step and determine whether a source can serve as an independent corroborator. Corroboration eligibility rules treat sources within the same institutional family as non-independent unless explicit evidence of independent observation is present. All such eligibility decisions are annotated with the inference confidence from the institution_inference stage.</p></section><section><h2>Clarity Axis</h2><p>A clarity axis assesses how explicitly an institutional claim is articulated in the source text (for example explicit attribution statements, byline metadata, or corporate branding). Clarity scores influence both credibility weighting and the stringency of corroboration rules; higher clarity reduces reliance on external inference while lower clarity raises the evidentiary bar for corroboration.</p></section><section><h2>Aggregation and Calibration</h2><p>Aggregation synthesizes weighted claims across the claim–evidence graph. Calibration procedures adjust institutional priors and combination rules using holdout datasets and simulated perturbations in source typologies. Calibration emphasizes robustness to misclassification of institutional type by favoring artifact-level signals when institutional inference confidence is low.</p></section><section><h2>Validation and Quality Assurance</h2><p>Validation includes unit checks of parsing fidelity, reconciliation of reference links, and spot audits of institution inference rationales. Quality-assurance procedures incorporate reproducibility tests using the recorded runtime_libraries and conversion logs, and measure score stability under alternate inference thresholds. All validation outcomes are logged and considered when updating scoring parameters.</p></section><section><h2>Limitations and Governance</h2><p>The methodology recognizes limits inherent to automated institution inference, including dependence on conversion fidelity and the potential for biased training signals in language models. Governance measures require human-in-the-loop review for high-impact decisions and mandate explicit documentation of inference confidence, chain-of-custody records, and any manual overrides. The procedure therefore combines algorithmic institution inference with formal audit trails and governance safeguards</section>
<section class="wiki-section" id="sec-claim_evidence_graph"><h2>Introduction</h2>
<p>This methodology section defines the construction and governance of a claim-evidence graph used for cyber-attribution scoring. The claim-evidence graph is a directed, typed graph that connects authored claims to discrete evidence items, source descriptors, and technical artifacts with explicit anchor-level pointers. The primary objectives of the graph are to enable machine-actionable traceability from claim to evidence, to support reproducible chain-of-custody assessment, and to enforce anti-circularity constraints so that evidence derived from the evaluated report is not treated as independent corroboration. The description below is procedural and methodological; it uses metadata samples contained in the supplied raw data (for example, identifiers such as claim_id values C001–C006, source records such as SRC0001, and artifact records such as ART00001–ART00014) to illustrate structure and controls rather than to restate substantive findings.</p>

<h2>Data processing and extraction</h2>
<p>Input processing proceeds in predictable stages. First, documents and auxiliary data are ingested into a normalized repository that preserves both the original file and an extracted text representation. The supplied bundle records this normalization step with entries under normalized.artifacts and normalized.sources; each artifact record (for example ART00001) carries an artifact_type, value, origin, location tuple (page, block_id) and an extraction confidence score. The extraction pipeline separates tasks conceptually: conversion of binary documents to a text/markdown form; structural parsing into logical blocks (paragraphs, lists, tables); and extraction of technical artifacts (CVE identifiers, domain names, email addresses, cryptographic hashes). The raw_artifacts_preview included with the input (artifact types and example_values such as "CVE-2018-4878" and "daum.net") is used to calibrate entity-extraction models and to validate that the extractor recognizes the relevant modal classes for subsequent linking.</p>
<p>Structural parsing yields discrete anchors: stable, addressable block identifiers that become nodes in the claim-evidence graph. For every artifact and for every claim anchor we persist the original location metadata (page, block_id) and the block text snippet. The normalized.artifacts entries in the provided scoring_bundle show the representation used in practice: artifact_id, artifact_type, value, location and extraction confidence. Anchor-level persistence supports fine-grained traceability and enables later human review to re-evaluate extraction correctness without re-processing the entire document.</p>

<h2>References and institution inference</h2>
<p>Reference parsing and source normalization are treated as first-class processes. Cited items discovered in text are canonicalized into source records and cross-referenced to publisher and author metadata where available. The provided raw_sources_preview contains a representative source entry (SRC0001: Mandiant APT1 Report) which illustrates the normalized source record schema: source_id, source_type, authoring_org, publisher, and publication date. Where explicit bibliographic metadata are absent, reference-disambiguation relies on URL and domain normalization; domain strings present in artifact lists (for example, "fireeye.com") are used as candidate organizational signals in institution inference.</p>
<p>Institution inference applies a conservative, multi-evidence heuristic. Domain ownership, publisher metadata, and in-document claims about provenance are combined to produce an inferred institutional attribution with an associated confidence band. The system records both the inferred institution and the provenance of that inference (for example which artifact_ids and which source records supported the inference). To minimize circularity, any institution inference that relies solely on artifacts extracted from the evaluated report is flagged and treated as report-derived. Those flagged inferences are not permitted to contribute to cross-source corroboration metrics unless validated by an independent source record or external registry lookup.</p>

<h2>Claim-evidence graph construction and traceability</h2>
<p>The claim-evidence graph is constructed by creating a claim node for each parsed claim (claim_id entries such as C001–C006) and linking claim nodes to evidence nodes via explicit relation edges. Evidence nodes correspond to normalized evidence_items (for example E-0001..E-0008 in the normalized bundle) and to atomic artifact nodes that represent extracted technical indicators (ART00001..ART00014). Each evidence node includes modality metadata (for example "cve", "infrastructure") and feature vectors used for probative weighting (the input contains features labeled I, A, M, P, T in the evidence_items records). Edges are typed to indicate the role of the evidence relative to the claim (for example supports, contextualizes, links_actor). All edges are annotated with anchor pointers (page, block_id) so that every claim→evidence relationship can be traced back to the original block in the source document; this anchor-level traceability permits deterministic human verification and preserves evidentiary context.</p>
<p>To preserve provenance fidelity the system stores an origin_id for every evidence node (for example ORIG:src0001 in the evidence_items) and maintains a mapping from evidence nodes to their source_ids and artifact_ids. During graph construction the pipeline enforces an invariant that every edge must reference at least one anchor and at least one origin. This invariant is foundational for chain-of-custody scoring because it prevents unanchored syntheses from being used as primary evidence and thereby preserves an auditable trail from claim to raw textual anchor.</p>

<h2>Anti-circularity safeguards</h2>
<p>Anti-circularity mechanisms are explicit and multi-layered. First, origin clustering is used to detect single-origin support: the scoring bundle contains origin_cluster_weights and unique_origin_count fields that the pipeline uses to mark claims with limited source independence. Second, the system distinguishes report-derived evidence from independently obtained artifacts by tracking recovered_reference_count and a report_derived_ratio; any evidence node with a report-derived flag is excluded from multi-source corroboration tallies unless an external reconciliation step validates it. Third, provenance validation uses</section>
<section class="wiki-section" id="sec-scoring_overview"><h2>Introduction</h2><p>This methodology section describes the scoring architecture used to translate extraction outputs into reproducible attribution scores. The objective is to articulate a transparent, auditable pipeline that moves from claim-level evidence anchors to document-level synthesis, while maintaining an explicit separation between raw extraction outputs and the inferential weighting that yields final scores. The exposition follows the pipeline manifests present in the supplied data (for example, the scoring_bundle and document_scores_v4 structures) and explains the rationale for each processing stage in general terms rather than restating substantive findings from the source document.</p><h2>Data processing and extraction</h2><p>Primary ingestion begins with a provider-backed PDF conversion to markdown, with an offline fallback, as recorded in the pipeline_methods. Structural parsing produces discrete text blocks, anchors, tables and figure references. From these anchors a schema-driven artifact extraction stage emits typed artifacts (artifact_type enumerations such as vulnerability identifiers, cryptographic hashes, domains and contact addresses) and attaches provenance metadata including extraction confidence and block anchors. Evidence items are then assembled by grouping anchors and artifacts under evidence identifiers; each evidence item records modality tags (for example, technical artifact, dataset or infrastructure), feature vectors (the dataset labels I, A, M, P, T are present in the evidence feature maps) and a computed probative_weight. Those extraction outputs (anchors, artifacts, evidence items and their raw feature scores) are treated as observable inputs to scoring and are retained without inferential interpretation in this stage so that downstream weighting can be independently examined and audited.</p><h2>References and institution inference</h2><p>Reference parsing and source registry linkage are performed after artifact extraction. The pipeline captures a normalized source registry entry for each declared source, including source_kind and authoring organization; the supplied pipeline_methods indicate an institution inference step that uses a model-backed heuristic process. Institution inference operates on the parsed citation metadata and the source registry to produce inferred authoring_org entries and to populate flags relevant to source characterization (for example, indicators of litigation preparation, stated conflicts, or whether a source is single). The distinction between extracted reference tokens and inferred institutional attributes is critical: parsed citations and anchors are deterministic outputs from the structural parser, whereas inferred institutional attributes are model-derived annotations used only in credibility- and corroboration-related axes of scoring and are recorded separately so they may be re-evaluated or replaced without re-running extraction.</p><h2>Scoring framework: claim-level axes to document-level synthesis</h2><p>Scoring is organized as claim-level vectors and a document-level aggregation. At the claim-level each claim is associated with a set of evidence items (evidence_ids) and per-evidence probative measures. The claim-level axes represented in the data include grounding (the extent and coverage of anchors and markers supporting the claim), custody (chain-of-custody and artifact identifier quality), credibility (source quality and independence), corroboration (multi-source and modality convergence), confidence (stated confidence and calibration), and clarity (granularity and specificity of actor–act–link relationships). Numerically, these axes appear as per-claim scores (for example grounding_0_100, custody_0_100, clarity_0_100) and as component vectors (the six-c or similar vectors) that feed an evidence_weight or evidence_support term.</p><p>Inferential weighting is explicitly separated from extraction outputs. The pipeline treats extracted indicators (anchors, artifact types and evidence probative_weight) as inputs to a weighting model that implements shrinkage and calibration. The statistical_calibration_v4 block in the supplied data documents mechanisms for reliability adjustment (reliability_factor), effective sample sizing (effective_evidence_n), and shrinkage lambdas applied to axes (shrinkage_lambda for custody, credibility, corroboration, clarity, confidence). Penalty multipliers (for example a single_source penalty factor) are applied to claim-level aggregates to encode known structural risks; these appear as penalty_multiplier and penalties in the claim scoring entries. Claim-level final_score values therefore reflect the composition of evidence weight aggregates, applied penalties, and calibrated shrinkage rather than any additional re-interpretation of extraction artifacts.</p><p>Document-level synthesis aggregates claim-level vectors into summary statistics that are designed to preserve the multi-axis structure. Aggregation methods present in the data include mean and geometric means across claim scores (for example overall_claim_score_mean and overall_claim_score_geometric) and a headline claim selection that propagates a headline_vector to the document-level. Bootstrap-derived uncertainty estimates (bootstrap_95ci) are computed for selected aggregates to communicate sampling and model uncertainty at the document-level. Where gating rules are required (for example seriousness or minimum credibility thresholds) those conditions are evaluated against the aggregated vectors and are recorded in gate structures so that decisions are auditable and reproducible.</p><h2>Validation and quality assurance</h2><p>Quality assurance is performed at multiple points. Extraction QA validates anchor coverage, artifact traceability and anchor-to-claim alignment; chain_provenance_diagnostics fields record context_completeness, integrity_signal, lineage_quality and anchor_quality metrics that support automated checks. Scoring QA validates internal consistency by checking that evidence_weight_aggregate equals the sum of constituent probative</section>
<section class="wiki-section" id="sec-chain_of_custody"><h2>Introduction</h2>
<p>This chapter defines the methodological approach used to evaluate chain-of-custody aspects of evidence that underlie cyber‑attribution scoring. The presentation is disciplinary and procedural rather than substantive: it explains how provenance, integrity, time anchors, artifact identifiers and versioning are operationalised, how those signals are combined into quantitative custody measures, and how quality controls and penalties are applied when signals are incomplete. Where appropriate, the method refers to diagnostic fields and aggregated metrics drawn from the supplied scoring bundle and document-level summaries to illustrate behaviour of the method, not to adjudicate particular factual claims.</p>

<h2>Data processing and extraction</h2>
<p>The pipeline begins with structured ingestion of the primary source objects and derived artifacts. Document‑level metadata and the normalized artifact table serve as primary inputs: artifact rows record artifact_type, value, extraction origin and extraction confidence; evidence items connect anchors to claims and enumerate modalities (for example, "cve" or "infrastructure"). Structural parsing produces anchor identifiers and block‑level locations that are preserved as the primary linkage between textual claim statements and supporting artifacts. Extraction confidence and explicit artifact identifiers (for example, hash values and CVE strings) are recorded and propagated through the pipeline so that downstream custody calculations can weight direct, verifiable artifacts more heavily than contextually inferred entities.</p>
<p>To protect the chain of custody we differentiate three extraction classes. Direct artifacts are those with explicit identifiers and high extraction confidence (artifact entries with confidence=1 and types such as hash_md5 or cve); contextual artifacts are textually proximate mentions that lack a stable identifier; and remote or derived artifacts are external references that require follow‑up retrieval to be validated. The pipeline tags each artifact with provenance metadata indicating the source record (source_id), the extraction method (extracted_from), and the anchor block(s) that justify claim linkage. The scoring inputs include counts and previews of artifacts (for example, counts by artifact_type and example values) to allow automated flagging of weakly identified chains (for example, absence of artifact identifiers or absence of hash values where they would be expected).</p>

<h2>Reference parsing and institution inference</h2>
<p>Reference parsing separates internal report anchors from external citations. The normalised source table records authoring_org and</section>
<section class="wiki-section" id="sec-credibility_corroboration"><section>
  <h3>Introduction</h3>
  <p>This methodology describes the Credibility axis with a dedicated Corroboration subcomponent as applied to cyber-attribution scoring. The approach prioritizes transparency of inputs and reproducibility of transformations while avoiding substantive adjudication of contested factual claims. The protocol distinguishes between structural processing steps and inference rules: the former converts raw artifacts and anchors into machine-readable evidence items, and the latter evaluates provenance, source quality and inter-source relationships. The following exposition uses the provided input metadata to illustrate procedural behavior (for example, a single documented source with type classification "internal_document_section" and an authoring organization listed as Mandiant), but it intentionally refrains from drawing operational conclusions about specific incidents referenced in the underlying report.</p>

  <h3>Data processing and extraction</h3>
  <p>Source ingestion begins with normalized metadata and a source type inventory. In the supplied input the source_type_counts map indicates one unit classified as internal_document_section; this triggers an ingestion pathway optimized for publisher-supplied technical reports. Documents are parsed into structural anchors (pages and blocks) and artifacts are extracted with modality labels (for example, CVE identifiers, hashes, domains and email addresses). Each artifact record carries provenance flags and an extraction confidence score; the scoring pipeline uses these values to weight downstream evidence. Evidence items are formed by clustering related anchors and artifacts, assigning modality vectors and computing a probative weight that combines artifact-level confidence with contextual indicators (anchor coverage, marker strength and modality relevance). This pipeline deliberately separates extraction confidence from interpretive credibility so that noisy or machine-extracted artifacts do not automatically inflate attribution measures.</p>

  <h3>References and institution inference</h3>
  <p>Reference parsing recognizes explicit citations, author lists and publisher fields and synthesizes an institutional identity when available. The normalized source record in the input includes an authoring_org field ("Mandiant") which permits an institutional label to be associated with that origin. Institution inference leverages explicit publisher metadata first, then secondary signals such as domain names and email addresses contained among extracted artifacts. The inferred institution, its declared role (publisher, vendor, academic etc.) and any declared conflicts of interest are recorded in a source record. These records form the basis of a source hierarchy: primary-origin sources (first-party technical reports and datasets) are placed above secondary-derivative items (aggregated summaries, citations) in the hierarchy used for credibility scoring. The source hierarchy is therefore explicit, auditable and used to compute scope factors such as claim coverage and the effective independence of corroborating chains.</p>

  <h3>Scoring framework: credibility, corroboration and claim coverage</h3>
  <p>The Credibility axis is computed from three components: chain-of-custody and provenance fidelity, source-quality and independence, and corroboration-convergence across distinct origins. Chain-of-custody metrics assess artifact identifiers, time anchors and lineage disclosure to produce a custody score. Source-quality and independence combine declared publisher type, domain diversity and the presence of independent corroborating origins into a credibility_independence measure. Independence is operationalized as the lack of shared origin identifiers across supporting sources, penalizing claims that derive from a single origin even if that origin contains multiple anchors.</p>
  <p>Corroboration is assessed by counting and weighting supporting sources and modalities, then applying diminishing returns via a saturation function. The corroboration metric rewards multi-origin, cross-modality support and explicit cross-language or cross-domain corroboration. Claim coverage scaling maps the fraction of a claim's anchors that are supported by eligible sources into a multiplicative coverage factor: claims with full anchor coverage receive full weight while partially covered claims are down-weighted proportionally. The supplied document scores illustrate the framework's behavior when only one origin is present: credibility and corroboration summary values may be driven to near-zero when eligible independent sources and explicit citations are absent, while grounding and custody measures remain calculable from artifact anchors and identifiers.</p>

  <h3>Exclusion criteria for low-value source classes</h3>
  <p>To preserve analytic integrity the pipeline applies explicit exclusion criteria that remove or de-prioritize low-value source classes. Examples of exclusion criteria include anonymous fora with unverifiable archives, social-media posts without persistent anchors, third-party summaries lacking original artefactual linkage, and automated feeds with insufficient provenance metadata. Excluded items are recorded and reported; they do not contribute to eligible_source_count used in credibility or corroboration computations but remain available for human review. The exclusion criteria are conservative: a source is excluded only when its provenance cannot be reconstructed to a minimum threshold required for independent corroboration or when its content lacks persistent anchors that can be traced to artifact identifiers.</p>

  <h3>Validation and quality assurance</h3>
  <p>Validation proceeds at two levels. First, technical validation verifies that extraction outputs are internally consistent: anchor coverage, artifact-to-evidence alignment and non-duplication heuristics are checked and reported in chain_provenance_diagnostics. Second, scoring validation uses a calibration process that applies prior distributions and shrinkage (reliability_factor and shrinkage_lambda parameters) to avoid overconfidence when evidence counts are small. Bootstrap intervals and effective evidence counts are computed to quantify uncertainty. Quality assurance also includes manual review gates for cases where exclusion criteria or single-source penalties materially alter claim-level inferences. All pipeline decisions, from source hierarchy placements to exclusion actions, are recorded to permit audit and independent replication.</p>
</section></section>
<section class="wiki-section" id="sec-clarity_axis"><h2>Introduction</h2>
<p>This methodology chapter defines a transparent approach to scoring the clarity of attribution narratives that implicate state actors and to distinguishing between three legal responsibility pathways: conduct by state organs, responsibility for non-state actors operating under state control, and state responsibility based on failure to exercise due diligence. The approach is designed to be instrumented against structured outputs from an automated extraction pipeline and to be auditable against underlying provenance metadata. To illustrate method behavior, the supplied scoring snapshot reports an overall clarity mean (clarity_avg_0_100) of 37.11 with a bootstrap 95% confidence interval of approximately 31.96–42.67; such summary statistics are used in the methodology discussion only to demonstrate calibration and uncertainty handling, not to substantively characterize particular allegations.</p>

<h2>Data processing and extraction</h2>
<p>The pipeline begins with document ingestion and automated structural parsing. Source metadata and document-level inputs (for example a reported claims_count of 6) are normalized into a canonical scoring bundle. Text-to-structure conversion yields identified artifacts (examples in the provided bundle include CVE identifiers, cryptographic hashes and domain strings) and evidence items that aggregate anchors and modalities. Each artifact and evidence item carries extraction confidence and feature vectors used downstream: the input dataset includes artifacts such as ART00001–ART00014 with extraction confidences at or near 1.0 and evidence items E-0001 through E-0008 that record modality, anchors, and a computed probative_weight. Chain-of-custody signals are derived from provenance fields in the chain_provenance_diagnostics (for example context_completeness, integrity_signal, lineage_quality and anchor_quality), and these signals feed the custody score component. The methodology explicitly separates syntactic extraction (artifact recognition and anchor mapping) from semantic linking (assignment of evidence to claims) so that errors in OCR, parsing or anchor placement are isolated and traceable through artifact_proximity_tiers and anchor identifiers.</p>

<h2>Reference parsing and institutional inference</h2>
<p>Reference parsing converts document citations and source metadata into structured source objects (for example normalized.sources entries such as SRC0001 with authoring_org fields). The scoring inputs in the provided bundle show a single indexed source and zero recovered citations; operationally this triggers a single-source penalty at the claim level. Institution inference is conservative: it uses explicit source metadata (authoring_org, publisher, domain strings) and corroborating infrastructure artifacts (domains and email addresses extracted from the text) to infer institutional association candidates. The method records a binary state-actor signal and a state_claim_flag when language or metadata in the claim explicitly asserts a state nexus; these flags are operational signals for the clarity axis rather than standalone determinations of state responsibility. Because institution inference relies on metadata rather than substantive narrative assertions, the system applies graded heuristics (for example probabilistic weighting when artifact domains are tied to particular institutions) and records the provenance of each inference to permit human review and contestation.</p>

<h2>Scoring framework and responsibility-pathway decomposition</h2>
<p>The scoring model partitions evidentiary quality into orthogonal components: grounding (evidence-to-claim anchoring), custody (provenance and integrity), credibility (source quality and independence), corroboration (multi-source and modality convergence), confidence (expression and calibration of uncertainty), and clarity (the specificity of acts, actors and links). The clarity axis is decomposed further into act_specificity, actor_specificity and link_specificity and into legal-path subcomponents that map to responsibility pathways: organ_path_clarity, control_path_clarity and due_diligence_path_clarity. Each claim receives per-component scores in the unit interval and an explanatory questions object that reports binary or graded answers to readability questions such as whether attribution to a named state is clear and whether a responsibility mode is indicated. For example, the score_details structure supplies numeric values for organ_path_cl</section>
<section class="wiki-section" id="sec-aggregation_calibration"><h2>Introduction</h2>
<p>This methodology chapter articulates the procedures for claim-to-document aggregation, weighting, calibration, and the treatment of uncertainty and dispersion in a cyber‑attribution scoring pipeline. The discussion is procedural and methodological: it draws on numerical artifacts provided in the raw scoring bundle to illustrate how aggregation and calibration operate, but it deliberately refrains from reporting or interpreting substantive case findings. The purpose is to make explicit the logic by which discrete evidence features and provenance signals are transformed into composite claim and document scores and how those aggregates are adjusted and reported with measures of statistical uncertainty.</p>

<h2>Data processing and extraction</h2>
<p>Input processing begins with ingestion of primary documents and technical artefacts that have been extracted from the native source. Structural parsing yields artifact records (for example, CVE identifiers, hashes, domains and email addresses) each annotated with an extraction confidence value. Anchors are preserved as pointers to the original structural blocks. The pipeline represents these items in a normalized evidence model in which evidence items carry modality and feature vectors (for instance I, A, M, P, T components) and a derived probative weight. In the supplied bundle, artifacts are recorded under the normalized artifact list and evidence items reference anchors and modalities; the evidence probative weights are then summed per claim to produce an evidence_weight_aggregate value used downstream. Chain tracing metadata such as provenance, integrity signals and anchor quality are retained to permit downstream custody diagnostics and to support saturation calculations when multiple anchors or artifacts are present for the same claim.</p>

<h2>References and institution inference</h2>
<p>Reference parsing distinguishes internal anchors (textual locations inside a single report) from external citations and unique origins. The pipeline records explicit citations and computes a sources_total and citations_total for each document. Where external authoring organizations are declared, the system captures authoring_org and origin_signature fields to support institutional inference. Institutional inference is operationalised to produce domain and origin clusters that inform credibility and corroboration axes without supplanting technical evidence: domain independence and eligible_source_count metrics are derived from the normalized sources table, and single_source conditions are detected (for example via a single source count value) and trigger predefined penalty rules that affect final claim-level aggregation.</p>

<h2>Scoring framework: aggregation, weighting and calibration</h2>
<p>Claim scoring proceeds by aggregating probative weights from constituent evidence items into a claim-level evidence weight (illustrated by the evidence_weight_aggregate field). Aggregation is not a simple sum: each evidence item contributes a feature-weighted score that is adjusted for modality, anchor proximity, and documented provenance quality. The pipeline computes core vectors (for example custody, credibility, corroboration) for each claim using sub‑components such as chain_of_custody_provenance, credibility_independence and corroboration_convergence. When originating sources are not independent or when a single origin supplies multiple items, an explicit single_source penalty multiplier (illustratively 0.85 in the provided bundle) is applied to reduce over‑confidence arising from non‑independent support. The aggregated output is then transformed into a normalized final_score for the claim after penalties and data contribution multipliers are applied.</p>
<p>Statistical calibration is layered onto the aggregated scores to control for small‑N variability and overfitting. The pipeline computes an effective_evidence_n that represents the information content of the aggregated chain (for example, an effective_evidence_n of 4.0 in the sample), and derives a reliability_factor that moderates shrinkage toward prior expectations. Shrinkage is implemented via lambda parameters per axis (for example custody, credibility, corroboration, clarity and confidence), which weight the observed score against a prior_scores vector. This empirical Bayes style shrinkage reduces variance when evidence is scarce or heterogeneous while preserving signal where evidence is substantive. Saturation factors are also tracked to ensure that additional items from the same provenance cluster produce diminishing marginal increases to the corroboration and custody axes rather than linear growth.</p>
<p>Measures of uncertainty and dispersion are produced at both the claim and document level. Bootstrap resampling is used to derive 95% confidence intervals for key document aggregates such as belief_weighted_0_100 and axis averages (as encoded in bootstrap_95ci in the supplied bundle). Those intervals quantify sampling and model uncertainty under the pipeline's resampling assumptions; dispersion diagnostics (for example variance of axis scores across claims and the spread of bootstrap replicates) inform interpretation thresholds and the construction of conservative reporting gates. For instance, when a document-level belief metric exhibits a narrow bootstrap interval around a low mean, the reporting logic treats the low belief as robust to sampling variability; conversely, wide intervals trigger explicit uncertainty language in the downstream product and, where relevant, additional calibration (increased shrinkage) to avoid over‑confident presentation.</p>

<h2>Validation and quality assurance</h2>
<p>Quality assurance includes procedural validation of extraction confidences and systematic audits of provenance. Chain_provenance_diagnostics fields such as context_completeness, lineage_quality and anchor_quality are used to flag chains that require human review; artifacts with low extraction confidence are excluded or down‑weighted in automated aggregation. Scoring calibration is validated by back‑testing shrinkage parameters and reliability_factors across a labeled corpus; bootstrap_95ci results are compared to empirical dispersion observed in hold</section>
<section class="wiki-section" id="sec-validation_quality_assurance"><section><h2>Introduction</h2><p>This methodology chapter describes the validation and quality assurance approach applied to the automated and semi-automated components of the cyber-attribution scoring pipeline. The description is deliberately procedural and normative: it explains the purpose and mechanics of the validation architecture, the rationale for sampling and human intervention, and the interface between automated validation gates and targeted review. Where appropriate, provenance to raw pipeline artifacts and validation outputs is indicated to demonstrate traceability of the procedural assertions.</p></section><section><h2>Data Processing and Extraction</h2><p>Data entering the validation regime originates from structured extraction artifacts produced by the pipeline; representative inputs are recorded in the pipeline output and report artifacts (for example, the extraction bundle at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.output.json and the consolidated report at /home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json). The validation process consumes these JSON artifacts and executes a set of deterministic checks that evaluate schema conformance, table integrity, citation presence, artifact enumeration, and grounding consistency. The rationale for these checks is to ensure structural and referential soundness prior to any substantive scoring or downstream synthesis. The validation artifact at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.validation_report.json documents the outcomes of those automated checks and provides the primary input to the QA protocol.</p></section><section><h2>References and Institution Inference</h2><p>Reference parsing and institutional inference are treated as distinct but interoperable tasks. Reference parsing extracts anchors and citation metadata from the structural parse and the artifact extraction stage, while institution inference applies rule-based and probabilistic heuristics to map reference metadata to institutional identifiers. Methodologically, the pipeline separates entity normalization from attribution scoring to avoid circular dependencies: institution inference outputs are stored alongside origin artifacts and are subject to the same validation gates as other components. Traceability is preserved by recording the provenance of reference parsing and institution inferences within the extraction bundle and by linking those outputs back to the validation report referenced above.</p></section><section><h2>Scoring Framework</h2><p>The scoring framework is agnostic to source content and operates over validated structural and evidentiary primitives. Scores are computed from component-level indicators such as schema integrity, corroboration diversity, and artifact completeness. The validation outputs inform confidence weighting within the scoring model: for example, higher scores on schema and integrity checks reduce uncertainty margins, while warnings regarding corroboration diversity or missing non-duplicative anchors trigger conservative adjustments. This separation ensures that scoring adjustments respond to measurable quality attributes rather than to substantive case conclusions. The scoring outputs are persisted with provenance pointers to the validation and extraction JSON files (for instance, the score reports in /home/pantera/projects/TEIA/annotarium/outputs/scoring/), enabling auditors to reconcile numerical adjustments against the underlying validation evidence.</p></section><section><h2>Validation and Quality Assurance</h2><p>The QA architecture comprises layered automated validation gates followed by a hybrid review regimen that combines continuous agent review with targeted human sampling. Automated validation gates execute deterministic checks on incoming extraction artifacts; these checks include schema validation, integrity verification, table and artifact counts, presence of citations, and a set of heuristics that identify potential corroboration weakness or duplicated anchor text. The outputs of those gates are materialized in a machine-readable validation report (path: /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.validation_report.json) and carry an overall certification indicator and category-level scores used to triage subsequent review activity.</p><p>Operational QA proceeds with enabled agent review for continuous monitoring and deterministic remediation of trivial failures; agent review operates as the first human-adjacent tier and flags artifacts for escalation when automated heuristics cannot resolve an anomaly. In parallel, a targeted human review protocol samples a defined fraction of processed items to provide an empirical estimate of residual error rates. The configured sampling fraction for targeted human review is 10% (reported as human_sample_fraction = 0.1 in the QA metadata). Results from the sampled reviews are recorded; in the sampled set for this run the observed error rate was zero (human_sample_observed_error_rate = 0.0), which is reported as no observed errors in the reviewed sample. That sampling outcome is reported alongside the automated certification (the validation bundle indicates certification = "PASS" and an overall_score = 92.09) and category-level diagnostics that can include warnings about repeated anchor duplication or limited corroboration diversity.</p><p>Methodologically, this hybrid approach balances scale and rigor: validation gates provide reproducible, high-throughput assurance that prevents malformed inputs from propagating, while agent review and the 10% human review sample provide an empirical check on algorithmic assumptions and capture error modes that automated rules may miss. The QA protocol records provenance to the raw payloads and score artifacts (see raw_payload_paths and raw_payload_files entries) so that any subsequent re-audit can re-run validation gates and replicate the targeted human sampling. Together, these elements form a defensible chain of custody and quality assurance strategy designed to support reproducible, transparent cyber-attribution scoring without conflating methodological controls with substantive analytic judgments.</p></section></section>
<section class="wiki-section" id="sec-limitations_governance"><h2>Introduction</h2>
<p>This section articulates the methodological limitations, governance controls, and a principled refinement roadmap for a reproducible cyber‑attribution scoring workflow. The objective is to describe, at a methodological level, how instrument and process fragilities are identified, controlled, and iteratively improved without asserting or adjudicating any substantive factual claims from the underlying dossier. The discussion situates the limits of automated and schema‑driven processing in relation to evidentiary legibility, explains governance measures adopted to preserve auditability, and proposes a staged refinement roadmap that balances determinacy, human oversight, and measured experimentation. Methodological emphasis is placed on transparency of assumptions, explicit delimitation of scope, and mechanisms for recording decisions so that subsequent reviewers can reconstruct how particular outputs were obtained from inputs.</p>

<h2>Data Processing and Extraction</h2>
<p>Data processing begins by converting source documents into a consistent internal representation that preserves original textual anchors and metadata. Extraction prioritizes explicit signals such as named entities, timestamps, hashes, and quoted assertions while recording extraction provenance at the field level. Where automated extraction produces low‑confidence outputs, those items are flagged for targeted human review rather than being promoted into downstream scoring without adjudication. Normalization routines are applied to harmonize formats and reduce spurious variability, and all transformation steps are logged to enable reproducibility. The rationale for these controls is to limit propagation of extraction errors into inferential stages and to maintain an auditable chain from raw input to derived artifacts.</p>

<h2>References and Institution Inference</h2>
<p>Inference of institutional associations from references and contextual mentions is treated as a probabilistic, model‑assisted exercise constrained by external registries and verifiable metadata. The system records which external sources were consulted and the confidence assigned to any inferred linkage, while explicitly excluding inferences that rely solely on contextual implication without corroborating anchors. Institutional inference workflows incorporate mechanisms to surface ambiguous or contradictory signals to human analysts and to prevent automated aggregation from amplifying tenuous linkages. This conservativism is intended to reduce false positive associations and to ensure that institutional attributions remain provisional until validated through adjudicative review.</p>

<h2>Scoring Framework</h2>
<p>The scoring framework is claim‑centric and provenance‑aware: each claim carries a set of evidentiary anchors, a gravity weight that encodes its potential significance, and a confidence metric derived from extraction quality and source credibility. Aggregation across claims applies explicit propagation rules for uncertainty and documents the mathematical form of aggregation so that score derivation is transparent. Thresholds used to trigger particular handling paths, such as human escalation or public disclosure, are configurable and justified by sensitivity analyses. All score components are accompanied by machine‑readable rationales that permit downstream audits and enable alternative aggregation strategies to be applied retroactively.</p>

<h2>Validation and Quality Assurance</h2>
<p>Validation combines systematic testing with ongoing quality assurance practices. Test suites exercise extraction and scoring pipelines using seeded inputs and synthetic cases designed to probe known failure modes and edge conditions. Continuous monitoring tracks metrics indicative of extraction drift, score stability, and source quality, and periodic independent audits verify that logs and provenance records suffice to reproduce analytic outcomes. Governance controls include documented escalation paths for disputed inferences, scheduled bias and robustness assessments, and a staged rollout plan for changes that could materially affect attribution outcomes. Together these measures aim to sustain confidence in methodological soundness while enabling iterative refinement.</p></section>
</article>