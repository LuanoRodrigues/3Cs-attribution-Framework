{
  "generated_at_utc": "2026-02-22T22:11:11Z",
  "mode": "aggregate",
  "title": "Annotarium Methodology: Portfolio View (5 documents)",
  "template_path": "/home/pantera/projects/TEIA/annotarium/docs/methodology_template.json",
  "input_reports": [
    "/home/pantera/projects/TEIA/annotarium/outputs/reports/apt29_hammertoss_stealthy_tactics_define_a_report.json",
    "/home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json",
    "/home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2013_apt1_exposing_one_of_china_s_cyber_espionage_units_report.json",
    "/home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2017_apt28_at_the_center_of_the_storm_report.json",
    "/home/pantera/projects/TEIA/annotarium/outputs/reports/panda_crowdstrike_intelligence_report_report.json"
  ],
  "template_sections": [
    {
      "id": "introduction",
      "title": "Introduction and Epistemic Framing",
      "instruction": "Introduce the methodology as an evidentiary framework for cyber attribution under contestation. Explain epistemic posture, burden-sensitive interpretation, and why structured extraction precedes legal inference.",
      "min_paragraphs": 3,
      "min_words": 260,
      "must_include": [
        "epistemic",
        "evidentiary",
        "contestable",
        "burden-sensitive"
      ],
      "context_keys": [
        "mode",
        "report_meta",
        "documents_preview",
        "methodology_reference_excerpt",
        "pipeline_methods"
      ]
    },
    {
      "id": "scope_units",
      "title": "Scope, Units of Analysis, and Output Semantics",
      "instruction": "Define units of analysis (claim, source, artifact, evidence item, document) and explain what each output score means and does not mean.",
      "min_paragraphs": 2,
      "min_words": 220,
      "must_include": [
        "claim",
        "source",
        "artifact",
        "evidence item",
        "document-level"
      ],
      "context_keys": [
        "mode",
        "raw_data_excerpt",
        "pipeline_counts",
        "document_scores_v4",
        "claim_score_preview_v4"
      ]
    },
    {
      "id": "data_ingestion",
      "title": "Data Ingestion and Corpus Handling",
      "instruction": "Explain report ingestion, deterministic file handling, and reproducibility guarantees from raw PDF inputs through intermediate artifacts.",
      "min_paragraphs": 2,
      "min_words": 200,
      "must_include": [
        "deterministic",
        "reproducibility",
        "input corpus"
      ],
      "context_keys": [
        "report_meta",
        "raw_payload_paths",
        "raw_payload_files",
        "pipeline_methods",
        "runtime_libraries"
      ]
    },
    {
      "id": "pdf_to_markdown",
      "title": "PDF-to-Markdown Conversion",
      "instruction": "Explain conversion from PDF to markdown, explicitly describing Mistral-based processing and offline fallback posture as methodological resilience rather than conceptual change.",
      "min_paragraphs": 2,
      "min_words": 210,
      "must_include": [
        "Mistral",
        "markdown",
        "offline fallback",
        "resilience"
      ],
      "context_keys": [
        "pipeline_methods",
        "pipeline_counts",
        "raw_payload_paths",
        "raw_payload_files"
      ]
    },
    {
      "id": "structural_parsing",
      "title": "Structural Parsing of Text, Tables, and Figures",
      "instruction": "Describe extraction of text blocks, tables, and figures/images with anchors and why anchor-preserving structure is required for auditability.",
      "min_paragraphs": 2,
      "min_words": 220,
      "must_include": [
        "anchors",
        "tables",
        "figures",
        "auditability"
      ],
      "context_keys": [
        "pipeline_counts",
        "raw_data_excerpt",
        "raw_payload_paths",
        "raw_claims_preview"
      ]
    },
    {
      "id": "artifact_extraction",
      "title": "Artifact Extraction and Technical Object Normalization",
      "instruction": "Explain artifact extraction across modalities, normalization of technical objects, and implications for custody/provenance evaluation.",
      "min_paragraphs": 2,
      "min_words": 220,
      "must_include": [
        "normalization",
        "technical objects",
        "provenance",
        "modalities"
      ],
      "context_keys": [
        "artifact_type_counts",
        "raw_artifacts_preview",
        "pipeline_methods",
        "document_scores_v4"
      ]
    },
    {
      "id": "reference_parsing",
      "title": "Footnote and Reference Parsing",
      "instruction": "Explain reference and footnote parsing, citation linkage, and resolution from rhetorical citation to analyzable source graph.",
      "min_paragraphs": 2,
      "min_words": 220,
      "must_include": [
        "footnote",
        "citation linkage",
        "source graph"
      ],
      "context_keys": [
        "raw_sources_preview",
        "source_type_counts",
        "pipeline_counts",
        "pipeline_methods"
      ]
    },
    {
      "id": "institution_inference",
      "title": "Institution Inference and Source Typology",
      "instruction": "Explain source institution inference, typology mapping, and how institutional class affects credibility weighting and corroboration eligibility.",
      "min_paragraphs": 2,
      "min_words": 210,
      "must_include": [
        "institution inference",
        "source typology",
        "credibility weighting",
        "corroboration eligibility"
      ],
      "context_keys": [
        "raw_sources_preview",
        "source_type_counts",
        "pipeline_methods",
        "runtime_libraries"
      ]
    },
    {
      "id": "claim_evidence_graph",
      "title": "Claim-Evidence Graph Construction",
      "instruction": "Describe how claims are linked to evidence, sources, and artifacts, including anchor-level traceability and anti-circularity safeguards.",
      "min_paragraphs": 2,
      "min_words": 230,
      "must_include": [
        "claim-evidence graph",
        "traceability",
        "anti-circularity"
      ],
      "context_keys": [
        "raw_claims_preview",
        "raw_sources_preview",
        "raw_artifacts_preview",
        "claim_score_preview_v4",
        "scoring_bundle"
      ]
    },
    {
      "id": "scoring_overview",
      "title": "Scoring Framework Overview",
      "instruction": "Present the overall scoring architecture from claim-level axes to document-level synthesis. Clarify separation between extraction outputs and inferential weighting.",
      "min_paragraphs": 2,
      "min_words": 230,
      "must_include": [
        "claim-level",
        "document-level",
        "inferential weighting"
      ],
      "context_keys": [
        "document_scores_v4",
        "claim_score_preview_v4",
        "scoring_bundle",
        "pipeline_methods"
      ]
    },
    {
      "id": "chain_of_custody",
      "title": "Chain of Custody Axis",
      "instruction": "Explain chain-of-custody variables (provenance, integrity, time anchors, artifact identifiers, versioning), quality controls, and penalties.",
      "min_paragraphs": 3,
      "min_words": 280,
      "must_include": [
        "provenance",
        "integrity",
        "time anchors",
        "versioning",
        "penalties"
      ],
      "context_keys": [
        "document_scores_v4",
        "claim_score_preview_v4",
        "raw_artifacts_preview",
        "scoring_bundle"
      ]
    },
    {
      "id": "credibility_corroboration",
      "title": "Credibility Axis with Corroboration Subcomponent",
      "instruction": "Explain source hierarchy, independence logic, corroboration rules, claim coverage scaling, and exclusion criteria for low-value source classes.",
      "min_paragraphs": 3,
      "min_words": 320,
      "must_include": [
        "source hierarchy",
        "independence",
        "corroboration",
        "claim coverage",
        "exclusion criteria"
      ],
      "context_keys": [
        "source_type_counts",
        "raw_sources_preview",
        "document_scores_v4",
        "claim_score_preview_v4",
        "scoring_bundle"
      ]
    },
    {
      "id": "clarity_axis",
      "title": "Clarity Axis and State Responsibility Pathways",
      "instruction": "Explain clarity scoring for attribution to state actors through organs, control over non-state actors, and due diligence failure pathways.",
      "min_paragraphs": 3,
      "min_words": 300,
      "must_include": [
        "state organs",
        "non-state actors",
        "control",
        "due diligence",
        "state responsibility"
      ],
      "context_keys": [
        "document_scores_v4",
        "claim_score_preview_v4",
        "scoring_bundle",
        "raw_claims_preview"
      ]
    },
    {
      "id": "aggregation_calibration",
      "title": "Aggregation, Calibration, and Uncertainty",
      "instruction": "Explain claim-to-document aggregation, weighting, calibration, and uncertainty handling (e.g., confidence intervals/dispersion diagnostics) in interpretation.",
      "min_paragraphs": 3,
      "min_words": 300,
      "must_include": [
        "aggregation",
        "calibration",
        "uncertainty",
        "dispersion"
      ],
      "context_keys": [
        "document_scores_v4",
        "scoring_bundle",
        "portfolio_summary",
        "documents_preview"
      ]
    },
    {
      "id": "validation_quality_assurance",
      "title": "Validation and Quality Assurance",
      "instruction": "Explain automated validation gates and QA protocol with agent review and targeted human review on a 10% sample with no observed errors in that sample.",
      "min_paragraphs": 3,
      "min_words": 260,
      "must_include": [
        "validation gates",
        "agent review",
        "human review",
        "10%",
        "no observed errors"
      ],
      "context_keys": [
        "validation_bundle",
        "qa_protocol",
        "raw_payload_paths",
        "raw_payload_files"
      ]
    },
    {
      "id": "limitations_governance",
      "title": "Limitations, Governance, and Future Refinement",
      "instruction": "Discuss methodological limits, governance controls, and principled refinement roadmap without overstating certainty.",
      "min_paragraphs": 2,
      "min_words": 220,
      "must_include": [
        "limitations",
        "governance",
        "refinement roadmap"
      ],
      "context_keys": [
        "validation_bundle",
        "document_scores_v4",
        "portfolio_summary",
        "runtime_libraries",
        "methodology_reference_excerpt"
      ]
    }
  ],
  "context_snapshot": {
    "mode": "aggregate",
    "document_count": 5,
    "documents_preview": [
      {
        "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/apt29_hammertoss_stealthy_tactics_define_a_report.json",
        "title": "APT1: Exposing One of China's Cyber Espionage Units",
        "belief_weighted_0_100": 0.05,
        "custody_avg_0_100": 61.43,
        "credibility_composite_avg_0_100": 0.0,
        "clarity_avg_0_100": 24.25,
        "sources_total": 1,
        "citations_total": 0,
        "citation_coverage_sources_0_1": 0.0
      },
      {
        "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json",
        "title": "APT1: Exposing One of China's Cyber Espionage Units",
        "belief_weighted_0_100": 0.06,
        "custody_avg_0_100": 34.5,
        "credibility_composite_avg_0_100": 0.0,
        "clarity_avg_0_100": 37.11,
        "sources_total": 1,
        "citations_total": 0,
        "citation_coverage_sources_0_1": 0.0
      },
      {
        "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2013_apt1_exposing_one_of_china_s_cyber_espionage_units_report.json",
        "title": "APT1: Exposing One of China's Cyber Espionage Units",
        "belief_weighted_0_100": 0.13,
        "custody_avg_0_100": 52.58,
        "credibility_composite_avg_0_100": 0.1,
        "clarity_avg_0_100": 51.74,
        "sources_total": 10,
        "citations_total": 17,
        "citation_coverage_sources_0_1": 0.9
      },
      {
        "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2017_apt28_at_the_center_of_the_storm_report.json",
        "title": "APT1: Exposing One of China's Cyber Espionage Units",
        "belief_weighted_0_100": 0.07,
        "custody_avg_0_100": 42.89,
        "credibility_composite_avg_0_100": 0.0,
        "clarity_avg_0_100": 42.24,
        "sources_total": 1,
        "citations_total": 0,
        "citation_coverage_sources_0_1": 0.0
      },
      {
        "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/panda_crowdstrike_intelligence_report_report.json",
        "title": "APT1: Exposing One of China's Cyber Espionage Units",
        "belief_weighted_0_100": 0.08,
        "custody_avg_0_100": 40.65,
        "credibility_composite_avg_0_100": 0.0,
        "clarity_avg_0_100": 42.34,
        "sources_total": 1,
        "citations_total": 0,
        "citation_coverage_sources_0_1": 0.0
      }
    ],
    "raw_documents_preview": [
      {
        "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/apt29_hammertoss_stealthy_tactics_define_a_report.json",
        "title": "APT1: Exposing One of China's Cyber Espionage Units",
        "belief_weighted_0_100": 0.05,
        "custody_avg_0_100": 61.43,
        "credibility_composite_avg_0_100": 0.0,
        "clarity_avg_0_100": 24.25,
        "sources_total": 1,
        "citations_total": 0,
        "citation_coverage_sources_0_1": 0.0
      },
      {
        "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json",
        "title": "APT1: Exposing One of China's Cyber Espionage Units",
        "belief_weighted_0_100": 0.06,
        "custody_avg_0_100": 34.5,
        "credibility_composite_avg_0_100": 0.0,
        "clarity_avg_0_100": 37.11,
        "sources_total": 1,
        "citations_total": 0,
        "citation_coverage_sources_0_1": 0.0
      },
      {
        "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2013_apt1_exposing_one_of_china_s_cyber_espionage_units_report.json",
        "title": "APT1: Exposing One of China's Cyber Espionage Units",
        "belief_weighted_0_100": 0.13,
        "custody_avg_0_100": 52.58,
        "credibility_composite_avg_0_100": 0.1,
        "clarity_avg_0_100": 51.74,
        "sources_total": 10,
        "citations_total": 17,
        "citation_coverage_sources_0_1": 0.9
      },
      {
        "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2017_apt28_at_the_center_of_the_storm_report.json",
        "title": "APT1: Exposing One of China's Cyber Espionage Units",
        "belief_weighted_0_100": 0.07,
        "custody_avg_0_100": 42.89,
        "credibility_composite_avg_0_100": 0.0,
        "clarity_avg_0_100": 42.24,
        "sources_total": 1,
        "citations_total": 0,
        "citation_coverage_sources_0_1": 0.0
      },
      {
        "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/panda_crowdstrike_intelligence_report_report.json",
        "title": "APT1: Exposing One of China's Cyber Espionage Units",
        "belief_weighted_0_100": 0.08,
        "custody_avg_0_100": 40.65,
        "credibility_composite_avg_0_100": 0.0,
        "clarity_avg_0_100": 42.34,
        "sources_total": 1,
        "citations_total": 0,
        "citation_coverage_sources_0_1": 0.0
      }
    ],
    "portfolio_summary": {
      "generated_at_utc": "2026-02-22T11:31:26Z",
      "source": "/home/pantera/projects/TEIA/annotarium/threec_electron_viewer/pipeline_files.json",
      "document_count": 5,
      "aggregated_totals": {
        "pages": 5,
        "images": 0,
        "tables": 31,
        "figures": 40,
        "claims": 38,
        "sources": 14,
        "artifacts": 25
      },
      "aggregated_averages_per_document": {
        "pages": 1.0,
        "images": 0.0,
        "tables": 6.2,
        "figures": 8.0,
        "claims": 7.6,
        "sources": 2.8,
        "artifacts": 5.0
      },
      "average_scores_0_1": {
        "chain_of_custody": 0.52182,
        "credibility": 0.00030000000000000003,
        "credibility_base": 0.0,
        "corroboration": 0.0006000000000000001,
        "clarity": 0.42838000000000004
      },
      "average_proxy_support_metrics": {
        "weighted_coverage_ratio": 0.04,
        "weighted_precision_proxy": 0.003,
        "weighted_recall_proxy": 0.040000000000000015,
        "weighted_f2_proxy": 0.006000000000000002
      },
      "cv_scores_0_1": {
        "chain_of_custody": 0.21489457593551423,
        "credibility": 2.23606797749979,
        "credibility_base": 0.0,
        "corroboration": 2.23606797749979,
        "clarity": 0.25122872077165803
      },
      "covariance_matrix": {
        "chain_of_custody": {
          "chain_of_custody": 0.012574546999999997,
          "credibility": 2.1442499999999993e-05,
          "credibility_base": 0.0,
          "corroboration": 4.288499999999999e-05,
          "clarity": -0.004505357000000001,
          "claims": -0.134865,
          "sources": 0.12865499999999994,
          "artifacts": 0.09739999999999999,
          "weighted_coverage_ratio": 0.002858999999999999,
          "weighted_precision_proxy": 0.0002144249999999999,
          "weighted_recall_proxy": 0.002859,
          "weighted_f2_proxy": 0.00042885
        },
        "credibility": {
          "chain_of_custody": 2.1442499999999993e-05,
          "credibility": 4.5000000000000003e-07,
          "credibility_base": 0.0,
          "corroboration": 9.000000000000001e-07,
          "clarity": 4.9995e-05,
          "claims": 0.0009,
          "sources": 0.0027000000000000006,
          "artifacts": 0.000375,
          "weighted_coverage_ratio": 6.000000000000001e-05,
          "weighted_precision_proxy": 4.500000000000001e-06,
          "weighted_recall_proxy": 6.000000000000002e-05,
          "weighted_f2_proxy": 9.000000000000005e-06
        },
        "credibility_base": {
          "chain_of_custody": 0.0,
          "credibility": 0.0,
          "credibility_base": 0.0,
          "corroboration": 0.0,
          "clarity": 0.0,
          "claims": 0.0,
          "sources": 0.0,
          "artifacts": 0.0,
          "weighted_coverage_ratio": 0.0,
          "weighted_precision_proxy": 0.0,
          "weighted_recall_proxy": 0.0,
          "weighted_f2_proxy": 0.0
        },
        "corroboration": {
          "chain_of_custody": 4.288499999999999e-05,
          "credibility": 9.000000000000001e-07,
          "credibility_base": 0.0,
          "corroboration": 1.8000000000000001e-06,
          "clarity": 9.999e-05,
          "claims": 0.0018,
          "sources": 0.005400000000000001,
          "artifacts": 0.00075,
          "weighted_coverage_ratio": 0.00012000000000000002,
          "weighted_precision_proxy": 9.000000000000002e-06,
          "weighted_recall_proxy": 0.00012000000000000004,
          "weighted_f2_proxy": 1.800000000000001e-05
        },
        "clarity": {
          "chain_of_custody": -0.004505357000000001,
          "credibility": 4.9995e-05,
          "credibility_base": 0.0,
          "corroboration": 9.999e-05,
          "clarity": 0.011582356999999996,
          "claims": 0.30071499999999995,
          "sources": 0.29996999999999996,
          "artifacts": -0.009849999999999998,
          "weighted_coverage_ratio": 0.006665999999999999,
          "weighted_precision_proxy": 0.0004999499999999999,
          "weighted_recall_proxy": 0.006666000000000001,
          "weighted_f2_proxy": 0.0009999
        },
        "claims": {
          "chain_of_custody": -0.134865,
          "credibility": 0.0009,
          "credibility_base": 0.0,
          "corroboration": 0.0018,
          "clarity": 0.30071499999999995,
          "claims": 9.299999999999999,
          "sources": 5.400000000000001,
          "artifacts": -1.0,
          "weighted_coverage_ratio": 0.12000000000000004,
          "weighted_precision_proxy": 0.009,
          "weighted_recall_proxy": 0.12000000000000002,
          "weighted_f2_proxy": 0.018000000000000006
        },
        "sources": {
          "chain_of_custody": 0.12865499999999994,
          "credibility": 0.0027000000000000006,
          "credibility_base": 0.0,
          "corroboration": 0.005400000000000001,
          "clarity": 0.29996999999999996,
          "claims": 5.400000000000001,
          "sources": 16.2,
          "artifacts": 2.25,
          "weighted_coverage_ratio": 0.36000000000000004,
          "weighted_precision_proxy": 0.027000000000000003,
          "weighted_recall_proxy": 0.36000000000000015,
          "weighted_f2_proxy": 0.05400000000000002
        },
        "artifacts": {
          "chain_of_custody": 0.09739999999999999,
          "credibility": 0.000375,
          "credibility_base": 0.0,
          "corroboration": 0.00075,
          "clarity": -0.009849999999999998,
          "claims": -1.0,
          "sources": 2.25,
          "artifacts": 2.0,
          "weighted_coverage_ratio": 0.049999999999999996,
          "weighted_precision_proxy": 0.0037500000000000007,
          "weighted_recall_proxy": 0.050000000000000024,
          "weighted_f2_proxy": 0.007500000000000002
        },
        "weighted_coverage_ratio": {
          "chain_of_custody": 0.002858999999999999,
          "credibility": 6.000000000000001e-05,
          "credibility_base": 0.0,
          "corroboration": 0.00012000000000000002,
          "clarity": 0.006665999999999999,
          "claims": 0.12000000000000004,
          "sources": 0.36000000000000004,
          "artifacts": 0.049999999999999996,
          "weighted_coverage_ratio": 0.008,
          "weighted_precision_proxy": 0.0006,
          "weighted_recall_proxy": 0.008000000000000004,
          "weighted_f2_proxy": 0.0012000000000000003
        },
        "weighted_precision_proxy": {
          "chain_of_custody": 0.0002144249999999999,
          "credibility": 4.500000000000001e-06,
          "credibility_base": 0.0,
          "corroboration": 9.000000000000002e-06,
          "clarity": 0.0004999499999999999,
          "claims": 0.009,
          "sources": 0.027000000000000003,
          "artifacts": 0.0037500000000000007,
          "weighted_coverage_ratio": 0.0006,
          "weighted_precision_proxy": 4.5e-05,
          "weighted_recall_proxy": 0.0006000000000000002,
          "weighted_f2_proxy": 9.000000000000003e-05
        },
        "weighted_recall_proxy": {
          "chain_of_custody": 0.002859,
          "credibility": 6.000000000000002e-05,
          "credibility_base": 0.0,
          "corroboration": 0.00012000000000000004,
          "clarity": 0.006666000000000001,
          "claims": 0.12000000000000002,
          "sources": 0.36000000000000015,
          "artifacts": 0.050000000000000024,
          "weighted_coverage_ratio": 0.008000000000000004,
          "weighted_precision_proxy": 0.0006000000000000002,
          "weighted_recall_proxy": 0.008000000000000005,
          "weighted_f2_proxy": 0.0012000000000000005
        },
        "weighted_f2_proxy": {
          "chain_of_custody": 0.00042885,
          "credibility": 9.000000000000005e-06,
          "credibility_base": 0.0,
          "corroboration": 1.800000000000001e-05,
          "clarity": 0.0009999,
          "claims": 0.018000000000000006,
          "sources": 0.05400000000000002,
          "artifacts": 0.007500000000000002,
          "weighted_coverage_ratio": 0.0012000000000000003,
          "weighted_precision_proxy": 9.000000000000003e-05,
          "weighted_recall_proxy": 0.0012000000000000005,
          "weighted_f2_proxy": 0.0001800000000000001
        }
      },
      "spearman_matrix": {
        "chain_of_custody": {
          "chain_of_custody": 0.9999999999999998,
          "credibility": 0.35355339059327373,
          "credibility_base": 0.0,
          "corroboration": 0.35355339059327373,
          "clarity": 0.0,
          "claims": -0.051298917604257706,
          "sources": 0.35355339059327373,
          "artifacts": 0.4472135954999579,
          "weighted_coverage_ratio": 0.35355339059327373,
          "weighted_precision_proxy": 0.35355339059327373,
          "weighted_recall_proxy": 0.35355339059327373,
          "weighted_f2_proxy": 0.35355339059327373
        },
        "credibility": {
          "chain_of_custody": 0.35355339059327373,
          "credibility": 0.9999999999999998,
          "credibility_base": 0.0,
          "corroboration": 0.9999999999999998,
          "clarity": 0.7071067811865475,
          "claims": 0.5441071875825088,
          "sources": 0.9999999999999998,
          "artifacts": 0.3952847075210474,
          "weighted_coverage_ratio": 0.9999999999999998,
          "weighted_precision_proxy": 0.9999999999999998,
          "weighted_recall_proxy": 0.9999999999999998,
          "weighted_f2_proxy": 0.9999999999999998
        },
        "credibility_base": {
          "chain_of_custody": 0.0,
          "credibility": 0.0,
          "credibility_base": 0.0,
          "corroboration": 0.0,
          "clarity": 0.0,
          "claims": 0.0,
          "sources": 0.0,
          "artifacts": 0.0,
          "weighted_coverage_ratio": 0.0,
          "weighted_precision_proxy": 0.0,
          "weighted_recall_proxy": 0.0,
          "weighted_f2_proxy": 0.0
        },
        "corroboration": {
          "chain_of_custody": 0.35355339059327373,
          "credibility": 0.9999999999999998,
          "credibility_base": 0.0,
          "corroboration": 0.9999999999999998,
          "clarity": 0.7071067811865475,
          "claims": 0.5441071875825088,
          "sources": 0.9999999999999998,
          "artifacts": 0.3952847075210474,
          "weighted_coverage_ratio": 0.9999999999999998,
          "weighted_precision_proxy": 0.9999999999999998,
          "weighted_recall_proxy": 0.9999999999999998,
          "weighted_f2_proxy": 0.9999999999999998
        },
        "clarity": {
          "chain_of_custody": 0.0,
          "credibility": 0.7071067811865475,
          "credibility_base": 0.0,
          "corroboration": 0.7071067811865475,
          "clarity": 0.9999999999999998,
          "claims": 0.9746794344808964,
          "sources": 0.7071067811865475,
          "artifacts": -0.11180339887498948,
          "weighted_coverage_ratio": 0.7071067811865475,
          "weighted_precision_proxy": 0.7071067811865475,
          "weighted_recall_proxy": 0.7071067811865475,
          "weighted_f2_proxy": 0.7071067811865475
        },
        "claims": {
          "chain_of_custody": -0.051298917604257706,
          "credibility": 0.5441071875825088,
          "credibility_base": 0.0,
          "corroboration": 0.5441071875825088,
          "clarity": 0.9746794344808964,
          "claims": 1.0000000000000002,
          "sources": 0.5441071875825088,
          "artifacts": -0.2867696673382022,
          "weighted_coverage_ratio": 0.5441071875825088,
          "weighted_precision_proxy": 0.5441071875825088,
          "weighted_recall_proxy": 0.5441071875825088,
          "weighted_f2_proxy": 0.5441071875825088
        },
        "sources": {
          "chain_of_custody": 0.35355339059327373,
          "credibility": 0.9999999999999998,
          "credibility_base": 0.0,
          "corroboration": 0.9999999999999998,
          "clarity": 0.7071067811865475,
          "claims": 0.5441071875825088,
          "sources": 0.9999999999999998,
          "artifacts": 0.3952847075210474,
          "weighted_coverage_ratio": 0.9999999999999998,
          "weighted_precision_proxy": 0.9999999999999998,
          "weighted_recall_proxy": 0.9999999999999998,
          "weighted_f2_proxy": 0.9999999999999998
        },
        "artifacts": {
          "chain_of_custody": 0.4472135954999579,
          "credibility": 0.3952847075210474,
          "credibility_base": 0.0,
          "corroboration": 0.3952847075210474,
          "clarity": -0.11180339887498948,
          "claims": -0.2867696673382022,
          "sources": 0.3952847075210474,
          "artifacts": 0.9999999999999998,
          "weighted_coverage_ratio": 0.3952847075210474,
          "weighted_precision_proxy": 0.3952847075210474,
          "weighted_recall_proxy": 0.3952847075210474,
          "weighted_f2_proxy": 0.3952847075210474
        },
        "weighted_coverage_ratio": {
          "chain_of_custody": 0.35355339059327373,
          "credibility": 0.9999999999999998,
          "credibility_base": 0.0,
          "corroboration": 0.9999999999999998,
          "clarity": 0.7071067811865475,
          "claims": 0.5441071875825088,
          "sources": 0.9999999999999998,
          "artifacts": 0.3952847075210474,
          "weighted_coverage_ratio": 0.9999999999999998,
          "weighted_precision_proxy": 0.9999999999999998,
          "weighted_recall_proxy": 0.9999999999999998,
          "weighted_f2_proxy": 0.9999999999999998
        },
        "weighted_precision_proxy": {
          "chain_of_custody": 0.35355339059327373,
          "credibility": 0.9999999999999998,
          "credibility_base": 0.0,
          "corroboration": 0.9999999999999998,
          "clarity": 0.7071067811865475,
          "claims": 0.5441071875825088,
          "sources": 0.9999999999999998,
          "artifacts": 0.3952847075210474,
          "weighted_coverage_ratio": 0.9999999999999998,
          "weighted_precision_proxy": 0.9999999999999998,
          "weighted_recall_proxy": 0.9999999999999998,
          "weighted_f2_proxy": 0.9999999999999998
        },
        "weighted_recall_proxy": {
          "chain_of_custody": 0.35355339059327373,
          "credibility": 0.9999999999999998,
          "credibility_base": 0.0,
          "corroboration": 0.9999999999999998,
          "clarity": 0.7071067811865475,
          "claims": 0.5441071875825088,
          "sources": 0.9999999999999998,
          "artifacts": 0.3952847075210474,
          "weighted_coverage_ratio": 0.9999999999999998,
          "weighted_precision_proxy": 0.9999999999999998,
          "weighted_recall_proxy": 0.9999999999999998,
          "weighted_f2_proxy": 0.9999999999999998
        },
        "weighted_f2_proxy": {
          "chain_of_custody": 0.35355339059327373,
          "credibility": 0.9999999999999998,
          "credibility_base": 0.0,
          "corroboration": 0.9999999999999998,
          "clarity": 0.7071067811865475,
          "claims": 0.5441071875825088,
          "sources": 0.9999999999999998,
          "artifacts": 0.3952847075210474,
          "weighted_coverage_ratio": 0.9999999999999998,
          "weighted_precision_proxy": 0.9999999999999998,
          "weighted_recall_proxy": 0.9999999999999998,
          "weighted_f2_proxy": 0.9999999999999998
        }
      },
      "bootstrap_95ci": {
        "scores_0_1": {
          "chain_of_custody": {
            "mean": 0.52182,
            "ci95_low": 0.44306,
            "ci95_high": 0.60232
          },
          "credibility": {
            "mean": 0.00030000000000000003,
            "ci95_low": 0.0,
            "ci95_high": 0.0009000000000000001
          },
          "credibility_base": {
            "mean": 0.0,
            "ci95_low": 0.0,
            "ci95_high": 0.0
          },
          "corroboration": {
            "mean": 0.0006000000000000001,
            "ci95_low": 0.0,
            "ci95_high": 0.0018000000000000002
          },
          "clarity": {
            "mean": 0.42838000000000004,
            "ci95_low": 0.34008000000000005,
            "ci95_high": 0.50924
          }
        },
        "proxy_support_metrics": {
          "weighted_coverage_ratio": {
            "mean": 0.04,
            "ci95_low": 0.0,
            "ci95_high": 0.12000000000000002
          },
          "weighted_precision_proxy": {
            "mean": 0.003,
            "ci95_low": 0.0,
            "ci95_high": 0.009000000000000001
          },
          "weighted_recall_proxy": {
            "mean": 0.040000000000000015,
            "ci95_low": 0.0,
            "ci95_high": 0.12000000000000004
          },
          "weighted_f2_proxy": {
            "mean": 0.006000000000000002,
            "ci95_low": 0.0,
            "ci95_high": 0.018000000000000006
          }
        }
      },
      "documents": [
        {
          "id": "apt29_hammertoss_stealthy_tactics_define_a",
          "label": "apt29-hammertoss-stealthy-tactics-define-a.pdf",
          "pdf_path": "/home/pantera/projects/TEIA/annotarium/Reports/apt29-hammertoss-stealthy-tactics-define-a.pdf",
          "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/apt29_hammertoss_stealthy_tactics_define_a_report.json",
          "status": "PASS",
          "last_run_at": "2026-02-22T10:15:08.178Z",
          "metrics": {
            "pages": 1,
            "images": 0,
            "tables": 2,
            "figures": 6,
            "claims": 3,
            "sources": 1,
            "artifacts": 6,
            "scores_0_1": {
              "chain_of_custody": 0.6817,
              "credibility": 0.0,
              "credibility_base": 0.0,
              "corroboration": 0.0,
              "clarity": 0.2644
            },
            "proxy_support_metrics": {
              "claims_count": 3,
              "weighted_coverage_ratio": 0.0,
              "weighted_precision_proxy": 0.0,
              "weighted_recall_proxy": 0.0,
              "weighted_f2_proxy": 0.0
            }
          }
        },
        {
          "id": "fireeye_rpt_apt37_02_20_2018",
          "label": "Fireeye_rpt_APT37(02-20-2018).pdf",
          "pdf_path": "/home/pantera/projects/TEIA/annotarium/Reports/Fireeye_rpt_APT37(02-20-2018).pdf",
          "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json",
          "status": "PASS",
          "last_run_at": "2026-02-22T10:15:08.450Z",
          "metrics": {
            "pages": 1,
            "images": 0,
            "tables": 3,
            "figures": 4,
            "claims": 6,
            "sources": 1,
            "artifacts": 4,
            "scores_0_1": {
              "chain_of_custody": 0.3875,
              "credibility": 0.0,
              "credibility_base": 0.0,
              "corroboration": 0.0,
              "clarity": 0.4086
            },
            "proxy_support_metrics": {
              "claims_count": 6,
              "weighted_coverage_ratio": 0.0,
              "weighted_precision_proxy": 0.0,
              "weighted_recall_proxy": 0.0,
              "weighted_f2_proxy": 0.0
            }
          }
        },
        {
          "id": "mandiant_2013_apt1_exposing_one_of_china_s_cyber_espionage_units",
          "label": "Mandiant - 2013 - APT1 Exposing One of China's Cyber Espionage Units.pdf",
          "pdf_path": "/home/pantera/projects/TEIA/annotarium/Reports/Mandiant - 2013 - APT1 Exposing One of China's Cyber Espionage Units.pdf",
          "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2013_apt1_exposing_one_of_china_s_cyber_espionage_units_report.json",
          "status": "PASS",
          "last_run_at": "2026-02-22T10:15:08.954Z",
          "metrics": {
            "pages": 1,
            "images": 0,
            "tables": 19,
            "figures": 29,
            "claims": 10,
            "sources": 10,
            "artifacts": 6,
            "scores_0_1": {
              "chain_of_custody": 0.579,
              "credibility": 0.0015,
              "credibility_base": 0.0,
              "corroboration": 0.003,
              "clarity": 0.5617
            },
            "proxy_support_metrics": {
              "claims_count": 10,
              "weighted_coverage_ratio": 0.2,
              "weighted_precision_proxy": 0.015000000000000001,
              "weighted_recall_proxy": 0.20000000000000007,
              "weighted_f2_proxy": 0.03000000000000001
            }
          }
        },
        {
          "id": "mandiant_2017_apt28_at_the_center_of_the_storm",
          "label": "Mandiant - 2017 - APT28 At the Center of the Storm.pdf",
          "pdf_path": "/home/pantera/projects/TEIA/annotarium/Reports/Mandiant - 2017 - APT28 At the Center of the Storm.pdf",
          "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2017_apt28_at_the_center_of_the_storm_report.json",
          "status": "PASS",
          "last_run_at": "2026-02-22T10:15:09.274Z",
          "metrics": {
            "pages": 1,
            "images": 0,
            "tables": 7,
            "figures": 0,
            "claims": 10,
            "sources": 1,
            "artifacts": 3,
            "scores_0_1": {
              "chain_of_custody": 0.4815,
              "credibility": 0.0,
              "credibility_base": 0.0,
              "corroboration": 0.0,
              "clarity": 0.4547
            },
            "proxy_support_metrics": {
              "claims_count": 10,
              "weighted_coverage_ratio": 0.0,
              "weighted_precision_proxy": 0.0,
              "weighted_recall_proxy": 0.0,
              "weighted_f2_proxy": 0.0
            }
          }
        },
        {
          "id": "panda_crowdstrike_intelligence_report",
          "label": "Panda - CrowdStrike Intelligence Report.pdf",
          "pdf_path": "/home/pantera/projects/TEIA/annotarium/Reports/Panda - CrowdStrike Intelligence Report.pdf",
          "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/panda_crowdstrike_intelligence_report_report.json",
          "status": "PASS",
          "last_run_at": "2026-02-22T10:15:09.630Z",
          "metrics": {
            "pages": 1,
            "images": 0,
            "tables": 0,
            "figures": 1,
            "claims": 9,
            "sources": 1,
            "artifacts": 6,
            "scores_0_1": {
              "chain_of_custody": 0.4794,
              "credibility": 0.0,
              "credibility_base": 0.0,
              "corroboration": 0.0,
              "clarity": 0.4525
            },
            "proxy_support_metrics": {
              "claims_count": 9,
              "weighted_coverage_ratio": 0.0,
              "weighted_precision_proxy": 0.0,
              "weighted_recall_proxy": 0.0,
              "weighted_f2_proxy": 0.0
            }
          }
        }
      ]
    },
    "raw_payload_paths": {
      "portfolio_summary_json": "/home/pantera/projects/TEIA/annotarium/outputs/reports/portfolio_summary.json",
      "reports": [
        "/home/pantera/projects/TEIA/annotarium/outputs/reports/apt29_hammertoss_stealthy_tactics_define_a_report.json",
        "/home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json",
        "/home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2013_apt1_exposing_one_of_china_s_cyber_espionage_units_report.json",
        "/home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2017_apt28_at_the_center_of_the_storm_report.json",
        "/home/pantera/projects/TEIA/annotarium/outputs/reports/panda_crowdstrike_intelligence_report_report.json"
      ]
    },
    "raw_payload_files": {
      "portfolio_summary_json": {
        "path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/portfolio_summary.json",
        "exists": true,
        "size_bytes": 20562
      }
    },
    "runtime_libraries": {
      "python_version": "3.10.12",
      "platform": "Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35",
      "python_packages": {
        "openai": "2.9.0",
        "pypdf": "6.0.0",
        "pymupdf4llm": "0.2.9",
        "pymupdf": "1.26.7",
        "plotly": "5.18.0"
      },
      "viewer_node_packages": {
        "name": "threec-electron-viewer",
        "version": "0.1.0",
        "dependencies": {
          "update-electron-app": "^3.1.1"
        },
        "devDependencies": {
          "electron": "^31.7.7",
          "electron-builder": "^26.0.12"
        }
      }
    },
    "pipeline_methods": {
      "pdf_to_markdown_primary": "process_pdf_mistral_ocr.py (Mistral OCR/provider-backed conversion)",
      "pdf_to_markdown_fallback": "offline fallback via PyMuPDF4LLM",
      "table_and_image_extraction": "stage1 markdown parse",
      "artifact_extraction": "schema extraction artifact indices",
      "reference_parsing": "citations + references linked to sources",
      "institution_inference": "gpt-5-mini institution classification (+ web fallback)"
    },
    "qa_protocol": {
      "agent_review_enabled": true,
      "human_sample_fraction": 0.1,
      "human_sample_observed_error_rate": 0.0,
      "note": "Human review is targeted and sampled; results reported for reviewed sample."
    },
    "methodology_reference_excerpt": "# Methodology\n\n## I. Methodological Position\n\nThis study treats cyber-attribution reporting as an evidentiary exercise rather than a narrative exercise. The central claim is that attribution assessments should be evaluated as structured arguments about State responsibility, not as standalone assertions of confidence. In consequence, the method asks, for each proposition advanced in a report, what evidentiary materials are relied upon, how those materials are connected to the proposition, and whether those connections can bear weight under adversarial scrutiny.\n\nThe analytical posture is therefore jurisprudential. It draws from recurring ICJ evidentiary practice: differential weighting of heterogeneous materials, caution toward single-origin or litigation-shaped records, and preference for convergent indications over repetition. The objective is not to replicate adjudication procedurally, but to translate judicially legible evidentiary logic into a reproducible scoring framework for cyber-attribution dossiers.\n\n## II. Corpus, Record Formation, and Constraints\n\nThe corpus consists of cybersecurity attribution reports in PDF form. Each report is transcribed into markdown and then transformed into a schema-constrained evidentiary record. The schema is strict by design. It separates claims, sources, artifacts, and evidence links into distinct objects and requires explicit anchors for evidentiary references.\n\nThis design choice has methodological significance. It prevents retrospective reconstruction of support through implicit model inference and compels the system to preserve an inspectable chain between proposition and proof. The method thus prioritizes evidentiary legibility over extraction breadth.\n\n## III. Procedural Architecture\n\nThe workflow proceeds in four phases. First, source documents are transcribed from PDF into markdown. Second, markdown is parsed into structured outputs containing metadata, source inventories, artifact inventories, and claim-level analytic blocks. Third, outputs undergo integrity checks. Fourth, validated outputs are scored under an ICJ-inspired weighting model.\n\nTwo procedural commitments govern all phases. The first is determinacy: where possible, deterministic transformations are preferred, and model-assisted components are constrained by schema and post-run verification. The second is persistence: all major intermediate and terminal artifacts are written to disk, so that any score can be reconstructed and contested from the underlying record.\n\n## IV. Integrity Controls as Admissibility Discipline\n\nNo score is produced absent structural integrity of the evidentiary record. In practical terms, runs are failed where identifier collisions occur, where artifact anchors are missing, where citation pathways cannot be traced, or where references resolve to nonexistent entities. These controls function analogously to admissibility discipline: they do not determine substantive truth, but they determine whether the record is fit to carry a substantive weighing exercise.\n\nThis stage is essential to avoid pseudo-precision. Without strict record integrity, downstream numerical outputs risk expressing parser convenience rather than evidentiary strength.\n\n## V. Evidentiary Weight Model\n\n### A. Item-Level Weight\n\nEach evidence item is evaluated along five bounded dimensions: independence, authentication/provenance, methodological soundness, procedural testing, and contemporaneity. Item-level probative force is computed multiplicatively. The multiplicative form is deliberate: strong performance on one dimension cannot fully compensate for critical weakness on another.\n\n### B. Corroboration and Anti-Circularity\n\nCorroboration is not treated as citation volume. Evidence is clustered by origin, origin-level contribution is aggregated with diminishing returns, and claim-level corroboration is then derived from convergence across origins. This implements an anti-circularity rule: repeated downstream reporting of one upstream source does not become independent support merely by repetition.\n\n### C. Core 3Cs\n\nThe principal outputs are the core 3Cs: Chain of Custody, Credibility, and Clarity. Corroboration is preserved as an explicit sub-calculation and audit surface, but it is integrated into the top-level Credibility axis rather than exposed as a separate top-level C.\n\nChain of Custody is modeled as claim-specific evidentiary handling quality, not as raw artifact quantity. In the current implementation, custody is computed from five normalized variables extracted from evidence text: provenance markers, integrity markers, temporal anchors, artifact identifiers, and versioning/update lineage. These dimensions are weighted and combined linearly, then bounded in `[0,1]`, so that the score remains auditable at claim level and cannot be inflated by a single indicator class.\n\nCredibility is modeled as a composite of source-quality support and corroborative convergence for each claim. The source-quality component is derived from source-type quality, the strongest source attached to the claim, source diversity, and domain independence, with a single-source penalty. Internal/auto sources and newspapers are excluded from credibility support. The model gives maximal weight to international institutions/judicial material and peer-reviewed academic material, intermediate weight to official government and NGO material, and lower weight to think-tank/other material.\n\nIn addition, credibility is calibrated at the document level by a weighted claim-coverage factor keyed to high-credibility source support. Let `Cred_raw_i` denote raw credibility for claim `i`, and let `w_i` denote claim gravity weight. Define a credibility-covered claim as one with at least one high-credibility source (`source_quality >= 0.90`). Then:\n\n`credibility_coverage_factor = (Σ w_i over credibility-covered claims) / (Σ w_i over all claims)`\n\nand:\n\n`Cred_i = Cred_raw_i × credibility_coverage_factor`\n\nThis ensures credibility is interpreted as weighted coverage across the claim set rather than isolated source quality in a small subset of claims.\n\nCorroboration is modeled as convergence constrained by support relevance. A claim with broad wording but narrow evidentiary support receives lower corroborative strength, even where artifact volume is high. In the current presentation model, corroboration is retained as a dedicated subscore and then merged into top-level credibility:\n\n`Credibility_top = 0.50 × Credibility_source_quality + 0.50 × Corroboration`\n\nClarity is modeled as legal attribution intelligibility. Operationally, it answers two linked questions: (i) whether attribution of attack `Z` to State `X` is clearly reasoned in the text, and (ii) whether the mode of responsibility is clear under state-responsibility doctrine. The scorer therefore checks not only act–actor–link specificity, but also whether the report clearly indicates one of three legal pathways: attribution through state organs, attribution through non-state actors operating under state direction/control, or state omission/failure of due diligence (knowledge plus failure to prevent within jurisdiction).\n\nIn practice, the clarity panel and score details expose explicit question-level outputs:\n\n- “Is attribution to State X given attack Z clear?”\n- “Was the state’s responsibility pathway clear: direct conduct by official organs, control/direction of non-state operators, or omission/due diligence failure in its territory?”\n- “Is it clear that the state knew of the activity and failed to prevent, investigate, or suppress it (due diligence)?”\n\nIn addition, corroboration is calibrated at the document level by a claim-coverage factor so that corroboration is interpreted as proportionate coverage across the claim set, not absolute citation mass in isolated claims. Let `C_raw_i` denote raw corroboration for claim `i`, and let `w_i` denote the claim gravity weight. Define a corroborated claim as one for which `C_raw_i > 0`. Th..."
  },
  "sections": [
    {
      "id": "introduction",
      "title": "Introduction and Epistemic Framing",
      "html": "<h2>Introduction and Epistemic Framing</h2>\n<p>This methodology is presented as an evidentiary framework for cyber attribution where findings are contestable and must survive adversarial scrutiny. The epistemic posture adopted is jurisprudential: the evaluator treats attribution claims as arguments composed of heterogeneous materials rather than as singular declarative acts. The analysis is burden-sensitive in that inferential weight is allocated according to the demands of contested proof — stronger connections are required where consequences are greater and where the record is thin. The approach privileges structured extraction of evidentiary units prior to any legal inference so that chain-of-reasoning remains transparent and reconstructible.</p>\n\n<h2>Data Processing and Extraction</h2>\n<p>Operationally, the corpus consists of PDF-format attribution reports that are transcribed and parsed into schema-constrained records. As reflected in the provided methodology excerpt, the pipeline proceeds from PDF to markdown and then into structured objects separating claims, artifacts, sources, and links. The pipeline methods supplied for this project include an OCR/transcription primary step (process_pdf_mistral_ocr.py) with a PyMuPDF fallback, stage-one markdown parsing for images and tables, and schema-driven artifact extraction. These deterministic and persistenced transformations are required to prevent implicit reconstruction of evidentiary support and to enable later audit of extraction choices.</p>\n\n<p>Schema constraints enforce explicit anchors for evidentiary references and surface integrity checks to fail runs with identifier collisions, missing anchors, or unresolved citation pathways. This preserves admissibility discipline: only structurally intact records move to scoring. The corpus preview accompanying this task shows heterogeneous metadata fields (for example, per-report metrics such as \"credibility_composite_avg_0_100\" and \"sources_total\") whose variation is used illustratively to demonstrate how the extraction pipeline must ingest, normalize, and retain provenance attributes for each item without collapsing them into a single undifferentiated score.</p>\n\n<h2>Reference Parsing and Institution Inference</h2>\n<p>Reference parsing is performed as a distinct stage that links in-text citations and bibliography entries to discrete source objects in the schema. The design separates citation parsing from institution inference so that source identity and institutional classification remain auditable. Institution inference is performed with a constrained language model assist (noted in the pipeline methods as gpt-5-mini institution classification) with a web-backed fallback for ambiguous cases; all inferred institution attributions persist as candidate labels with provenance metadata recording the inference method and confidence bounds. This preserves contestability: an institution label is an evidentiary claim about origin, subject to cross-checking against primary source anchors.</p>\n\n<h2>Scoring Framework</h2>\n<p>The scoring layer is ICJ-inspired and multi-axial. Item-level weightings evaluate independence, authentication/provenance, methodological soundness, procedural testing, and contemporaneity; the multiplicative formulation prevents compensation across critical axes. Corroboration is computed as convergence across independent origins with diminishing returns and an explicit anti-circularity rule to avoid treating downstream repetition as independent support. The top-level axes — Chain of Custody, Credibility, and Clarity — are computed from bounded, auditable subcomponents. Chain of Custody combines textual provenance markers, integrity indicators, temporal anchors, artifact identifiers, and update lineage into a normalized custody score. Credibility combines source-type quality, source diversity, and corroborative convergence, and is calibrated by a claim-coverage factor so that high-quality sources must cover a proportion of claims to materially raise document-level credibility. Clarity operationalizes legal intelligibility by mapping claim-level reasoning to doctrinal pathways (state organ conduct, direction/control of non-state actors, omission/due diligence) so that legal inferences are only scored where the report supplies the requisite act–actor–link specificity.</p>\n\n<h2>Validation and Quality Assurance</h2>\n<p>Integrity checks and validation are treated as admissibility controls. Runs that exhibit identifier collisions, unresolved anchors, or citation mismatches are failed and returned for remediation. Post-parsing validation includes provenance re-walks, institution inference audits, and spot-checks of artifact extraction against original markup. Scoring outputs are reproducible because intermediate artifacts are persisted: every top-level score can be traced to the claim-level records and the extracted evidence items. Quality assurance includes unit tests for deterministic pipeline stages, manual review of model-assisted inferences, and calibration exercises that demonstrate how the framework responds to variation in source quality metrics (as seen in the corpus preview) without converting parser convenience into evidentiary strength.</p>\n\n<p>Finally, the methodology acknowledges limits: the framework is designed to render contestable assessments transparent and auditable, not to resolve contested factual questions. It therefore records uncertainty explicitly, treats high-impact inferences as burden-sensitive, and preserves the evidentiary chain for subsequent legal or scholarly adjudication.</p>",
      "model": "gpt-5-mini"
    },
    {
      "id": "scope_units",
      "title": "Scope, Units of Analysis, and Output Semantics",
      "html": "<section><h2>Introduction</h2><p>This methodology describes the conceptual and operational treatment of units of analysis and output semantics for a cyber-attribution scoring pipeline. It articulates definitions, processing stages, provenance controls, and validation procedures that govern how discrete inputs are converted into scored outputs. The exposition is presented at a methodological level and is intentionally generic: it does not report or interpret any case-specific factual findings from source materials. The accompanying data manifest supplied to this section indicated an aggregate processing mode and did not include raw excerpts for specific documents; consequently, the procedures described emphasize robustness to varying input completeness while referencing the provided manifest fields.</p></section><section><h2>Scope, Units of Analysis, and Output Semantics</h2><p>Five primary units of analysis are defined and used throughout the pipeline: claim, source, artifact, evidence item, and document-level object. A claim is an asserted proposition regarding an event, behavior, attribution, or inference that a consumer of the analysis might seek to evaluate. A source denotes an origin of information, which may be an organization, publication, individual, or technical feed; sources are tracked separately from documents to permit cross-document provenance aggregation. An artifact is any discrete extracted object such as a binary sample, configuration snippet, code fragment, image, or log excerpt; artifacts are treated as first-class inputs to technical analyses. An evidence item is a parsed and normalized representation of a factual or technical finding extracted from a document or artifact; such items are the atomic evidentiary units linked to claims. The document-level unit denotes the container (for example, a report, email, or file) from which artifacts and evidence items are extracted, and it carries document-specific metadata (author, timestamp, format, and chain-of-custody markers).</p><p>Each output score is scoped to one of these units. A claim score quantifies the degree to which the available evidence items and their provenance patterns support the substantive content of a claim; it is not a definitive legal ruling or a binary truth determination. A source score communicates assessed reliability and consistency across documents and artifacts attributed to that source; it does not imply completeness of knowledge about that source or an immutable trustworthiness label. An artifact-level score captures confidence in the artifact’s integrity, origin inference, and relevance to particular claims; it does not substitute for specialized forensic analysis of the artifact. A document-level score aggregates artifact and evidence-item assessments together with document metadata to indicate the overall internal coherence and evidentiary weight of the document. In all cases scores are probabilistic, contextual, and explicitly accompanied by uncertainty bounds and provenance metadata; they are intended to inform, not to substitute for, human judgment or legal determinations.</p></section><section><h2>Data Ingestion</h2><p>The ingestion stage normalizes incoming payloads and metadata into an internal representation that preserves source identifiers and document-level metadata. When the raw_data_excerpt is unavailable, as in the present manifest, the pipeline adopts conservatively broad parsing heuristics and flags missing content for secondary review. The pipeline records the declared processing mode (for example, aggregate) and ingests manifest fields (including pipeline_counts, when present) to parameterize downstream parsing fidelity and validation expectations. All ingested objects are assigned persistent identifiers to facilitate immutable references in subsequent processing stages.</p></section><section><h2>PDF to Markdown Conversion and Text Normalization</h2><p>Binary document formats undergo staged conversion intended to preserve structural cues and metadata while producing searchable text. Optical character recognition (OCR) processes are applied where necessary with explicit confidence scores retained. Conversion produces a normalized, timestamped markdown-like representation that retains location pointers to original offsets in the source PDF or native file. Low-confidence regions are annotated and preserved as evidence items of reduced textual clarity rather than discarded, ensuring the chain of custody and certainty metadata remain intact.</p></section><section><h2>Structural Parsing</h2><p>Structural parsing segments normalized text into document-level components (title, abstract or summary, body sections, footnotes, appendices, and bibliography) and produces a document object model that supports fine-grained referencing. This stage is responsible for generating the document-level metadata record noted above. Structural parsing associates candidate claim segments with proximate evidence items and records inter-segment linkages, enabling downstream claim–evidence graph construction while preserving document-level provenance and positional indices.</p></section><section><h2>Artifact Extraction</h2><p>Artifact extraction identifies and isolates technical objects embedded in or referenced by documents. Extraction processes preserve original binary payloads where available and compute cryptographic hashes and format identifiers. Each artifact receives metadata including extraction method, extraction confidence, and linkage to the parent document-level container and source. Artifacts are classified by type to determine applicable forensic or analytic modules; their",
      "model": "gpt-5-mini"
    },
    {
      "id": "data_ingestion",
      "title": "Data Ingestion and Corpus Handling",
      "html": "<section>\n  <h2>Introduction</h2>\n  <p>This methodology chapter describes the end-to-end procedures used to ingest a defined input corpus of technical reports and to produce reproducible intermediate artifacts that serve as the basis for cyber-attribution scoring. The objective is to set out deterministic, auditable operations from raw document inputs through structured extractions, reference resolution, institution inference and scoring so that downstream inferences are reproducible by independent reviewers. Throughout the document the term input corpus denotes the set of original report files and their attendant summary metadata as recorded at ingestion.</p>\n\n  <h2>Scope Units</h2>\n  <p>The unit of analysis for the pipeline is the report-level document as represented in the input corpus. For the present implementation the recorded input corpus entries include a portfolio summary JSON (/home/pantera/projects/TEIA/annotarium/outputs/reports/portfolio_summary.json) and a set of document JSON representations produced from original PDFs (for example: /home/pantera/projects/TEIA/annotarium/outputs/reports/apt29_hammertoss_stealthy_tactics_define_a_report.json, /home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json, /home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2013_apt1_exposing_one_of_china_s_cyber_espionage_units_report.json, /home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2017_apt28_at_the_center_of_the_storm_report.json, /home/pantera/projects/TEIA/annotarium/outputs/reports/panda_crowdstrike_intelligence_report_report.json). The portfolio summary artifact is present at the specified path and recorded with its size in bytes (20562) as part of the ingestion ledger. Each report is treated as an immutable input object whose metadata and binary content are captured before any transformation to preserve reproducibility and chain of custody.</p>\n\n  <h2>Data Ingestion and Corpus Handling</h2>\n  <p>Ingestion proceeds by enumerating the input corpus and recording stable identifiers for each file. Deterministic artifact identifiers combine the file system path, a cryptographic checksum computed at the time of ingestion, and a timestamped manifest entry. This manifest captures the exact raw payload paths and the recorded file attributes (for example existence, size in bytes, and original path). Existence checks and size measurements are recorded for reproducibility; the portfolio summary JSON is validated for presence at /home/pantera/projects/TEIA/annotarium/outputs/reports/portfolio_summary.json and its file size is logged to the manifest. All subsequent processing references artifacts by manifest identifier rather than by mutable filesystem paths to ensure that transformations are traceable to the precise input byte sequences.</p>",
      "model": "gpt-5-mini"
    },
    {
      "id": "pdf_to_markdown",
      "title": "PDF-to-Markdown Conversion",
      "html": "<section><h2>Introduction</h2><p>This methodology chapter defines the procedures and rationale for converting source documents into machine-interpretable representations used in a cyber-attribution scoring pipeline. The objective is to describe the technical and procedural choices that produce markdown-structured artifacts and downstream extractions while preserving evidentiary integrity, reproducibility, and analytic transparency. The description provides a roadmap from ingestion through validation and quality assurance while emphasizing methodological rationale rather than case-specific substantive claims.</p></section>\n\n<section><h2>Scope and Units of Analysis</h2><p>The unit of analysis for the conversion pipeline is the individual report or document artifact provided as an electronic PDF. The pipeline treats each document as an atomic source for structural parsing, artifact extraction, reference linking, and institution inference. In the present configuration the available raw payloads include a portfolio summary and a set of report JSON outputs referenced in the pipeline inputs (for example: /home/pantera/projects/TEIA/annotarium/outputs/reports/portfolio_summary.json and a set of report JSON files enumerated in the raw payload paths). These file references are incorporated into the chain-of-custody records maintained for each processing stage.</p></section>\n\n<section><h2>Data Ingestion</h2><p>Document ingestion begins with acquisition of the original PDF files or their first-order JSON exports. Metadata about the source files is recorded at intake to support later provenance assertions; where available, filesystem path, existence flag, and byte-size are captured (for example, the portfolio summary JSON at /home/pantera/projects/TEIA/annotarium/outputs/reports/portfolio_summary.json with an indicated size of 20562 bytes). All ingestion events are timestamped and associated with the ingestion tool version to establish baseline reproducibility.</p></section>\n\n<section><h2>PDF-to-Markdown Conversion</h2><p>The conversion from PDF to markdown is implemented with a primary Mistral-based processing step followed by an offline fallback posture. The primary pathway leverages a Mistral OCR/provider-backed conversion implementation (noted in the pipeline methods as process_pdf_mistral_ocr.py) to produce a first-pass markdown representation that preserves textual flow, inline citations, and preliminary structural markers such as headings, tables, and figure captions. This Mistral-enabled processing is documented as the preferred channel for its throughput and provider capabilities; however, the pipeline explicitly treats an offline fallback as a matter of methodological resilience rather than as a conceptual change in the conversion model. The offline fallback uses PyMuPDF4LLM to reconstitute textual content and structural cues into markdown when provider-backed processing is unavailable or when policy and operational constraints require air-gapped processing. Both pathways emit a standardized markdown schema to ensure downstream components operate on a consistent intermediate representation regardless of execution environment.</p></section>\n\n<section><h2>Structural Parsing</h2><p>After markdown generation, stage1 markdown parse routines identify and tag major structural elements: sections, subsections, tables, figures, captions, and inline reference tokens. The structural parser yields a hierarchical document model that encodes offsets and relative positions to preserve evidentiary context for extracted claims. Structural parsing is designed to be idempotent across the two conversion pathways so that identical content yields a consistent parsed model whether produced by Mistral processing or the offline PyMuPDF4LLM fallback.</p></section>\n\n<section><h2>Artifact Extraction</h2><p>Artifact extraction operates on the parsed markdown structure to produce indexed artifacts conforming to a schema extraction artifact index. This includes named entities, technical indicators, IOCs, code snippets, and table-extracted data. Each artifact is recorded with provenance metadata linking back to the location in the markdown and the upstream conversion pathway. Artifact indices are constructed to support subsequent claim-evidence graph formation and to facilitate selective reprocessing if retraction or correction is required.</p></section>\n\n<section><h2>Reference Parsing</h2><p>Reference parsing isolates citations and bibliographic sections and links inline reference tokens to structured source descriptors. The pipeline captures both the citation text as rendered in the document and normalized reference metadata when resolvable. References are stored with pointers to their source text locations and, where available, to external identifiers or URLs. This linking supports automated cross-checking and external corroboration while preserving the original document anchor for auditability.</p></section>\n\n<section><h2>Institution Inference</h2><p>Institution inference is performed as a classification task that maps named organizations and author affiliations identified in the artifact extraction phase to institutional identifiers. The configured method employs a transformer-based classifier (documented as gpt-5-mini institution classification) with an optional web fallback to resolve ambiguous cases. This two-tier approach is designed to maximize inferential accuracy while maintaining an auditable decision trail: model-derived inferences are accompanied by confidence scores and, where the web fallback is used, by the retrieval evidence that informed the classification.</p></section>\n\n<section><h2>Claim–Evidence Graph Construction</h2><p>Extracted artifacts, parsed claims, and linked references are assembled into a claim–evidence graph. The graph is a directed structure in which claim nodes are connected to evidence nodes that include quoted passages, artifacts, and external references. Each edge is annotated with provenance metadata capturing document origin, markdown offsets, conversion pathway, and extraction routine. The graph model supports incremental updates, enabling later augmentation or correction without wholesale reprocessing of the original documents.</p></section>\n\n<section><h2>Scoring Framework Overview</h2><p>The scoring framework evaluates claims and institutions along axes such as source integrity, extractive confidence, provenance completeness, and external corroboration. Scores are computed from component reliability measures (for example, conversion-path consistency, parser confidence, and classifier probabilities) and are combined using calibrated aggregation functions. The framework is deliberately modular so that individual score components can be reweighted or replaced as the empirical performance of conversion and inference components is assessed.</p></section>\n\n<section><h2>Chain of Custody</h2><p>Every processing action is recorded in a chain-of-custody log that includes the tool version (for example, process_pdf_mistral_ocr.py or PyMuPDF4LLM), the time of operation, the operator or service identity, and the resulting artifact identifiers. This log provides the evidentiary trail necessary for auditing and supports reproducibility by enabling exact re-execution of processing steps against preserved inputs.</p",
      "model": "gpt-5-mini"
    },
    {
      "id": "structural_parsing",
      "title": "Structural Parsing of Text, Tables, and Figures",
      "html": "<h2 id=\"introduction\">Introduction</h2>\n<p>This methodology section situates the structural parsing stage within a broader cyber‑attribution scoring pipeline and explains why preserving document anchors is central to auditability. The approach described here treats each report and supporting artifact as a compound document comprising text, tables, figures, and embedded metadata. The methodological rationale is to enable reproducible extraction, to maintain provenance from original payloads to derived evidence nodes, and to permit retrospective review by independent analysts or auditors without invoking substantive findings from any particular report. The procedures below are grounded in the available raw payload paths and intended to interoperate with downstream reference and institution inference stages.</p>\n\n<h2 id=\"scope_units\">Scope and Units of Analysis</h2>\n<p>The unit of analysis is the atomic document component: block‑level text segments, tabular data units, and discrete image or figure objects. Each unit is treated as an addressable entity by virtue of an anchor that locates it within the original payload. Anchors are defined so as to survive format conversions and to record file‑level provenance, byte offsets where available, page numbers, and unique identifiers derived from the source file path. This unitization supports consistent handling of heterogeneous sources, including the reports referenced in the ingestion manifest. It also establishes the minimal element that can carry extracted metadata, analytic annotations, and links into the claim–evidence graph.</p>\n\n<h2 id=\"data_ingestion\">Data Ingestion</h2>\n<p>Ingestion begins from a manifest of raw payloads. For the present pipeline, the manifest entries include the portfolio summary and a set of report JSON payloads identified by their filesystem paths. Ingestion records the source path, file checksum, timestamp of acquisition, and declared content type. These fields form the initial layer of chain‑of‑custody metadata and become part of each unitʼs anchor. The ingestion stage validates file integrity against checksums and logs any anomalies that would affect subsequent parsing. This procedural step is deliberately agnostic to the substantive contents of any report, and its purpose is to ensure robust linkage between source material and extracted units.</p>\n\n<h2 id=\"pdf_to_markdown\">PDF to Structured Text Conversion</h2>\n<p>Where source files are encoded in page‑based formats such as PDF, a two‑stage conversion process is applied to produce structured text and candidate anchors. First, an optical or native text extraction engine renders page images and identifies layout features including columns, headers, and footers. Second, a logical reflow algorithm groups text runs into candidate blocks, preserving positional metadata. At every stage anchors are propagated and augmented; for example, a page coordinate anchor is retained alongside a derived block identifier that references the original byte range. This redundancy reduces the risk that subsequent reflow operations will sever provenance links necessary for auditability.</p>\n\n<h2 id=\"structural_parsing\">Structural Parsing of Text, Tables, and Figures</h2>\n<p>Structural parsing decomposes documents into discrete, anchored objects: text blocks, tables, and figures. Text blocks are defined by contiguous text runs with coherent layout and typographic cues. Tables are detected by spatial alignment and cell boundary heuristics and are extracted as structured matrices with cell‑level anchors. Figures and images are extracted as binary objects with an associated descriptive caption block and coordinate anchor. Crucially, every extracted object is annotated with a stable anchor that encodes source path, page number, byte offset where available, and an assigned object identifier. These anchors enable an audit trail from any analytic assertion back to the precise location in the original payload, which is essential for external review and for resolving disputes about interpretation.</p>\n\n<p>The requirement to preserve anchors throughout parsing is methodological and epistemic. Methodologically, anchors permit reconstitution of the original context when a downstream consumer needs to re‑evaluate an excerpt, validate a table transcription, or re‑examine an image. Epistemically, anchor preservation supports auditability by providing an unambiguous mapping between derived claims and source material. This mapping is necessary for credible chain‑of‑custody records, for assessments of extraction fidelity, and for harmonizing multiple extractions of the same source performed by independent teams or tools.</p>\n\n<h2 id=\"artifact_extraction\">Artifact Extraction and Normalization</h2>\n<p>After structural segmentation, artifacts are normalized into canonical representations suitable for analysis. Text blocks are normalized for whitespace and encoding while retaining original offsets; tables are normalized into row‑column schemas with cell type annotations; figures are encoded with format metadata and image hashes. Normalization explicitly avoids altering substantive content beyond character encoding corrections and layout de‑noise. Each normalized artifact keeps its anchor metadata and is linked to the ingestion and conversion logs that justify any transformation performed.</p>\n\n<h2 id=\"reference_parsing\">Reference Parsing</h2>\n<p>References cited within text blocks and captioned near tables or figures are parsed to extract bibliographic entities, URLs, and identifiers. Parsing annotates each citation with its anchor and records the citation span within the parent block. This granular anchoring allows downstream adjudication of which specific phrase or table cell supports a bibliographic claim and enables tracing of evidence provenance through the reference network without asserting substantive connections among institutions or actors.</p>\n\n<h2 id=\"institution_inference\">Institution Inference</h2>\n<p>Institutional inference operates on anchored artifacts to suggest candidate organizational associations for authors, publishers, or cited entities. The inference engine uses anchored mentions, document metadata, and external authority lists to produce scored hypotheses, each linked to the originating anchor. By tying inferences to anchors rather than to entire documents, the methodology preserves the capacity to review and contest individual association hypotheses and to evaluate how particular passages or tables produced specific inference outputs.</p>\n\n<h2 id=\"claim_evidence_graph\">Claim–Evidence Graph Construction</h2>\n<p>Extracted, normalized artifacts and parsed references are assembled into a claim–evidence graph in which nodes represent anchored artifacts, inferred entities, and candidate claims, while edges represent evidential or provenance relationships. Anchors function as immutable identifiers for nodes derived from source documents, enabling deterministic reconstruction of the graph from the original payloads. The graph model is designed to accommodate multiple, potentially conflicting claims while maintaining traceable links to source anchors for each supporting artifact.</p>\n\n<h2 id=\"scoring_overview\">Scoring Framework Overview</h2>\n<p>The scoring framework assigns quantitative indicators to claims and inferences based on features of their anchored supporting artifacts. Features include extraction confidence, the degree of corroboration across independent anchors, temporal proximity",
      "model": "gpt-5-mini"
    },
    {
      "id": "artifact_extraction",
      "title": "Artifact Extraction and Technical Object Normalization",
      "html": "<h2 id=\"introduction\">Introduction</h2>\n<p>This methodology chapter sets out a repeatable, defensible approach to artifact extraction and the normalization of technical objects in multi‑modal evidentiary collections. It is written in the register of legal and academic exposition and is intended to support attribution scoring systems while preserving chain of custody and enabling provenance assessment. The chapter describes the processing pipeline from ingestion through structural parsing, artifact extraction, reference linkage and institution inference, and explains how these steps together produce normalized technical objects suitable for downstream claim–evidence graphing and scoring. The description is methodological and deliberately refrains from addressing or characterizing any specific substantive finding from the underlying corpus.</p>\n\n<h3 id=\"scope_units\">Scope and Units of Analysis</h3>\n<p>The unit of analysis in this chapter is the technical object. A technical object is any discrete artifact that can carry technical indicators or metadata: for example, files, images, attachments embedded in documents, text segments, tables, extracted strings, timestamps, hashes, network indicators, and structured records produced by automated extraction routines. The modalities considered include rendered text (OCR outputs), native text, tabular data, images, and metadata streams. For the purposes of provenance and custody evaluation, each technical object is treated as an atomic unit that may be linked to source documents, extraction steps, and transformation operations. The methodology emphasizes normalization of these technical objects so that comparison, aggregation, and scoring functions operate over consistent, machine‑interpretable representations irrespective of modality of origin.</p>\n\n<h3 id=\"data_ingestion\">Data Ingestion and Data Contracts</h3>\n<p>Ingestion begins with the receipt of source documents and associated metadata. The pipeline operates under an explicit data contract that defines required and optional keys for downstream stages; in this engagement the contract keys included artifact_type_counts, raw_artifacts_preview, pipeline_methods and document_scores_v4. In practice, downstream routines must be resilient to missing keys: when artifact_type_counts or raw_artifacts_preview are absent, the pipeline records those absences in provenance metadata and triggers candidate discovery routines to determine modality composition empirically. All ingestion steps log source identifiers and preservation conditions to support later chain‑of‑custody review.</p>\n\n<h3 id=\"pdf_to_markdown\">PDF Conversion and Text Normalization (pdf_to_markdown)</h3>\n<p>PDF and other fixed‑layout formats are converted into processable text and structural markup. The primary conversion tool indicated in the supplied pipeline_methods is a provider‑backed Mistral OCR workflow implemented as process_pdf_mistral_ocr.py; an offline fallback implemented via PyMuPDF4LLM is available when the primary service is not usable. The methodology requires that each conversion include confidence metrics, page‑level hashes, and extraction provenance records identifying which conversion routine and parameters were used. OCR and conversion outputs are normalized for whitespace, Unicode normalization forms, and linebreak conventions to create a canonical textual representation that is carried forward as the canonical source of textual technical objects for that document. Such normalization reduces spurious variance when comparing textual artifacts across conversion methods or providers.</p>\n\n<h3 id=\"structural_parsing\">Structural Parsing and Segmentation</h3>\n<p>After conversion to markdown or structured text, stage1 markdown parse routines perform structural analysis: segmentation into paragraphs, headings, captions, table regions and embedded object placeholders. Table and image extraction are initiated from this parsed structure; the supplied pipeline notes that table_and_image_extraction is handled in stage1 markdown parse. Structural parsing annotates each segment with source coordinates, page numbers, and transformation lineage. This segmentation step delineates the technical objects that will undergo further normalization, and it preserves the mapping between extracted artifacts and their physical or logical locations within the source document—critical information for provenance assertions and potential evidentiary review.</p>\n\n<h3 id=\"artifact_extraction\">Artifact Extraction and Technical Object Normalization</h3>\n<p>Artifact extraction is implemented through schema extraction and artifact indexing routines (described in the pipeline as schema extraction artifact indices). Each extracted artifact becomes a technical object instance and is assigned a globally unique identifier, a stable content fingerprint (cryptographic hash where feasible), modality label, and a provenance record that includes the originating file identifier, conversion routine, and page/offset coordinates. Normalization processes transform modality‑specific representations into canonical forms: images are represented by standardized image encodings plus embedded OCR text where applicable; tables are represented as normalized row/column matrices with normalized cell encodings; dates, IP addresses, hashes and other structured indicators are normalized to canonical lexical forms. This normalization supports deterministic comparison across items that originate in different modalities or conversion pipelines. The methodology also differentiates between normalization (a reversible or bounded transformation that preserves original content and mapping) and irreversible summarization; provenance metadata records the exact transformation applied so the original technical object can be reconstructed or re‑extracted if necessary.</p>\n\n<h3 id=\"reference_parsing\">Reference Parsing and Source Linkage</h3>\n<p>References and citations are parsed from textual segments by routines that extract citation tokens and attempt linkage to external sources; the supplied pipeline specifies that reference_parsing produces citations + references linked to sources. Each reference token becomes an ancillary technical object with linkage hypothesis metadata documenting the matching criteria used (string similarity, DOI lookup, URL resolution, or repository identifiers). Reference linkage is recorded with confidence scores and with the evidence chain that justifies the linkage. Where automated linkage is uncertain, the system captures alternative candidates and the rationale for each to support later human review. Reference parsing is integral to provenance because it creates explicit, auditable connections between extracted claims, cited evidence, and external sources.</p>\n\n<h3 id=\"institution_inference\">Institution Inference and Attribution Candidates</h3>\n<p>Institutional inference routines apply classification models and, where necessary, web fallback checks to infer organizational origin metadata for artifacts. The pipeline inventory indicates the use of gpt-5-mini institution classification with web fallback. Methodologically, inferred institutional metadata are treated as probabilistic assertions with associated provenance that records model version, prompt templates, and any external evidence used for verification. The system separates inferred institutional labels from verified institutional statements; inferred labels serve as candidate inputs to attribution scoring but are not elevated to verified status without corroborating evidence. All inferred outputs include uncertainty characterization to ensure appropriate weight in downstream scoring.</p>\n\n<h3 id=\"claim_evidence_graph\">Claim–Evidence Graph Construction</h3>\n<p>Normalized technical objects and parsed references are assembled into a claim–evidence graph. Nodes represent technical objects, inferred claims, and external source records; edges represent extraction lineage, referential citation, transformation, and inferred institutional association. The graph explicitly encodes provenance and transformation attributes so that any claim node can be traced back to the precise technical object versions, conversion steps, and reference linkages that support it. This graph structure provides the substrate for scoring algorithms that evaluate credibility, corroboration, and chain‑of‑custody integrity.</p>\n\n<h3 id=\"scoring_overview\">Scoring Framework Overview</h3>\n<p>The scoring framework consumes normalized technical objects and the claim–evidence graph to generate reproducible metrics for attribution tasks. Scores are produced along multiple axes: provenance",
      "model": "gpt-5-mini"
    },
    {
      "id": "reference_parsing",
      "title": "Footnote and Reference Parsing",
      "html": "<h2>Introduction</h2>\n<p>This methodology chapter sets out a reproducible approach to parsing footnote and reference material from primary documents and to resolving rhetorical citations into an analyzable source graph. The account that follows describes procedural choices, data transformations, and quality controls in general methodological terms. It does not report or analyze case-specific substantive findings; rather, it explains how information flowing from document ingestion to structured citation objects is managed, validated, and represented for subsequent attribution scoring.</p>\n\n<h2>Scope and Units of Analysis</h2>\n<p>The unit of analysis is the discrete referential element within a document: a footnote marker, an in-text parenthetical citation, a reference list entry, or an embedded hyperlink. These referential elements are treated as propositional connectors between a rhetorical claim in the source document and an external object (for example, another document, dataset, web resource, or institutional artifact). The methodology therefore operationalizes the transformation from rhetorical citation to nodes and edges in a source graph that can be used for evidence linking and scoring.</p>\n\n<h2>Data Ingestion</h2>\n<p>Document ingestion follows a documented pipeline that privileges optical character recognition and structural recovery suited for machine parsing. The primary conversion method is process_pdf_mistral_ocr.py (Mistral OCR/provider-backed conversion); an explicit fallback is available using offline fallback via PyMuPDF4LLM. Metadata captured during ingestion—file provenance, timestamp, and conversion logs—are retained to support chain-of-custody assertions and to assign initial confidence levels to extracted referential elements.</p>\n\n<h2>PDF-to-Markdown Conversion</h2>\n<p>The conversion stage produces an intermediate markdown-like representation in which typographic cues (superscript numerals, bracketed citations, italics) are preserved as tokens for downstream structural parsing. The chosen OCR and conversion methods are selected for their ability to preserve footnote markers and inline citation tokens; however, the methodology prescribes explicit downstream normalization to address OCR-induced artifacts such as misrecognized numerals, misplaced punctuation, and broken URLs.</p>\n\n<h2>Structural Parsing</h2>\n<p>Structural parsing segments the converted text into document zones (body, header, footer, footnote region, bibliography/reference section, tables, and figures). The segmentation is guided by a stage1 markdown parse for table_and_image_extraction, and by heuristics that identify contiguous reference lists and footnote blocks. Structural labeling is a prerequisite for robust footnote resolution because the same glyphic token (for example, a bracketed number) may serve different rhetorical roles depending on its spatial-contextual zone.</p>\n\n<h2>Artifact Extraction</h2>\n<p>Artifact extraction converts segmented text regions into schema-compliant artifact indices. The artifact extraction stage produces normalized fields such as cited title, author string, year, publisher, DOI, URL, and contextual anchor text. Extracted artifacts are indexed with provenance links to the originating document and the pipeline stage that produced the extraction, following the schema extraction artifact indices pattern. These indices form the atomic record units that populate the nascent source graph.</p>\n\n<h2>Reference Parsing</h2>\n<p>The reference parsing process links rhetorical tokens (footnote markers, in-text citations) to the artifact indices generated from the reference list or footnote region. The parser first performs token disambiguation—resolving whether a given symbol denotes a footnote or an inline citation—then associates the token with one or more candidate reference entries. Resolution strategies combine deterministic matching on normalized strings (title, author, year) with heuristic matching (partial title overlap, publisher name variants) and provenance heuristics (positional proximity within the same page or column). Citation linkage is represented as edges that connect a claim-span in the body text to artifact nodes; these edges carry attributes for match confidence, matching method, and evidence of ambiguity. The resulting structure is an explicit source graph suitable for graph-based analyses of evidentiary support.</p>\n\n<h2>Institution Inference</h2>\n<p>Institutional inference is applied to extracted artifacts that include organizational names or accounts. The primary classifier used for institution inference is gpt-5-mini institution classification (+ web fallback). The classifier is employed to normalize naming variants, to resolve corporate ownership hierarchies where appropriate, and to attach standardized institutional identifiers when available. Web-based fallback checks are used conservatively and logged, and institutional resolution outputs include confidence metrics and provenance to the underlying classifier call or web query.</p>\n\n<h2>Claim–Evidence Graph Construction</h2>\n<p>Claims are identified as annotated text spans in the body of the document; these spans are linked via citation linkage edges to one or more artifact nodes. The claim–evidence graph represents claims, artifacts, and institutions as typed nodes, and R-type edges (cites, supports, rebuts, clarifies) as necessary for downstream scoring. Each edge and node retains provenance metadata: originating file, pipeline stage (for example, process_pdf_mistral_ocr.py), timestamp, and confidence scores aggregated from OCR, parsing, and matching stages. This ensures that the graph is not only analyzable but auditable.</p>\n\n<h2>Scoring Overview</h2>\n<p>Scoring transforms the claim–evidence graph into quantitative and qualitative indicators for attribution assessment. Scores are computed from component measures: extraction confidence, citation linkage confidence, institutional reliability, and corroboration counts across independent artifacts. The scoring framework is modular such that each component score can be calibrated independently and combined using transparent rules. Scores are documented with their contributing components and the provenance of each contribution.</p>\n\n<h2>Chain of Custody</h2>\n<p>The chain-of-custody record captures each pipeline handoff and transformation, including the use of process_pdf_mistral_ocr.py (Mistral OCR/provider-backed conversion) and offline fallback via PyMuPDF4LLM, along with timestamps and operator identifiers where applicable.",
      "model": "gpt-5-mini"
    },
    {
      "id": "institution_inference",
      "title": "Institution Inference and Source Typology",
      "html": "<section aria-labelledby=\"introduction\"><h2 id=\"introduction\">Introduction</h2><p>This methodology chapter sets out a principled, reproducible approach to institution inference and source typology as components of a broader cyber‑attribution scoring framework. It frames the problem in evidentiary terms: source materials are ingested, structurally parsed, and linked into a claim–evidence graph; institution inference and source typology then situate each source within an institutional class that informs credibility weighting and corroboration eligibility. The exposition below emphasizes methodological rationale, data provenance, and the criteria by which institutional class alters downstream scoring without advancing case‑specific attribution claims.</p></section><section aria-labelledby=\"data_processing_extraction\"><h2 id=\"data_processing_extraction\">Data Processing and Extraction</h2><p>Raw source materials undergo a staged pipeline designed to preserve provenance and to enable structured downstream analysis. Primary conversion from PDF to machine‑readable text is performed using a provider‑backed OCR and conversion script (process_pdf_mistral_ocr.py), with an offline fallback implemented via a PyMuPDF‑based routine (PyMuPDF4LLM) when provider conversion is unavailable. Tabular and image content are flagged during initial markdown parsing (stage1 markdown parse) and routed to specialized extraction routines. Artifacts discovered during parsing are indexed in a schema extraction artifact index to retain location, page coordinates, and conversion metadata for chain‑of‑custody purposes. Reference parsing is performed to extract citations and to produce explicit links from document assertions to external sources; these links are retained as discrete evidence nodes in the claim–evidence graph. The selection of these tools and steps is motivated by the need to balance fidelity of transcription, repeatability of extraction, and explicit recording of transformation steps for later audit.</p></section><section aria-labelledby=\"references_institution_inference\"><h2 id=\"references_institution_inference\">References and Institution Inference</h2><p>Reference parsing yields candidate source identifiers (DOIs, URLs, author names, organizational bylines) which are normalized and then subjected to institution inference. Institution inference is the process by which a source is classified into institutional categories (for example, academic, commercial vendor, independent researcher, government, intergovernmental organization, open‑source collective) based on extracted metadata and supplemental checks. The operational implementation combines a model‑based classifier (reported here as gpt‑5‑mini institution classification) with a web‑fallback verification step when classifier confidence is below a predefined threshold. The classifier receives structured inputs derived from parsed references and artifact indices, and returns an institutional label and a confidence score; the web fallback performs targeted lookups to validate or refine the label when necessary.</p><p>Source typology is mapped to a finite set of institutional classes that have been pre‑defined in the scoring ontology. Mapping rules encode observable heuristics (publisher domain reputation, organizational registration, stated affiliation, presence of peer review, and declared funding or sponsorship) and are applied deterministically where possible and probabilistically where metadata are incomplete. Institutional class determines whether a source is eligible to participate in particular corroboration patterns and sets baseline credibility weighting. For example, sources classified as peer‑reviewed academic outputs or official government technical reports are assigned higher baseline weights than anonymous forum posts, but each assignment remains subject to adjustment through corroboration and artifact‑level quality assessment. This approach preserves methodological neutrality by separating institutional priors from evidence‑level corroboration steps.</p></section><section aria-labelledby=\"scoring_framework\"><h2 id=\"scoring_framework\">Scoring Framework</h2><p>The scoring framework combines three principal elements: (1) base credibility weighting derived from source typology and institution inference, (2) corroboration adjustments based on independent evidence links in the claim–evidence graph, and (3) clarity and artifact‑level quality modifiers drawn from extraction metadata. Base credibility weighting encodes the initial trust assigned to a source contingent on its institutional class and associated confidence score from the institution inference process. Corroboration adjustments depend on corroboration eligibility, a rule set that specifies which institutional classes may corroborate one another and the strength imparted by independent, methodologically distinct traces. Clarifying modifiers account for transcription fidelity, completeness of referenced artifact indices, and any detected transformations during PDF‑to‑markdown conversion. The result is a composite, auditable score per claim that reflects both institutional priors and evidence‑level linkage strength.</p></section><section aria-labelledby=\"validation_quality_assurance\"><h2 id=\"validation_quality_assurance\">Validation and Quality Assurance</h2><p>Validation is implemented at multiple levels. Unit checks verify that conversion and parsing steps preserve location and parsing metadata as recorded in the schema extraction artifact indices. Cross‑validation exercises compare institution inference outputs against curated ground truth sets and web‑verified labels to calibrate classifier thresholds and fallback invocation policies. Corroboration eligibility rules and credibility weighting parameters are stress‑tested using synthetic and historical datasets to observe sensitivity to misclassification and missing metadata. All transformation steps, model versions, and runtime environments (for example, the Python runtime and package versions used for extraction and inference) are recorded to enable reproducible re‑runs and forensic audit. Periodic human review of institution inference edge cases is mandated to correct systematic biases and to refine mapping heuristics. Together these measures ensure that institution inference, source typology mapping, credibility weighting, and corroboration eligibility are transparent, auditable, and subject to continuous improvement without reliance on case‑specific findings.</p></section>",
      "model": "gpt-5-mini"
    },
    {
      "id": "claim_evidence_graph",
      "title": "Claim-Evidence Graph Construction",
      "html": "<h2>Introduction</h2>\n<p>This methodological chapter sets out the approach used to construct a claim-evidence graph for cyber-attribution scoring. The goal is to describe, in legally oriented and academically rigorous terms, how discrete analytic units—claims, evidentiary fragments, source records, and extracted artifacts—are represented and linked to preserve auditable traceability while preventing inferential circularity. The discussion emphasizes methodological rationale rather than substantive findings drawn from any particular report.</p>\n\n<h2>Scope and Units of Analysis</h2>\n<p>The primary units of analysis are defined as follows: a claim is a discrete, testable assertion derived from reporting or analytic synthesis; a source is any documentary or digital record that provides information (for example, reports, raw feeds, or archival records); an artifact is an extracted data object such as a file hash, network indicator, binary fragment, or image; and an anchor is the minimal locatable unit of a source (for example, a page-and-paragraph locator, byte offsets in a file, or a timestamped log entry) that permits deterministic retracing of an evidentiary fragment. These units are the nodes and atomic identifiers used to construct the claim-evidence graph.</p>\n\n<h2>Data Ingestion</h2>\n<p>Data ingestion is governed by a written protocol that requires preservation of original file containers and capture of ingest metadata. In the present data package, fields named raw_claims_preview, raw_sources_preview, raw_artifacts_preview, claim_score_preview_v4, and scoring_bundle were present in the data contract; the preview fields were null at time of ingestion. The methodology therefore mandates explicit handling of null or missing preview fields: ingestion records must log the absence, attempt automated retrieval of the upstream artifact where a pointer exists, and, where automated retrieval is not possible, instantiate a flagged manual-curation workflow. All steps are recorded in the ingestion log to ensure later auditability and reproducibility.</p>\n\n<h2>PDF-to-Markdown Conversion</h2>\n<p>Where sources are provided as PDF or other document images, conversion to text uses a two-stage approach combining optical character recognition with structural retention. The objective is not only to obtain linear text but to preserve locators (page numbers, column markers, figure and table captions) that serve as anchors. Conversion output is retained alongside checksums of original files and conversion artifacts to support later verification of traceability.</p>\n\n<h2>Structural Parsing</h2>\n<p>Structural parsing identifies document components such as headings, footnotes, bibliographic references, figures, and tables and links them to anchor metadata. Parsing is designed to be conservative: when automatic delineation is ambiguous, the parser preserves original bounding metadata and marks the fragment as low-confidence, triggering human review. The structural representation feeds the downstream claim-evidence graph so that edges can reference specific structural anchors rather than coarse document identifiers.</p>\n\n<h2>Artifact Extraction</h2>\n<p>Artifact extraction isolates technical indicators and data objects from sources with explicit provenance markers. Each extracted artifact is recorded with a provenance tuple comprising source identifier, anchor locator, extraction timestamp, extractor identity (tool and version), and a content checksum. Artifacts are retained in a separate, append-only artifact store. The methodological rationale is to separate evidentiary items from interpretive narrative so that linkages in the claim-evidence graph can reference raw artifacts directly, increasing traceability and reducing interpretive opacity.</p>\n\n<h2>Reference Parsing</h2>\n<p>Reference parsing resolves in-text citations and bibliographic entries into canonical source identifiers where possible. The parser attempts to normalize reference strings to persistent identifiers (for example, DOIs or URIs), and when normalization fails records the original citation string plus the structural anchor. The consistent normalization of references permits automated detection of common sources across multiple claims, enabling correlation without reliance on paraphrase matching alone.</p>\n\n<h2>Institutional Inference</h2>\n<p>Institutional inference is performed using metadata appearing in sources—author lines, affiliation footnotes, issuing domains, and organisational signatures—supplemented by external registries where permitted. Inference is probabilistic and accompanied by confidence scores; uncertain inferences are retained but flagged. The rationale is to avoid hard-coded attribution of institutional origin without explicit evidence, and to make any institutional linkage transparent and reviewable within the claim-evidence graph.</p>\n\n<h2>Claim-Evidence Graph Construction</h2>\n<p>The claim-evidence graph is a directed acyclic representation in which nodes represent claims, evidentiary fragments (anchored source excerpts), source records, and artifacts. Edges encode explicit relations: a claim is supported_by an evidentiary fragment; a fragment is contained_in a source at a specified anchor; an artifact is extracted_from an anchor. Anchor-level traceability is implemented by ensuring every support edge references an anchor locator that maps to a specific offset or structural component in the original source. Nodes carry provenance metadata including ingest checksum, timestamps, extractor identity, and confidence annotations. The design objective is that any claim can be algorithmically and humanly traced back to the exact fragment and file that served as its immediate support, thereby enabling reproducibility and evidentiary inspection.</p>\n\n<h2>Anti-circularity Safeguards</h2>\n<p>Anti-circularity is enforced at multiple stages. First, the graph construction enforces acyclicity for support relations: a claim may not be supported_by a fragment that itself derives its content from the claim (direct cycle). Second, provenance chains are validated by examining extraction timestamps and source timestamps to ensure chronological plausibility; any reverse temporal dependencies are flagged for review. Third, the methodology distinguishes independent corroboration from re-publication by annotating sources with origin labels and measuring independence of provenance paths; support that depends solely on secondary re-publications of a single primary record is down-weighted or marked",
      "model": "gpt-5-mini"
    },
    {
      "id": "scoring_overview",
      "title": "Scoring Framework Overview",
      "html": "<section><h2>Introduction</h2><p>This methodology chapter sets out the architecture and procedural safeguards for a cyber-attribution scoring system. The account that follows is descriptive and methodological: it explains how source documents are ingested, how discrete claim-level signals are extracted and normalized, how references and institutional provenance are inferred, and how those signals are synthesized into document-level judgments using explicit inferential weighting. The rationale for each design choice is presented in general terms so as to permit reproducibility and independent critique while avoiding exposition of any case-specific substantive findings in the underlying report.</p></section><section><h2>Scope and Units of Analysis</h2><p>The primary units of analysis are claims and documents. A claim-level unit corresponds to a discrete assertion or proposition expressed within a source, which can be mapped to evidence spans during extraction. A document-level unit corresponds to the complete authored artifact from which one or more claims are derived. The scoring architecture maintains a strict separation between claim-level outputs produced by extraction processes and the document-level synthesis performed by aggregation and calibration routines; this separation preserves traceability from final scores back to the originating evidence spans.</p></section><section><h2>Data Ingestion</h2><p>Data ingestion is designed to preserve original artifacts and to capture a chain of custody. Raw inputs include native PDFs, images, figures, and ancillary metadata. Metadata recording captures provenance fields and timestamps at point of receipt. The pipeline is instrumented to log which conversion path was applied to each artifact so that downstream consumers can account for modality-specific error characteristics. The section_data_contract_keys supplied with source metadata are recorded as part of the ingest manifest to ensure schema conformance during subsequent processing.</p></section><section><h2>PDF-to-Markdown Conversion</h2><p>Conversion of PDFs into machine-readable text proceeds along a primary and fallback path. The primary conversion is performed using a provider-backed Mistral OCR conversion (denoted process_pdf_mistral_ocr.py in the pipeline_methods), with an offline fallback via PyMuPDF4LLM when the primary path is unavailable. Each conversion run emits quality metrics that capture OCR confidence, detected page layout structures, and any warnings about non-text elements. The explicit recording of which converter was used enables later calibration of extraction confidence and allows inferential weighting to account for modality-dependent noise.</p></section><section><h2>Structural Parsing</h2><p>Structural parsing organizes converted text into logical regions such as headings, paragraphs, tables, and figures. The stage1 markdown parse identifies candidate tables and images and produces a structural token stream that downstream extractors consume. Detected structural elements are annotated with spatial and semantic attributes so that claim extraction can prefer contiguous narrative spans for assertions and dedicated table parsers for tabular evidence. Structural parsing also records uncertainty where layout ambiguity is present; these uncertainties propagate to claim-level confidence scores rather than being resolved deterministically.</p></section><section><h2>Artifact Extraction</h2><p>Artifact extraction uses schema extraction artifact indices to identify and extract discrete artifacts: named entities, asserted actions, temporal markers, technical indicators, and referenced artifacts such as hash values or network indicators. Extraction outputs are represented in a normalized schema that aligns with the claim-level unit model; each extracted artifact includes provenance pointers to the original document coordinates and to the conversion method logged during ingestion. The separation between extraction outputs and subsequent inferential weighting is deliberate: extraction reports are treated as observed signals, not as calibrated judgments.</p></section><section><h2>Reference Parsing</h2><p>Reference parsing links in-text citations and bibliographic entries to external sources. The pipeline records citations + references linked to sources and attempts to resolve DOIs, URLs, and publisher identifiers where present. Resolution outcomes are recorded with confidence metadata and with a provenance chain showing whether the reference was resolved during the initial pass or flagged for manual review. Reference parsing supports both direct corroboration of claims and the construction of evidence graphs that enumerate inter-document relations.</p></section><section><h2>Institution Inference</h2><p>Institution inference classifies the apparent institutional origin or affiliation associated with a document or reference. The pipeline employs a primary automated classifier (gpt-5-mini institution classification) with a recorded web fallback when the automated classification yields low confidence. Institutional labels are accompanied by confidence scores and provenance traces that show which textual features or external signals informed the classification. Institutional inference is encoded as an independent signal that can inform inferential weighting but is not determinative by itself.</p></section><section><h2>Claim Evidence Graph</h2><p>Extracted claims and references are assembled into a claim evidence graph that represents claims as nodes and evidentiary relations as typed edges. Each edge encodes the nature of support (direct quotation, paraphrase, citation, corroboration, contradiction) and is annotated with the extraction provenance and quality metrics derived from earlier stages. The claim evidence graph preserves the distinction between observed extraction outputs and interpretive relations that are introduced during synthesis; this enables auditors to review both raw signals and the inferential steps that connect them.</p></section><section><h2>Scoring Framework Overview</h2><p>The scoring architecture is hierarchical. At the claim-level, discrete axes are scored for evidentiary support, temporal proximity, technical plausibility, and provenance reliability. These axes are calculated from normalized extraction outputs and from quantitative metadata such as OCR confidence and reference resolution success. Document-level synthesis aggregates claim-level axes using explicit inferential weighting that is specified a priori and documented in the scoring bundle. Inferential weighting is applied only after claim-level signals have been produced and logged; weights are parameterized so that alternative calibrations may be applied for sensitivity analysis without re-running extraction.</p></section><section><h2>Chain of Custody</h2><p>The chain of custody records the provenance of each artifact and each derived signal from receipt through conversion, parsing, extraction, and scoring. Logs record the exact pipeline method invoked (including process_pdf_mistral_ocr.py and PyMuPDF4LLM where applicable), the version of the parsers, and any manual interventions. The custody record ensures reproducibility and supports later forensic review of any score by linking outputs unambiguously back to file-level inputs and processing steps.</p></section><section><h2>Credibility Corroboration</h2><p>Credibility corroboration procedures evaluate corroboration across independent sources and across distinct evidence types (e.g., textual, tabular, digital artifacts). The system differentiates corroboration that arises from shared references from corroboration that arises from independent observation. Corroboration strength is represented as a separate axis that feeds into document-level aggregation but is maintained as an auditable signal distinct from extraction confidence to prevent conflation of correlated errors.</p></section><section><h2>Clarity Axis</h2><p>The clarity axis measures how explicitly a claim is stated versus implicitly inferred. Clarity is scored on the basis of syntactic and discourse markers identified during structural parsing and by the proximity of supporting evidence spans. This axis is used to distinguish well-articulated claims, which warrant higher baseline weight in aggregation, from speculative or inferential language, which receives conservative down-weighting in inferential weighting schemes.</p></section><section><h2>Aggregation and Calibration</h2><p>Document-level aggregation combines claim-level axis scores using the pre-specified weighting scheme. Calibration is performed on held-out validation sets and through expert adjudication to align aggregated scores with domain-relevant benchmarks. Calibration datasets and their performance statistics are recorded separately from operational run data so that policy changes to inferential weighting can",
      "model": "gpt-5-mini"
    },
    {
      "id": "chain_of_custody",
      "title": "Chain of Custody Axis",
      "html": "<h2>Introduction</h2>\n<p>This methodology chapter defines the chain-of-custody axis as an integral component of a broader cyber-attribution scoring framework. The exposition below treats chain-of-custody as a set of measurable variables and procedural controls that support inferential claims without presenting or adjudicating case-specific factual findings. The approach is grounded in the available input schema and raw data contract; where the provided raw data fields are null or absent, the method specifies required metadata collection and fallback procedures to preserve analytical rigor.</p>\n\n<h2>Scope and Units of Analysis</h2>\n<p>The scope of analysis encompasses discrete artifacts and document-level units that contribute evidence to attribution assertions. Units of analysis include source documents, extracted artifacts, cryptographic artifacts, network captures, and metadata records. Each unit is represented by a minimal set of identifiers and metadata sufficient to evaluate provenance, integrity, time anchors, artifact identifiers, and versioning. In the supplied context the keys document_scores_v4, claim_score_preview_v4, raw_artifacts_preview, and scoring_bundle are present in the contract but their current values are null; this absence informs the treatment of missing data and the imposition of conservative assumptions in downstream scoring.</p>\n\n<h2>Data Ingestion</h2>\n<p>Data ingestion prescribes canonicalized intake procedures for raw files and metadata. Ingested records must capture the original source location, acquisition method, acquisition timestamp, operator identity, and any intermediary processing steps. When the ingestion pipeline encounters null or absent fields (as observed in the provided raw data contract), it triggers provenance flags that downgrade automatic confidence and initiate auxiliary acquisition requests. The rationale for conservative downgrading is to ensure that scores reflect not only content but also the quality of custody metadata.</p>\n\n<h2>PDF to Markdown Conversion</h2>\n<p>Conversion of opaque formats to machine-readable representations follows a documented transformation pipeline that preserves source identifiers and records transformation checksums. Each conversion step appends metadata that records the conversion tool, versioning, configuration parameters, and a checksum of the output. These recorded artifacts feed into integrity assessments and provide time anchors that are used to demonstrate temporal continuity from acquisition to analysis. Conversion artifacts are treated as first-class items for chain-of-custody evaluation.</p>\n\n<h2>Structural Parsing</h2>\n<p>Structural parsing isolates semantic components (headers, footnotes, captions, embedded media) while preserving positional metadata. The parser annotates each extracted element with an artifact identifier and attaches provenance attributes back to the source document. Structural parsing thereby enables granular integrity checks and supports later reconstructions of the evidence trail should questions about tampering or misattribution arise.</p>\n\n<h2>Artifact Extraction</h2>\n<p>Artifact extraction produces discrete objects suitable for cryptographic hashing and independent validation. For each artifact the pipeline records an artifact identifier, creation and acquisition timestamps, the applied extraction routine and its versioning, and an integrity checksum. These records are fundamental to linking content to custody metadata and to establishing time anchors that bound the artifact's temporal provenance.</p>\n\n<h2>Reference Parsing</h2>\n<p>Reference parsing identifies citations, external links, and embedded references and maps them to canonical external identifiers where possible. The parser logs match confidence and source provenance for each resolved reference. In the event of unresolved references or ambiguous mappings, the system propagates uncertainty to the chain-of-custody assessment to avoid overstating linkage strength.</p>\n\n<h2>Institution Inference</h2>\n<p>Institution inference is conducted on the basis of explicit metadata, document signatures, infrastructure indicators, and corroborating references. The inference procedure is probabilistic and records the contributing features and their weights. Because institution inference is sensitive to provenance and integrity, the model conditions its output on the quality flags produced during ingestion and artifact extraction; null or missing upstream metadata (as in the provided contract) yields wider posterior uncertainty distributions.</p>\n\n<h2>Claim–Evidence Graph</h2>\n<p>The claim–evidence graph formalizes links between inferential claims and supporting artifacts. Nodes in the graph are annotated with provenance and integrity metadata and with time anchors that indicate when the artifact was observed and when it was asserted into the graph. Edges encode the nature of support (direct quote, paraphrase, derived analysis) and are weighted by the assessed quality of custody metadata and by any versioning relationships between nodes.</p>\n\n<h2>Scoring Overview</h2>\n<p>Scoring integrates multiple axes including technical indicators, contextual corroboration, and chain-of-custody quality. The scoring function treats chain-of-custody attributes as modifiers that scale evidentiary weight. Where raw data fields required for chain-of-custody computation are null, the scoring function applies conservative priors and documents the uncertainty adjustment applied to final scores. Counts or scores may be used illustratively to demonstrate how a missing provenance flag reduces effective weight, but such numeric exemplars are methodological, not substantive.</p>\n\n<h2>Chain of Custody Variables and Operationalization</h2>\n<p>The core chain-of-custody variables are operationalized as follows. Provenance denotes the documented origin and acquisition pathway of an artifact and is represented by source identifiers, acquisition logs, and operator credentials. Integrity is evaluated through cryptographic checksums, digital signatures, and checksum lineage; failures or absence of integrity evidence are recorded as integrity exceptions. Time anchors comprise acquisition timestamps, embedded document timestamps, and externally verifiable temporal references; the methodology distinguishes creation time, acquisition time, and ingestion time and treats discrepancies as temporal uncertainty. Artifact identifiers are stable, collision-resistant labels assigned at intake and preserved through transformations. Versioning captures both semantic and binary changes to artifacts and is tracked via diffs, version tags, and change logs.</p>\n\n<h2>Quality Controls and Penalties</h2>\n<p>Quality controls mandate that each artifact carry minimally required metadata to avoid automatic penalization. Verification steps include checksum recomputation, signature validation, operator attestation checks, and cross-source temporal reconciliation. Penalties are structured as graduated score reductions and provenance flags: missing provenance metadata results in an initial penalty, failed integrity checks produce larger deductions, ambiguous or conflicting time anchors incur uncertainty penalties, unversioned artifacts receive procedural penalties proportional to potential impact, and identifier collisions trigger investigation and further score suppression. Penalties are applied transparently and recorded in audit logs so that recalibration or contestation can be performed with a clear evidentiary trail.</p>\n\n<h2>Credibility Corroboration</h2>\n<p>Corroboration procedures synthesize independent lines of evidence and prefer artifacts with higher provenance and integrity scores. The methodology favors redundancy and temporal sequencing to strengthen credibility. When corroborating",
      "model": "gpt-5-mini"
    },
    {
      "id": "credibility_corroboration",
      "title": "Credibility Axis with Corroboration Subcomponent",
      "html": "<div>\n  <h2>Introduction</h2>\n  <p>This methodology chapter specifies a reproducible approach for assessing the credibility of sources and the corroboration of claims within a cyber-attribution scoring framework. The approach is normative and procedural: it defines how input artifacts are transformed into structured evidence, how source attributes inform weighting decisions, and how corroboration is operationalized without adjudicating or reporting case-specific factual findings. The methodology is grounded in the supplied data contract keys; however, in the present instance the preview fields (source_type_counts, raw_sources_preview, document_scores_v4, claim_score_preview_v4, scoring_bundle) are null. Where those raw data elements are present, they are the primary inputs to the processes described below and determine quantitative parameterization of the procedures.</p>\n\n  <h2>Scope and Units</h2>\n  <p>The unit of analysis is the claim-evidence pair: an asserted fact or inference (a claim) together with the document-level and artifact-level evidence that purports to support it. Scope is limited to documentary and digital artifacts ingested under the system’s data contract. The framework distinguishes source-level attributes (authorship, provenance, publication channel) from evidence-level attributes (timestamping, cryptographic fingerprints, embedded metadata) so that scoring remains sensitive to both the origin and the content of artifacts.</p>\n\n  <h2>Data Ingestion</h2>\n  <p>Ingestion begins with canonicalizing incoming documents according to the data contract keys. The system expects structured counts and previews (source_type_counts and raw_sources_preview) to inform downstream heuristics; when present, document_scores_v4 and claim_score_preview_v4 provide prior assessments that may be used as features for calibration. All ingested items receive a persistent identifier and metadata record capturing capture time, source channel, and the ingesting agent.</p>\n\n  <h2>PDF-to-Markdown Conversion</h2>\n  <p>Binary or image-based documents are converted into machine-readable text using OCR and layout-aware PDF parsing. Conversion preserves structural elements (headings, captions, footnotes) and records confidence scores for each extracted element. The conversion step produces a normalized markdown-like intermediate, which facilitates consistent downstream parsing while retaining provenance metadata from the original file.</p>\n\n  <h2>Structural Parsing</h2>\n  <p>Structural parsing decomposes documents into title, abstract, body, figures, tables, and references. Each element inherits provenance metadata and a token-level confidence measure. Structural parsing also identifies sections likely to contain claims (executive summaries, conclusions, captions) so that extraction prioritizes high-yield regions for claim detection and reference linking.</p>\n\n  <h2>Artifact Extraction</h2>\n  <p>Artifact extraction isolates discrete evidentiary items: quoted text segments, images with embedded metadata, header fields, and binary payloads. Each artifact is hashed and stored in an artifact registry to facilitate deduplication and to enable chains of custody. Extraction records sources of uncertainty such as OCR errors or image compression artifacts and attaches these as attributes used later in scoring.</p>\n\n  <h2>Reference Parsing</h2>\n  <p>References and in-text citations are parsed into structured reference records. Where DOI, URL, or bibliographic entries are present, the system resolves these to canonical identifiers. Reference parsing records the citation context (supporting/contrasting) and links back to the artifact registry. In the present instance the raw_sources_preview field is null; when available, it supplies sample references that are used to validate parsing heuristics and train disambiguation models.</p>\n\n  <h2>Institution Inference</h2>\n  <p>Institution inference maps author and publisher metadata to canonical institutional identifiers. This uses name authority lists, domain registration records, and cross-referencing with known organizational registries. Inference produces an institution confidence score reflecting ambiguity (for example, multiple entities sharing similar names or domains). Institution-level attributes feed into the source hierarchy by distinguishing independent institutional provenance from potentially colocated or affiliated outputs.</p>\n\n  <h2>Claim–Evidence Graph Construction</h2>\n  <p>Extracted claims and their supporting artifacts are represented as a directed graph: nodes are claims or evidence artifacts and edges denote support, contradiction, or derivation. Each edge is annotated with provenance metadata and an evidence confidence derived from extraction and parsing scores. The claim evidence graph enables efficient evaluation of corroboration across multiple, potentially heterogenous sources and supports visibility into chains of inference.</p>\n\n  <h2>Scoring Overview</h2>\n  <p>The scoring framework produces multi-axial outputs; principal among them for this chapter is the Credibility Axis with a Corroboration subcomponent. Scores combine source-level priors (source hierarchy placement, institutional inference), artifact-level integrity attributes (hashing, metadata), and graph-level measures (number of independent supporting nodes, depth of derivation). Where available, document_scores_v4 and claim_score_preview_v4 serve as prior feature inputs to supervised calibration; in their absence, the framework uses domain-informed default priors and transparent uncertainty bounds.</p>\n\n  <h2>Chain of Custody</h2>\n  <p>Every artifact and derived node in the claim evidence graph maintains a chain-of-custody record recording ingest timestamps, processing actors, and transformation operations. Chain-of-custody attributes are incorporated into credibility calculations: artifacts with incomplete or ambiguous custody records receive down-weighting commensurate with the assessed risk of tampering or misattribution.</p>\n\n  <h2>Credibility Axis with Corroboration Subcomponent</h2>\n  <p>The credibility model operationalizes the terms source hierarchy, independence, corroboration, claim coverage, and exclusion criteria as follows. Source hierarchy is a taxonomy that classifies sources into primary (originator materials such as original logs or signed statements), secondary (reports that synthesize or interpret primary materials), and tertiary (summaries, repostings, or derivative analyses). Institutional inference informs placement within this hierarchy and supplies an institution-level confidence. Independence is assessed by analyzing shared provenance signals: identical artifacts, shared author lists, common hosting domains, or reuse of institutional boilerplate are indicators of non-independence. Independence is treated as a structural attribute on edges in the claim evidence graph and quantifies the marginal evidentiary value added by an additional supporting node.</p>\n\n  <p>Corroboration rules combine independence-adjusted counts with qualitative alignment of supporting artifacts. A corroborative endorsement increments a claim’s corroboration score only if it (a) originates from a source classified at or above a minimum hierarchy threshold, (b) passes a minimal chain-of-custody integrity test, and (c) is sufficiently independent from existing supporters as measured by provenance overlap metrics. Corroboration therefore rewards convergence from distinct, credible lines of evidence rather than mere multiplicity of copies.</p>\n\n  <p>Claim coverage scaling addresses the breadth and directness of support. Direct, substantive matches between an artifact and a claim (for example, a timestamped log entry that explicitly records the asserted action) contribute more to coverage than indirect or inferential references. Coverage is scaled by the proportion of claim elements (actors, actions, dates, locations, technical signatures",
      "model": "gpt-5-mini"
    },
    {
      "id": "clarity_axis",
      "title": "Clarity Axis and State Responsibility Pathways",
      "html": "<h2>Introduction</h2>\n<p>This methodological chapter sets out an operational approach to scoring clarity in cyber-attribution where allegations implicate state responsibility. It is written in legal-academic register and directed at producing reproducible, transparent judgments about how clearly available evidence supports attribution along three recognized state-responsibility pathways: acts by state organs, control over non-state actors, and failures of due diligence. The objective is not to adjudicate particular events but to define the evidentiary architecture, processing pipeline, and scoring conventions that translate heterogeneous documentary artifacts into a graded clarity measure suitable for comparative analysis and legal consideration.</p>\n\n<h2>Scope and Measurement Units</h2>\n<p>The unit of measurement for the clarity axis is the claim-evidence unit, defined as a discrete attribution claim paired with the set of primary and corroborative artifacts that speak to it. Each claim-evidence unit is assessed for (a) the legal pathway invoked (state organs, control over non-state actors, or due diligence failure), (b) the evidentiary modalities present (technical forensics, command-and-control metadata, human source testimony, documentary linkage, or pattern-of-life contextualization), and (c) provenance and processing metadata necessary for chain-of-custody assessment. These structured units permit aggregation while preserving the provenance needed for interpretive caution in matters of state responsibility.</p>\n\n<h2>Data Ingestion and Document Processing</h2>\n<p>Data ingestion begins with the acquisition of source documents in native formats. Documents are normalized into a working representation that preserves original layout, timestamps, authorship metadata, and embedded objects. When source material is PDF, optical character recognition and layout-aware conversion are applied; the process is designed to preserve page ordering, headings, footnotes, figures, and annexes so that subsequent structural parsing can reference original offsets. Given the supplied contextual metadata in this instance—specifically the document-level keys document_scores_v4, claim_score_preview_v4, scoring_bundle, and raw_claims_preview—were present but contain null values, the pipeline must shift to a mode that emphasizes explicit artifact extraction and manual validation rather than automated score inheritance.</p>\n\n<h2>Structural Parsing and Artifact Extraction</h2>\n<p>Structural parsing segments converted documents into hierarchical elements—titles, paragraphs, tables, figures, captions, and bibliographic references—while capturing positional and formatting cues that assist in identifying assertions, quotations, and evidentiary exhibits. Artifact extraction then identifies candidate evidence objects for attribution analysis: technical artifacts (hashes, IP addresses, timestamps, malware signatures), human-sourced materials (declarations, intercepted communications), and documentary linkages (contracts, organizational charts, procurement records). Each extracted artifact is recorded with provenance metadata sufficient to reconstruct the extraction pathway, including the original document identifier and the extraction method used.</p>\n\n<h2>Reference Parsing and Institution Inference</h2>\n<p>Reference parsing isolates cited materials and cross-references within and across documents to build a preliminary citation graph. Institution inference applies named-entity recognition and contextual heuristics to map parties, organizational roles, and institutional",
      "model": "gpt-5-mini"
    },
    {
      "id": "aggregation_calibration",
      "title": "Aggregation, Calibration, and Uncertainty",
      "html": "<h2>Introduction</h2>\n<p>This methodology describes the principled steps used for claim-to-document aggregation, weighting, calibration, and uncertainty quantification within a cyber-attribution scoring pipeline. The procedure is intended to be transparent, reproducible, and explicitly grounded in the available portfolio-level and document-level measurements extracted from the source artefacts. To illustrate the scale and heterogeneity that the method must accommodate, the supplied portfolio summary reports a document_count of 5 and aggregated_totals that include 38 claims, 14 sources, and 25 artifacts; these counts demonstrate the common situation of unequal claim density and evidence richness across documents. The exposition below emphasizes methodological rationale rather than case-specific findings and uses the supplied numeric diagnostics to illustrate how aggregation, calibration, and dispersion controls are operationalised. The goal of the pipeline is to convert heterogeneous, partially observed evidence into calibrated, uncertainty-aware scores while preserving provenance and enabling independent replication.</p>\n\n<h2>Data processing and evidence extraction</h2>\n<p>Raw inputs are first normalised through a sequence of extraction stages that produce structured claim records, artefact linkages, and source metadata. Each document is parsed to produce standardized fields such as pages, tables, figures, claims, sources, and artifacts. Per-document claim counts in the supplied set vary across documents, motivating per-claim weighting rather than uniform aggregation. Claim extraction assigns an evidence list to each claim by linking artefact identifiers and source citations, and documents with incomplete or missing numerical fields are flagged for separate handling. The data processing stage emphasises deterministic extraction rules, canonical identifier mapping for artefacts and sources, and explicit recording of missingness so that downstream scoring can treat absent indicators as informative rather than silently dropping them. All intermediate outputs include provenance pointers to their originating document and extraction step to enable later audit and review.</p>\n\n<h2>References and institution inference</h2>\n<p>Reference resolution and institution inference operate on the normalised source metadata to produce inferred institutional attributions and confidence annotations. The method first canonicalises citation strings and maps organization names to controlled vocabularies where possible, then exploits co-occurrence across cited sources and artefacts to strengthen or weaken institutional links. When explicit institutional metadata is unavailable, the pipeline applies conservative heuristics based on recurring affiliations, email domains, and publisher identifiers, and attaches an uncertainty score reflecting the strength of the inference. This approach preserves the distinction between direct source identity, inferred institution, and purely provenance-level citations, and it propagates inference uncertainty into the later scoring stages rather than collapsing it prematurely into binary assignments.</p>\n\n<h2>Scoring framework</h2>\n<p>The scoring framework translates structured claims, artefact reliability indicators, and source/institution confidence into probabilistic scores for attribution-related assertions. Aggregation applies claim-level weights that reflect document-specific claim density, source reliability, and artifact-level trust metrics; documents with higher claim counts are not permitted to dominate simply by volume because weights include normalization factors to correct for unequal claim density. Calibration is implemented using holdout diagnostics and within-portfolio calibration curves, with score transforms designed to produce well-calibrated probabilities under varied evidence richness. Uncertainty quantification uses ensemble and resampling techniques to estimate score dispersion and to produce interval estimates; these dispersion measures inform downstream decision thresholds and are reported alongside point estimates to avoid overconfident interpretations.</p>\n\n<h2>Validation and quality assurance</h2>\n<p>Validation proceeds through layered checks that include automated consistency tests, held-out calibration assessment, and targeted manual review of high-impact cases. Quality assurance requires that all claims preserve provenance chains back to original artefacts and that any imputation of missing fields is logged with method and assumed distribution. Sensitivity analyses examine how score outcomes shift under alternative weighting schemes and under removal of individual high-leverage documents or artefacts. Operational controls include periodic recalibration against new held-out data, logging of threshold adjustments, and explicit metrics for calibration error and discrimination that are used to gate release of scores. Together these practices aim to ensure the scoring pipeline is robust, auditable, and conservative where evidence is sparse or ambiguous.</p>",
      "model": "gpt-5-mini"
    },
    {
      "id": "validation_quality_assurance",
      "title": "Validation and Quality Assurance",
      "html": "<section aria-labelledby=\"validation-quality-assurance\"><h2 id=\"validation-quality-assurance\">Validation and Quality Assurance</h2><p>This Validation and Quality Assurance section describes the procedural safeguards and automated controls applied to the attribution scoring pipeline. The protocol is grounded in the project data contract and the available raw payload metadata, including the portfolio summary manifest located at <code>/home/pantera/projects/TEIA/annotarium/outputs/reports/portfolio_summary.json</code> and the set of input report artifacts enumerated under raw payload paths. The validation design operates independently of substantive case findings: it is intended to provide reproducible, auditable assurance that structural parsing, artifact extraction, and downstream scoring computations were executed in accordance with schema and integrity expectations.</p><p>Automated validation gates form the primary line of defense against data corruption and processing errors. These gates are implemented as deterministic, idempotent checks that run at defined stages of the pipeline: immediately after ingestion, following pdf-to-markdown conversion and structural parsing, and after the construction of the claim–evidence graph and scoring aggregation. Typical gate checks include schema conformance (field presence and type constraints against the processing contract), checksum and size validation against the supplied file metadata, timestamp and provenance consistency, duplicate artifact detection, and basic plausibility checks (for example, that extracted artifact types correspond to allowed categories). The validation bundle parameter is recorded as null in the supplied section metadata; nevertheless, the gates reference the explicit raw payload paths and file metadata to verify that the expected inputs were present and unmodified prior to scoring.</p><p>Where automated validation gates detect anomalies, the system routes affected items into an agent review queue for triage. The agent review capability is enabled in the documented QA protocol and executes a layered set of remediation actions: automated re-ingestion attempts, adaptive parser selection, and escalations that append diagnostic metadata to the artifact so that subsequent reviewers see the precise failure mode. The design rationale for agent review is to maximize throughput while ensuring that automated corrective steps are attempted before human resources are committed.</p><p>Human review is applied in a targeted, sampled fashion to provide an independent check on automated processing and to estimate residual error rates. The QA protocol specifies a targeted human review fraction of 10% of processed artifacts, selected by a stratified sampling strategy that prioritizes items with high analytic impact and those that triggered non-fatal validation warnings. For the sampled subset the observed error rate reported in the protocol is zero; explicitly, the human_sample_observed_error_rate is 0.0 and the documentation states that the reviewed 10% sample displayed no observed errors. This outcome is reported with the caveat that sampling uncertainty remains: a zero observed error rate in the sample reduces but does not eliminate the possibility of unobserved faults in the population. Consequently, the protocol defines escalation thresholds—if any error is detected during human review, the pipeline will reprocess the entire affected batch and increase the human sampling fraction for one or more subsequent runs.</p><p>Quality assurance practice mandates full auditability: all validation gates, agent review actions, human reviewer annotations, and reprocessing events are persisted in an immutable audit log tied to the original raw payload paths and file metadata (for example, the portfolio_summary.json path and attributes). Error classification taxonomy and remediation outcomes are recorded to enable retrospective calibration of validation gates and to support statistical estimates of residual uncertainty. Together, these elements comprise a defensible QA posture that combines deterministic validation gates, agent review automation, and targeted human review at a 10% sampling rate, with the sampled review yielding no observed errors in the reviewed subset.</p></section>",
      "model": "gpt-5-mini"
    },
    {
      "id": "limitations_governance",
      "title": "Limitations, Governance, and Future Refinement",
      "html": "<h2>Introduction</h2>\n<p>This methodological supplement describes limits, governance controls, and a principled refinement roadmap for a reproducible cyber‑attribution scoring pipeline. It is written as a legal‑academic procedural statement that emphasizes evidentiary legibility and auditable transformations rather than asserting substantive conclusions about particular cases. To ground the discussion, the remarks below refer to metadata and summary statistics drawn from the present processing run, including a corpus footprint (document_count = 5, generated_at_utc = 2026-02-22T11:31:26Z) and aggregated totals for the record (pages = 5, tables = 31, figures = 40, claims = 38, sources = 14, artifacts = 25). These figures are cited only to illustrate how the method behaves under modest corpus scale and heterogeneity and not to support or repeat any factual claims within the underlying reports.</p>\n\n<h2>Scope and Units of Analysis</h2>\n<p>The unit of analysis is the proposition as encoded in a claim object and its attached evidentiary graph. The pipeline enforces a strict schema that separates claims, sources, artifacts, and explicit evidence links; this design decision preserves inspectable chains of reasoning and compels analysts to situate every attribution proposition in discrete, referenceable terms. At the portfolio level, the present run produced 38 claim objects across five documents, a distribution that the scoring model treats as a weighted claim set rather than an undifferentiated pool of assertions. Treating claims as the primary analytic units clarifies the locus of limitations and governance controls discussed below.</p>\n\n<h2>Data Ingestion</h2>\n<p>Data ingestion begins with source acquisition and checksumed archival of native PDF files. All ingestion events are time‑stamped and persisted. The pipeline records a provenance envelope for each run: the path to the source bundle, the generated_at_utc marker, and the derived metadata. These persisted artifacts permit later reconstruction of any score and function as an initial control against accidental or clandestine modification of source materials.</p>\n\n<h2>PDF to Markdown Transcription</h2>\n<p>PDF→markdown transcription is deterministic where possible and supplemented by constrained, schema‑aware model assistance. The transcription stage outputs a markdown representation and an index of artifact anchors. The system fails runs if artifact anchors are missing or if identifier collisions are detected, enforcing an input integrity requirement prior to structural parsing.</p>\n\n<h2>Structural Parsing</h2>\n<p>Structural parsing maps markdown into schema objects: claims, sources, artifacts, and evidence links. The parser enforces required fields and runs automated integrity checks. In the present run, integrity controls prevented propagation of malformed records; these checks are a core governance control because they delimit which records may enter the scoring pipeline.</p>\n\n<h2>Artifact Extraction</h2>\n<p>Artifact extraction isolates primary materials (e.g., logs, binaries, indicators) and records available provenance signals such as timestamps, identifiers, and asserted handling histories. The pipeline treats extracted artifact counts (present run: artifacts = 25 across the portfolio) as contextual information that informs item‑level weight but does not by itself determine claim weight.</p>\n\n<h2>Reference Parsing</h2>\n<p>Reference parsing identifies cited documents and metadata and attempts to resolve them to canonical identifiers. The system records unresolved references and treats them as suspended support rather than discarding them. By doing so the method maintains conservative evidence accounting and surfaces weaknesses in the citation network.</p>\n\n<h2>Institution Inference</h2>\n<p>Institution inference maps source identifiers to institutional types used in the source‑quality model (e.g., international institution, peer‑reviewed academic, government, NGO). This mapping is explicit and auditable. Institutional classifications influence the source‑quality axis but are applied according to a documented taxonomy; the present portfolio-level averages (for example, average chain_of_custody ≈ 0.52182 and average clarity",
      "model": "gpt-5-mini"
    }
  ],
  "html": "<article class=\"wiki-page\">\n<header><h1>Annotarium Methodology: Portfolio View (5 documents)</h1><div class=\"wiki-meta\">Generated at 2026-02-22T22:11:11Z</div></header>\n<nav class=\"wiki-toc\"><h2>Contents</h2><ol>\n<li><a href=\"#sec-introduction\">Introduction and Epistemic Framing</a></li><li><a href=\"#sec-scope_units\">Scope, Units of Analysis, and Output Semantics</a></li><li><a href=\"#sec-data_ingestion\">Data Ingestion and Corpus Handling</a></li><li><a href=\"#sec-pdf_to_markdown\">PDF-to-Markdown Conversion</a></li><li><a href=\"#sec-structural_parsing\">Structural Parsing of Text, Tables, and Figures</a></li><li><a href=\"#sec-artifact_extraction\">Artifact Extraction and Technical Object Normalization</a></li><li><a href=\"#sec-reference_parsing\">Footnote and Reference Parsing</a></li><li><a href=\"#sec-institution_inference\">Institution Inference and Source Typology</a></li><li><a href=\"#sec-claim_evidence_graph\">Claim-Evidence Graph Construction</a></li><li><a href=\"#sec-scoring_overview\">Scoring Framework Overview</a></li><li><a href=\"#sec-chain_of_custody\">Chain of Custody Axis</a></li><li><a href=\"#sec-credibility_corroboration\">Credibility Axis with Corroboration Subcomponent</a></li><li><a href=\"#sec-clarity_axis\">Clarity Axis and State Responsibility Pathways</a></li><li><a href=\"#sec-aggregation_calibration\">Aggregation, Calibration, and Uncertainty</a></li><li><a href=\"#sec-validation_quality_assurance\">Validation and Quality Assurance</a></li><li><a href=\"#sec-limitations_governance\">Limitations, Governance, and Future Refinement</a></li>\n</ol></nav>\n<section class=\"wiki-section\" id=\"sec-introduction\"><h2>Introduction and Epistemic Framing</h2>\n<p>This methodology is presented as an evidentiary framework for cyber attribution where findings are contestable and must survive adversarial scrutiny. The epistemic posture adopted is jurisprudential: the evaluator treats attribution claims as arguments composed of heterogeneous materials rather than as singular declarative acts. The analysis is burden-sensitive in that inferential weight is allocated according to the demands of contested proof — stronger connections are required where consequences are greater and where the record is thin. The approach privileges structured extraction of evidentiary units prior to any legal inference so that chain-of-reasoning remains transparent and reconstructible.</p>\n\n<h2>Data Processing and Extraction</h2>\n<p>Operationally, the corpus consists of PDF-format attribution reports that are transcribed and parsed into schema-constrained records. As reflected in the provided methodology excerpt, the pipeline proceeds from PDF to markdown and then into structured objects separating claims, artifacts, sources, and links. The pipeline methods supplied for this project include an OCR/transcription primary step (process_pdf_mistral_ocr.py) with a PyMuPDF fallback, stage-one markdown parsing for images and tables, and schema-driven artifact extraction. These deterministic and persistenced transformations are required to prevent implicit reconstruction of evidentiary support and to enable later audit of extraction choices.</p>\n\n<p>Schema constraints enforce explicit anchors for evidentiary references and surface integrity checks to fail runs with identifier collisions, missing anchors, or unresolved citation pathways. This preserves admissibility discipline: only structurally intact records move to scoring. The corpus preview accompanying this task shows heterogeneous metadata fields (for example, per-report metrics such as \"credibility_composite_avg_0_100\" and \"sources_total\") whose variation is used illustratively to demonstrate how the extraction pipeline must ingest, normalize, and retain provenance attributes for each item without collapsing them into a single undifferentiated score.</p>\n\n<h2>Reference Parsing and Institution Inference</h2>\n<p>Reference parsing is performed as a distinct stage that links in-text citations and bibliography entries to discrete source objects in the schema. The design separates citation parsing from institution inference so that source identity and institutional classification remain auditable. Institution inference is performed with a constrained language model assist (noted in the pipeline methods as gpt-5-mini institution classification) with a web-backed fallback for ambiguous cases; all inferred institution attributions persist as candidate labels with provenance metadata recording the inference method and confidence bounds. This preserves contestability: an institution label is an evidentiary claim about origin, subject to cross-checking against primary source anchors.</p>\n\n<h2>Scoring Framework</h2>\n<p>The scoring layer is ICJ-inspired and multi-axial. Item-level weightings evaluate independence, authentication/provenance, methodological soundness, procedural testing, and contemporaneity; the multiplicative formulation prevents compensation across critical axes. Corroboration is computed as convergence across independent origins with diminishing returns and an explicit anti-circularity rule to avoid treating downstream repetition as independent support. The top-level axes — Chain of Custody, Credibility, and Clarity — are computed from bounded, auditable subcomponents. Chain of Custody combines textual provenance markers, integrity indicators, temporal anchors, artifact identifiers, and update lineage into a normalized custody score. Credibility combines source-type quality, source diversity, and corroborative convergence, and is calibrated by a claim-coverage factor so that high-quality sources must cover a proportion of claims to materially raise document-level credibility. Clarity operationalizes legal intelligibility by mapping claim-level reasoning to doctrinal pathways (state organ conduct, direction/control of non-state actors, omission/due diligence) so that legal inferences are only scored where the report supplies the requisite act–actor–link specificity.</p>\n\n<h2>Validation and Quality Assurance</h2>\n<p>Integrity checks and validation are treated as admissibility controls. Runs that exhibit identifier collisions, unresolved anchors, or citation mismatches are failed and returned for remediation. Post-parsing validation includes provenance re-walks, institution inference audits, and spot-checks of artifact extraction against original markup. Scoring outputs are reproducible because intermediate artifacts are persisted: every top-level score can be traced to the claim-level records and the extracted evidence items. Quality assurance includes unit tests for deterministic pipeline stages, manual review of model-assisted inferences, and calibration exercises that demonstrate how the framework responds to variation in source quality metrics (as seen in the corpus preview) without converting parser convenience into evidentiary strength.</p>\n\n<p>Finally, the methodology acknowledges limits: the framework is designed to render contestable assessments transparent and auditable, not to resolve contested factual questions. It therefore records uncertainty explicitly, treats high-impact inferences as burden-sensitive, and preserves the evidentiary chain for subsequent legal or scholarly adjudication.</p></section>\n<section class=\"wiki-section\" id=\"sec-scope_units\"><section><h2>Introduction</h2><p>This methodology describes the conceptual and operational treatment of units of analysis and output semantics for a cyber-attribution scoring pipeline. It articulates definitions, processing stages, provenance controls, and validation procedures that govern how discrete inputs are converted into scored outputs. The exposition is presented at a methodological level and is intentionally generic: it does not report or interpret any case-specific factual findings from source materials. The accompanying data manifest supplied to this section indicated an aggregate processing mode and did not include raw excerpts for specific documents; consequently, the procedures described emphasize robustness to varying input completeness while referencing the provided manifest fields.</p></section><section><h2>Scope, Units of Analysis, and Output Semantics</h2><p>Five primary units of analysis are defined and used throughout the pipeline: claim, source, artifact, evidence item, and document-level object. A claim is an asserted proposition regarding an event, behavior, attribution, or inference that a consumer of the analysis might seek to evaluate. A source denotes an origin of information, which may be an organization, publication, individual, or technical feed; sources are tracked separately from documents to permit cross-document provenance aggregation. An artifact is any discrete extracted object such as a binary sample, configuration snippet, code fragment, image, or log excerpt; artifacts are treated as first-class inputs to technical analyses. An evidence item is a parsed and normalized representation of a factual or technical finding extracted from a document or artifact; such items are the atomic evidentiary units linked to claims. The document-level unit denotes the container (for example, a report, email, or file) from which artifacts and evidence items are extracted, and it carries document-specific metadata (author, timestamp, format, and chain-of-custody markers).</p><p>Each output score is scoped to one of these units. A claim score quantifies the degree to which the available evidence items and their provenance patterns support the substantive content of a claim; it is not a definitive legal ruling or a binary truth determination. A source score communicates assessed reliability and consistency across documents and artifacts attributed to that source; it does not imply completeness of knowledge about that source or an immutable trustworthiness label. An artifact-level score captures confidence in the artifact’s integrity, origin inference, and relevance to particular claims; it does not substitute for specialized forensic analysis of the artifact. A document-level score aggregates artifact and evidence-item assessments together with document metadata to indicate the overall internal coherence and evidentiary weight of the document. In all cases scores are probabilistic, contextual, and explicitly accompanied by uncertainty bounds and provenance metadata; they are intended to inform, not to substitute for, human judgment or legal determinations.</p></section><section><h2>Data Ingestion</h2><p>The ingestion stage normalizes incoming payloads and metadata into an internal representation that preserves source identifiers and document-level metadata. When the raw_data_excerpt is unavailable, as in the present manifest, the pipeline adopts conservatively broad parsing heuristics and flags missing content for secondary review. The pipeline records the declared processing mode (for example, aggregate) and ingests manifest fields (including pipeline_counts, when present) to parameterize downstream parsing fidelity and validation expectations. All ingested objects are assigned persistent identifiers to facilitate immutable references in subsequent processing stages.</p></section><section><h2>PDF to Markdown Conversion and Text Normalization</h2><p>Binary document formats undergo staged conversion intended to preserve structural cues and metadata while producing searchable text. Optical character recognition (OCR) processes are applied where necessary with explicit confidence scores retained. Conversion produces a normalized, timestamped markdown-like representation that retains location pointers to original offsets in the source PDF or native file. Low-confidence regions are annotated and preserved as evidence items of reduced textual clarity rather than discarded, ensuring the chain of custody and certainty metadata remain intact.</p></section><section><h2>Structural Parsing</h2><p>Structural parsing segments normalized text into document-level components (title, abstract or summary, body sections, footnotes, appendices, and bibliography) and produces a document object model that supports fine-grained referencing. This stage is responsible for generating the document-level metadata record noted above. Structural parsing associates candidate claim segments with proximate evidence items and records inter-segment linkages, enabling downstream claim–evidence graph construction while preserving document-level provenance and positional indices.</p></section><section><h2>Artifact Extraction</h2><p>Artifact extraction identifies and isolates technical objects embedded in or referenced by documents. Extraction processes preserve original binary payloads where available and compute cryptographic hashes and format identifiers. Each artifact receives metadata including extraction method, extraction confidence, and linkage to the parent document-level container and source. Artifacts are classified by type to determine applicable forensic or analytic modules; their</section>\n<section class=\"wiki-section\" id=\"sec-data_ingestion\"><section>\n  <h2>Introduction</h2>\n  <p>This methodology chapter describes the end-to-end procedures used to ingest a defined input corpus of technical reports and to produce reproducible intermediate artifacts that serve as the basis for cyber-attribution scoring. The objective is to set out deterministic, auditable operations from raw document inputs through structured extractions, reference resolution, institution inference and scoring so that downstream inferences are reproducible by independent reviewers. Throughout the document the term input corpus denotes the set of original report files and their attendant summary metadata as recorded at ingestion.</p>\n\n  <h2>Scope Units</h2>\n  <p>The unit of analysis for the pipeline is the report-level document as represented in the input corpus. For the present implementation the recorded input corpus entries include a portfolio summary JSON (/home/pantera/projects/TEIA/annotarium/outputs/reports/portfolio_summary.json) and a set of document JSON representations produced from original PDFs (for example: /home/pantera/projects/TEIA/annotarium/outputs/reports/apt29_hammertoss_stealthy_tactics_define_a_report.json, /home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json, /home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2013_apt1_exposing_one_of_china_s_cyber_espionage_units_report.json, /home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2017_apt28_at_the_center_of_the_storm_report.json, /home/pantera/projects/TEIA/annotarium/outputs/reports/panda_crowdstrike_intelligence_report_report.json). The portfolio summary artifact is present at the specified path and recorded with its size in bytes (20562) as part of the ingestion ledger. Each report is treated as an immutable input object whose metadata and binary content are captured before any transformation to preserve reproducibility and chain of custody.</p>\n\n  <h2>Data Ingestion and Corpus Handling</h2>\n  <p>Ingestion proceeds by enumerating the input corpus and recording stable identifiers for each file. Deterministic artifact identifiers combine the file system path, a cryptographic checksum computed at the time of ingestion, and a timestamped manifest entry. This manifest captures the exact raw payload paths and the recorded file attributes (for example existence, size in bytes, and original path). Existence checks and size measurements are recorded for reproducibility; the portfolio summary JSON is validated for presence at /home/pantera/projects/TEIA/annotarium/outputs/reports/portfolio_summary.json and its file size is logged to the manifest. All subsequent processing references artifacts by manifest identifier rather than by mutable filesystem paths to ensure that transformations are traceable to the precise input byte sequences.</p></section>\n<section class=\"wiki-section\" id=\"sec-pdf_to_markdown\"><section><h2>Introduction</h2><p>This methodology chapter defines the procedures and rationale for converting source documents into machine-interpretable representations used in a cyber-attribution scoring pipeline. The objective is to describe the technical and procedural choices that produce markdown-structured artifacts and downstream extractions while preserving evidentiary integrity, reproducibility, and analytic transparency. The description provides a roadmap from ingestion through validation and quality assurance while emphasizing methodological rationale rather than case-specific substantive claims.</p></section>\n\n<section><h2>Scope and Units of Analysis</h2><p>The unit of analysis for the conversion pipeline is the individual report or document artifact provided as an electronic PDF. The pipeline treats each document as an atomic source for structural parsing, artifact extraction, reference linking, and institution inference. In the present configuration the available raw payloads include a portfolio summary and a set of report JSON outputs referenced in the pipeline inputs (for example: /home/pantera/projects/TEIA/annotarium/outputs/reports/portfolio_summary.json and a set of report JSON files enumerated in the raw payload paths). These file references are incorporated into the chain-of-custody records maintained for each processing stage.</p></section>\n\n<section><h2>Data Ingestion</h2><p>Document ingestion begins with acquisition of the original PDF files or their first-order JSON exports. Metadata about the source files is recorded at intake to support later provenance assertions; where available, filesystem path, existence flag, and byte-size are captured (for example, the portfolio summary JSON at /home/pantera/projects/TEIA/annotarium/outputs/reports/portfolio_summary.json with an indicated size of 20562 bytes). All ingestion events are timestamped and associated with the ingestion tool version to establish baseline reproducibility.</p></section>\n\n<section><h2>PDF-to-Markdown Conversion</h2><p>The conversion from PDF to markdown is implemented with a primary Mistral-based processing step followed by an offline fallback posture. The primary pathway leverages a Mistral OCR/provider-backed conversion implementation (noted in the pipeline methods as process_pdf_mistral_ocr.py) to produce a first-pass markdown representation that preserves textual flow, inline citations, and preliminary structural markers such as headings, tables, and figure captions. This Mistral-enabled processing is documented as the preferred channel for its throughput and provider capabilities; however, the pipeline explicitly treats an offline fallback as a matter of methodological resilience rather than as a conceptual change in the conversion model. The offline fallback uses PyMuPDF4LLM to reconstitute textual content and structural cues into markdown when provider-backed processing is unavailable or when policy and operational constraints require air-gapped processing. Both pathways emit a standardized markdown schema to ensure downstream components operate on a consistent intermediate representation regardless of execution environment.</p></section>\n\n<section><h2>Structural Parsing</h2><p>After markdown generation, stage1 markdown parse routines identify and tag major structural elements: sections, subsections, tables, figures, captions, and inline reference tokens. The structural parser yields a hierarchical document model that encodes offsets and relative positions to preserve evidentiary context for extracted claims. Structural parsing is designed to be idempotent across the two conversion pathways so that identical content yields a consistent parsed model whether produced by Mistral processing or the offline PyMuPDF4LLM fallback.</p></section>\n\n<section><h2>Artifact Extraction</h2><p>Artifact extraction operates on the parsed markdown structure to produce indexed artifacts conforming to a schema extraction artifact index. This includes named entities, technical indicators, IOCs, code snippets, and table-extracted data. Each artifact is recorded with provenance metadata linking back to the location in the markdown and the upstream conversion pathway. Artifact indices are constructed to support subsequent claim-evidence graph formation and to facilitate selective reprocessing if retraction or correction is required.</p></section>\n\n<section><h2>Reference Parsing</h2><p>Reference parsing isolates citations and bibliographic sections and links inline reference tokens to structured source descriptors. The pipeline captures both the citation text as rendered in the document and normalized reference metadata when resolvable. References are stored with pointers to their source text locations and, where available, to external identifiers or URLs. This linking supports automated cross-checking and external corroboration while preserving the original document anchor for auditability.</p></section>\n\n<section><h2>Institution Inference</h2><p>Institution inference is performed as a classification task that maps named organizations and author affiliations identified in the artifact extraction phase to institutional identifiers. The configured method employs a transformer-based classifier (documented as gpt-5-mini institution classification) with an optional web fallback to resolve ambiguous cases. This two-tier approach is designed to maximize inferential accuracy while maintaining an auditable decision trail: model-derived inferences are accompanied by confidence scores and, where the web fallback is used, by the retrieval evidence that informed the classification.</p></section>\n\n<section><h2>Claim–Evidence Graph Construction</h2><p>Extracted artifacts, parsed claims, and linked references are assembled into a claim–evidence graph. The graph is a directed structure in which claim nodes are connected to evidence nodes that include quoted passages, artifacts, and external references. Each edge is annotated with provenance metadata capturing document origin, markdown offsets, conversion pathway, and extraction routine. The graph model supports incremental updates, enabling later augmentation or correction without wholesale reprocessing of the original documents.</p></section>\n\n<section><h2>Scoring Framework Overview</h2><p>The scoring framework evaluates claims and institutions along axes such as source integrity, extractive confidence, provenance completeness, and external corroboration. Scores are computed from component reliability measures (for example, conversion-path consistency, parser confidence, and classifier probabilities) and are combined using calibrated aggregation functions. The framework is deliberately modular so that individual score components can be reweighted or replaced as the empirical performance of conversion and inference components is assessed.</p></section>\n\n<section><h2>Chain of Custody</h2><p>Every processing action is recorded in a chain-of-custody log that includes the tool version (for example, process_pdf_mistral_ocr.py or PyMuPDF4LLM), the time of operation, the operator or service identity, and the resulting artifact identifiers. This log provides the evidentiary trail necessary for auditing and supports reproducibility by enabling exact re-execution of processing steps against preserved inputs.</p</section>\n<section class=\"wiki-section\" id=\"sec-structural_parsing\"><h2 id=\"introduction\">Introduction</h2>\n<p>This methodology section situates the structural parsing stage within a broader cyber‑attribution scoring pipeline and explains why preserving document anchors is central to auditability. The approach described here treats each report and supporting artifact as a compound document comprising text, tables, figures, and embedded metadata. The methodological rationale is to enable reproducible extraction, to maintain provenance from original payloads to derived evidence nodes, and to permit retrospective review by independent analysts or auditors without invoking substantive findings from any particular report. The procedures below are grounded in the available raw payload paths and intended to interoperate with downstream reference and institution inference stages.</p>\n\n<h2 id=\"scope_units\">Scope and Units of Analysis</h2>\n<p>The unit of analysis is the atomic document component: block‑level text segments, tabular data units, and discrete image or figure objects. Each unit is treated as an addressable entity by virtue of an anchor that locates it within the original payload. Anchors are defined so as to survive format conversions and to record file‑level provenance, byte offsets where available, page numbers, and unique identifiers derived from the source file path. This unitization supports consistent handling of heterogeneous sources, including the reports referenced in the ingestion manifest. It also establishes the minimal element that can carry extracted metadata, analytic annotations, and links into the claim–evidence graph.</p>\n\n<h2 id=\"data_ingestion\">Data Ingestion</h2>\n<p>Ingestion begins from a manifest of raw payloads. For the present pipeline, the manifest entries include the portfolio summary and a set of report JSON payloads identified by their filesystem paths. Ingestion records the source path, file checksum, timestamp of acquisition, and declared content type. These fields form the initial layer of chain‑of‑custody metadata and become part of each unitʼs anchor. The ingestion stage validates file integrity against checksums and logs any anomalies that would affect subsequent parsing. This procedural step is deliberately agnostic to the substantive contents of any report, and its purpose is to ensure robust linkage between source material and extracted units.</p>\n\n<h2 id=\"pdf_to_markdown\">PDF to Structured Text Conversion</h2>\n<p>Where source files are encoded in page‑based formats such as PDF, a two‑stage conversion process is applied to produce structured text and candidate anchors. First, an optical or native text extraction engine renders page images and identifies layout features including columns, headers, and footers. Second, a logical reflow algorithm groups text runs into candidate blocks, preserving positional metadata. At every stage anchors are propagated and augmented; for example, a page coordinate anchor is retained alongside a derived block identifier that references the original byte range. This redundancy reduces the risk that subsequent reflow operations will sever provenance links necessary for auditability.</p>\n\n<h2 id=\"structural_parsing\">Structural Parsing of Text, Tables, and Figures</h2>\n<p>Structural parsing decomposes documents into discrete, anchored objects: text blocks, tables, and figures. Text blocks are defined by contiguous text runs with coherent layout and typographic cues. Tables are detected by spatial alignment and cell boundary heuristics and are extracted as structured matrices with cell‑level anchors. Figures and images are extracted as binary objects with an associated descriptive caption block and coordinate anchor. Crucially, every extracted object is annotated with a stable anchor that encodes source path, page number, byte offset where available, and an assigned object identifier. These anchors enable an audit trail from any analytic assertion back to the precise location in the original payload, which is essential for external review and for resolving disputes about interpretation.</p>\n\n<p>The requirement to preserve anchors throughout parsing is methodological and epistemic. Methodologically, anchors permit reconstitution of the original context when a downstream consumer needs to re‑evaluate an excerpt, validate a table transcription, or re‑examine an image. Epistemically, anchor preservation supports auditability by providing an unambiguous mapping between derived claims and source material. This mapping is necessary for credible chain‑of‑custody records, for assessments of extraction fidelity, and for harmonizing multiple extractions of the same source performed by independent teams or tools.</p>\n\n<h2 id=\"artifact_extraction\">Artifact Extraction and Normalization</h2>\n<p>After structural segmentation, artifacts are normalized into canonical representations suitable for analysis. Text blocks are normalized for whitespace and encoding while retaining original offsets; tables are normalized into row‑column schemas with cell type annotations; figures are encoded with format metadata and image hashes. Normalization explicitly avoids altering substantive content beyond character encoding corrections and layout de‑noise. Each normalized artifact keeps its anchor metadata and is linked to the ingestion and conversion logs that justify any transformation performed.</p>\n\n<h2 id=\"reference_parsing\">Reference Parsing</h2>\n<p>References cited within text blocks and captioned near tables or figures are parsed to extract bibliographic entities, URLs, and identifiers. Parsing annotates each citation with its anchor and records the citation span within the parent block. This granular anchoring allows downstream adjudication of which specific phrase or table cell supports a bibliographic claim and enables tracing of evidence provenance through the reference network without asserting substantive connections among institutions or actors.</p>\n\n<h2 id=\"institution_inference\">Institution Inference</h2>\n<p>Institutional inference operates on anchored artifacts to suggest candidate organizational associations for authors, publishers, or cited entities. The inference engine uses anchored mentions, document metadata, and external authority lists to produce scored hypotheses, each linked to the originating anchor. By tying inferences to anchors rather than to entire documents, the methodology preserves the capacity to review and contest individual association hypotheses and to evaluate how particular passages or tables produced specific inference outputs.</p>\n\n<h2 id=\"claim_evidence_graph\">Claim–Evidence Graph Construction</h2>\n<p>Extracted, normalized artifacts and parsed references are assembled into a claim–evidence graph in which nodes represent anchored artifacts, inferred entities, and candidate claims, while edges represent evidential or provenance relationships. Anchors function as immutable identifiers for nodes derived from source documents, enabling deterministic reconstruction of the graph from the original payloads. The graph model is designed to accommodate multiple, potentially conflicting claims while maintaining traceable links to source anchors for each supporting artifact.</p>\n\n<h2 id=\"scoring_overview\">Scoring Framework Overview</h2>\n<p>The scoring framework assigns quantitative indicators to claims and inferences based on features of their anchored supporting artifacts. Features include extraction confidence, the degree of corroboration across independent anchors, temporal proximity</section>\n<section class=\"wiki-section\" id=\"sec-artifact_extraction\"><h2 id=\"introduction\">Introduction</h2>\n<p>This methodology chapter sets out a repeatable, defensible approach to artifact extraction and the normalization of technical objects in multi‑modal evidentiary collections. It is written in the register of legal and academic exposition and is intended to support attribution scoring systems while preserving chain of custody and enabling provenance assessment. The chapter describes the processing pipeline from ingestion through structural parsing, artifact extraction, reference linkage and institution inference, and explains how these steps together produce normalized technical objects suitable for downstream claim–evidence graphing and scoring. The description is methodological and deliberately refrains from addressing or characterizing any specific substantive finding from the underlying corpus.</p>\n\n<h3 id=\"scope_units\">Scope and Units of Analysis</h3>\n<p>The unit of analysis in this chapter is the technical object. A technical object is any discrete artifact that can carry technical indicators or metadata: for example, files, images, attachments embedded in documents, text segments, tables, extracted strings, timestamps, hashes, network indicators, and structured records produced by automated extraction routines. The modalities considered include rendered text (OCR outputs), native text, tabular data, images, and metadata streams. For the purposes of provenance and custody evaluation, each technical object is treated as an atomic unit that may be linked to source documents, extraction steps, and transformation operations. The methodology emphasizes normalization of these technical objects so that comparison, aggregation, and scoring functions operate over consistent, machine‑interpretable representations irrespective of modality of origin.</p>\n\n<h3 id=\"data_ingestion\">Data Ingestion and Data Contracts</h3>\n<p>Ingestion begins with the receipt of source documents and associated metadata. The pipeline operates under an explicit data contract that defines required and optional keys for downstream stages; in this engagement the contract keys included artifact_type_counts, raw_artifacts_preview, pipeline_methods and document_scores_v4. In practice, downstream routines must be resilient to missing keys: when artifact_type_counts or raw_artifacts_preview are absent, the pipeline records those absences in provenance metadata and triggers candidate discovery routines to determine modality composition empirically. All ingestion steps log source identifiers and preservation conditions to support later chain‑of‑custody review.</p>\n\n<h3 id=\"pdf_to_markdown\">PDF Conversion and Text Normalization (pdf_to_markdown)</h3>\n<p>PDF and other fixed‑layout formats are converted into processable text and structural markup. The primary conversion tool indicated in the supplied pipeline_methods is a provider‑backed Mistral OCR workflow implemented as process_pdf_mistral_ocr.py; an offline fallback implemented via PyMuPDF4LLM is available when the primary service is not usable. The methodology requires that each conversion include confidence metrics, page‑level hashes, and extraction provenance records identifying which conversion routine and parameters were used. OCR and conversion outputs are normalized for whitespace, Unicode normalization forms, and linebreak conventions to create a canonical textual representation that is carried forward as the canonical source of textual technical objects for that document. Such normalization reduces spurious variance when comparing textual artifacts across conversion methods or providers.</p>\n\n<h3 id=\"structural_parsing\">Structural Parsing and Segmentation</h3>\n<p>After conversion to markdown or structured text, stage1 markdown parse routines perform structural analysis: segmentation into paragraphs, headings, captions, table regions and embedded object placeholders. Table and image extraction are initiated from this parsed structure; the supplied pipeline notes that table_and_image_extraction is handled in stage1 markdown parse. Structural parsing annotates each segment with source coordinates, page numbers, and transformation lineage. This segmentation step delineates the technical objects that will undergo further normalization, and it preserves the mapping between extracted artifacts and their physical or logical locations within the source document—critical information for provenance assertions and potential evidentiary review.</p>\n\n<h3 id=\"artifact_extraction\">Artifact Extraction and Technical Object Normalization</h3>\n<p>Artifact extraction is implemented through schema extraction and artifact indexing routines (described in the pipeline as schema extraction artifact indices). Each extracted artifact becomes a technical object instance and is assigned a globally unique identifier, a stable content fingerprint (cryptographic hash where feasible), modality label, and a provenance record that includes the originating file identifier, conversion routine, and page/offset coordinates. Normalization processes transform modality‑specific representations into canonical forms: images are represented by standardized image encodings plus embedded OCR text where applicable; tables are represented as normalized row/column matrices with normalized cell encodings; dates, IP addresses, hashes and other structured indicators are normalized to canonical lexical forms. This normalization supports deterministic comparison across items that originate in different modalities or conversion pipelines. The methodology also differentiates between normalization (a reversible or bounded transformation that preserves original content and mapping) and irreversible summarization; provenance metadata records the exact transformation applied so the original technical object can be reconstructed or re‑extracted if necessary.</p>\n\n<h3 id=\"reference_parsing\">Reference Parsing and Source Linkage</h3>\n<p>References and citations are parsed from textual segments by routines that extract citation tokens and attempt linkage to external sources; the supplied pipeline specifies that reference_parsing produces citations + references linked to sources. Each reference token becomes an ancillary technical object with linkage hypothesis metadata documenting the matching criteria used (string similarity, DOI lookup, URL resolution, or repository identifiers). Reference linkage is recorded with confidence scores and with the evidence chain that justifies the linkage. Where automated linkage is uncertain, the system captures alternative candidates and the rationale for each to support later human review. Reference parsing is integral to provenance because it creates explicit, auditable connections between extracted claims, cited evidence, and external sources.</p>\n\n<h3 id=\"institution_inference\">Institution Inference and Attribution Candidates</h3>\n<p>Institutional inference routines apply classification models and, where necessary, web fallback checks to infer organizational origin metadata for artifacts. The pipeline inventory indicates the use of gpt-5-mini institution classification with web fallback. Methodologically, inferred institutional metadata are treated as probabilistic assertions with associated provenance that records model version, prompt templates, and any external evidence used for verification. The system separates inferred institutional labels from verified institutional statements; inferred labels serve as candidate inputs to attribution scoring but are not elevated to verified status without corroborating evidence. All inferred outputs include uncertainty characterization to ensure appropriate weight in downstream scoring.</p>\n\n<h3 id=\"claim_evidence_graph\">Claim–Evidence Graph Construction</h3>\n<p>Normalized technical objects and parsed references are assembled into a claim–evidence graph. Nodes represent technical objects, inferred claims, and external source records; edges represent extraction lineage, referential citation, transformation, and inferred institutional association. The graph explicitly encodes provenance and transformation attributes so that any claim node can be traced back to the precise technical object versions, conversion steps, and reference linkages that support it. This graph structure provides the substrate for scoring algorithms that evaluate credibility, corroboration, and chain‑of‑custody integrity.</p>\n\n<h3 id=\"scoring_overview\">Scoring Framework Overview</h3>\n<p>The scoring framework consumes normalized technical objects and the claim–evidence graph to generate reproducible metrics for attribution tasks. Scores are produced along multiple axes: provenance</section>\n<section class=\"wiki-section\" id=\"sec-reference_parsing\"><h2>Introduction</h2>\n<p>This methodology chapter sets out a reproducible approach to parsing footnote and reference material from primary documents and to resolving rhetorical citations into an analyzable source graph. The account that follows describes procedural choices, data transformations, and quality controls in general methodological terms. It does not report or analyze case-specific substantive findings; rather, it explains how information flowing from document ingestion to structured citation objects is managed, validated, and represented for subsequent attribution scoring.</p>\n\n<h2>Scope and Units of Analysis</h2>\n<p>The unit of analysis is the discrete referential element within a document: a footnote marker, an in-text parenthetical citation, a reference list entry, or an embedded hyperlink. These referential elements are treated as propositional connectors between a rhetorical claim in the source document and an external object (for example, another document, dataset, web resource, or institutional artifact). The methodology therefore operationalizes the transformation from rhetorical citation to nodes and edges in a source graph that can be used for evidence linking and scoring.</p>\n\n<h2>Data Ingestion</h2>\n<p>Document ingestion follows a documented pipeline that privileges optical character recognition and structural recovery suited for machine parsing. The primary conversion method is process_pdf_mistral_ocr.py (Mistral OCR/provider-backed conversion); an explicit fallback is available using offline fallback via PyMuPDF4LLM. Metadata captured during ingestion—file provenance, timestamp, and conversion logs—are retained to support chain-of-custody assertions and to assign initial confidence levels to extracted referential elements.</p>\n\n<h2>PDF-to-Markdown Conversion</h2>\n<p>The conversion stage produces an intermediate markdown-like representation in which typographic cues (superscript numerals, bracketed citations, italics) are preserved as tokens for downstream structural parsing. The chosen OCR and conversion methods are selected for their ability to preserve footnote markers and inline citation tokens; however, the methodology prescribes explicit downstream normalization to address OCR-induced artifacts such as misrecognized numerals, misplaced punctuation, and broken URLs.</p>\n\n<h2>Structural Parsing</h2>\n<p>Structural parsing segments the converted text into document zones (body, header, footer, footnote region, bibliography/reference section, tables, and figures). The segmentation is guided by a stage1 markdown parse for table_and_image_extraction, and by heuristics that identify contiguous reference lists and footnote blocks. Structural labeling is a prerequisite for robust footnote resolution because the same glyphic token (for example, a bracketed number) may serve different rhetorical roles depending on its spatial-contextual zone.</p>\n\n<h2>Artifact Extraction</h2>\n<p>Artifact extraction converts segmented text regions into schema-compliant artifact indices. The artifact extraction stage produces normalized fields such as cited title, author string, year, publisher, DOI, URL, and contextual anchor text. Extracted artifacts are indexed with provenance links to the originating document and the pipeline stage that produced the extraction, following the schema extraction artifact indices pattern. These indices form the atomic record units that populate the nascent source graph.</p>\n\n<h2>Reference Parsing</h2>\n<p>The reference parsing process links rhetorical tokens (footnote markers, in-text citations) to the artifact indices generated from the reference list or footnote region. The parser first performs token disambiguation—resolving whether a given symbol denotes a footnote or an inline citation—then associates the token with one or more candidate reference entries. Resolution strategies combine deterministic matching on normalized strings (title, author, year) with heuristic matching (partial title overlap, publisher name variants) and provenance heuristics (positional proximity within the same page or column). Citation linkage is represented as edges that connect a claim-span in the body text to artifact nodes; these edges carry attributes for match confidence, matching method, and evidence of ambiguity. The resulting structure is an explicit source graph suitable for graph-based analyses of evidentiary support.</p>\n\n<h2>Institution Inference</h2>\n<p>Institutional inference is applied to extracted artifacts that include organizational names or accounts. The primary classifier used for institution inference is gpt-5-mini institution classification (+ web fallback). The classifier is employed to normalize naming variants, to resolve corporate ownership hierarchies where appropriate, and to attach standardized institutional identifiers when available. Web-based fallback checks are used conservatively and logged, and institutional resolution outputs include confidence metrics and provenance to the underlying classifier call or web query.</p>\n\n<h2>Claim–Evidence Graph Construction</h2>\n<p>Claims are identified as annotated text spans in the body of the document; these spans are linked via citation linkage edges to one or more artifact nodes. The claim–evidence graph represents claims, artifacts, and institutions as typed nodes, and R-type edges (cites, supports, rebuts, clarifies) as necessary for downstream scoring. Each edge and node retains provenance metadata: originating file, pipeline stage (for example, process_pdf_mistral_ocr.py), timestamp, and confidence scores aggregated from OCR, parsing, and matching stages. This ensures that the graph is not only analyzable but auditable.</p>\n\n<h2>Scoring Overview</h2>\n<p>Scoring transforms the claim–evidence graph into quantitative and qualitative indicators for attribution assessment. Scores are computed from component measures: extraction confidence, citation linkage confidence, institutional reliability, and corroboration counts across independent artifacts. The scoring framework is modular such that each component score can be calibrated independently and combined using transparent rules. Scores are documented with their contributing components and the provenance of each contribution.</p>\n\n<h2>Chain of Custody</h2>\n<p>The chain-of-custody record captures each pipeline handoff and transformation, including the use of process_pdf_mistral_ocr.py (Mistral OCR/provider-backed conversion) and offline fallback via PyMuPDF4LLM, along with timestamps and operator identifiers where applicable.</section>\n<section class=\"wiki-section\" id=\"sec-institution_inference\"><section aria-labelledby=\"introduction\"><h2 id=\"introduction\">Introduction</h2><p>This methodology chapter sets out a principled, reproducible approach to institution inference and source typology as components of a broader cyber‑attribution scoring framework. It frames the problem in evidentiary terms: source materials are ingested, structurally parsed, and linked into a claim–evidence graph; institution inference and source typology then situate each source within an institutional class that informs credibility weighting and corroboration eligibility. The exposition below emphasizes methodological rationale, data provenance, and the criteria by which institutional class alters downstream scoring without advancing case‑specific attribution claims.</p></section><section aria-labelledby=\"data_processing_extraction\"><h2 id=\"data_processing_extraction\">Data Processing and Extraction</h2><p>Raw source materials undergo a staged pipeline designed to preserve provenance and to enable structured downstream analysis. Primary conversion from PDF to machine‑readable text is performed using a provider‑backed OCR and conversion script (process_pdf_mistral_ocr.py), with an offline fallback implemented via a PyMuPDF‑based routine (PyMuPDF4LLM) when provider conversion is unavailable. Tabular and image content are flagged during initial markdown parsing (stage1 markdown parse) and routed to specialized extraction routines. Artifacts discovered during parsing are indexed in a schema extraction artifact index to retain location, page coordinates, and conversion metadata for chain‑of‑custody purposes. Reference parsing is performed to extract citations and to produce explicit links from document assertions to external sources; these links are retained as discrete evidence nodes in the claim–evidence graph. The selection of these tools and steps is motivated by the need to balance fidelity of transcription, repeatability of extraction, and explicit recording of transformation steps for later audit.</p></section><section aria-labelledby=\"references_institution_inference\"><h2 id=\"references_institution_inference\">References and Institution Inference</h2><p>Reference parsing yields candidate source identifiers (DOIs, URLs, author names, organizational bylines) which are normalized and then subjected to institution inference. Institution inference is the process by which a source is classified into institutional categories (for example, academic, commercial vendor, independent researcher, government, intergovernmental organization, open‑source collective) based on extracted metadata and supplemental checks. The operational implementation combines a model‑based classifier (reported here as gpt‑5‑mini institution classification) with a web‑fallback verification step when classifier confidence is below a predefined threshold. The classifier receives structured inputs derived from parsed references and artifact indices, and returns an institutional label and a confidence score; the web fallback performs targeted lookups to validate or refine the label when necessary.</p><p>Source typology is mapped to a finite set of institutional classes that have been pre‑defined in the scoring ontology. Mapping rules encode observable heuristics (publisher domain reputation, organizational registration, stated affiliation, presence of peer review, and declared funding or sponsorship) and are applied deterministically where possible and probabilistically where metadata are incomplete. Institutional class determines whether a source is eligible to participate in particular corroboration patterns and sets baseline credibility weighting. For example, sources classified as peer‑reviewed academic outputs or official government technical reports are assigned higher baseline weights than anonymous forum posts, but each assignment remains subject to adjustment through corroboration and artifact‑level quality assessment. This approach preserves methodological neutrality by separating institutional priors from evidence‑level corroboration steps.</p></section><section aria-labelledby=\"scoring_framework\"><h2 id=\"scoring_framework\">Scoring Framework</h2><p>The scoring framework combines three principal elements: (1) base credibility weighting derived from source typology and institution inference, (2) corroboration adjustments based on independent evidence links in the claim–evidence graph, and (3) clarity and artifact‑level quality modifiers drawn from extraction metadata. Base credibility weighting encodes the initial trust assigned to a source contingent on its institutional class and associated confidence score from the institution inference process. Corroboration adjustments depend on corroboration eligibility, a rule set that specifies which institutional classes may corroborate one another and the strength imparted by independent, methodologically distinct traces. Clarifying modifiers account for transcription fidelity, completeness of referenced artifact indices, and any detected transformations during PDF‑to‑markdown conversion. The result is a composite, auditable score per claim that reflects both institutional priors and evidence‑level linkage strength.</p></section><section aria-labelledby=\"validation_quality_assurance\"><h2 id=\"validation_quality_assurance\">Validation and Quality Assurance</h2><p>Validation is implemented at multiple levels. Unit checks verify that conversion and parsing steps preserve location and parsing metadata as recorded in the schema extraction artifact indices. Cross‑validation exercises compare institution inference outputs against curated ground truth sets and web‑verified labels to calibrate classifier thresholds and fallback invocation policies. Corroboration eligibility rules and credibility weighting parameters are stress‑tested using synthetic and historical datasets to observe sensitivity to misclassification and missing metadata. All transformation steps, model versions, and runtime environments (for example, the Python runtime and package versions used for extraction and inference) are recorded to enable reproducible re‑runs and forensic audit. Periodic human review of institution inference edge cases is mandated to correct systematic biases and to refine mapping heuristics. Together these measures ensure that institution inference, source typology mapping, credibility weighting, and corroboration eligibility are transparent, auditable, and subject to continuous improvement without reliance on case‑specific findings.</p></section></section>\n<section class=\"wiki-section\" id=\"sec-claim_evidence_graph\"><h2>Introduction</h2>\n<p>This methodological chapter sets out the approach used to construct a claim-evidence graph for cyber-attribution scoring. The goal is to describe, in legally oriented and academically rigorous terms, how discrete analytic units—claims, evidentiary fragments, source records, and extracted artifacts—are represented and linked to preserve auditable traceability while preventing inferential circularity. The discussion emphasizes methodological rationale rather than substantive findings drawn from any particular report.</p>\n\n<h2>Scope and Units of Analysis</h2>\n<p>The primary units of analysis are defined as follows: a claim is a discrete, testable assertion derived from reporting or analytic synthesis; a source is any documentary or digital record that provides information (for example, reports, raw feeds, or archival records); an artifact is an extracted data object such as a file hash, network indicator, binary fragment, or image; and an anchor is the minimal locatable unit of a source (for example, a page-and-paragraph locator, byte offsets in a file, or a timestamped log entry) that permits deterministic retracing of an evidentiary fragment. These units are the nodes and atomic identifiers used to construct the claim-evidence graph.</p>\n\n<h2>Data Ingestion</h2>\n<p>Data ingestion is governed by a written protocol that requires preservation of original file containers and capture of ingest metadata. In the present data package, fields named raw_claims_preview, raw_sources_preview, raw_artifacts_preview, claim_score_preview_v4, and scoring_bundle were present in the data contract; the preview fields were null at time of ingestion. The methodology therefore mandates explicit handling of null or missing preview fields: ingestion records must log the absence, attempt automated retrieval of the upstream artifact where a pointer exists, and, where automated retrieval is not possible, instantiate a flagged manual-curation workflow. All steps are recorded in the ingestion log to ensure later auditability and reproducibility.</p>\n\n<h2>PDF-to-Markdown Conversion</h2>\n<p>Where sources are provided as PDF or other document images, conversion to text uses a two-stage approach combining optical character recognition with structural retention. The objective is not only to obtain linear text but to preserve locators (page numbers, column markers, figure and table captions) that serve as anchors. Conversion output is retained alongside checksums of original files and conversion artifacts to support later verification of traceability.</p>\n\n<h2>Structural Parsing</h2>\n<p>Structural parsing identifies document components such as headings, footnotes, bibliographic references, figures, and tables and links them to anchor metadata. Parsing is designed to be conservative: when automatic delineation is ambiguous, the parser preserves original bounding metadata and marks the fragment as low-confidence, triggering human review. The structural representation feeds the downstream claim-evidence graph so that edges can reference specific structural anchors rather than coarse document identifiers.</p>\n\n<h2>Artifact Extraction</h2>\n<p>Artifact extraction isolates technical indicators and data objects from sources with explicit provenance markers. Each extracted artifact is recorded with a provenance tuple comprising source identifier, anchor locator, extraction timestamp, extractor identity (tool and version), and a content checksum. Artifacts are retained in a separate, append-only artifact store. The methodological rationale is to separate evidentiary items from interpretive narrative so that linkages in the claim-evidence graph can reference raw artifacts directly, increasing traceability and reducing interpretive opacity.</p>\n\n<h2>Reference Parsing</h2>\n<p>Reference parsing resolves in-text citations and bibliographic entries into canonical source identifiers where possible. The parser attempts to normalize reference strings to persistent identifiers (for example, DOIs or URIs), and when normalization fails records the original citation string plus the structural anchor. The consistent normalization of references permits automated detection of common sources across multiple claims, enabling correlation without reliance on paraphrase matching alone.</p>\n\n<h2>Institutional Inference</h2>\n<p>Institutional inference is performed using metadata appearing in sources—author lines, affiliation footnotes, issuing domains, and organisational signatures—supplemented by external registries where permitted. Inference is probabilistic and accompanied by confidence scores; uncertain inferences are retained but flagged. The rationale is to avoid hard-coded attribution of institutional origin without explicit evidence, and to make any institutional linkage transparent and reviewable within the claim-evidence graph.</p>\n\n<h2>Claim-Evidence Graph Construction</h2>\n<p>The claim-evidence graph is a directed acyclic representation in which nodes represent claims, evidentiary fragments (anchored source excerpts), source records, and artifacts. Edges encode explicit relations: a claim is supported_by an evidentiary fragment; a fragment is contained_in a source at a specified anchor; an artifact is extracted_from an anchor. Anchor-level traceability is implemented by ensuring every support edge references an anchor locator that maps to a specific offset or structural component in the original source. Nodes carry provenance metadata including ingest checksum, timestamps, extractor identity, and confidence annotations. The design objective is that any claim can be algorithmically and humanly traced back to the exact fragment and file that served as its immediate support, thereby enabling reproducibility and evidentiary inspection.</p>\n\n<h2>Anti-circularity Safeguards</h2>\n<p>Anti-circularity is enforced at multiple stages. First, the graph construction enforces acyclicity for support relations: a claim may not be supported_by a fragment that itself derives its content from the claim (direct cycle). Second, provenance chains are validated by examining extraction timestamps and source timestamps to ensure chronological plausibility; any reverse temporal dependencies are flagged for review. Third, the methodology distinguishes independent corroboration from re-publication by annotating sources with origin labels and measuring independence of provenance paths; support that depends solely on secondary re-publications of a single primary record is down-weighted or marked</section>\n<section class=\"wiki-section\" id=\"sec-scoring_overview\"><section><h2>Introduction</h2><p>This methodology chapter sets out the architecture and procedural safeguards for a cyber-attribution scoring system. The account that follows is descriptive and methodological: it explains how source documents are ingested, how discrete claim-level signals are extracted and normalized, how references and institutional provenance are inferred, and how those signals are synthesized into document-level judgments using explicit inferential weighting. The rationale for each design choice is presented in general terms so as to permit reproducibility and independent critique while avoiding exposition of any case-specific substantive findings in the underlying report.</p></section><section><h2>Scope and Units of Analysis</h2><p>The primary units of analysis are claims and documents. A claim-level unit corresponds to a discrete assertion or proposition expressed within a source, which can be mapped to evidence spans during extraction. A document-level unit corresponds to the complete authored artifact from which one or more claims are derived. The scoring architecture maintains a strict separation between claim-level outputs produced by extraction processes and the document-level synthesis performed by aggregation and calibration routines; this separation preserves traceability from final scores back to the originating evidence spans.</p></section><section><h2>Data Ingestion</h2><p>Data ingestion is designed to preserve original artifacts and to capture a chain of custody. Raw inputs include native PDFs, images, figures, and ancillary metadata. Metadata recording captures provenance fields and timestamps at point of receipt. The pipeline is instrumented to log which conversion path was applied to each artifact so that downstream consumers can account for modality-specific error characteristics. The section_data_contract_keys supplied with source metadata are recorded as part of the ingest manifest to ensure schema conformance during subsequent processing.</p></section><section><h2>PDF-to-Markdown Conversion</h2><p>Conversion of PDFs into machine-readable text proceeds along a primary and fallback path. The primary conversion is performed using a provider-backed Mistral OCR conversion (denoted process_pdf_mistral_ocr.py in the pipeline_methods), with an offline fallback via PyMuPDF4LLM when the primary path is unavailable. Each conversion run emits quality metrics that capture OCR confidence, detected page layout structures, and any warnings about non-text elements. The explicit recording of which converter was used enables later calibration of extraction confidence and allows inferential weighting to account for modality-dependent noise.</p></section><section><h2>Structural Parsing</h2><p>Structural parsing organizes converted text into logical regions such as headings, paragraphs, tables, and figures. The stage1 markdown parse identifies candidate tables and images and produces a structural token stream that downstream extractors consume. Detected structural elements are annotated with spatial and semantic attributes so that claim extraction can prefer contiguous narrative spans for assertions and dedicated table parsers for tabular evidence. Structural parsing also records uncertainty where layout ambiguity is present; these uncertainties propagate to claim-level confidence scores rather than being resolved deterministically.</p></section><section><h2>Artifact Extraction</h2><p>Artifact extraction uses schema extraction artifact indices to identify and extract discrete artifacts: named entities, asserted actions, temporal markers, technical indicators, and referenced artifacts such as hash values or network indicators. Extraction outputs are represented in a normalized schema that aligns with the claim-level unit model; each extracted artifact includes provenance pointers to the original document coordinates and to the conversion method logged during ingestion. The separation between extraction outputs and subsequent inferential weighting is deliberate: extraction reports are treated as observed signals, not as calibrated judgments.</p></section><section><h2>Reference Parsing</h2><p>Reference parsing links in-text citations and bibliographic entries to external sources. The pipeline records citations + references linked to sources and attempts to resolve DOIs, URLs, and publisher identifiers where present. Resolution outcomes are recorded with confidence metadata and with a provenance chain showing whether the reference was resolved during the initial pass or flagged for manual review. Reference parsing supports both direct corroboration of claims and the construction of evidence graphs that enumerate inter-document relations.</p></section><section><h2>Institution Inference</h2><p>Institution inference classifies the apparent institutional origin or affiliation associated with a document or reference. The pipeline employs a primary automated classifier (gpt-5-mini institution classification) with a recorded web fallback when the automated classification yields low confidence. Institutional labels are accompanied by confidence scores and provenance traces that show which textual features or external signals informed the classification. Institutional inference is encoded as an independent signal that can inform inferential weighting but is not determinative by itself.</p></section><section><h2>Claim Evidence Graph</h2><p>Extracted claims and references are assembled into a claim evidence graph that represents claims as nodes and evidentiary relations as typed edges. Each edge encodes the nature of support (direct quotation, paraphrase, citation, corroboration, contradiction) and is annotated with the extraction provenance and quality metrics derived from earlier stages. The claim evidence graph preserves the distinction between observed extraction outputs and interpretive relations that are introduced during synthesis; this enables auditors to review both raw signals and the inferential steps that connect them.</p></section><section><h2>Scoring Framework Overview</h2><p>The scoring architecture is hierarchical. At the claim-level, discrete axes are scored for evidentiary support, temporal proximity, technical plausibility, and provenance reliability. These axes are calculated from normalized extraction outputs and from quantitative metadata such as OCR confidence and reference resolution success. Document-level synthesis aggregates claim-level axes using explicit inferential weighting that is specified a priori and documented in the scoring bundle. Inferential weighting is applied only after claim-level signals have been produced and logged; weights are parameterized so that alternative calibrations may be applied for sensitivity analysis without re-running extraction.</p></section><section><h2>Chain of Custody</h2><p>The chain of custody records the provenance of each artifact and each derived signal from receipt through conversion, parsing, extraction, and scoring. Logs record the exact pipeline method invoked (including process_pdf_mistral_ocr.py and PyMuPDF4LLM where applicable), the version of the parsers, and any manual interventions. The custody record ensures reproducibility and supports later forensic review of any score by linking outputs unambiguously back to file-level inputs and processing steps.</p></section><section><h2>Credibility Corroboration</h2><p>Credibility corroboration procedures evaluate corroboration across independent sources and across distinct evidence types (e.g., textual, tabular, digital artifacts). The system differentiates corroboration that arises from shared references from corroboration that arises from independent observation. Corroboration strength is represented as a separate axis that feeds into document-level aggregation but is maintained as an auditable signal distinct from extraction confidence to prevent conflation of correlated errors.</p></section><section><h2>Clarity Axis</h2><p>The clarity axis measures how explicitly a claim is stated versus implicitly inferred. Clarity is scored on the basis of syntactic and discourse markers identified during structural parsing and by the proximity of supporting evidence spans. This axis is used to distinguish well-articulated claims, which warrant higher baseline weight in aggregation, from speculative or inferential language, which receives conservative down-weighting in inferential weighting schemes.</p></section><section><h2>Aggregation and Calibration</h2><p>Document-level aggregation combines claim-level axis scores using the pre-specified weighting scheme. Calibration is performed on held-out validation sets and through expert adjudication to align aggregated scores with domain-relevant benchmarks. Calibration datasets and their performance statistics are recorded separately from operational run data so that policy changes to inferential weighting can</section>\n<section class=\"wiki-section\" id=\"sec-chain_of_custody\"><h2>Introduction</h2>\n<p>This methodology chapter defines the chain-of-custody axis as an integral component of a broader cyber-attribution scoring framework. The exposition below treats chain-of-custody as a set of measurable variables and procedural controls that support inferential claims without presenting or adjudicating case-specific factual findings. The approach is grounded in the available input schema and raw data contract; where the provided raw data fields are null or absent, the method specifies required metadata collection and fallback procedures to preserve analytical rigor.</p>\n\n<h2>Scope and Units of Analysis</h2>\n<p>The scope of analysis encompasses discrete artifacts and document-level units that contribute evidence to attribution assertions. Units of analysis include source documents, extracted artifacts, cryptographic artifacts, network captures, and metadata records. Each unit is represented by a minimal set of identifiers and metadata sufficient to evaluate provenance, integrity, time anchors, artifact identifiers, and versioning. In the supplied context the keys document_scores_v4, claim_score_preview_v4, raw_artifacts_preview, and scoring_bundle are present in the contract but their current values are null; this absence informs the treatment of missing data and the imposition of conservative assumptions in downstream scoring.</p>\n\n<h2>Data Ingestion</h2>\n<p>Data ingestion prescribes canonicalized intake procedures for raw files and metadata. Ingested records must capture the original source location, acquisition method, acquisition timestamp, operator identity, and any intermediary processing steps. When the ingestion pipeline encounters null or absent fields (as observed in the provided raw data contract), it triggers provenance flags that downgrade automatic confidence and initiate auxiliary acquisition requests. The rationale for conservative downgrading is to ensure that scores reflect not only content but also the quality of custody metadata.</p>\n\n<h2>PDF to Markdown Conversion</h2>\n<p>Conversion of opaque formats to machine-readable representations follows a documented transformation pipeline that preserves source identifiers and records transformation checksums. Each conversion step appends metadata that records the conversion tool, versioning, configuration parameters, and a checksum of the output. These recorded artifacts feed into integrity assessments and provide time anchors that are used to demonstrate temporal continuity from acquisition to analysis. Conversion artifacts are treated as first-class items for chain-of-custody evaluation.</p>\n\n<h2>Structural Parsing</h2>\n<p>Structural parsing isolates semantic components (headers, footnotes, captions, embedded media) while preserving positional metadata. The parser annotates each extracted element with an artifact identifier and attaches provenance attributes back to the source document. Structural parsing thereby enables granular integrity checks and supports later reconstructions of the evidence trail should questions about tampering or misattribution arise.</p>\n\n<h2>Artifact Extraction</h2>\n<p>Artifact extraction produces discrete objects suitable for cryptographic hashing and independent validation. For each artifact the pipeline records an artifact identifier, creation and acquisition timestamps, the applied extraction routine and its versioning, and an integrity checksum. These records are fundamental to linking content to custody metadata and to establishing time anchors that bound the artifact's temporal provenance.</p>\n\n<h2>Reference Parsing</h2>\n<p>Reference parsing identifies citations, external links, and embedded references and maps them to canonical external identifiers where possible. The parser logs match confidence and source provenance for each resolved reference. In the event of unresolved references or ambiguous mappings, the system propagates uncertainty to the chain-of-custody assessment to avoid overstating linkage strength.</p>\n\n<h2>Institution Inference</h2>\n<p>Institution inference is conducted on the basis of explicit metadata, document signatures, infrastructure indicators, and corroborating references. The inference procedure is probabilistic and records the contributing features and their weights. Because institution inference is sensitive to provenance and integrity, the model conditions its output on the quality flags produced during ingestion and artifact extraction; null or missing upstream metadata (as in the provided contract) yields wider posterior uncertainty distributions.</p>\n\n<h2>Claim–Evidence Graph</h2>\n<p>The claim–evidence graph formalizes links between inferential claims and supporting artifacts. Nodes in the graph are annotated with provenance and integrity metadata and with time anchors that indicate when the artifact was observed and when it was asserted into the graph. Edges encode the nature of support (direct quote, paraphrase, derived analysis) and are weighted by the assessed quality of custody metadata and by any versioning relationships between nodes.</p>\n\n<h2>Scoring Overview</h2>\n<p>Scoring integrates multiple axes including technical indicators, contextual corroboration, and chain-of-custody quality. The scoring function treats chain-of-custody attributes as modifiers that scale evidentiary weight. Where raw data fields required for chain-of-custody computation are null, the scoring function applies conservative priors and documents the uncertainty adjustment applied to final scores. Counts or scores may be used illustratively to demonstrate how a missing provenance flag reduces effective weight, but such numeric exemplars are methodological, not substantive.</p>\n\n<h2>Chain of Custody Variables and Operationalization</h2>\n<p>The core chain-of-custody variables are operationalized as follows. Provenance denotes the documented origin and acquisition pathway of an artifact and is represented by source identifiers, acquisition logs, and operator credentials. Integrity is evaluated through cryptographic checksums, digital signatures, and checksum lineage; failures or absence of integrity evidence are recorded as integrity exceptions. Time anchors comprise acquisition timestamps, embedded document timestamps, and externally verifiable temporal references; the methodology distinguishes creation time, acquisition time, and ingestion time and treats discrepancies as temporal uncertainty. Artifact identifiers are stable, collision-resistant labels assigned at intake and preserved through transformations. Versioning captures both semantic and binary changes to artifacts and is tracked via diffs, version tags, and change logs.</p>\n\n<h2>Quality Controls and Penalties</h2>\n<p>Quality controls mandate that each artifact carry minimally required metadata to avoid automatic penalization. Verification steps include checksum recomputation, signature validation, operator attestation checks, and cross-source temporal reconciliation. Penalties are structured as graduated score reductions and provenance flags: missing provenance metadata results in an initial penalty, failed integrity checks produce larger deductions, ambiguous or conflicting time anchors incur uncertainty penalties, unversioned artifacts receive procedural penalties proportional to potential impact, and identifier collisions trigger investigation and further score suppression. Penalties are applied transparently and recorded in audit logs so that recalibration or contestation can be performed with a clear evidentiary trail.</p>\n\n<h2>Credibility Corroboration</h2>\n<p>Corroboration procedures synthesize independent lines of evidence and prefer artifacts with higher provenance and integrity scores. The methodology favors redundancy and temporal sequencing to strengthen credibility. When corroborating</section>\n<section class=\"wiki-section\" id=\"sec-credibility_corroboration\"><div>\n  <h2>Introduction</h2>\n  <p>This methodology chapter specifies a reproducible approach for assessing the credibility of sources and the corroboration of claims within a cyber-attribution scoring framework. The approach is normative and procedural: it defines how input artifacts are transformed into structured evidence, how source attributes inform weighting decisions, and how corroboration is operationalized without adjudicating or reporting case-specific factual findings. The methodology is grounded in the supplied data contract keys; however, in the present instance the preview fields (source_type_counts, raw_sources_preview, document_scores_v4, claim_score_preview_v4, scoring_bundle) are null. Where those raw data elements are present, they are the primary inputs to the processes described below and determine quantitative parameterization of the procedures.</p>\n\n  <h2>Scope and Units</h2>\n  <p>The unit of analysis is the claim-evidence pair: an asserted fact or inference (a claim) together with the document-level and artifact-level evidence that purports to support it. Scope is limited to documentary and digital artifacts ingested under the system’s data contract. The framework distinguishes source-level attributes (authorship, provenance, publication channel) from evidence-level attributes (timestamping, cryptographic fingerprints, embedded metadata) so that scoring remains sensitive to both the origin and the content of artifacts.</p>\n\n  <h2>Data Ingestion</h2>\n  <p>Ingestion begins with canonicalizing incoming documents according to the data contract keys. The system expects structured counts and previews (source_type_counts and raw_sources_preview) to inform downstream heuristics; when present, document_scores_v4 and claim_score_preview_v4 provide prior assessments that may be used as features for calibration. All ingested items receive a persistent identifier and metadata record capturing capture time, source channel, and the ingesting agent.</p>\n\n  <h2>PDF-to-Markdown Conversion</h2>\n  <p>Binary or image-based documents are converted into machine-readable text using OCR and layout-aware PDF parsing. Conversion preserves structural elements (headings, captions, footnotes) and records confidence scores for each extracted element. The conversion step produces a normalized markdown-like intermediate, which facilitates consistent downstream parsing while retaining provenance metadata from the original file.</p>\n\n  <h2>Structural Parsing</h2>\n  <p>Structural parsing decomposes documents into title, abstract, body, figures, tables, and references. Each element inherits provenance metadata and a token-level confidence measure. Structural parsing also identifies sections likely to contain claims (executive summaries, conclusions, captions) so that extraction prioritizes high-yield regions for claim detection and reference linking.</p>\n\n  <h2>Artifact Extraction</h2>\n  <p>Artifact extraction isolates discrete evidentiary items: quoted text segments, images with embedded metadata, header fields, and binary payloads. Each artifact is hashed and stored in an artifact registry to facilitate deduplication and to enable chains of custody. Extraction records sources of uncertainty such as OCR errors or image compression artifacts and attaches these as attributes used later in scoring.</p>\n\n  <h2>Reference Parsing</h2>\n  <p>References and in-text citations are parsed into structured reference records. Where DOI, URL, or bibliographic entries are present, the system resolves these to canonical identifiers. Reference parsing records the citation context (supporting/contrasting) and links back to the artifact registry. In the present instance the raw_sources_preview field is null; when available, it supplies sample references that are used to validate parsing heuristics and train disambiguation models.</p>\n\n  <h2>Institution Inference</h2>\n  <p>Institution inference maps author and publisher metadata to canonical institutional identifiers. This uses name authority lists, domain registration records, and cross-referencing with known organizational registries. Inference produces an institution confidence score reflecting ambiguity (for example, multiple entities sharing similar names or domains). Institution-level attributes feed into the source hierarchy by distinguishing independent institutional provenance from potentially colocated or affiliated outputs.</p>\n\n  <h2>Claim–Evidence Graph Construction</h2>\n  <p>Extracted claims and their supporting artifacts are represented as a directed graph: nodes are claims or evidence artifacts and edges denote support, contradiction, or derivation. Each edge is annotated with provenance metadata and an evidence confidence derived from extraction and parsing scores. The claim evidence graph enables efficient evaluation of corroboration across multiple, potentially heterogenous sources and supports visibility into chains of inference.</p>\n\n  <h2>Scoring Overview</h2>\n  <p>The scoring framework produces multi-axial outputs; principal among them for this chapter is the Credibility Axis with a Corroboration subcomponent. Scores combine source-level priors (source hierarchy placement, institutional inference), artifact-level integrity attributes (hashing, metadata), and graph-level measures (number of independent supporting nodes, depth of derivation). Where available, document_scores_v4 and claim_score_preview_v4 serve as prior feature inputs to supervised calibration; in their absence, the framework uses domain-informed default priors and transparent uncertainty bounds.</p>\n\n  <h2>Chain of Custody</h2>\n  <p>Every artifact and derived node in the claim evidence graph maintains a chain-of-custody record recording ingest timestamps, processing actors, and transformation operations. Chain-of-custody attributes are incorporated into credibility calculations: artifacts with incomplete or ambiguous custody records receive down-weighting commensurate with the assessed risk of tampering or misattribution.</p>\n\n  <h2>Credibility Axis with Corroboration Subcomponent</h2>\n  <p>The credibility model operationalizes the terms source hierarchy, independence, corroboration, claim coverage, and exclusion criteria as follows. Source hierarchy is a taxonomy that classifies sources into primary (originator materials such as original logs or signed statements), secondary (reports that synthesize or interpret primary materials), and tertiary (summaries, repostings, or derivative analyses). Institutional inference informs placement within this hierarchy and supplies an institution-level confidence. Independence is assessed by analyzing shared provenance signals: identical artifacts, shared author lists, common hosting domains, or reuse of institutional boilerplate are indicators of non-independence. Independence is treated as a structural attribute on edges in the claim evidence graph and quantifies the marginal evidentiary value added by an additional supporting node.</p>\n\n  <p>Corroboration rules combine independence-adjusted counts with qualitative alignment of supporting artifacts. A corroborative endorsement increments a claim’s corroboration score only if it (a) originates from a source classified at or above a minimum hierarchy threshold, (b) passes a minimal chain-of-custody integrity test, and (c) is sufficiently independent from existing supporters as measured by provenance overlap metrics. Corroboration therefore rewards convergence from distinct, credible lines of evidence rather than mere multiplicity of copies.</p>\n\n  <p>Claim coverage scaling addresses the breadth and directness of support. Direct, substantive matches between an artifact and a claim (for example, a timestamped log entry that explicitly records the asserted action) contribute more to coverage than indirect or inferential references. Coverage is scaled by the proportion of claim elements (actors, actions, dates, locations, technical signatures</section>\n<section class=\"wiki-section\" id=\"sec-clarity_axis\"><h2>Introduction</h2>\n<p>This methodological chapter sets out an operational approach to scoring clarity in cyber-attribution where allegations implicate state responsibility. It is written in legal-academic register and directed at producing reproducible, transparent judgments about how clearly available evidence supports attribution along three recognized state-responsibility pathways: acts by state organs, control over non-state actors, and failures of due diligence. The objective is not to adjudicate particular events but to define the evidentiary architecture, processing pipeline, and scoring conventions that translate heterogeneous documentary artifacts into a graded clarity measure suitable for comparative analysis and legal consideration.</p>\n\n<h2>Scope and Measurement Units</h2>\n<p>The unit of measurement for the clarity axis is the claim-evidence unit, defined as a discrete attribution claim paired with the set of primary and corroborative artifacts that speak to it. Each claim-evidence unit is assessed for (a) the legal pathway invoked (state organs, control over non-state actors, or due diligence failure), (b) the evidentiary modalities present (technical forensics, command-and-control metadata, human source testimony, documentary linkage, or pattern-of-life contextualization), and (c) provenance and processing metadata necessary for chain-of-custody assessment. These structured units permit aggregation while preserving the provenance needed for interpretive caution in matters of state responsibility.</p>\n\n<h2>Data Ingestion and Document Processing</h2>\n<p>Data ingestion begins with the acquisition of source documents in native formats. Documents are normalized into a working representation that preserves original layout, timestamps, authorship metadata, and embedded objects. When source material is PDF, optical character recognition and layout-aware conversion are applied; the process is designed to preserve page ordering, headings, footnotes, figures, and annexes so that subsequent structural parsing can reference original offsets. Given the supplied contextual metadata in this instance—specifically the document-level keys document_scores_v4, claim_score_preview_v4, scoring_bundle, and raw_claims_preview—were present but contain null values, the pipeline must shift to a mode that emphasizes explicit artifact extraction and manual validation rather than automated score inheritance.</p>\n\n<h2>Structural Parsing and Artifact Extraction</h2>\n<p>Structural parsing segments converted documents into hierarchical elements—titles, paragraphs, tables, figures, captions, and bibliographic references—while capturing positional and formatting cues that assist in identifying assertions, quotations, and evidentiary exhibits. Artifact extraction then identifies candidate evidence objects for attribution analysis: technical artifacts (hashes, IP addresses, timestamps, malware signatures), human-sourced materials (declarations, intercepted communications), and documentary linkages (contracts, organizational charts, procurement records). Each extracted artifact is recorded with provenance metadata sufficient to reconstruct the extraction pathway, including the original document identifier and the extraction method used.</p>\n\n<h2>Reference Parsing and Institution Inference</h2>\n<p>Reference parsing isolates cited materials and cross-references within and across documents to build a preliminary citation graph. Institution inference applies named-entity recognition and contextual heuristics to map parties, organizational roles, and institutional</section>\n<section class=\"wiki-section\" id=\"sec-aggregation_calibration\"><h2>Introduction</h2>\n<p>This methodology describes the principled steps used for claim-to-document aggregation, weighting, calibration, and uncertainty quantification within a cyber-attribution scoring pipeline. The procedure is intended to be transparent, reproducible, and explicitly grounded in the available portfolio-level and document-level measurements extracted from the source artefacts. To illustrate the scale and heterogeneity that the method must accommodate, the supplied portfolio summary reports a document_count of 5 and aggregated_totals that include 38 claims, 14 sources, and 25 artifacts; these counts demonstrate the common situation of unequal claim density and evidence richness across documents. The exposition below emphasizes methodological rationale rather than case-specific findings and uses the supplied numeric diagnostics to illustrate how aggregation, calibration, and dispersion controls are operationalised. The goal of the pipeline is to convert heterogeneous, partially observed evidence into calibrated, uncertainty-aware scores while preserving provenance and enabling independent replication.</p>\n\n<h2>Data processing and evidence extraction</h2>\n<p>Raw inputs are first normalised through a sequence of extraction stages that produce structured claim records, artefact linkages, and source metadata. Each document is parsed to produce standardized fields such as pages, tables, figures, claims, sources, and artifacts. Per-document claim counts in the supplied set vary across documents, motivating per-claim weighting rather than uniform aggregation. Claim extraction assigns an evidence list to each claim by linking artefact identifiers and source citations, and documents with incomplete or missing numerical fields are flagged for separate handling. The data processing stage emphasises deterministic extraction rules, canonical identifier mapping for artefacts and sources, and explicit recording of missingness so that downstream scoring can treat absent indicators as informative rather than silently dropping them. All intermediate outputs include provenance pointers to their originating document and extraction step to enable later audit and review.</p>\n\n<h2>References and institution inference</h2>\n<p>Reference resolution and institution inference operate on the normalised source metadata to produce inferred institutional attributions and confidence annotations. The method first canonicalises citation strings and maps organization names to controlled vocabularies where possible, then exploits co-occurrence across cited sources and artefacts to strengthen or weaken institutional links. When explicit institutional metadata is unavailable, the pipeline applies conservative heuristics based on recurring affiliations, email domains, and publisher identifiers, and attaches an uncertainty score reflecting the strength of the inference. This approach preserves the distinction between direct source identity, inferred institution, and purely provenance-level citations, and it propagates inference uncertainty into the later scoring stages rather than collapsing it prematurely into binary assignments.</p>\n\n<h2>Scoring framework</h2>\n<p>The scoring framework translates structured claims, artefact reliability indicators, and source/institution confidence into probabilistic scores for attribution-related assertions. Aggregation applies claim-level weights that reflect document-specific claim density, source reliability, and artifact-level trust metrics; documents with higher claim counts are not permitted to dominate simply by volume because weights include normalization factors to correct for unequal claim density. Calibration is implemented using holdout diagnostics and within-portfolio calibration curves, with score transforms designed to produce well-calibrated probabilities under varied evidence richness. Uncertainty quantification uses ensemble and resampling techniques to estimate score dispersion and to produce interval estimates; these dispersion measures inform downstream decision thresholds and are reported alongside point estimates to avoid overconfident interpretations.</p>\n\n<h2>Validation and quality assurance</h2>\n<p>Validation proceeds through layered checks that include automated consistency tests, held-out calibration assessment, and targeted manual review of high-impact cases. Quality assurance requires that all claims preserve provenance chains back to original artefacts and that any imputation of missing fields is logged with method and assumed distribution. Sensitivity analyses examine how score outcomes shift under alternative weighting schemes and under removal of individual high-leverage documents or artefacts. Operational controls include periodic recalibration against new held-out data, logging of threshold adjustments, and explicit metrics for calibration error and discrimination that are used to gate release of scores. Together these practices aim to ensure the scoring pipeline is robust, auditable, and conservative where evidence is sparse or ambiguous.</p></section>\n<section class=\"wiki-section\" id=\"sec-validation_quality_assurance\"><section aria-labelledby=\"validation-quality-assurance\"><h2 id=\"validation-quality-assurance\">Validation and Quality Assurance</h2><p>This Validation and Quality Assurance section describes the procedural safeguards and automated controls applied to the attribution scoring pipeline. The protocol is grounded in the project data contract and the available raw payload metadata, including the portfolio summary manifest located at <code>/home/pantera/projects/TEIA/annotarium/outputs/reports/portfolio_summary.json</code> and the set of input report artifacts enumerated under raw payload paths. The validation design operates independently of substantive case findings: it is intended to provide reproducible, auditable assurance that structural parsing, artifact extraction, and downstream scoring computations were executed in accordance with schema and integrity expectations.</p><p>Automated validation gates form the primary line of defense against data corruption and processing errors. These gates are implemented as deterministic, idempotent checks that run at defined stages of the pipeline: immediately after ingestion, following pdf-to-markdown conversion and structural parsing, and after the construction of the claim–evidence graph and scoring aggregation. Typical gate checks include schema conformance (field presence and type constraints against the processing contract), checksum and size validation against the supplied file metadata, timestamp and provenance consistency, duplicate artifact detection, and basic plausibility checks (for example, that extracted artifact types correspond to allowed categories). The validation bundle parameter is recorded as null in the supplied section metadata; nevertheless, the gates reference the explicit raw payload paths and file metadata to verify that the expected inputs were present and unmodified prior to scoring.</p><p>Where automated validation gates detect anomalies, the system routes affected items into an agent review queue for triage. The agent review capability is enabled in the documented QA protocol and executes a layered set of remediation actions: automated re-ingestion attempts, adaptive parser selection, and escalations that append diagnostic metadata to the artifact so that subsequent reviewers see the precise failure mode. The design rationale for agent review is to maximize throughput while ensuring that automated corrective steps are attempted before human resources are committed.</p><p>Human review is applied in a targeted, sampled fashion to provide an independent check on automated processing and to estimate residual error rates. The QA protocol specifies a targeted human review fraction of 10% of processed artifacts, selected by a stratified sampling strategy that prioritizes items with high analytic impact and those that triggered non-fatal validation warnings. For the sampled subset the observed error rate reported in the protocol is zero; explicitly, the human_sample_observed_error_rate is 0.0 and the documentation states that the reviewed 10% sample displayed no observed errors. This outcome is reported with the caveat that sampling uncertainty remains: a zero observed error rate in the sample reduces but does not eliminate the possibility of unobserved faults in the population. Consequently, the protocol defines escalation thresholds—if any error is detected during human review, the pipeline will reprocess the entire affected batch and increase the human sampling fraction for one or more subsequent runs.</p><p>Quality assurance practice mandates full auditability: all validation gates, agent review actions, human reviewer annotations, and reprocessing events are persisted in an immutable audit log tied to the original raw payload paths and file metadata (for example, the portfolio_summary.json path and attributes). Error classification taxonomy and remediation outcomes are recorded to enable retrospective calibration of validation gates and to support statistical estimates of residual uncertainty. Together, these elements comprise a defensible QA posture that combines deterministic validation gates, agent review automation, and targeted human review at a 10% sampling rate, with the sampled review yielding no observed errors in the reviewed subset.</p></section></section>\n<section class=\"wiki-section\" id=\"sec-limitations_governance\"><h2>Introduction</h2>\n<p>This methodological supplement describes limits, governance controls, and a principled refinement roadmap for a reproducible cyber‑attribution scoring pipeline. It is written as a legal‑academic procedural statement that emphasizes evidentiary legibility and auditable transformations rather than asserting substantive conclusions about particular cases. To ground the discussion, the remarks below refer to metadata and summary statistics drawn from the present processing run, including a corpus footprint (document_count = 5, generated_at_utc = 2026-02-22T11:31:26Z) and aggregated totals for the record (pages = 5, tables = 31, figures = 40, claims = 38, sources = 14, artifacts = 25). These figures are cited only to illustrate how the method behaves under modest corpus scale and heterogeneity and not to support or repeat any factual claims within the underlying reports.</p>\n\n<h2>Scope and Units of Analysis</h2>\n<p>The unit of analysis is the proposition as encoded in a claim object and its attached evidentiary graph. The pipeline enforces a strict schema that separates claims, sources, artifacts, and explicit evidence links; this design decision preserves inspectable chains of reasoning and compels analysts to situate every attribution proposition in discrete, referenceable terms. At the portfolio level, the present run produced 38 claim objects across five documents, a distribution that the scoring model treats as a weighted claim set rather than an undifferentiated pool of assertions. Treating claims as the primary analytic units clarifies the locus of limitations and governance controls discussed below.</p>\n\n<h2>Data Ingestion</h2>\n<p>Data ingestion begins with source acquisition and checksumed archival of native PDF files. All ingestion events are time‑stamped and persisted. The pipeline records a provenance envelope for each run: the path to the source bundle, the generated_at_utc marker, and the derived metadata. These persisted artifacts permit later reconstruction of any score and function as an initial control against accidental or clandestine modification of source materials.</p>\n\n<h2>PDF to Markdown Transcription</h2>\n<p>PDF→markdown transcription is deterministic where possible and supplemented by constrained, schema‑aware model assistance. The transcription stage outputs a markdown representation and an index of artifact anchors. The system fails runs if artifact anchors are missing or if identifier collisions are detected, enforcing an input integrity requirement prior to structural parsing.</p>\n\n<h2>Structural Parsing</h2>\n<p>Structural parsing maps markdown into schema objects: claims, sources, artifacts, and evidence links. The parser enforces required fields and runs automated integrity checks. In the present run, integrity controls prevented propagation of malformed records; these checks are a core governance control because they delimit which records may enter the scoring pipeline.</p>\n\n<h2>Artifact Extraction</h2>\n<p>Artifact extraction isolates primary materials (e.g., logs, binaries, indicators) and records available provenance signals such as timestamps, identifiers, and asserted handling histories. The pipeline treats extracted artifact counts (present run: artifacts = 25 across the portfolio) as contextual information that informs item‑level weight but does not by itself determine claim weight.</p>\n\n<h2>Reference Parsing</h2>\n<p>Reference parsing identifies cited documents and metadata and attempts to resolve them to canonical identifiers. The system records unresolved references and treats them as suspended support rather than discarding them. By doing so the method maintains conservative evidence accounting and surfaces weaknesses in the citation network.</p>\n\n<h2>Institution Inference</h2>\n<p>Institution inference maps source identifiers to institutional types used in the source‑quality model (e.g., international institution, peer‑reviewed academic, government, NGO). This mapping is explicit and auditable. Institutional classifications influence the source‑quality axis but are applied according to a documented taxonomy; the present portfolio-level averages (for example, average chain_of_custody ≈ 0.52182 and average clarity</section>\n</article>"
}
