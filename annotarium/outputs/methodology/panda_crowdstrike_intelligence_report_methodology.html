<article class="wiki-page">
<header><h1>Annotarium Methodology: APT1: Exposing One of China's Cyber Espionage Units</h1><div class="wiki-meta">Generated at 2026-02-22T21:57:11Z</div></header>
<nav class="wiki-toc"><h2>Contents</h2><ol>
<li><a href="#sec-introduction">Introduction and Epistemic Framing</a></li><li><a href="#sec-scope_units">Scope, Units of Analysis, and Output Semantics</a></li><li><a href="#sec-data_ingestion">Data Ingestion and Corpus Handling</a></li><li><a href="#sec-pdf_to_markdown">PDF-to-Markdown Conversion</a></li><li><a href="#sec-structural_parsing">Structural Parsing of Text, Tables, and Figures</a></li><li><a href="#sec-artifact_extraction">Artifact Extraction and Technical Object Normalization</a></li><li><a href="#sec-reference_parsing">Footnote and Reference Parsing</a></li><li><a href="#sec-institution_inference">Institution Inference and Source Typology</a></li><li><a href="#sec-claim_evidence_graph">Claim-Evidence Graph Construction</a></li><li><a href="#sec-scoring_overview">Scoring Framework Overview</a></li><li><a href="#sec-chain_of_custody">Chain of Custody Axis</a></li><li><a href="#sec-credibility_corroboration">Credibility Axis with Corroboration Subcomponent</a></li><li><a href="#sec-clarity_axis">Clarity Axis and State Responsibility Pathways</a></li><li><a href="#sec-aggregation_calibration">Aggregation, Calibration, and Uncertainty</a></li><li><a href="#sec-validation_quality_assurance">Validation and Quality Assurance</a></li><li><a href="#sec-limitations_governance">Limitations, Governance, and Future Refinement</a></li>
</ol></nav>
<section class="wiki-section" id="sec-introduction"><div>
  <h2>Introduction and Epistemic Framing</h2>
  <p>This methodology is presented as an evidentiary framework for cyber attribution in contexts where findings are contestable and parties may dispute interpretive moves. It adopts an explicitly epistemic posture: scores and labels are not prima facie assertions of fact but structured indices of probative force derived from an auditable evidentiary record. The approach is burden-sensitive in the jurisprudential sense: evaluative thresholds and weighting choices are selected to reflect the asymmetric consequences that flow from attributive claims, and to make visible where inferential steps carry most weight. Structured extraction therefore precedes legal inference so that adjudicable pathways—that is, the mappings from artifacts to claims to legal responsibility—are preserved, inspectable, and reproducible rather than reconstructed after the fact through implicit interpretation.</p>

  <h2>Data Processing and Extraction</h2>
  <p>Data ingestion follows a deterministic, multi-stage pipeline that converts primary documents into a schema-constrained evidentiary record. The pipeline stages documented in the supplied materials include PDF transcription to markdown (primary and fallback methods), table and image extraction, artifact indexing, and structural parsing into distinct objects for claims, sources, artifacts, and evidentiary links. These transformations emphasize persistence and determinacy: intermediate artifacts are written to disk and identifier schemas are enforced so that any downstream score can be traced back to a specific transcription, anchor, or artifact. The methodological rationale for this ordering is epistemic: by externalizing every extraction decision, the method limits implicit model inference and constrains post hoc reinterpretation, thereby preserving the evidentiary chain needed for burden-sensitive adjudication.</p>

  <h2>References, Citation Parsing, and Institution Inference</h2>
  <p>Reference parsing and institution inference are treated as discrete, auditable sub-processes. Citations and footnote-like references are parsed into a source registry that records bibliographic anchors, URL or identifier resolution status, and any provenance metadata discovered during parsing. Institution inference is performed as an auxiliary step that maps parsed references to organizational actors and institutional types; the documented pipeline specifies an inferencing tool informed by large language models with optional web fallback. These inferred institution labels are not final qualifications for source quality; they serve as inputs to a credibility taxonomy and must be subject to human verification. The taxonomy itself encodes normative preferences for international judicial and peer-reviewed academic material, intermediate weight for official government and NGO material, and lower weight for less authoritative outlets, reflecting a burden-sensitive prioritization of sources suitable for contested attribution.</p>

  <h2>Scoring Framework and Aggregation Rationale</h2>
  <p>The scoring framework is derived from an item-level evaluative model and a claim-level corroboration model designed to avoid circularity. Item-level probative force is assessed along bounded dimensions including independence, authentication/provenance, methodological soundness, procedural testing, and contemporaneity. The multiplicative combination of those dimensions is an intentional design choice: it prevents excellence along one axis from masking critical deficiency along another. Corroboration is computed by clustering evidence by origin and aggregating origin-level contributions with diminishing returns, so that successive repetitions of a single upstream source do not artificially inflate independent support. At the claim level, the framework produces three primary axes—Chain of Custody, Credibility, and Clarity—each defined to be auditable and bounded. Chain of Custody operationalizes custody-related indicators (for example provenance markers, temporal anchors, and artifact identifiers) into a normalized quality score. Credibility is a composite of source-quality signals and corroborative convergence, calibrated at the document level by a coverage factor that weights claims by gravity so the measure reflects proportional coverage rather than isolated high-quality citations. Clarity captures legal intelligibility and maps analytic text to clear responsibility pathways under state-responsibility doctrine: direct organ conduct, control/direction of non-state actors, or omission/due-diligence failure. Aggregation rules preserve auditability by bounding intermediate subscores in [0,1] and by exposing the claim–evidence graph so that each claim’s score can be decomposed into item-level contributions and origin-level adjustments.</p>

  <h2>Validation, Integrity Controls, and Quality Assurance</h2>
  <p>Validation operates at both structural and substantive layers. Structural integrity checks enforce identifier uniqueness, anchor presence, and resolvable citation pathways; runs that fail these checks are treated as inadmissible for scoring to avoid pseudo-precision. Procedural safeguards emphasize determinacy where possible and require persistence of intermediate artifacts to enable full reconstruction of any scoring run. Substantive quality assurance includes human review of institution inference, spot-checks of transcription accuracy, and verification of artifact anchors against extracted images or tables. The process is designed so that any numeric output is reconstructible from the persisted record</section>
<section class="wiki-section" id="sec-scope_units"><h1>Introduction</h1>
<p>This methodological chapter sets out the conceptual definitions, processing workflow, attribution scoring architecture, and validation procedures that govern automated and semi-automated assessment of cyber-attribution claims. The exposition is procedural and normative: it explains why particular units of analysis are defined, how textual and structural inputs are transformed into structured evidence, how institutional provenance is inferred from references, and how multi-axis scores are computed and quality-assured. For purposes of illustration and validation, the methodology is grounded in the provided raw metadata and pipeline diagnostics from a single vendor report, whose document-level metadata records a title of "APT1: Exposing One of China's Cyber Espionage Units," an authoring entity of "Mandiant," and a publication date inferred from filename heuristics. Those metadata and pipeline artifacts are used solely to illustrate the method; the methodological text does not adopt or evaluate substantive factual claims appearing in the underlying report.</p>

<h2>Scope and Units of Analysis</h2>
<p>Analytic clarity requires explicit unit definitions. A claim is a discrete propositional assertion extracted from a document; it is the principal unit to which attribution weights are applied. A source is an identified originator or repository of information referenced in the document or discovered during ingestion; source attributes include declared authorship, organizational affiliation, and any available custody metadata. An artifact denotes a technical object or trace (for example, a file hash, malware sample signature, or network indicator) that is described or reproduced in the document and that can be used to link claims to observable phenomena. An evidence item is the minimal evidentiary unit that links an artifact, a passage of text, or an external reference to a claim; evidence items capture provenance, context, and the explicit statement of support (or contradiction) for a claim. Document-level outputs aggregate metrics across the claims, sources, artifacts, and evidence items contained in a single document, delivering summary descriptors that facilitate triage and further review.</p>
<p>It is essential to state what these outputs mean and do not mean. A score attached to a claim does not constitute an adjudication of truth in a legal sense nor does it substitute for independent forensic validation; it is a quantitative expression of confidence and provenance based on defined input features and weighting rules. A document-level score is a descriptive summary of the evidentiary profile of that document under the scoring rubric and is not, by itself, a statement about external entities beyond the document. The methodology distinguishes uncertainty attributable to weak grounding from uncertainty attributable to source custody or clarity of expression; thus scores are intended to inform, not to conclude.</p>

<h2>Data Processing and Extraction</h2>
<p>The processing pipeline begins with ingestion of the source file and its metadata. In the provided example, pipeline diagnostics report a single input file and local metadata attributes, including the inferred publication date. The ingestion stage captures structural metadata (file path, declared document type, authoring entity) and records counts of pages, claims, sources, artifacts and other structural elements produced by downstream parsers. Text normalization and structural parsing convert heterogeneous inputs—here supplied as markdown from an offline vendor report—into a canonical intermediate representation. Optical character recognition and layout-preserving extraction are applied where necessary, but in the present instance the pipeline operated on markdown, producing one sampled page and a set of parsed objects including nine extracted claims and six artifacts as indicated in the pipeline counts.</p>
<p>Structural parsing includes segmentation into headings, paragraphs, captions, and figures, and the extraction of potential evidence items where the text contains artifact descriptors, indicator strings, or explicit source citations. Artifact extraction applies deterministic patterns for technical indicators and named-entity recognition for organizational and person names. Each extracted artifact is normalized to a canonical representation and annotated with provenance: the originating document, the page or section location, and the local parsing confidence score. Throughout extraction the system preserves original text offsets to enable human review and reattachment of context.</p>

<h2>References and Institution Inference</h2>
<p>Reference parsing isolates in-text citations, footnotes, bibliographic entries, and explicit attributions. When explicit bibliographic data are absent, heuristics combine authoring entity fields and contextual phrasing to infer institution-level provenance. In the raw metadata associated with the example report, the authoring_entity field provides an explicit institution string. Institution inference combines declared author strings with corroborating signals such as attached source locators, document type tags, and custody metadata to form a structured source record. The inference pipeline is conservative: where metadata are incomplete or derived (for example, an inferred publication date from filename heuristics), the pipeline annotates the resulting source record with provenance flags and lower custody scores rather than substituting definitive institutional assignments.</p>
<p>All inferred institutional attributions are treated as inputs to scoring rather than as authoritative labels. The rationale for conservatism is methodological transparency: institution inference affects custody and credibility axes of scoring, and overconfident institution labels can distort downstream composite measures. Therefore the system records both the inferred institution and the basis for that inference to support re-evaluation by human analysts.</p>

<h2>Scoring Framework</h2>
<p>The scoring framework is multi-axial, decomposing claim assessment into distinct, interpretable components. The principal axes are grounding, custody, credibility, clarity, corroboration, and belief. Grounding measures the degree to which a claim is linked to concrete evidence items and artifacts; custody quantifies the strength of provenance metadata for the evidence; credibility assesses the historical reliability of the source when external calibration data are available</section>
<section class="wiki-section" id="sec-data_ingestion"><h2>Introduction</h2>
<p>This methodology chapter describes the processes and controls applied to ingest, parse, structure, and score a document-level input corpus for the purposes of cyber-attribution analysis. The account that follows addresses the theory and implementation of deterministic handling of source files, the provenance assurances applied to intermediate artifacts, the mechanisms used to extract references and infer institutional attribution, the conceptual basis for scoring and calibration, and the validation and quality-assurance procedures that together enable reproducibility. The exposition is intentionally general and methodological in nature; it does not present or interpret case-specific factual findings from the underlying report.</p>

<h2>Scope and Unit Definitions</h2>
<p>The principal unit of analysis is the input corpus item, here instantiated as a single PDF report and its derivative artifacts. For methodological clarity, the corpus is described in terms of discrete, addressable artifacts: the original PDF, primary text extraction outputs, structural markdown or tokenized representations, extracted tables and figures, artifact indices (evidence items), reference registries, and scoring payloads. Each artifact is assigned a stable identifier at ingestion to permit deterministic references across pipeline stages. The terms deterministic, reproducibility, and input corpus are deliberately emphasized to align provenance commitments with operational practice.</p>

<h2>Data Ingestion and Corpus Handling</h2>
<p>Ingestion commences with the immutable intake of the original report file, which in this instance is recorded at the path /home/pantera/projects/TEIA/annotarium/Reports/Panda - CrowdStrike Intelligence Report.pdf with a byte size noted in the ingestion manifest. Deterministic file handling is achieved by recording the absolute path, byte-size, and cryptographic checksum of the binary PDF at the moment of intake and persisting that metadata alongside a record-level identifier (for example, report_id). Downstream artifacts are named and timestamped deterministically relative to that intake record so that a sequence of pipeline stages can be replayed to produce identical artifacts from the same input corpus item. The ingestion manifest and report-level metadata are stored in a structured report JSON file, here reflected at /home/pantera/projects/TEIA/annotarium/outputs/reports/panda_crowdstrike_intelligence_report_report.json and annotated with generation timestamp metadata (generated_at_utc). These entries function as the canonical starting point for reproducibility and provide the first link in the chain of custody.</p>

<h2>PDF-to-Markdown and Primary Extraction</h2>
<p>Primary content extraction is implemented with a prioritized, deterministic operator sequence: a provider-backed conversion using process_pdf_mistral_ocr.py is attempted first and, on defined failure modes, a deterministic offline fallback using PyMuPDF4LLM is invoked. Each operation emits a markdown representation together with a machine-parseable extraction JSON. For the present corpus item, outputs are recorded at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/panda_crowdstrike_intelligence_report/panda_crowdstrike_intelligence_report.output.json and a rendered markdown at the corresponding .md path. The markdown stage is designed to preserve structural anchors for tables and images; these anchors allow reproducible reconstitution of figures and the table payload by position and identifier rather than by heuristic re-parsing. The extraction stage also records validation artifacts (for example, a validation JSON manifest) to indicate whether the primary extraction met pre-specified quality gates.</p>

<h2>Structural Parsing and Artifact Extraction</h2>
<p>After text extraction, a structural parsing stage converts markdown anchors and layout signals into a normalized document object model. This model emits indexed artifacts: named tables, figure descriptors, caption text, and candidate evidence items identified from body text and tables. Artifact extraction adheres to a deterministic schema: each artifact is assigned an index, positional coordinates, and a hashed identifier derived from the source extraction segment. The artifact index is captured in an artifact output (schema extraction stage) so that downstream reviewers and automated scorers can reference the same evidence item across repeated runs without ambiguity. The artifact extraction outputs for the present corpus were persisted in the pipeline outputs and are referenced by path in the ingestion manifest to ensure traceability between the original PDF and the extracted items.</p>

<h2>Reference Parsing</h2>
<p>Reference parsing identifies citations, footnotes, and other internal cross-references and links them to a source registry. Parsing is performed by an explicit ruleset that recognizes in-text citation patterns, enumerated reference blocks, and footnote indicators, and then normalizes those references into a structured registry that records the cited text, its anchor location in the extracted text, and any external identifiers. This registry is stored as a first-class artifact (validation and reference JSON) and is designed to be idempotent: repeated parsing of the same extraction artifact yields the same registry entries. The record in the pipeline (for example, a validation_report_json) documents parsing success, errors, and the number of recognized references as a diagnostic metric; such counts are used for quality assessment and to trigger human review when thresholds are not met.</p>

<h2>Institution Inference</h2>
<p>Institutional-source inference is performed by a deterministic orchestration script (infer_source_institutions.py) that combines pattern-based heuristics with model-assisted natural language processing. The method employs a documented prompt-and-evidence approach whereby candidate institutional mentions, contact details, and domain-level signals extracted from the input corpus are assembled into a scoring input. The process permits an optional external web fallback for corroboration but records when that fallback was used. All inference outputs are accompanied by provenance metadata that records the inputs, model versions used (for example, gpt-5-mini where applicable), any web queries performed, and a timestamped decision record so that the inference can be reproduced or audited from the same input corpus and configuration.</p>

<h2>Claim–Evidence Graph Construction</h2>
<p>Extracted claims and their candidate supporting evidence items are represented in a directed claim–evidence graph. Nodes represent textual claims, artifact indices, and external references; edges encode the claimed</section>
<section class="wiki-section" id="sec-pdf_to_markdown"><h2>Introduction</h2>
<p>This methodology chapter describes the conversion, processing, and scoring posture employed for cyber-attribution analysis in a manner consistent with academic and legal standards. The purpose of the methods presented is to document reproducible procedures for transforming a primary report into structured evidence, for extracting and linking artifacts and references, for inferring probable institutional origins of sources, and for operationalizing an attribution scoring framework. The exposition emphasizes methodological rationale and resilience rather than asserting or summarizing any substantive findings contained in the underlying report.</p>

<h2>Data Processing and Extraction</h2>
<p>The pipeline adopts a two-tiered approach to document conversion and structural parsing that privileges provider-backed optical character recognition while retaining a robust offline fallback. Primary conversion from PDF to a working markup form is performed using a Mistral-based process, implemented operationally as process_pdf_mistral_ocr.py, which yields a markdown representation suitable for downstream natural language processing and human review. When provider conversion fails or exceeds operational time bounds, an offline fallback posture is invoked via PyMuPDF4LLM; this offline fallback is explicitly a resilience mechanism to maintain continuity of extraction rather than a conceptual departure from the primary method. The stage1 markdown parse intentionally emits table structures and figure/image anchors to preserve spatial and tabular context, and a subsequent schema extraction stage emits artifact indices derived from text, tables, and images. The pipeline metadata documents processing volumes (for example, an instance of this pipeline recorded one page, nine identified claims, six artifacts, and one figure), which are used here to illustrate method behavior and throughput rather than to summarize report content.</p>

<h2>References and Institution Inference</h2>
<p>Reference parsing is treated as a discrete automated stage in which citations and footnote-like references are parsed and linked to a source registry. This registry supports provenance tracking by connecting parsed references to canonical records and to the original document anchors emitted by the markdown stage. Institutional inference is performed by infer_source_institutions.py, which leverages a language model (gpt-5-mini) as the primary inference engine and may optionally consult web-based resources when necessary. The institutional inference step is presented as an analytic layer that produces candidate source attributions along with confidence metadata; the method emphasizes traceability by recording the markdown anchors, artifact indices, and the inference inputs used for each candidate attribution.</p>

<h2>Scoring Framework</h2>
<p>The scoring framework translates structured inputs into calibrated attribution scores while preserving an auditable chain of evidence. Inputs into scoring are derived from the schema-extracted artifacts, the structured claim set produced by paragraph- and anchor-level parsing, and the institutional inference outputs. Scoring artifacts and intermediate outputs are persisted to named artifacts in the repository; for example, score inputs for a reviewed run are retained at the stated score_input_v3 path and scored outputs are retained at the declared full_scores and full_scores_v3 locations. The framework distinguishes between evidentiary presence (qualitative linkage of a claim to an artifact), corroboration (independent corroborative signals), and clarity (degree to which the claim is explicitly supported by a cited artifact); these axes are combined according to pre-specified calibration rules to produce an overall attribution score while preserving per-axis diagnostics for later review.</p>

<h2>Validation and Quality Assurance</h2>
<p>Quality assurance comprises automated validation, manual review checkpoints, and archival of provenance metadata. Automated validation reports are produced for each run and are retained alongside upstream artifacts; reference is made here to the validation_report_json to indicate that a formal validation artifact is created and stored. Validation checks include concordance of extracted artifacts with the markdown anchors, completeness of reference parsing, and consistency of institutional inference with documented inputs. The system logs file-level provenance (for example, original PDF and markdown artifacts saved at the declared pdf and markdown paths and with the associated file-size metadata) to permit reproducibility audits. Where provider conversions are used, the offline fallback path is exercised as a quality-assurance control to ensure that timeouts or provider failures do not introduce undetected data-loss; this redundancy is an explicit resilience design choice. Together, the validation steps and retained artifacts enable third-party audit and re-computation of scores under alternative parameterizations while maintaining a documented chain of custody for each extracted datum.</p></section>
<section class="wiki-section" id="sec-structural_parsing"><h2>Introduction</h2><p>This methodology chapter outlines a reproducible approach to parsing and scoring attribution-relevant content from intelligence artifacts. The procedures described below are grounded in the available extraction outputs and metadata, and are intended to support auditability, repeatability, and defensible chain-of-custody practices. Where raw extraction outputs are cited, they are used to illustrate methodological behavior rather than to restate substantive findings from the source materials. The approach emphasizes preservation of anchors that map extracted content back to its precise locations in the original document, and treats structural elements such as text blocks, tables, and figures as first-class objects in downstream scoring and review workflows.</p><h2>Data processing and extraction</h2><p>The ingestion pipeline begins with a deterministic capture of the source file and associated metadata. For example, document metadata fields such as title, authoring entity, publication date provenance, and source_locator are recorded alongside the extracted content. Early-stage counts (for instance, pages: 1, claims: 9, sources: 1, artifacts: 6, tables: 0, figures: 1) are retained as provenance indicators to guide sampling and QA, and to demonstrate the pipeline behaviour on an individual document. The extraction workflow performs structural parsing to identify discrete text blocks, table constructs, and embedded figures or images. Each extracted object is represented with an anchor that records the extraction method (e.g., OCR, markup conversion, manual description), a stable anchor_id, and a location tuple (such as page_index and object identifier). The example publication_date_anchor in the metadata shows the anchor pattern: an anchor_id, an extraction_method of manual_description, and a location with page_index 0. Anchors are intentionally preserved as machine-readable tokens so that any downstream assessor can re-locate the exact source text or artifact in the original file for independent review.</p><p>Structural parsing treats tables and figures differently from linear text. Tables are parsed into row/column models with their original coordinates and surrounding caption text retained as anchors. Figures and images are stored as artifact objects with captions, nearby narrative anchors, and image-level hashes for integrity checking. When tables are absent but tabular content exists in text blocks, heuristics detect columnar patterns and emit a synthetic table object together with an anchor to the original text span. Anchors therefore function both as provenance and as an audit trail that supports later human verification and automated re-extraction.</p><h2>References and institution inference</h2><p>Reference parsing separates explicit citations, in-text references, and institution mentions into discrete, anchored tokens. Citation strings and surrounding context are normalized and linked to source_locator entries where available. Inference of institutional association proceeds only from linguistically grounded signals (named entities, organizational mentions, footnotes) and corroborating anchors; it does not rely on opaque heuristics that obscure provenance. All inferred links to institutions are accompanied by a confidence metric derived from the quantity and quality of anchored references—quantity measured by distinct anchor occurrences and quality by anchor provenance (for example, whether the anchor derives from a caption, metadata field, or body text). These steps create a transparent mapping from observed textual indicators to institution-level hypotheses, enabling later reviewers to follow every inference back to the anchored evidence.</p><h2>Scoring framework</h2><p>The scoring framework integrates anchored evidence into a layered representation that separates claim extraction, evidence linking, and attribution scoring. Evidence items retain their anchors and an evidence-type tag (text block, table, figure, artifact). Claim-to-evidence links are stored as explicit edges in a claim evidence graph that preserves anchor identifiers for both claim and evidence nodes. Scores are computed from calibrated functions that weigh anchor provenance, artifact integrity, and corroboration across independent sources. Weights are tunable and documented; for instance, anchors originating from structured metadata or figure captions may be assigned different baseline trust than those from OCR-transcribed body text. Crucially, numeric scores are presented as diagnostic indicators of confidence and are accompanied by the underlying anchored evidence list so that the numerical output can be audited to the originating anchors.</p><h2>Validation and quality assurance</h2><p>Validation procedures exercise the anchor-preserving pipeline end-to-end. QA includes automated consistency checks (anchor uniqueness, location plausibility, hash matching for image artifacts) and manual spot checks that re-open the original source at the anchor location to confirm correct extraction. Validation artifacts and pipeline outputs are stored alongside their original file paths (for example, paths to raw_extraction and validation_report_json) to maintain a documented chain of custody. The QA regimen further includes sensitivity analyses that demonstrate how score outputs change when specific anchored evidence items are removed or re-weighted, thereby making clear the dependence of attribution scores on particular anchored elements. Together, these measures ensure that the methodology supports auditability and accountable decision-making while preserving the ability of independent reviewers to trace any claim or score back to the exact anchored source.</p></section>
<section class="wiki-section" id="sec-artifact_extraction"><section><h2>Introduction</h2><p>This methodology chapter describes a principled approach to artifact extraction and technical object normalization across modalities, and the implications of those processes for provenance and custody evaluation in cyber-attribution scoring. The exposition emphasizes methodological rationale rather than case-specific findings. It specifies the units of analysis, the ingestion and conversion pipeline, structural parsing, extraction and normalization strategies for technical objects, and the downstream treatment of references and inferred institutional attributions. Where applicable, the text cites and uses supplied processing metadata and artifact summaries to illustrate how methods operate on heterogeneous inputs.</p></section><section><h2>Scope and Units</h2><p>The scope of this methodology is the transformation of raw document inputs into structured, normalized technical objects suitable for attribution scoring. Units of analysis consist of discrete artifacts (for example: domains, email addresses, file names, cryptographic hashes, IP addresses, and URLs) together with their contextual spans in source documents and associated provenance metadata. The intent is to treat such items as technical objects whose normalization and linkage are prerequisites to any credible provenance or custody assessment. The methodological focus is on repeatable, auditable transformations, including provenance metadata capture at each processing stage.</p></section><section><h2>Data Ingestion</h2><p>The ingestion layer receives heterogeneous source formats and routes them into conversion processes that preserve original timestamps, original file identifiers, and conversion provenance. Pipeline metadata provided with the source material indicates primary and fallback conversion tools; these are recorded as part of the object-level provenance. The supplied artifact summary shows multiple artifact classes (for example, 53 domains, 11 emails, 2 file names, 9 MD5 hashes, 4 IPs, and 15 URLs), which exemplify the multimodal profile commonly encountered and the need to retain modality-specific contextual cues during ingestion.</p></section><section><h2>PDF-to-Markdown Conversion</h2><p>Primary conversion of PDF sources into a machine-tractable format is undertaken with a provider-backed OCR/conversion process, with an offline fallback when provider conversion fails or times out. The methodology records which conversion path was executed for each document and attaches conversion confidence metadata. Conversion artifacts — including extracted text, table/figure anchors, and OCR confidence metrics — are retained as provenance events. This approach ensures that subsequent normalization steps can reference back to the conversion artifact when assessing the fidelity of extracted technical objects.</p></section><section><h2>Structural Parsing</h2><p>After conversion, structural parsing identifies document components such as sections, captions, tables, figures, footnotes, and inline citations. The parser emits anchors for tables and images so that artifact extraction can link artifacts to their structural origin. Structural parsing preserves relative offsets and neighborhood text to support contextual disambiguation: for example, whether a domain appears in a bibliography entry, a running text paragraph, or a table cell influences downstream treatment of that domain as an evidentiary item versus an incidental string.</p></section><section><h2>Artifact Extraction and Technical Object Normalization</h2><p>Artifact extraction operates across modalities by applying modality-appropriate regularization and canonicalization. For domains and URLs, extraction normalizes case, strips protocol prefixes where appropriate, and records both raw and canonical forms. Email addresses are normalized to a canonical local@domain form while retaining original punctuation and surrounding context. File names are recorded verbatim and tokenized into components (for example, organization, topic tags, and date-like strings) to aid matching; cryptographic hashes (MD5 in the provided material) are validated for format and canonicalized to lower-case hex representation. IP addresses are normalized to canonical dotted-quad notation and, where available, associated with ASN and geo-resolution artifacts in a secondary enrichment step. The normalization vocabulary explicitly records the original representation, the normalized token, and the applied rule set so that every transformation is reversible or auditable for provenance review.</p><p>The normalization of technical objects is driven by the dual goals of deduplication and stable identity for scoring. For instance, the example artifact inventory supplied demonstrates modality heterogeneity that benefits from dedicated normalization rules: multiple URL variants often map to a single canonical domain, and file name variants may encode identical source documents. The normalization process therefore produces an object registry that maps variant textual representations to canonical identifiers and documents the mapping rules and evidence that underlies each canonicalization decision.</p></section><section><h2>Reference Parsing</h2><p>Reference parsing extracts bibliographic-like citations, footnote constructs, and inline references, linking them to a source registry. The parser records the span of the reference, any DOI/URL/hash identifiers present, and linkage confidence. This enables the construction of directed provenance edges between claims or quoted passages and their cited artifacts. The method distinguishes between explicit citations that point to external sources and incidental references (for example, mentions of a domain in passing) and records that distinction as part of the provenance model.</p></section><section><h2>Institution Inference</h2><p>Institution inference is performed as a downstream, optional augmentation that attempts to map extracted artifacts and references to probable source institutions. The methodology treats any inferred institution as a hypothesis that must be accompanied by its inference provenance (the model/tool used, input features, and confidence). In the supplied processing metadata, institution inference is noted as an automated step using a large language model-based utility with optional web fallback; this usage is recorded in the provenance ledger for transparency and later validation. Institutional attribution is therefore stored alongside confidence scores and links back to the supporting artifacts and references.</p></section><section><h2>Claim–Evidence Graph Construction</h2><p>Extracted artifacts, normalized technical objects, and parsed references are represented in a claim–evidence graph where nodes are claims, artifacts, and sources, and edges represent citation, derivation, or co-occurrence relationships. The graph encodes provenance at the edge level, including extraction method, structural origin (for example, table row, figure caption), and conversion lineage. The graph model supports queries that surface chains of custody, enable path-based corroboration analysis, and allow the scoring system to weight evidence by its provenance pedigree and modality.</p></section><section><h2>Scoring Overview</h2><p>The scoring framework consumes the claim–evidence graph and applies modular axes such as grounding, custody, credibility, corroboration, and clarity. The supplied document-level summary metrics illustrate the kinds of composite outputs the scoring framework produces (for example, grounding and custody averages, and a belief-weighted measure), but these numbers are included here only to exemplify structural behavior: axes are intended to be independently interpretable, and each score is accompanied by bootstrap confidence intervals and provenance annotations. Scores are derived from quantifiable features such as presence of explicit references, conversion confidence, normalization ambiguity, and the density of corroborating artifacts across independent sources.</p></section><section><h2>Chain of Custody and Provenance</h2><p>Chain-of-custody recording is integral to the methodology. At every transformation stage—ingestion, conversion, structural parsing, extraction, normalization, enrichment, and inference—the system records an immutable provenance event containing actor, timestamp, method identifier, and parameterization. This chain allows auditors to reconstruct how a canonical technical object was derived from raw inputs and to assess whether intervening transformations materially affect attribution conclusions. Provenance metadata is also used to adjust custody axis scores when conversion or extraction steps introduce uncertainty.</p></section><section><h2>Credibility and Corroboration Axes</h2><p>The credibility axis evaluates the trustworthiness of sources and artifacts, considering both internal document signals (for example, explicit citations and structural context) and external signals obtained through enrichment. Corroboration measures the degree</section>
<section class="wiki-section" id="sec-reference_parsing"><h3>Introduction</h3>
<p>This methodology describes a reproducible approach to parsing footnotes and references from document-derived data, resolving rhetorical citations into analyzable constructs, and integrating those constructs into a coherent source graph usable for cyber-attribution scoring. The exposition is framed as a general method and does not draw conclusions about particular allegations or actors. Methodological decisions are explained with reference to pipeline metadata and the available raw-source preview to justify tradeoffs in sensitivity, precision, and provenance tracking.</p>

<h3>Scope and Units of Analysis</h3>
<p>The unit of analysis comprises discrete document pages, extracted artifacts (for example, code snippets, network indicators, images), declared claims, and canonicalized source records. Pipeline instrumentation reports a small ingest in the exemplar dataset (pages: 1; claims: 9; sources: 1; artifacts: 6; citations: 0; figures: 1). These counts are used here only to illustrate how processing stages scale and to motivate robustness to missing explicit citation markers rather than to summarize substantive content.</p>

<h3>Data Ingestion</h3>
<p>Input materials arrive via the documented pipeline and are registered against a source registry keyed by source_id, title, type, entity_name, venue, year, and any provided URL or identifier. The pipeline contract enumerates the fields that must be preserved (section_data_contract_keys). Each ingest event appends a provenance record that timestamps the pull, identifies the conversion method invoked, and retains a copy-hash of the original PDF when available. This provenance seed is essential to later chain-of-custody assertions and to tie canonicalized references back to original byte-level artifacts.</p>

<h3>PDF-to-Markdown Conversion</h3>
<p>Conversion is performed using a primary provider-backed OCR/markup stage (pdf_to_markdown_primary: process_pdf_mistral_ocr.py) with an offline fallback (pdf_to_markdown_fallback: PyMuPDF4LLM). The methodology prescribes retaining both the primary conversion and fallback outputs when discrepancies are detected. Page-level alignment tables map token spans to original page coordinates and are preserved to support later artifact extraction, figure anchoring, and footnote localization.</p>

<h3>Structural Parsing</h3>
<p>Structural parsing identifies document constructs such as headings, paragraphs, tables, figures, captions, and explicit footnote regions. The parser creates a token stream augmented with structural tags and coordinate anchors. Where the primary conversion produces ambiguous segmentation (for example, merged columns or split captions), heuristics informed by lexicon statistics and layout geometry are applied conservatively to avoid creating spurious citation anchors.</p>

<h3>Artifact Extraction</h3>
<p>Artifact extraction emits typed artifact records (the pipeline lists six artifacts in the preview) that include an artifact identifier, type, bounding coordinates, extracted content, and a linkage to the parent page and source record. Artifacts are indexed separately from citations so that evidence attached to artifacts (for instance, an indicator in a figure) can be linked into the citation resolution process when the rhetorical citation references a figure or table rather than a bibliographic entry.</p>

<h3>Reference Parsing, Citation Linkage, and Resolution</h3>
<p>The reference parsing stage detects both explicit bibliographic references and footnote-like in-text markers. Detection combines rule-based pattern matching (for numeric and author-year tokens), layout-driven association of superscripts to footnote blocks, and language-model-assisted disambiguation when layout cues are absent. When a footnote region is found, the text is parsed into a bibliographic candidate that is then normalized against the source registry. Citation linkage resolves rhetorical citations (for example, parenthetical mentions or footnote markers) to concrete source records or, when no registry match exists, to provisional external identifiers constructed from normalized bibliographic strings and document metadata. The result is a directed set of edges from citing spans (claims or artifacts) to cited nodes; these edges form the basis of the source graph.</p>

<h3>Institution Inference</h3>
<p>Institution inference leverages a hybrid approach: deterministic normalization of publisher and corporate names is followed by a language-model-assisted disambiguation (pipeline method: infer_source_institutions.py using gpt-5-mini with optional web fallback). The model is used only to propose canonical institution identifiers and to generate confidence scores; any model-proposed mappings are validated against the registry and, where possible, external authority files. This approach preserves human-auditable rationale while allowing the pipeline to handle name variants and abbreviations automatically.</p>

<h3>Claim–Evidence Graph Construction</h3>
<p>Claims extracted by the pipeline (nine in the preview) are represented as nodes in a claim–evidence graph. Evidence nodes include artifacts, cited sources, and document spans. Directed edges indicate support, contradiction, or reference relationships. Each edge is annotated with provenance (which conversion and parsing methods produced it), a confidence assessment from the citation linkage step, and any positional anchors. The graph model is designed to be queryable for provenance traces, enabling analysts to follow citation linkage from a rhetorical citation to the underlying artifact bytes.</p>

<h3>Scoring Framework Overview</h3>
<p>The scoring framework translates features of the claim–evidence graph into interpretable metrics for attribution analysis. Features include citation density, proximity of supporting artifacts to the claim span, independence of corroborating sources, and the strength of institution inference. Scores are computed modularly so that individual factors can be audited and recalibrated; numerical exemplars from the pipeline_counts are used in testing to verify scoring behavior but are not presented as substantive findings.</p>

<h3>Chain of Custody and Provenance</h3>
<p>Every node and edge in the source graph records chain-of-custody metadata: ingest timestamp, conversion method, converter version, source identifier, and document hash. These provenance fields permit downstream consumers to assert the origin of particular citation linkages and to rerun conversion steps when required for dispute resolution or deeper analysis.</p>

<h3>Credibility Corroboration</h3>
<p>Credibility assessment aggregates signals from independent corroboration, publication venue reputation (normalized via institution inference), recency, and direct artifact corroboration. The methodology emphasizes transparent scoring rules and thresholds so that corroboration can be recomputed as new sources are added to the registry or as institution mappings are refined.</p>

<h3>Clarity Axis and Ambiguity Handling</h3>
<p>The clarity axis quantifies how explicit a citation linkage is, ranging from unambiguous bibliographic matches to weak rhetorical allusions. Ambiguity is managed by retaining multiple candidate linkages with associated likelihoods rather than forcing premature dis</section>
<section class="wiki-section" id="sec-institution_inference"><h2>Introduction</h2>
<p>This methodology chapter explicates the procedures and epistemic rationale employed for institution inference and source typology within a cyber-attribution scoring pipeline. It situates institution inference as a component of evidence triage that informs subsequent credibility weighting and corroboration eligibility decisions. The exposition remains procedural and methodological: it draws on pipeline artifacts and metadata while intentionally avoiding discussion of any case-specific adversary attributions or substantive findings from underlying reports.</p>

<h2>Scope Units</h2>
<p>The unit of analysis for the methods described here is the discrete source object as represented in the pipeline registry. Examples of such units include internal document sections, whole reports, parsed tables and images, and extracted artifacts. The raw data previewed for this work includes a single internal document section identified as SRC0001 with associated metadata (title: "APT1 Executive Summary and Key Findings", entity_name: "Mandiant", publication_or_venue: "Mandiant APT1 Report", year: 2013). Aggregate counts of source types in the current dataset are captured in source_type_counts, which in the present preview shows one internal_document_section. Methodological rules scale from single-source handling to batched adjudication across many such units.</p>

<h2>Data Ingestion and Processing</h2>
<p>Raw source ingestion follows an automated conversion and parsing pipeline that is designed to preserve provenance metadata and structural elements. The primary conversion path is provider-backed OCR and markdown conversion (pdf_to_markdown_primary documented as process_pdf_mistral_ocr.py). When the primary path fails or times out, the pipeline employs an offline fallback (pdf_to_markdown_fallback via PyMuPDF4LLM). Both paths emit a normalized markdown-like representation together with anchors for tables and figures. Runtime environment details are recorded to support reproducibility and chain-of-custody assessments; the runtime_libraries manifest includes versions for Python, PDF tools, and model client libraries, which are persisted in the evidence registry.</p>

<h2>Structural Parsing and Artifact Extraction</h2>
<p>Following markdown normalization, a structural parsing stage identifies semantic elements such as headings, enumerations, tables and figure anchors. The table_and_image_extraction stage emits discrete references to tabular and graphical content to facilitate fine-grained review. Artifact_extraction then generates schema-conformant artifact indices that map textual references and table cells to canonical artifact identifiers. These indices are essential for linking claims to observable artifacts in the claim-evidence graph and for enabling traceable corroboration queries.</p>

<h2>Reference Parsing and Linking</h2>
<p>Reference_parsing processes citations, footnotes and cross-references, producing a linked registry entry for each cited item. Parsed references are normalized and cross-checked against the source registry to detect intra-document and inter-document citations. This linked reference graph informs later steps, particularly when evaluating whether independent institutional sources corroborate a given claim.</p>

<h2>Institution Inference and Source Typology</h2>
<p>Institution inference uses a hybrid approach that combines heuristics derived from explicit metadata with model-assisted inference. The pipeline component infer_source_institutions.py performs initial classification by consulting structured metadata fields (entity_name, publication_or_venue, year) and by applying a generative model for disambiguation (documented use of gpt-5-mini, with optional web fallback when local evidence is insufficient). Source typology maps each inferred institution into a taxonomy of institutional classes (for example, commercial forensic firm, academic research group, government agency, independent analyst, and raw archival material). Typology definitions are operationalized as mutually exclusive labels with accompanying provenance confidence scores.</p>

<h2>Claim–Evidence Graph Construction</h2>
<p>Extracted claims are represented as nodes in a directed claim–evidence graph connected to artifact nodes and to source nodes. Institutional class labels are attached to source nodes and propagated through the graph so that downstream adjudication algorithms can condition on institutional attributes. Chain-of-custody metadata are retained at node and edge levels to document processing history and the transformations applied to each evidence item.</p>

<h2>Scoring Overview: Credibility Weighting and Corroboration Eligibility</h2>
<p>The scoring framework separates two related mechanisms: credibility weighting and corroboration eligibility. Credibility weighting assigns a continuous score to source nodes that reflects institutional class priors, documented methodological transparency, temporal proximity, and internal provenance indicators. These priors are informed by the source typology and calibrated using an external reference corpus when available. Corroboration eligibility is a binary gating function that determines which sources may act as independent corroborants for a claim. Eligibility rules explicitly exclude sources that share provenance chains or that are derivative renditions of the same primary evidence unless independent verification metadata are present. The interaction between weighting and eligibility is structured so that high-credibility sources can contribute more strongly to aggregate scores, while eligibility prevents double-counting of effectively identical attestations.</p>

<h2>Chain of Custody</h2>
<p>Chain-of-custody records are integral to both credibility weighting and corroboration eligibility. The pipeline persists transformation logs (ingestion path, parsing tools, model-assisted inferences) and associates them with source nodes. When institution inference relies on model-generated inferences, the system records model identity, prompt templates, and fallback actions to allow retrospective auditing and to adjust confidence scores when models or dependencies change.</p>

<h2>Credibility and Corroboration Calibration</h2>
<p>Aggregation and calibration mechanisms combine weighted contributions from eligible sources using a probabilistic fusion model. Calibration procedures include holdout validation against a labeled corpus, sensitivity analysis to institutional class prior settings, and re-weighting heuristics to penalize shared provenance. The design ensures that the system maintains separability between score components so that stakeholders can trace how institutional attributes influenced the final attribution score.</p>

<h2>Validation and Quality Assurance</h2>
<p>Validation and quality assurance encompass deterministic checks and empirical evaluation. Deterministic checks verify schema conformance for artifact indices, completeness of provenance records, and consistency of typology assignments against the source_type_counts manifest. Emp</section>
<section class="wiki-section" id="sec-claim_evidence_graph"><div><p>Introduction. This subsection explains the purpose and formal definition of the claim-evidence graph used in the attribution scoring pipeline. The graph is a directed, typed data structure whose principal node classes are claims, evidence items, artifacts, sources, and anchors. Edges encode explicit linkages: claims cite evidence items, evidence items reference anchors (page/block identifiers), anchors map to extracted artifacts (for example, email and domain strings), and evidence items inherit provenance metadata from source records. The structure is intended to support rigorous traceability from any claim statement back to the smallest extractable artifact and to enable reproducible scoring operations. In describing the construction below, the methodology references example records present in the supplied raw data (for instance, normalized evidence items such as E-0005 and artifact anchors such as ART00037) only as illustrations of method behavior and not as substantive findings.</p><p>Data processing and extraction. The construction pipeline begins with structural parsing of an input document into page and block anchors (the block identifiers captured as anchor-level locators). Textual and binary artifact extraction populates an artifacts table (examples in the raw_artifacts_preview include domain, email, ip, url, file_name and hash_md5 types). Each artifact record records extraction provenance (page, block_id and extraction confidence) so that any downstream assertion may be traced back to an anchor. Evidence items are then assembled by grouping one or more anchors into a single evidence node and computing an evidence feature vector. The scoring bundle demonstrates this design: evidence items carry a feature vector (I, A, M, P, T), an origin identifier (for example ORIG:src0001), an explicit list of anchors, and a computed probative_weight. Anchor-level traceability is preserved by recording the block_id values (for example ART00066, ART00030) alongside each evidence node so that consumers can present or re-inspect the original anchor context.</p><p>Reference parsing and institution inference. References and attribution metadata are parsed from both explicit citations and embedded source metadata. Normalized source records include authoring_org and publisher fields (the supplied normalized sources provide an authoring_org value). Institution inference uses structured source attributes (authoring_org, domain, publication date) and heuristics that weigh internal metadata ahead of derived paraphrases. The pipeline distinguishes between an evidence origin (origin_id) and recovered references: recovered_reference_count and recovered_reference identifiers are recorded where the chain links to external material. This explicit separation supports independent verification and prevents conflation of inferred institutional ties with primary extraction provenance.</p><p>Claim-evidence graph formation, traceability and anti-circularity safeguards. Claims are represented as distinct nodes with pointers to their supporting evidence_ids. Each evidence node retains anchor lists and artifact identifiers so that any claim-to-artifact path can be enumerated deterministically. Traceability is enforced by storing for each edge the anchor-level locator (page/block_id), the extraction confidence, and the source origin_id. Anti-circularity safeguards operate at two levels. First, origin clustering detects when multiple evidence nodes share the same origin signature (for example ORIG:src0001) and records origin_cluster_weights; scoring then applies a configurable single-source penalty_multiplier (the scoring bundle includes a single_source penalty factor of 0.85 in sampled claim_scores) to reduce undue inflation of apparent corroboration. Second, recovered_reference_count and explicit citation links are used to detect reuse of material extracted from the target document itself; the system treats evidence that merely restates a claim within the same origin as non-independent and deprioritizes it in corroboration tallies. These anti-circularity rules are enforced before aggregation to the claim level so that chain-of-evidence measures reflect independence and provenance rather than duplication.</p><p>Scoring framework and aggregation. Evidence-level probative_weight values are combined into claim-level aggregates using weighted sums and origin-aware normalization. The pipeline computes multiple axes that are separately interpretable: grounding (anchor coverage and marker strength), custody (provenance and integrity), clarity (actor/act/link specificity), credibility (source quality and diversity) and corroboration (modality and domain independence). The scoring bundle shows these outputs as numeric bands and as composite vectors (for example the six_c_vector and core_3c items). Aggregation applies shrinkage and calibration: statistical_calibration parameters such as reliability_factor, effective_evidence_n, and shrinkage_lambda are used to pull sparse estimates toward prior expectations and to compute bootstrap confidence intervals when requested. The methodology retains per-claim diagnostics (chain_provenance_diagnostics, anchor_quality, artifact_proximity_tiers) so that aggregated scores remain explainable to reviewers.</p><p>Validation and quality assurance. Quality assurance proceeds by automated and manual checks. Automated validators confirm anchor completeness, verify integrity signals (for example presence of hash identifiers), and flag low-confidence extractions for human review. Chain_provenance_diagnostics fields (context_completeness, lineage_quality, anchor_quality) provide quantitative thresholds that trigger escalation. Statistical bootstrap and saturation diagnostics (present in the v4 calibration outputs) inform whether</section>
<section class="wiki-section" id="sec-scoring_overview"><section><h3>Introduction</h3><p>This methodology chapter describes the architecture and operational logic used to produce cyber‑attribution scores, tracing a path from extraction outputs to inferential synthesis. The design separates deterministic extraction of source material (text, artifacts, anchors) from probabilistic and normative inferential weighting applied at the claim‑level and the document‑level. The procedures below are stated in general methodological terms and are grounded in the supplied scoring data and pipeline metadata; they do not rehearse or interpret the substantive allegations contained in the underlying report.</p></section><section><h3>Data processing and extraction</h3><p>Input processing begins with provider and fallback PDF conversion, followed by structural parsing and artifact extraction. The pipeline metadata (pipeline_methods) records a primary Mistral OCR conversion and an offline PyMuPDF fallback. The system emits a structured artifact registry (scoring_bundle.normalized.artifacts) and an evidence item registry (scoring_bundle.normalized.evidence_items). In the supplied bundle these registries enumerate multiple artifacts (for example artifact identifiers ART00001 through ART00094) and a set of evidence items (E-0001 through E-0037). Each artifact entry preserves extraction provenance (location anchors and extraction confidence) and each evidence item documents the modalities and anchor blocks that tie artifacts to a named claim. These deterministic outputs—text blocks, anchors, domains, IPs, hashes, URLs, and extracted emails—constitute the material basis for scoring but are not themselves inferential judgments.</p></section><section><h3>References and institution inference</h3><p>Reference parsing is applied to identify cited sources and to link anchors to source records in a canonical registry. The normalized source table (scoring_bundle.normalized.sources) records origin metadata (authoring organization, date_published, domain). When explicit citations are missing or incomplete, an institution inference stage (institution_inference) proposes provenance labels using an inference model with optional web fallback. The pipeline log shows single‑source coverage in this instance (document-level field sources_total = 1 and citations_total = 0), which is surfaced as a structural signal in downstream credibility and corroboration calculations and as an input to single‑source penalty mechanisms.</p></section><section><h3>Scoring framework: claim-level axes to document-level synthesis</h3><p>The scoring architecture is hierarchical. At the bottom, claim-level scoring computes axis scores along a set of orthogonal dimensions: grounding (evidence anchoring and marker strength), custody (provenance and artifact integrity), credibility (source quality and independence), corroboration (multi‑source and modality convergence), confidence (explicitness and uncertainty transparency), and clarity (act/actor/link specificity and responsibility mode signals). Each evidence item contributes a measured probative weight and modality features (as in evidence_items: features and probative_weight). The system combines evidence‑level probative weights into a claim‑level evidence_weight_0_100 and derives auxiliary measures such as belief_0_100 and evidence_support_0_1. A preview of claim scores (claim_score_preview_v4) shows the range of derived claim belief values (examples in the supplied preview range from approximately 0.06 to 0.23) and demonstrates how different evidence weights and custody scores affect belief at the claim-level.</p><p>Inferential weighting is separated from extraction: extraction yields anchors, artifacts, and provenance metadata; inferential weighting uses those deterministic inputs but applies calibrated statistical transforms and normative penalties. Examples of inferential mechanisms present in the data include a single_source penalty multiplier (penalty_multiplier = 0.85 for single‑source claims), origin_cluster_weights that apportion evidence weight by origin cluster, and probabilistic shrinkage (statistical_calibration_v4 fields such as shrinkage_lambda and reliability_factor). These mechanisms operate on the measurable axes (grounding, custody, etc.) to produce a normalized claim score (final_score) and the six‑C vectors (credibility, corroboration, coherence, confidence_discipline, compliance_mapping, etc.).</p><p>Document-level synthesis aggregates claim-level vectors. Aggregation computes central tendency measures (overall_claim_score_mean, overall_claim_score_geometric) and composes a headline_vector from a designated headline_claim_id. The document scoring also applies seriousness gates and threshold checks (for example seriousness_gate thresholds for overall_claim_score_mean, credibility_independence_median, and corroboration_convergence_median) to determine whether the document meets pre‑specified evidentiary minima. Bootstrap confidence intervals and reliability factors (document_scores_v4.bootstrap_95ci and per‑claim statistical_calibration_v4.reliability_factor) are used to express uncertainty at the document-level.</p></section><section><h3>Validation and quality assurance</h3><p>Quality assurance comprises deterministic checks on extraction completeness and</section>
<section class="wiki-section" id="sec-chain_of_custody"><p>Introduction: This methodology chapter sets out the structured approach used to quantify chain-of-custody reliability within a cyber-attribution scoring system. The aim is to describe the instruments, procedures and inferential safeguards used to assess provenance, integrity, time anchors, artifact identifiers and versioning while preserving separation between methodological description and any case-specific factual conclusions. The analysis treats the document as an evidentiary object that supplies artifacts, anchors and source metadata; it therefore emphasizes reproducibility, traceability and calibrated uncertainty when translating raw artefacts into numeric custody measures.</p>

<p>Data processing and extraction: Ingestion proceeds from the published document through an automated extraction pipeline and into structured records. The raw_artifacts_preview and the normalized artifacts tables provide the operational inputs: artifact_type, value, artifact_id, location, extracted_from and a per-artifact confidence score are recorded for each extracted item. Structural parsing (page/block identifiers and anchor labels) is preserved so that each artifact is associated with explicit anchors; evidence_items capture clustered anchors, modality tags and a probative_weight value. These structural elements enable computation of anchor_coverage, evidence_anchor_count and related grounding diagnostics. To illustrate how the pipeline calibrates custody metrics, document-level metadata such as sources_total and citations_total are tracked as upstream flags for further provenance analysis rather than as provenance conclusions themselves.</p>

<p>References and institution inference: Reference parsing collects declared sources and any in-text citations, and creates normalized source records with attributes such as authoring_org, publisher, date_published and domain. Where explicit bibliographic citations are absent, heuristics for recovered references and for institution inference operate on artifact metadata (domains, emails, file names, and embedded URLs). The system records recovered_reference_count and a recovered-reference workflow is executed when claim-support anchors point to external resources. Institutional inference is treated as a probabilistic classification problem: candidate organizational assignments are retained with provenance metadata and diagnostic scores rather than being hard assertions. This stage is sensitive to sparse citation coverage; document-level indicators (for example, sources_total = 1 and citations_total = 0) are used as triggers for conservative scoring and additional manual review steps.</p>

<p>Scoring framework (chain-of-custody focus): Custody is decomposed into constituent submetrics—provenance, integrity, time anchors, artifact identifiers and versioning—and these components are explicitly scored and reported. Score pipelines compute per-claim custody components from score_details.custody and report an aggregated custody value at claim and document level. The framework also implements multi-factor adjustments: credibility, corroboration and coherence vectors are combined with custody in a configurable aggregation. Penalties are applied when structural or source-quality conditions warrant downgrading; an operational example of such a mechanism is the single_source penalty, represented in the scoring bundle as a multiplier (for example, a penalty factor recorded as 0.85) that reduces the aggregate score. Penalties are documented with name, factor and rationale; their application is deterministic and reproducible from the recorded diagnostics.</p>

<p>Methodological rationale: The decomposition of custody into provenance, integrity, time anchors, artifact identifiers and versioning reflects legal-academic evidence norms: provenance establishes a traceable chain linking artifact to source; integrity establishes tamper-resistance or meaningful checks (hashes, checksums, internal consistency); time anchors permit temporal alignment of artifacts to the claimed events; artifact identifiers and versioning enable unique reference and support reproducibility. Where any component is weak or absent, the system reduces the effective contribution of that evidence chain to the final attribution score and records the justification in chain_provenance_diagnostics (for example, low integrity_signal or missing artifact_traceability triggers follow-up routines). Statistical calibration layers (reliability_factor and shrinkage parameters) are used to attenuate estimates when evidence quantity or quality is limited.</p>

<p>Validation and quality assurance: Quality controls include automated checks (anchor_quality, context_completeness, lineage_quality), bootstrapped uncertainty estimates at the document level, and manual review gates for cases with small source counts or absent citations. The document_scores_v4 bootstrap_95ci values (for custody and other axes) are retained to inform confidence intervals around aggregated measures. Reproducibility is supported by preserving the normalized artifacts and evidence_items with explicit anchors and by recording versioning metadata for any derived datasets or intermediate transforms. Integrity monitoring is signaled through explicit integrity indicators and by cross-validating artifact identifiers (</section>
<section class="wiki-section" id="sec-credibility_corroboration"><h2>Introduction</h2>
<p>This methodology section defines the approach used to operationalize the Credibility axis with a corroboration subcomponent for cyber-attribution scoring. The objective is to describe, in legal-academic prose, a repeatable analytic pipeline that translates heterogeneous documentary evidence into scored vectors that reflect provenance, independence and convergence of evidence while explicitly addressing exclusion criteria for low-value source classes. The descriptions below are framed as methodological rules and diagnostics; they do not adjudicate or repeat substantive allegations from any assessed report.</p>

<h2>Data processing and extraction</h2>
<p>Source ingestion begins with canonicalizing the document inventory and the extracted artifact set. Raw input records such as source metadata (for example, a single entry identifying an internal document section and its authoring organisation) are normalized to a common schema that records source_kind, authoring_org, date_published and citation links. Structural parsing of the document produces discrete anchors and artifacts (emails, domains, IPs, hashes, URLs) that are recorded with page/block provenance. The normalized artifacts and anchors are then clustered into evidence items that carry modality tags (for example, infrastructure, communications, ip, malware), a measured probative_weight, and feature vectors (I, A, M, P, T) that drive downstream weighting. The pipeline maintains explicit chain-of-custody diagnostics for each evidence item, including provenance quality, anchor_quality and artifact_proximity_tiers, so that custody scoring can be decomposed and audited independently of corroboration and credibility computations.</p>

<h2>References and institution inference</h2>
<p>Reference parsing implements two linked processes: (1) recovery of explicit citations and (2) inference of institutional provenance for unnamed or partially described references. Citation recovery scores the document-level citation coverage (for example, a reported citation_coverage of zero reduces the pool of independently verifiable sources). Institution inference uses authoring_org fields and domain tokens from artifacts to infer institutional class (academic, vendor, government, unknown) and to populate a source hierarchy. That source hierarchy is the ordered set of source classes used in credibility calculations and consists of high-credence classes (primary government/forensic chains, litigated exhibits), established third-party technical vendors, peer-reviewed literature, and lower-tier items (forums, personal blogs, aggregated webcache results). The implemented logic records eligible_source_count and marks the effective source_kind (for example, vendor) in the normalized source list to enable automated application of class-based adjustments.</p>

<h2>Scoring framework (source hierarchy, independence, corroboration, claim coverage, exclusion criteria)</h2>
<p>The scoring framework separates three core metrics: custody (chain-of-custody and artifact integrity), credibility (source quality and independence), and corroboration (convergence across independent sources and modalities). The source hierarchy establishes baseline priors for credibility: sources mapped to high-credence classes receive higher prior quality scores; vendors and single-report sections are placed in an intermediate class; unverified online forums are lower. Independence logic operationalizes domain and origin independence: unique_origin_count and unique_domain_count are computed for each claim and a domain_independence multiplier reduces credit for evidence that shares a common origin. Corroboration rules quantify multi-source convergence by combining source_quantity, modality_diversity, and crosscheck_language into a corroboration_convergence score; the claim-level corroboration component increases with independent origins and with evidence modality heterogeneity.
</p>
<p>Claim coverage scaling maps the proportion of a claim’s anchors that are tied to eligible sources (claim_support_coverage) into a claim_coverage_factor that modulates the contribution of credibility and corroboration to final scores. When claim coverage is low or citation_coverage is zero, the framework shrinks credibility and corroboration contributions toward empirical priors via shrinkage_lambda parameters (statistical calibration) to avoid overconfidence. Single-source penalties are applied multiplicatively (a documented penalty_multiplier is used where a single unique_origin yields an explicit factor), and examples in the diagnostic outputs show how a single-source multiplier reduces final claim scores.
</p>
<p>Exclusion criteria for low-value source classes are explicit and rule-based. A source is excluded from credibility/corroboration if it falls into a predetermined low-quality taxonomy (uninferable provenance, anonymous forum posts without verifiable anchors, or aggregated cache-only links) or if it lacks minimal custody diagnostics (artifact_identifiers = 0 and provenance quality below threshold). The system also applies a credibility_quality_gate: if eligible_source_count is zero or credible_claims_count remains zero after filtering, the credibility composite is set to its prior (often zero in the absence of qualifying sources) and corroboration contributions are suppressed pending external validation.</p>

<h2>Validation and quality assurance</h2>
<p>Validation is implemented at multiple layers. Internally, chain_provenance_diagnostics record context_completeness, integrity_signal and lineage_quality for every evidence item and are surfaced in per-claim diagnostics. Statistical calibration employs effective_evidence_n, reliability_factor and bootstrap_95ci calculations to estimate uncertainty and to compute shrinkage_lambda values that pull sparse estimates toward population priors. Independent QA checks include automated detection of single-source dominance (triggering penalty application), citation-coverage audits, and modality-consistency checks. The pipeline produces human-readable diagnostics (forensic anchors, probative weights, excluded_source_ids and the rationale for exclusion) so that reviewers can trace how a final score arose from input artifacts and applied thresholds. Periodic re-run with held-out evidence and bootstrap resampling are used to validate aggregate stability and to surface cases where corroboration or credibility would materially change if additional independent sources were added.</p>

<p>Together, these elements—explicit source hierarchy, a defensible independence model, formal corroboration aggregation rules, claim coverage scaling, and transparent exclusion criteria—create a reproducible, auditable methodology for assigning and calibrating Credibility and Corroboration sub-scores</section>
<section class="wiki-section" id="sec-clarity_axis"><p>Introduction: This methodology chapter defines the Clarity Axis and the operational scoring approach used to evaluate attribution to state actors via three distinct state-responsibility pathways: acts of state organs, non-state actors subject to state control, and state failure to exercise due diligence. The Clarity Axis is situated within a multi-dimensional scoring architecture that also measures grounding, custody, credibility, corroboration and confidence; in the supplied data the document-level clarity average is reported as 42.34 (document_scores_v4.clarity_avg_0_100), which indicates moderate overall explicitness of legal-path signals and motivates a measured, disaggregated treatment of state-responsibility claims. The rationale for separating legal-path clarity from other evidentiary dimensions is that questions of state responsibility are legal in form and require explicit linkage evidence that differs in character from technical provenance or artifact integrity.</p><p>Data processing and extraction: The pipeline begins with ingestion and structural parsing of the source, conversion of the report into an indexed internal representation, and artifact extraction. In the supplied bundle the normalized artifact table (scoring_bundle.full_icj.normalized.artifacts) contains a large set of artifacts (ART00001 through ART00094), and the evidence inventory (scoring_bundle.full_icj.normalized.evidence_items) contains discrete evidence items (E-0001 through E-0037). These raw extractions provide the anchors that connect textual claims to technical and documentary items. Methodologically, extraction yields three categories of inputs evaluated by downstream modules: textual anchors (citations and claim anchors), infrastructure and technical artifacts (domains, IPs, hashes), and communications/identity indicators (email addresses, usernames). Structural parsing records anchor locations and confidence flags so that subsequent scoring modules can assess anchor coverage, non-duplication, and anchor-proximity hierarchies; the grounding metrics in the data (for example a per-claim grounding score and anchor coverage statistics) are used to weight how much of a claim is directly supported by identifiable anchors.</p><p>Reference parsing and institution inference: Reference parsing maps anchors to source origins and recovers metadata about authorship and provenance. The normalized source table in the bundle indicates a single originating source (sources_total = 1, citations_total = 0 in document_scores_v4), and claim-level diagnostics record single-source penalty behavior (for example penalty_multiplier 0.85 in scoring_bundle.full_icj.claim_scores entries). Institution inference proceeds from explicit organizational identifiers in anchors and from patterns across artifacts (shared domains, repeated usernames, common hosting) to propose candidate institutional relationships. These inferences are treated as hypotheses, represented as features rather than adjudicated facts, and are encoded in the claim evidence graph so that they can be inspected independently of downstream legal-path scoring. The methodology preserves provenance: each inferred institutional link retains the evidence anchors and the origin_id (for example ORIG:src0001) so that chain provenance diagnostics (context_completeness, lineage_quality) remain auditable.</p><p>Scoring framework overview: The scoring framework composes multiple vectors. Chain-of-custody (custody) assesses provenance and artifact integrity, corroboration aggregates independent source support, and credibility measures source quality. The Clarity Axis is constructed as an orthogonal legal-path vector whose purpose is to render explicit which pathway of state responsibility—state organs, control of non-state actors, or due diligence failure—is supported by the available evidence. At the claim level the system computes granular clarity components (examples present in claim score_details.clarity) such as act_specificity, actor_specificity, link_specificity, and a state_actor_signal. These components are combined into three pathway-specific clarity scores reported as organ_path_clarity, control_path_clarity, and due_diligence_path_clarity. The aggregation uses multiplicative and coverage-weighted terms so that high actor specificity and explicit link evidence raise organ_path_clarity, whereas persistent infrastructure-control indicators and repeated actor-infrastructure co-occurrence raise control_path_clarity. By contrast, due_diligence_path_clarity is scored only when the evidence contains both an affirmative signal of state knowledge or attribution and an evidentiary indicator of failure to prevent; in the supplied scoring outputs due_diligence_path_clarity values are frequently low, reflecting the methodological requirement for bivariate evidence to establish that pathway.</p><p>Operationalization of the three responsibility pathways: The organ pathway (conducted_by_state_organs) emphasizes direct evidence tying the actor to formal state organs. Scoring factors include actor specificity (named individual or unit linkage), documentary evidence of formal appointment or assignment, and sustained operational alignment with state infrastructure; these elements are reflected in the clarity subcomponents that populate organ_path_clarity. The control pathway (non-state actors under state control) emphasizes structural and behavioral control signals: shared tooling, command-and-control continuity, common infrastructure, and operational directionality. Control is modeled as a probabilistic linkage distinct from formal appointment; control_path_clarity therefore increases with repeated, independent markers of operability and operational direction, and it is attenuated if provenance or source independence</section>
<section class="wiki-section" id="sec-aggregation_calibration"><h2>Introduction</h2>
<p>This methodology document explains the procedures used to aggregate claim-level evidence into document-level attribution scores, the weighting and calibration rules applied during computation, and the treatment of uncertainty and dispersion diagnostics. The exposition is methodological and refrains from asserting substantive findings about any specific actor or incident. Wherever practical, the description is grounded in the scoring artifacts and diagnostic outputs present in the supplied scoring bundle and document-level summaries so that readers can map conceptual steps to recorded metrics and diagnostics.</p>

<h2>Data processing and evidence extraction</h2>
<p>Initial processing begins with structured extraction of artifacts and anchors from the source document(s). The normalized artifact list and evidence_items entries serve as the primary inputs to subsequent aggregation: artifacts are recorded with type (for example, domain, email, ip, url, hash) and an extraction confidence; evidence items are composed from one or more anchors and are assigned modality and feature vectors. The evidence items include a per-item probative weight and modality tags that permit modality-aware aggregation. Extracted attributes such as evidence_anchor_count, anchors, artifact identifiers and anchor_coverage are retained as provenance metadata used later for custody diagnostics and for calculating evidence support fractions.</p>

<h2>References and institution inference</h2>
<p>Reference parsing proceeds from the normalized source records and any explicit citations. Source-level metadata (authoring organization, publisher, date_published and origin signatures) are recorded and used to derive institution-level indicators of provenance and independence. When multiple source records are present, domain_independence and multi-source diversity metrics are computed from source domain counts and type distributions. In cases where the scoring bundle records only a single source origin, that condition is propagated to the credibility and corroboration modules (for example, single_source penalties and domain_independence effects) so that downstream aggregation reflects reduced cross-source corroboration potential.</p>

<h2>Scoring framework: aggregation, weighting and calibration</h2>
<p>Aggregation transforms evidence-level probative weights and claim mappings into claim-level scores and then into a document-level summary. At the claim level, the system sums or otherwise aggregates evidence_weight_aggregate values for the evidence items associated with each claim, applies provenance multipliers (for example, a single_source penalty_multiplier), and computes core vectors along axes such as grounding, custody, credibility, corroboration, confidence and clarity. The claim_scores object records evidence_count, source_count, evidence_weight_aggregate and final_score, reflecting the direct aggregation outcome. To reduce overconfidence from limited evidence, the framework applies statistical calibration: reliability factors, effective_evidence_n, and shrinkage lambdas are used to pull raw per-axis scores toward historical priors. This is evident in the statistical_calibration_v4 entries, which list a reliability_factor, effective_evidence_n, shrinkage_lambda per axis, and prior_scores. Shrinkage reduces variance when evidence is few or concentrated; saturation factors limit the incremental effect of additional redundant chains.</p>

<p>Calibration steps use both deterministic rules and resampling diagnostics. Deterministic adjustments include penalties for single-source dependence and chain_saturation multipliers; the resampling-based calibration is expressed through bootstrap_95ci outputs for selected aggregated metrics (for example, belief-weighted measures and custody and clarity averages). The bootstrap CI provides an empirical dispersion estimate (ci95_low, ci95_high) that quantifies uncertainty in the document-level aggregates and enables reporting of both point estimates and interval ranges rather than single-point assertions.</p>

<h2>Uncertainty, dispersion diagnostics and reporting</h2>
<p>Uncertainty is represented along multiple axes. First, per-evidence extraction confidence and chain provenance diagnostics (context_completeness, artifact_traceability, lineage_quality) feed into custody and provenance scores that capture uncertainty about integrity and traceability. Second, calibration parameters (reliability_factor, effective_evidence_n) adjust how strongly raw evidence aggregates influence final belief scores; lower effective_evidence_n increases shrinkage toward priors. Third, bootstrap-derived 95% confidence intervals are used as a formal dispersion diagnostic for document-level aggregates, and summary fields that present mean and CI bounds are retained so that consumers can observe dispersion and incorporate it into decision thresholds. Finally, the system reports diagnostic scalar fields such as evidence_support_0_1, anchor_coverage, and claim_support_coverage to make explicit the extent of direct anchor support versus inferred linkages.</p>

<h2>Validation and quality assurance</h2>
<p>Quality assurance combines automated consistency checks and manual review triggers. Automated checks confirm that provenance metadata exist for each evidence item, that evidence counts and anchor coverage are coherent with computed anchor_coverage metrics, and that shrinkage and calibration parameters lie within expected ranges. Manual review is flagged when diagnostics indicate concentrated risk—for example, when source_count is one and a single_source penalty is applied, or when bootstrap intervals are wide relative to point estimates. The scoring bundle records such indicators (penalties array, penalty_multiplier) and chain_provenance_diagnostics entries that are used to triage items for expert review. Periodic revalidation uses resampling (bootstrap) and sensitivity analyses (varying shrinkage_lambda within plausible bounds) to verify that document-level conclusions are robust to reasonable methodological choices.</p>

<p>Together, these modules—artifact extraction, reference parsing and institution inference, evidence aggregation and weighting, statistical calibration, and uncertainty reporting—constitute an auditable pipeline. Each stage emits explicit diagnostics (evidence counts, probative_weight aggregates, shrinkage and bootstrap outputs, custody diagnostics, and penalty records) that permit independent inspection and support defensible interpretation under uncertainty and dispersion-aware decision-making.</p></section>
<section class="wiki-section" id="sec-validation_quality_assurance"><section>
  <h2>Validation and Quality Assurance</h2>
  <p>This validation and quality assurance section describes the automated and human-reviewed checkpoints applied to the extraction, grounding, and scoring processes. The validation architecture rests on a layered design that begins with automated validation gates operating against a formal schema and proceeds to an agent-mediated review stage and targeted human review sampling. The approach is oriented toward reproducibility and traceability: each validation artifact and its originating payload are recorded and available for independent inspection, with primary artifacts referenced from the project output paths (for example, the validation report at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/panda_crowdstrike_intelligence_report/panda_crowdstrike_intelligence_report.validation_report.json and the primary report JSON at /home/pantera/projects/TEIA/annotarium/outputs/reports/panda_crowdstrike_intelligence_report_report.json).</p>

  <p>Automated validation gates are formal checks executed immediately after extraction and before scoring. These gates validate schema conformance, table integrity, citation presence, and a set of grounding and corroboration constraints. Gate outcomes are summarized in a machine-readable validation bundle; in the case of the current artefact that bundle reports a certification of PASS and an overall score (93.1726), with discrete category scores used to determine pass/fail gating for downstream operations. Gate outputs include both hard failures (which block progression) and findings at warning severity (which do not block progression but are elevated for review). Warnings—such as duplication of support anchors or limited anchor diversity—are captured as structured findings and inform targeted follow-up without being used as substantive claims in attribution decisions.</p>

  <p>Downstream of automated gates, the QA protocol employs an agent review tier and a sampled human review tier. Agent review is enabled as part of the normal pipeline to apply contextual heuristics and to triage validation findings programmatically; this tier mitigates routine extraction artefacts and marks items for possible human inspection. Human review is targeted and sampled: a 10% human review fraction was applied to extracted claim and artifact units, selected by stratified random sampling to preserve representation across claim types and source artifacts. The sampled human review observed no observed errors in that sample, a result that is recorded in the QA metadata and used to calibrate short-term acceptance thresholds for subsequent batches. The absence of observed errors in the sample does not preclude further inspection of items flagged by automated gates; rather, it reduces the prior probability of systemic extraction error and informs the allocation of human review resources.</p>

  <p>Quality assurance is operationalized through explicit decision rules and audit trails. Items failing hard validation gates are retained out of the scoring pipeline until remedied; items producing warning-level findings are routed for agent review and, where warranted by finding severity or rarity, promoted to human review irrespective of sampling. All validation artifacts and QA decisions are logged to the repository of raw payloads and validation outputs (see, for example, the raw extraction and scoring inputs at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/panda_crowdstrike_intelligence_report/panda_crowdstrike_intelligence_report.output.json and /home/pantera/projects/TEIA/annotarium/outputs/pipeline/panda_crowdstrike_intelligence_report/panda_crowdstrike_intelligence_report.score_input_v3.json) to preserve chain of custody and enable retrospective re-analysis.</p>

  <p>Finally, the methodology recognises limits and prescribes periodic revalidation. Summary-level indicators from the validation bundle (including category scores and structured findings) are reviewed periodically to detect drift in extraction quality or in grounding/corroboration behavior; such reviews may trigger adjustments to the validation gates, sampling fraction, or agent heuristics. The validation and QA protocols described here are designed to provide a defensible, auditable pipeline that combines deterministic gates with human judgment, leveraging automated validation gates, agent review, and targeted human review to balance scale with rigor while documenting outcomes such as the 10% sample and the recorded result of no observed errors in that sample.</p>
</section></section>
<section class="wiki-section" id="sec-limitations_governance"><section id="introduction"><h2>Introduction</h2><p>This methodological addendum addresses limitations, governance, and a refinement roadmap for a reproducible cyber-attribution scoring system. It situates the analytic design within an evidentiary posture that privileges structured argumentation over narrative assertion, and it explains how downstream numerical outputs are constrained by upstream record integrity. The discussion that follows is intentionally procedural and normative: it explains scope, processing steps, reference and institution inference, scoring rationale, and quality controls without adjudicating any case-specific facts.</p></section><section id="scope_units"><h2>Scope and Analytical Units</h2><p>The unit of analysis is the evidentiary record derived from a single attribution report, where the report is decomposed into discrete claims, sources, artifacts, and explicit anchors linking artifacts to claims. The validation metadata accompanying each run documents corpus characteristics that shape methodological tradeoffs. For the illustrative validation bundle associated with this run, the top-level certification status is recorded as "PASS" and summary counts enumerate one source, nine claims, and ninety-four artifacts. These attributes inform admissibility discipline and the manner in which limitations and governance controls are calibrated across claim-level and document-level outputs.</p></section><section id="data_ingestion"><h2>Data Ingestion</h2><p>Data ingestion is constrained to PDF-origin reports. The process enforces deterministic persistence of intermediate artifacts so that any score can be reconstructed from the underlying record. The validation bundle confirms the practical effects of these constraints through automated integrity checks: the run includes category-level scores for schema, integrity, tables and other controls, and it reports specific warnings where anchors are duplicated or corroboration exhibits low anchor diversity. These ingestion-stage findings drive the governance requirement that records with missing anchors or unresolved citation pathways be remediated before substantive weighting occurs.</p></section><section id="pdf_to_markdown"><h2>PDF-to-Markdown Transcription</h2><p>The first transformation step converts PDF text into a normalized markdown transcript constrained by a strict schema. That schema separates claims, sources, artifacts, and anchors into distinct, auditable objects. The validation metadata documents a successful transcription pass in the present instance but records warnings that highlight common failure modes: duplicated support anchors and limited anchor diversity across corroborative components. Methodologically, these observations motivate conservative handling in subsequent scoring and form part of the limitations discussion below.</p></section><section id="structural_parsing"><h2>Structural Parsing</h2><p>Structural parsing maps the markdown transcript into typed entities and relational links. The parser enforces identifier uniqueness, anchor presence, and traceable citation pathways; runs that would violate these constraints are flagged or failed. The validation bundle reports perfect category scores for schema and integrity, which demonstrates the parser’s ability to detect structural defects. Nevertheless, the parser’s sensitivity to implicit contextual cues is intentionally limited to avoid retrofitting support where explicit anchors are absent, a design choice that produces known methodological tradeoffs documented under limitations.</p></section><section id="artifact_extraction"><h2>Artifact Extraction</h2><p>Artifacts are extracted as discrete evidentiary items and annotated with provenance markers, temporal anchors, and identifiers. The system records ninety-four artifacts in the exemplifying validation bundle; this artifact inventory is used to compute chain-of-custody indicators but is not treated as a raw proxy for credibility. The extraction step emphasizes capture of provenance metadata because multiplicative item-level weighting penalizes artifacts lacking authentication dimensions even when counts are high.</p></section><section id="reference_parsing"><h2>Reference Parsing</h2><p>Reference parsing identifies cited works and links them to artifacts and claims. The validation report indicates zero explicit citations despite source presence, which informs a governance rule: claims lacking high-quality, distinct source anchors cannot be elevated through repetition alone. This parsing stage preserves citation coverage metrics that later feed the credibility_coverage_factor and corroboration calculations of the scoring model.</p></section><section id="institution_inference"><h2>Institution Inference</h2><p>Institution inference classifies sources by type and assigns ordinal quality priors used in source-quality calculations. The scoring model gives maximal weight to judicial and peer-reviewed material, intermediate weight to official institutional outputs, and lower weight to other outlets. The validation bundle’s category breakdown for source-year and artifacts provides illustrative calibration targets for the institution inference subsystem and informs governance controls that exclude certain classes (for example, internal auto-generated content) from contributing to the credibility axis.</p></section><section id="claim_evidence_graph"><h2>Claim–Evidence Graph Construction</h2><p>Claims and their supporting anchors are assembled into an explicit claim–evidence graph that serves as the auditable input to item-level and claim-level weight calculations. The validation findings note repeated warnings that some claims have duplicated anchor text and limited anchor diversity; such structural patterns become flagged nodes within the graph and trigger downstream conservative weighting to mitigate over-counting and anti-circularity risks. The claim–evidence graph is therefore both the analytic object and the principal governance surface for contestation.</p></section><section id="scoring_overview"><h2>Scoring Overview</h2><p>The scoring architecture implements an ICJ-inspired evidentiary weighting model with three core axes: Chain of Custody, Credibility (integrating corroboration), and Clarity. Item-level weights are computed multiplicatively across bounded dimensions such as independence and authentication to prevent compensation of critical weaknesses by strengths in other dimensions. Corroboration is defined as convergence across independent origins with diminishing returns for common-origin repetition; this anti-circularity rule is enforced by origin clustering during corroboration calculation.</p></section><section id="chain_of_custody"><h2>Chain of Custody</h2><p>Chain-of-custody scoring operationalizes custody as a claim-specific quality measure derived from provenance markers, integrity markers, temporal anchors, artifact identifiers, and versioning lineage. These five variables are normalized and combined into a bounded custody score. The validation metadata records custody-related averages and bootstrap confidence intervals that are used to test the sensitivity of custody aggregations to missing markers. In practice, custody shortfalls identified at parsing time restrict downstream credibility gains for the affected claims.</p></section><section id="credibility_corroboration"><h2>Credibility and Corroboration</h2><p>Credibility is a composite of source-quality measures and corroborative convergence. Source quality is informed by inferred institution type, source diversity, and domain independence with explicit penalties for single-source reliance. The model applies a credibility_coverage_factor to calibrate document-level interpretability so that high-quality source support must cover a proportion of weighted claims to materially affect top-level credibility. Corroboration is retained as an explicit subscore and merged into credibility according to a fixed mixing rule; the validation bundle’s corroboration-related warnings about low anchor diversity are used to justify conservative corroboration scaling in affected runs.</p></section><section id="clarity_axis"><h2>Clarity Axis</h2><p>Clarity evaluates whether attribution reasoning is legally intelligible with respect to pathways of state responsibility. The clarity panel records question-level outputs that operationalize whether the text provides sufficiently specified act–actor–link narrative and whether one of the legal pathways (state organ, state direction/control of non-state actors, or omission/due-diligence failure) is addressed. Clarity scores are therefore legal-analytic judgments about presentation and reasoning rather than factual determinations;</section>
</article>