{
  "generated_at_utc": "2026-02-22T21:20:40Z",
  "mode": "single",
  "title": "Annotarium Methodology: APT1: Exposing One of China's Cyber Espionage Units",
  "template_path": "/home/pantera/projects/TEIA/annotarium/docs/methodology_template.json",
  "input_reports": [
    "/home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json"
  ],
  "template_sections": [
    {
      "id": "introduction",
      "title": "Introduction and Epistemic Framing",
      "instruction": "Introduce the methodology as an evidentiary framework for cyber attribution under contestation. Explain epistemic posture, burden-sensitive interpretation, and why structured extraction precedes legal inference.",
      "min_paragraphs": 3,
      "min_words": 260,
      "must_include": [
        "epistemic",
        "evidentiary",
        "contestable",
        "burden-sensitive"
      ],
      "context_keys": [
        "mode",
        "report_meta",
        "documents_preview",
        "methodology_reference_excerpt",
        "pipeline_methods"
      ]
    },
    {
      "id": "scope_units",
      "title": "Scope, Units of Analysis, and Output Semantics",
      "instruction": "Define units of analysis (claim, source, artifact, evidence item, document) and explain what each output score means and does not mean.",
      "min_paragraphs": 2,
      "min_words": 220,
      "must_include": [
        "claim",
        "source",
        "artifact",
        "evidence item",
        "document-level"
      ],
      "context_keys": [
        "mode",
        "raw_data_excerpt",
        "pipeline_counts",
        "document_scores_v4",
        "claim_score_preview_v4"
      ]
    },
    {
      "id": "data_ingestion",
      "title": "Data Ingestion and Corpus Handling",
      "instruction": "Explain report ingestion, deterministic file handling, and reproducibility guarantees from raw PDF inputs through intermediate artifacts.",
      "min_paragraphs": 2,
      "min_words": 200,
      "must_include": [
        "deterministic",
        "reproducibility",
        "input corpus"
      ],
      "context_keys": [
        "report_meta",
        "raw_payload_paths",
        "raw_payload_files",
        "pipeline_methods",
        "runtime_libraries"
      ]
    },
    {
      "id": "pdf_to_markdown",
      "title": "PDF-to-Markdown Conversion",
      "instruction": "Explain conversion from PDF to markdown, explicitly describing Mistral-based processing and offline fallback posture as methodological resilience rather than conceptual change.",
      "min_paragraphs": 2,
      "min_words": 210,
      "must_include": [
        "Mistral",
        "markdown",
        "offline fallback",
        "resilience"
      ],
      "context_keys": [
        "pipeline_methods",
        "pipeline_counts",
        "raw_payload_paths",
        "raw_payload_files"
      ]
    },
    {
      "id": "structural_parsing",
      "title": "Structural Parsing of Text, Tables, and Figures",
      "instruction": "Describe extraction of text blocks, tables, and figures/images with anchors and why anchor-preserving structure is required for auditability.",
      "min_paragraphs": 2,
      "min_words": 220,
      "must_include": [
        "anchors",
        "tables",
        "figures",
        "auditability"
      ],
      "context_keys": [
        "pipeline_counts",
        "raw_data_excerpt",
        "raw_payload_paths",
        "raw_claims_preview"
      ]
    },
    {
      "id": "artifact_extraction",
      "title": "Artifact Extraction and Technical Object Normalization",
      "instruction": "Explain artifact extraction across modalities, normalization of technical objects, and implications for custody/provenance evaluation.",
      "min_paragraphs": 2,
      "min_words": 220,
      "must_include": [
        "normalization",
        "technical objects",
        "provenance",
        "modalities"
      ],
      "context_keys": [
        "artifact_type_counts",
        "raw_artifacts_preview",
        "pipeline_methods",
        "document_scores_v4"
      ]
    },
    {
      "id": "reference_parsing",
      "title": "Footnote and Reference Parsing",
      "instruction": "Explain reference and footnote parsing, citation linkage, and resolution from rhetorical citation to analyzable source graph.",
      "min_paragraphs": 2,
      "min_words": 220,
      "must_include": [
        "footnote",
        "citation linkage",
        "source graph"
      ],
      "context_keys": [
        "raw_sources_preview",
        "source_type_counts",
        "pipeline_counts",
        "pipeline_methods"
      ]
    },
    {
      "id": "institution_inference",
      "title": "Institution Inference and Source Typology",
      "instruction": "Explain source institution inference, typology mapping, and how institutional class affects credibility weighting and corroboration eligibility.",
      "min_paragraphs": 2,
      "min_words": 210,
      "must_include": [
        "institution inference",
        "source typology",
        "credibility weighting",
        "corroboration eligibility"
      ],
      "context_keys": [
        "raw_sources_preview",
        "source_type_counts",
        "pipeline_methods",
        "runtime_libraries"
      ]
    },
    {
      "id": "claim_evidence_graph",
      "title": "Claim-Evidence Graph Construction",
      "instruction": "Describe how claims are linked to evidence, sources, and artifacts, including anchor-level traceability and anti-circularity safeguards.",
      "min_paragraphs": 2,
      "min_words": 230,
      "must_include": [
        "claim-evidence graph",
        "traceability",
        "anti-circularity"
      ],
      "context_keys": [
        "raw_claims_preview",
        "raw_sources_preview",
        "raw_artifacts_preview",
        "claim_score_preview_v4",
        "scoring_bundle"
      ]
    },
    {
      "id": "scoring_overview",
      "title": "Scoring Framework Overview",
      "instruction": "Present the overall scoring architecture from claim-level axes to document-level synthesis. Clarify separation between extraction outputs and inferential weighting.",
      "min_paragraphs": 2,
      "min_words": 230,
      "must_include": [
        "claim-level",
        "document-level",
        "inferential weighting"
      ],
      "context_keys": [
        "document_scores_v4",
        "claim_score_preview_v4",
        "scoring_bundle",
        "pipeline_methods"
      ]
    },
    {
      "id": "chain_of_custody",
      "title": "Chain of Custody Axis",
      "instruction": "Explain chain-of-custody variables (provenance, integrity, time anchors, artifact identifiers, versioning), quality controls, and penalties.",
      "min_paragraphs": 3,
      "min_words": 280,
      "must_include": [
        "provenance",
        "integrity",
        "time anchors",
        "versioning",
        "penalties"
      ],
      "context_keys": [
        "document_scores_v4",
        "claim_score_preview_v4",
        "raw_artifacts_preview",
        "scoring_bundle"
      ]
    },
    {
      "id": "credibility_corroboration",
      "title": "Credibility Axis with Corroboration Subcomponent",
      "instruction": "Explain source hierarchy, independence logic, corroboration rules, claim coverage scaling, and exclusion criteria for low-value source classes.",
      "min_paragraphs": 3,
      "min_words": 320,
      "must_include": [
        "source hierarchy",
        "independence",
        "corroboration",
        "claim coverage",
        "exclusion criteria"
      ],
      "context_keys": [
        "source_type_counts",
        "raw_sources_preview",
        "document_scores_v4",
        "claim_score_preview_v4",
        "scoring_bundle"
      ]
    },
    {
      "id": "clarity_axis",
      "title": "Clarity Axis and State Responsibility Pathways",
      "instruction": "Explain clarity scoring for attribution to state actors through organs, control over non-state actors, and due diligence failure pathways.",
      "min_paragraphs": 3,
      "min_words": 300,
      "must_include": [
        "state organs",
        "non-state actors",
        "control",
        "due diligence",
        "state responsibility"
      ],
      "context_keys": [
        "document_scores_v4",
        "claim_score_preview_v4",
        "scoring_bundle",
        "raw_claims_preview"
      ]
    },
    {
      "id": "aggregation_calibration",
      "title": "Aggregation, Calibration, and Uncertainty",
      "instruction": "Explain claim-to-document aggregation, weighting, calibration, and uncertainty handling (e.g., confidence intervals/dispersion diagnostics) in interpretation.",
      "min_paragraphs": 3,
      "min_words": 300,
      "must_include": [
        "aggregation",
        "calibration",
        "uncertainty",
        "dispersion"
      ],
      "context_keys": [
        "document_scores_v4",
        "scoring_bundle",
        "portfolio_summary",
        "documents_preview"
      ]
    },
    {
      "id": "validation_quality_assurance",
      "title": "Validation and Quality Assurance",
      "instruction": "Explain automated validation gates and QA protocol with agent review and targeted human review on a 10% sample with no observed errors in that sample.",
      "min_paragraphs": 3,
      "min_words": 260,
      "must_include": [
        "validation gates",
        "agent review",
        "human review",
        "10%",
        "no observed errors"
      ],
      "context_keys": [
        "validation_bundle",
        "qa_protocol",
        "raw_payload_paths",
        "raw_payload_files"
      ]
    },
    {
      "id": "limitations_governance",
      "title": "Limitations, Governance, and Future Refinement",
      "instruction": "Discuss methodological limits, governance controls, and principled refinement roadmap without overstating certainty.",
      "min_paragraphs": 2,
      "min_words": 220,
      "must_include": [
        "limitations",
        "governance",
        "refinement roadmap"
      ],
      "context_keys": [
        "validation_bundle",
        "document_scores_v4",
        "portfolio_summary",
        "runtime_libraries",
        "methodology_reference_excerpt"
      ]
    }
  ],
  "context_snapshot": {
    "mode": "single",
    "report_meta": {
      "report_path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json",
      "report_id": "fireeye_rpt_apt37_02_20_2018_report",
      "generated_at_utc": "2026-02-21T23:33:11.629Z",
      "document_title": "APT1: Exposing One of China's Cyber Espionage Units",
      "publication_date": "2013-01-01"
    },
    "pipeline_counts": {
      "pages": 1,
      "claims": 6,
      "sources": 1,
      "artifacts": 4,
      "citations": 0,
      "tables": 3,
      "figures": 4
    },
    "source_type_counts": {
      "internal_document_section": 1
    },
    "artifact_type_counts": {
      "cve": 6,
      "domain": 5,
      "email": 1,
      "hash_md5": 2
    },
    "document_scores_v4": {
      "belief_weighted_0_100": 0.06,
      "grounding_avg_0_100": 56.68,
      "custody_avg_0_100": 34.5,
      "credibility_avg_0_100": 0.0,
      "corroboration_avg_0_100": 0.0,
      "confidence_avg_0_100": 59.03,
      "clarity_avg_0_100": 37.11,
      "citation_coverage_sources_0_1": 0.0,
      "sources_total": 1,
      "citations_total": 0,
      "credibility_composite_avg_0_100": 0.0,
      "bootstrap_95ci": {
        "belief_weighted_0_100": {
          "mean": 0.06,
          "ci95_low": 0.05,
          "ci95_high": 0.07
        },
        "custody_avg_0_100": {
          "mean": 34.5,
          "ci95_low": 33.0,
          "ci95_high": 35.76
        },
        "credibility_avg_0_100": {
          "mean": 0.0,
          "ci95_low": 0.0,
          "ci95_high": 0.0
        },
        "corroboration_avg_0_100": {
          "mean": 0.0,
          "ci95_low": 0.0,
          "ci95_high": 0.0
        },
        "clarity_avg_0_100": {
          "mean": 37.11,
          "ci95_low": 31.96,
          "ci95_high": 42.67
        }
      }
    },
    "claim_score_preview_v4": [
      {
        "claim_id": "C001",
        "belief_0_100": 0.07,
        "custody_0_100": 31.27,
        "credibility_0_100": 0.0,
        "clarity_0_100": 47.62,
        "grounding_0_100": 55.56,
        "evidence_support_0_1": 0.0976
      },
      {
        "claim_id": "C002",
        "belief_0_100": 0.05,
        "custody_0_100": 35.76,
        "credibility_0_100": 0.0,
        "clarity_0_100": 30.49,
        "grounding_0_100": 56.75,
        "evidence_support_0_1": 0.0673
      },
      {
        "claim_id": "C003",
        "belief_0_100": 0.05,
        "custody_0_100": 35.73,
        "credibility_0_100": 0.0,
        "clarity_0_100": 31.35,
        "grounding_0_100": 55.78,
        "evidence_support_0_1": 0.0686
      },
      {
        "claim_id": "C004",
        "belief_0_100": 0.05,
        "custody_0_100": 35.79,
        "credibility_0_100": 0.0,
        "clarity_0_100": 31.39,
        "grounding_0_100": 57.64,
        "evidence_support_0_1": 0.0699
      },
      {
        "claim_id": "C005",
        "belief_0_100": 0.07,
        "custody_0_100": 35.75,
        "credibility_0_100": 0.0,
        "clarity_0_100": 44.27,
        "grounding_0_100": 56.52,
        "evidence_support_0_1": 0.0975
      },
      {
        "claim_id": "C006",
        "belief_0_100": 0.06,
        "custody_0_100": 32.73,
        "credibility_0_100": 0.0,
        "clarity_0_100": 37.56,
        "grounding_0_100": 57.81,
        "evidence_support_0_1": 0.0803
      }
    ],
    "raw_claims_preview": [
      {
        "claim_id": "C001",
        "section_heading": null,
        "claim_text": "On Feb. 2, 2018, we published a blog detailing the use of an Adobe Flash zero-day vulnerability (CVE-2018-4878) by a suspected North Korean cyber espionage group that we now track as APT37 (Reaper). Recent examination of this group's activities by FireEye iSIGHT Intelligence reveals APT37 has expanded its operations in both scope and sophistication. APT37's toolset, which includes access to zero-day vulnerabilitie...",
        "actor": null,
        "actor_type": null
      },
      {
        "claim_id": "C002",
        "section_heading": null,
        "claim_text": "In May 2017, APT37 used a bank liquidation letter as a spear phishing lure against a board member of a Middle Eastern financial company. The specially crafted email included an attachment containing exploit code for CVE-2017-0199, a vulnerability in Microsoft Office that had been disclosed just a month earlier. Once opened, the malicious document communicated with a compromised website, most likely to surreptitiou...",
        "actor": null,
        "actor_type": null
      },
      {
        "claim_id": "C003",
        "section_heading": null,
        "claim_text": "Numerous campaigns have employed social engineering tactics tailored specifically to desired targets. Lures and websites of particular interest to South Korean organizations (e.g. reunification) are regularly leveraged in campaigns. Multiple South Korean websites were abused in strategic web compromises to deliver newer variants of KARAE and POORAIM malware. Identified sites included South Korean conservative medi...",
        "actor": null,
        "actor_type": null
      },
      {
        "claim_id": "C004",
        "section_heading": null,
        "claim_text": "APT37 relies on compromised websites to host second stage malware. Small websites focused on subjects such as aromatherapy and scuba diving have been leveraged, and were most likely compromised opportunistically and made to host malicious payloads.",
        "actor": null,
        "actor_type": null
      },
      {
        "claim_id": "C005",
        "section_heading": null,
        "claim_text": "We assess with high confidence that APT37 acts in support of the North Korean government and is primarily based in North Korea. This assessment is based on multiple factors, including APT37's targeting profile, insight into the group's malware development and probable links to a North Korean individual believed to be the developer of several of APT37's proprietary malware families:",
        "actor": null,
        "actor_type": null
      },
      {
        "claim_id": "C006",
        "section_heading": null,
        "claim_text": "Many of the compromised domains in the command and control infrastructure are linked to South Korean companies. Most of these domains host a fake webpage pertinent to targets. | FE_APT_Backdoor_SHUTTERSPEED",
        "actor": null,
        "actor_type": null
      }
    ],
    "raw_sources_preview": [
      {
        "source_id": "SRC0001",
        "title": "APT1 Executive Summary and Key Findings",
        "source_type": "internal_document_section",
        "entity_name": "Mandiant",
        "publication_or_venue": "Mandiant APT1 Report",
        "year": 2013,
        "url_or_identifier": null
      }
    ],
    "raw_artifacts_preview": [
      {
        "artifact_type": "cve",
        "count": 6,
        "example_values": [
          "CVE-2018-4878",
          "CVE-2017-0199",
          "CVE-2018-0802",
          "CVE-2016-4117",
          "CVE-2016-1019"
        ]
      },
      {
        "artifact_type": "domain",
        "count": 5,
        "example_values": [
          "daum.net",
          "hmamail.com",
          "india.com",
          "fireeye.com",
          "www.fireeye.com"
        ]
      },
      {
        "artifact_type": "email",
        "count": 1,
        "example_values": [
          "info@fireeye.com"
        ]
      },
      {
        "artifact_type": "hash_md5",
        "count": 2,
        "example_values": [
          "183be2035d5a546670d2b9deeca4eb59",
          "7c2ebfc7960aac6f8d58b37a3f092a9c"
        ]
      }
    ],
    "raw_data_excerpt": {
      "document_metadata": {
        "title": "APT1: Exposing One of China's Cyber Espionage Units",
        "authoring_entity": "Mandiant",
        "publication_date": "2013-01-01",
        "publication_date_source": "filename",
        "publication_date_anchor_role": "other_date",
        "publication_date_anchor": {
          "anchor_id": "P000-A999",
          "extraction_method": "manual_description",
          "verbatim_text": "Filename contains publication year marker: 'Mandiant - 2013 - APT1 ...'; exact month/day not stated.",
          "location": {
            "page_index": 0,
            "page_label": null,
            "section_heading": null,
            "object_id": null
          },
          "notes": "Publication date inferred from local metadata/filename heuristic."
        },
        "version": "offline_v1",
        "document_type": "vendor_report",
        "audience": "mixed",
        "source_locator": {
          "source_type": "file",
          "source_value": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.md"
        },
        "input_format": "markdown"
      },
      "sample_pages_count": 1,
      "sample_claims_count": 6,
      "sample_sources_count": 1,
      "sample_artifacts_count": 4
    },
    "score_version": "v4",
    "source_files": {
      "pdf": "/home/pantera/projects/TEIA/annotarium/Reports/Fireeye_rpt_APT37(02-20-2018).pdf",
      "markdown": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.md",
      "raw_extraction": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.output.json",
      "validation_report_json": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.validation_report.json",
      "validation_report_md": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.validation_report.md",
      "full_scores": "/home/pantera/projects/TEIA/annotarium/outputs/scoring/fireeye_rpt_apt37_02_20_2018.icj_score_report.json",
      "score_input_v3": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.score_input_v3.json",
      "full_scores_v3": "/home/pantera/projects/TEIA/annotarium/outputs/scoring/fireeye_rpt_apt37_02_20_2018.icj_score_report_v3.json"
    },
    "raw_payload_paths": {
      "report_json": "/home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json",
      "raw_extraction": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.output.json",
      "validation_report_json": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.validation_report.json",
      "full_scores": "/home/pantera/projects/TEIA/annotarium/outputs/scoring/fireeye_rpt_apt37_02_20_2018.icj_score_report.json",
      "full_scores_v3": "/home/pantera/projects/TEIA/annotarium/outputs/scoring/fireeye_rpt_apt37_02_20_2018.icj_score_report_v3.json",
      "full_scores_v4": "",
      "score_input_v3": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.score_input_v3.json",
      "markdown": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.md",
      "pdf": "/home/pantera/projects/TEIA/annotarium/Reports/Fireeye_rpt_APT37(02-20-2018).pdf"
    },
    "raw_payload_files": {
      "report_json": {
        "path": "/home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json",
        "exists": true,
        "size_bytes": 520983
      },
      "raw_extraction": {
        "path": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.output.json",
        "exists": true,
        "size_bytes": 316434
      },
      "validation_report_json": {
        "path": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.validation_report.json",
        "exists": true,
        "size_bytes": 3983
      },
      "full_scores": {
        "path": "/home/pantera/projects/TEIA/annotarium/outputs/scoring/fireeye_rpt_apt37_02_20_2018.icj_score_report.json",
        "exists": true,
        "size_bytes": 30870
      },
      "full_scores_v3": {
        "path": "/home/pantera/projects/TEIA/annotarium/outputs/scoring/fireeye_rpt_apt37_02_20_2018.icj_score_report_v3.json",
        "exists": true,
        "size_bytes": 34751
      },
      "full_scores_v4": {
        "path": "",
        "exists": false,
        "size_bytes": 0
      },
      "score_input_v3": {
        "path": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.score_input_v3.json",
        "exists": true,
        "size_bytes": 21210
      },
      "markdown": {
        "path": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.md",
        "exists": true,
        "size_bytes": 22477
      },
      "pdf": {
        "path": "/home/pantera/projects/TEIA/annotarium/Reports/Fireeye_rpt_APT37(02-20-2018).pdf",
        "exists": true,
        "size_bytes": 2920140
      }
    },
    "scoring_bundle": {
      "available_score_objects": [
        "full_icj",
        "full_icj_v3",
        "full_icj_v4"
      ],
      "full_icj": {
        "icj_scoring_version": "v1",
        "inputs": {
          "document_title": "APT1: Exposing One of China's Cyber Espionage Units",
          "document_publication_date": "2013-01-01",
          "claims_count": 6,
          "profile": "balanced"
        },
        "readiness_validation": {
          "findings": []
        },
        "normalized": {
          "sources": [
            {
              "source_id": "SRC0001",
              "source_kind": "vendor",
              "title": "APT1 Executive Summary and Key Findings",
              "authors": [],
              "authoring_org": "Mandiant",
              "publisher": "Mandiant",
              "date_published": "2013-01-01",
              "url": "",
              "domain": "",
              "is_litigation_prepared": 0,
              "is_single_source": 0,
              "has_stated_conflict": 0,
              "has_countervailing_detail": 0,
              "cites": [],
              "origin_signature": [
                "src0001"
              ]
            }
          ],
          "artifacts": [
            {
              "artifact_id": "ART00001",
              "artifact_type": "cve",
              "value": "CVE-2018-4878",
              "location": {
                "page": 0,
                "block_id": "ART00001",
                "kind": "text",
                "table_id": ""
              },
              "extracted_from": "text",
              "confidence": 1
            },
            {
              "artifact_id": "ART00002",
              "artifact_type": "hash_md5",
              "value": "183be2035d5a546670d2b9deeca4eb59",
              "location": {
                "page": 0,
                "block_id": "ART00002",
                "kind": "text",
                "table_id": ""
              },
              "extracted_from": "text",
              "confidence": 1
            },
            {
              "artifact_id": "ART00003",
              "artifact_type": "cve",
              "value": "CVE-2017-0199",
              "location": {
                "page": 0,
                "block_id": "ART00003",
                "kind": "text",
                "table_id": ""
              },
              "extracted_from": "text",
              "confidence": 1
            },
            {
              "artifact_id": "ART00004",
              "artifact_type": "hash_md5",
              "value": "7c2ebfc7960aac6f8d58b37a3f092a9c",
              "location": {
                "page": 0,
                "block_id": "ART00004",
                "kind": "text",
                "table_id": ""
              },
              "extracted_from": "text",
              "confidence": 1
            },
            {
              "artifact_id": "ART00005",
              "artifact_type": "cve",
              "value": "CVE-2018-0802",
              "location": {
                "page": 0,
                "block_id": "ART00005",
                "kind": "text",
                "table_id": ""
              },
              "extracted_from": "text",
              "confidence": 1
            },
            {
              "artifact_id": "ART00006",
              "artifact_type": "cve",
              "value": "CVE-2016-4117",
              "location": {
                "page": 0,
                "block_id": "ART00006",
                "kind": "text",
                "table_id": ""
              },
              "extracted_from": "text",
              "confidence": 1
            },
            {
              "artifact_id": "ART00007",
              "artifact_type": "cve",
              "value": "CVE-2016-1019",
              "location": {
                "page": 0,
                "block_id": "ART00007",
                "kind": "text",
                "table_id": ""
              },
              "extracted_from": "text",
              "confidence": 1
            },
            {
              "artifact_id": "ART00008",
              "artifact_type": "cve",
              "value": "CVE-2015-3043",
              "location": {
                "page": 0,
                "block_id": "ART00008",
                "kind": "text",
                "table_id": ""
              },
              "extracted_from": "text",
              "confidence": 1
            },
            {
              "artifact_id": "ART00009",
              "artifact_type": "domain",
              "value": "daum.net",
              "location": {
                "page": 0,
                "block_id": "ART00009",
                "kind": "text",
                "table_id": ""
              },
              "extracted_from": "text",
              "confidence": 1
            },
            {
              "artifact_id": "ART00010",
              "artifact_type": "domain",
              "value": "hmamail.com",
              "location": {
                "page": 0,
                "block_id": "ART00010",
                "kind": "text",
                "table_id": ""
              },
              "extracted_from": "text",
              "confidence": 1
            },
            {
              "artifact_id": "ART00011",
              "artifact_type": "domain",
              "value": "india.com",
              "location": {
                "page": 0,
                "block_id": "ART00011",
                "kind": "text",
                "table_id": ""
              },
              "extracted_from": "text",
              "confidence": 1
            },
            {
              "artifact_id": "ART00012",
              "artifact_type": "email",
              "value": "info@fireeye.com",
              "location": {
                "page": 0,
                "block_id": "ART00012",
                "kind": "text",
                "table_id": ""
              },
              "extracted_from": "text",
              "confidence": 1
            },
            {
              "artifact_id": "ART00013",
              "artifact_type": "domain",
              "value": "fireeye.com",
              "location": {
                "page": 0,
                "block_id": "ART00013",
                "kind": "text",
                "table_id": ""
              },
              "extracted_from": "text",
              "confidence": 1
            },
            {
              "artifact_id": "ART00014",
              "artifact_type": "domain",
              "value": "www.fireeye.com",
              "location": {
                "page": 0,
                "block_id": "ART00014",
                "kind": "text",
                "table_id": ""
              },
              "extracted_from": "text",
              "confidence": 1
            }
          ],
          "evidence_items": [
            {
              "evidence_id": "E-0001",
              "evidence_kind": "dataset",
              "source_ids": [
                "SRC0001"
              ],
              "artifact_ids": [],
              "origin_id": "ORIG:src0001",
              "modalities": [
                "cve"
              ],
              "features": {
                "I": 0.42999999999999994,
                "A": 0.89,
                "M": 0.56,
                "P": 0.52,
                "T": 0.95
              },
              "anchors": [
                {
                  "page": 0,
                  "block_id": "ART00001"
                },
                {
                  "page": 0,
                  "block_id": "ART00005"
                },
                {
                  "page": 0,
                  "block_id": "ART00003"
                },
                {
                  "page": 0,
                  "block_id": "ART00006"
                }
              ],
              "probative_weight": 0.10587012799999998,
              "claim_id": "C001"
            },
            {
              "evidence_id": "E-0002",
              "evidence_kind": "technical_artifact",
              "source_ids": [
                "SRC0001"
              ],
              "artifact_ids": [],
              "origin_id": "ORIG:src0001",
              "modalities": [
                "infrastructure"
              ],
              "features": {
                "I": 0.42999999999999994,
                "A": 0.89,
                "M": 0.65,
                "P": 0.52,
                "T": 0.95
              },
              "anchors": [
                {
                  "page": 0,
                  "block_id": "ART00013"
                },
                {
                  "page": 0,
                  "block_id": "ART00014"
                },
                {
                  "page": 0,
                  "block_id": "ART00009"
                },
                {
                  "page": 0,
                  "block_id": "ART00010"
                }
              ],
              "probative_weight": 0.12288496999999998,
              "claim_id": "C001"
            },
            {
              "evidence_id": "E-0003",
              "evidence_kind": "dataset",
              "source_ids": [
                "SRC0001"
              ],
              "artifact_ids": [],
              "origin_id": "ORIG:src0001",
              "modalities": [
                "cve"
              ],
              "features": {
                "I": 0.42999999999999994,
                "A": 0.89,
                "M": 0.56,
                "P": 0.62,
                "T": 0.95
              },
              "anchors": [
                {
                  "page": 0,
                  "block_id": "ART00003"
                },
                {
                  "page": 0,
                  "block_id": "ART00001"
                },
                {
                  "page": 0,
                  "block_id": "ART00005"
                },
                {
                  "page": 0,
                  "block_id": "ART00006"
                }
              ],
              "probative_weight": 0.12622976799999996,
              "claim_id": "C002"
            },
            {
              "evidence_id": "E-0004",
              "evidence_kind": "technical_artifact",
              "source_ids": [
                "SRC0001"
              ],
              "artifact_ids": [],
              "origin_id": "ORIG:src0001",
              "modalities": [
                "infrastructure"
              ],
              "features": {
                "I": 0.42999999999999994,
                "A": 0.89,
                "M": 0.65,
                "P": 0.62,
                "T": 0.95
              },
              "anchors": [
                {
                  "page": 0,
                  "block_id": "ART00009"
                },
                {
                  "page": 0,
                  "block_id": "ART00010"
                },
                {
                  "page": 0,
                  "block_id": "ART00011"
                },
                {
                  "page": 0,
                  "block_id": "ART00013"
                }
              ],
              "probative_weight": 0.14651669499999997,
              "claim_id": "C002"
            },
            {
              "evidence_id": "E-0005",
              "evidence_kind": "technical_artifact",
              "source_ids": [
                "SRC0001"
              ],
              "artifact_ids": [],
              "origin_id": "ORIG:src0001",
              "modalities": [
                "infrastructure"
              ],
              "features": {
                "I": 0.42999999999999994,
                "A": 0.89,
                "M": 0.65,
                "P": 0.52,
                "T": 0.95
              },
              "anchors": [
                {
                  "page": 0,
                  "block_id": "ART00009"
                },
                {
                  "page": 0,
                  "block_id": "ART00010"
                },
                {
                  "page": 0,
                  "block_id": "ART00011"
                },
                {
                  "page": 0,
                  "block_id": "ART00013"
                }
              ],
              "probative_weight": 0.12288496999999998,
              "claim_id": "C003"
            },
            {
              "evidence_id": "E-0006",
              "evidence_kind": "technical_artifact",
              "source_ids": [
                "SRC0001"
              ],
              "artifact_ids": [],
              "origin_id": "ORIG:src0001",
              "modalities": [
                "infrastructure"
              ],
              "features": {
                "I": 0.42999999999999994,
                "A": 0.89,
                "M": 0.65,
                "P": 0.62,
                "T": 0.95
              },
              "anchors": [
                {
                  "page": 0,
                  "block_id": "ART00009"
                },
                {
                  "page": 0,
                  "block_id": "ART00010"
                },
                {
                  "page": 0,
                  "block_id": "ART00011"
                },
                {
                  "page": 0,
                  "block_id": "ART00013"
                }
              ],
              "probative_weight": 0.14651669499999997,
              "claim_id": "C004"
            },
            {
              "evidence_id": "E-0007",
              "evidence_kind": "technical_artifact",
              "source_ids": [
                "SRC0001"
              ],
              "artifact_ids": [],
              "origin_id": "ORIG:src0001",
              "modalities": [
                "infrastructure"
              ],
              "features": {
                "I": 0.42999999999999994,
                "A": 0.89,
                "M": 0.65,
                "P": 0.62,
                "T": 0.95
              },
              "anchors": [
                {
                  "page": 0,
                  "block_id": "ART00009"
                },
                {
                  "page": 0,
                  "block_id": "ART00010"
                },
                {
                  "page": 0,
                  "block_id": "ART00011"
                },
                {
                  "page": 0,
                  "block_id": "ART00013"
                }
              ],
              "probative_weight": 0.14651669499999997,
              "claim_id": "C005"
            },
            {
              "evidence_id": "E-0008",
              "evidence_kind": "technical_artifact",
              "source_ids": [
                "SRC0001"
              ],
              "artifact_ids": [],
              "origin_id": "ORIG:src0001",
              "modalities": [
                "infrastructure"
              ],
              "features": {
                "I": 0.42999999999999994,
                "A": 0.89,
                "M": 0.65,
                "P": 0.52,
                "T": 0.95
              },
              "anchors": [
                {
                  "page": 0,
                  "block_id": "ART00013"
                },
                {
                  "page": 0,
                  "block_id": "ART00014"
                },
                {
                  "page": 0,
                  "block_id": "ART00009"
                },
                {
                  "page": 0,
                  "block_id": "ART00010"
                }
              ],
              "probative_weight": 0.12288496999999998,
              "claim_id": "C006"
            }
          ],
          "claims": [
            {
              "claim_id": "C001",
              "claim_kind": "intrusion_set_attribution",
              "claim_text": "On Feb. 2, 2018, we published a blog detailing the use of an Adobe Flash zero-day vulnerability (CVE-2018-4878) by a suspected North Korean cyber espionage group that we now track as APT37 (Reaper). Recent examination of this group's activities by FireEye iSIGHT Intelligence reveals APT37 has expanded its operations in both scope and sophistication. APT37's toolset, which includes access to zero-day vulnerabilities and wiper malware, combined with heightened tensions in Northeast Asia and North Korea's penchant for norm breaking, means this group should be taken seriously.",
              "actor": {
                "name": "APT1",
                "level": "None"
              },
              "object": {
                "operation": "intrusion_set",
                "time_start": "",
                "time_end": ""
              },
              "citations": [],
              "evidence_ids": [
                "E-0001",
                "E-0002"
              ],
              "claim_features": {
                "coherence": 0.48,
                "confidence_discipline": 0.5700000000000001,
                "compliance_mapping": 1,
                "allegation_gravity": "medium"
              }
            },
            {
              "claim_id": "C002",
              "claim_kind": "mixed",
              "claim_text": "In May 2017, APT37 used a bank liquidation letter as a spear phishing lure against a board member of a Middle Eastern financial company. The specially crafted email included an attachment containing exploit code for CVE-2017-0199, a vulnerability in Microsoft Office that had been disclosed just a month earlier. Once opened, the malicious document communicated with a compromised website, most likely to surreptitiously download and install a backdoor called SHUTTERSPEED (MDS: 7c2ebfc7960aac6f8d58b37a3f092a9c). The tool would enable APT37 to collect system information, take screenshots and download additional malicious files to the victim computer.",
              "actor": {
                "name": "APT1 campaign",
                "level": "None"
              },
              "object": {
                "operation": "mixed",
                "time_start": "",
                "time_end": ""
              },
              "citations": [],
              "evidence_ids": [
                "E-0003",
                "E-0004"
              ],
              "claim_features": {
                "coherence": 0.6,
                "confidence_discipline": 0.5700000000000001,
                "compliance_mapping": 0.3,
                "allegation_gravity": "medium"
              }
            },
            {
              "claim_id": "C003",
              "claim_kind": "mixed",
              "claim_text": "Numerous campaigns have employed social engineering tactics tailored specifically to desired targets. Lures and websites of particular interest to South Korean organizations (e.g. reunification) are regularly leveraged in campaigns. Multiple South Korean websites were abused in strategic web compromises to deliver newer variants of KARAE and POORAIM malware. Identified sites included South Korean conservative media and a news site for North Korean refugees and defectors. In one instance, APT37 weaponized a video downloader application with KARAE malware that was indiscriminately distributed to South Korean victims through torrent websites.",
              "actor": {
                "name": "APT1 campaign",
                "level": "None"
              },
              "object": {
                "operation": "mixed",
                "time_start": "",
                "time_end": ""
              },
              "citations": [],
              "evidence_ids": [
                "E-0005"
              ],
              "claim_features": {
                "coherence": 0.48,
                "confidence_discipline": 0.5700000000000001,
                "compliance_mapping": 0.3,
                "allegation_gravity": "medium"
              }
            },
            {
              "claim_id": "C004",
              "claim_kind": "mixed",
              "claim_text": "APT37 relies on compromised websites to host second stage malware. Small websites focused on subjects such as aromatherapy and scuba diving have been leveraged, and were most likely compromised opportunistically and made to host malicious payloads.",
              "actor": {
                "name": "APT1 campaign",
                "level": "None"
              },
              "object": {
                "operation": "mixed",
                "time_start": "",
                "time_end": ""
              },
              "citations": [],
              "evidence_ids": [
                "E-0006"
              ],
              "claim_features": {
                "coherence": 0.6,
                "confidence_discipline": 0.5700000000000001,
                "compliance_mapping": 0.3,
                "allegation_gravity": "medium"
              }
            },
            {
              "claim_id": "C005",
              "claim_kind": "intrusion_set_attribution",
              "claim_text": "We assess with high confidence that APT37 acts in support of the North Korean government and is primarily based in North Korea. This assessment is based on multiple factors, including APT37's targeting profile, insight into the group's malware development and probable links to a North Korean individual believed to be the developer of several of APT37's proprietary malware families:",
              "actor": {
                "name": "APT1",
                "level": "None"
              },
              "object": {
                "operation": "intrusion_set",
                "time_start": "",
                "time_end": ""
              },
              "citations": [],
              "evidence_ids": [
                "E-0007"
              ],
              "claim_features": {
                "coherence": 0.6,
                "confidence_discipline": 0.5700000000000001,
                "compliance_mapping": 0.3,
                "allegation_gravity": "medium"
              }
            },
            {
              "claim_id": "C006",
              "claim_kind": "mixed",
              "claim_text": "Many of the compromised domains in the command and control infrastructure are linked to South Korean companies. Most of these domains host a fake webpage pertinent to targets. | FE_APT_Backdoor_SHUTTERSPEED",
              "actor": {
                "name": "APT1 campaign",
                "level": "None"
              },
              "object": {
                "operation": "mixed",
                "time_start": "",
                "time_end": ""
              },
              "citations": [],
              "evidence_ids": [
                "E-0008"
              ],
              "claim_features": {
                "coherence": 0.48,
                "confidence_discipline": 0.5700000000000001,
                "compliance_mapping": 0.3,
                "allegation_gravity": "medium"
              }
            }
          ]
        },
        "scoring": {
          "claim_scores": [
            {
              "claim_id": "C001",
              "evidence_count": 2,
              "source_count": 1,
              "unique_origin_count": 1,
              "source_unique_origin_count": 1,
              "evidence_weight_aggregate": 0.2157452504968238,
              "origin_cluster_weights": {
                "ORIG:src0001": 0.215745
              },
              "penalty_multiplier": 0.85,
              "penalties": [
                {
                  "name": "single_source",
                  "factor": 0.85
                }
              ],
              "final_score": 0.183383,
              "recovered_reference_count": 0,
              "data_contribution_score": 1,
              "data_contribution_multiplier": 1,
              "core_3c": {
                "chain_of_custody_provenance": {
                  "score": 0.041241,
                  "band_0_5": 0
                },
                "credibility_independence": {
                  "score": 0.5499,
                  "band_0_5": 3
                },
                "corroboration_convergence": {
                  "score": 0.40743,
                  "band_0_5": 2
                }
              },
              "chain_provenance_diagnostics": {
                "context_completeness": 1,
                "integrity_signal": 0,
                "lineage_quality": 0.8,
                "report_derived_ratio": 0,
                "artifact_traceability": 0,
                "provenance_quality": 0.51,
                "anchor_quality": 1,
                "chain_disclosure_quality": 0.4,
                "claim_anchor_alignment": 1,
                "artifact_proximity_hierarchy": 1,
                "artifact_proximity_tiers": {
                  "direct": 1,
                  "contextual": 0,
                  "local": 0,
                  "remote": 0
                }
              },
              "claim_support_coverage": 1,
              "six_c_vector": {
                "credibility": 0.114378,
                "corroboration": 0.415745,
                "coherence": 0.48,
                "confidence_discipline": 0.57,
                "compliance_mapping": 1
              }
            },
            {
              "claim_id": "C002",
              "evidence_count": 2,
              "source_count": 1,
              "unique_origin_count": 1,
              "source_unique_origin_count": 1,
              "evidence_weight_aggregate": 0.25425169458202324,
              "origin_cluster_weights": {
                "ORIG:src0001": 0.254252
              },
              "penalty_multiplier": 0.85,
              "penalties": [
                {
                  "name": "single_source",
                  "factor": 0.85
                }
              ],
              "final_score": 0.216114,
              "recovered_reference_count": 0,
              "data_contribution_score": 1,
              "data_contribution_multiplier": 1,
              "core_3c": {
                "chain_of_custody_provenance": {
                  "score": 0.044201,
                  "band_0_5": 0
                },
                "credibility_independence": {
                  "score": 0.4977,
                  "band_0_5": 2
                },
                "corroboration_convergence": {
                  "score": 0.445167,
                  "band_0_5": 2
                }
              },
              "chain_provenance_diagnostics": {
                "context_completeness": 1,
                "integrity_signal": 0,
                "lineage_quality": 0.8,
                "report_derived_ratio": 0,
                "artifact_traceability": 0,
                "provenance_quality": 0.51,
                "anchor_quality": 1,
                "chain_disclosure_quality": 0.8,
                "claim_anchor_alignment": 1,
                "artifact_proximity_hierarchy": 1,
                "artifact_proximity_tiers": {
                  "direct": 1,
                  "contextual": 0,
                  "local": 0,
                  "remote": 0
                }
              },
              "claim_support_coverage": 1,
              "six_c_vector": {
                "credibility": 0.136373,
                "corroboration": 0.454252,
                "coherence": 0.6,
                "confidence_discipline": 0.57,
                "compliance_mapping": 0.3
              }
            },
            {
              "claim_id": "C003",
              "evidence_count": 1,
              "source_count": 1,
              "unique_origin_count": 1,
              "source_unique_origin_count": 1,
              "evidence_weight_aggregate": 0.12288496999999998,
              "origin_cluster_weights": {
                "ORIG:src0001": 0.122885
              },
              "penalty_multiplier": 0.85,
              "penalties": [
                {
                  "name": "single_source",
                  "factor": 0.85
                }
              ],
              "final_score": 0.104452,
              "recovered_reference_count": 0,
              "data_contribution_score": 1,
              "data_contribution_multiplier": 1,
              "core_3c": {
                "chain_of_custody_provenance": {
                  "score": 0.041241,
                  "band_0_5": 0
                },
                "credibility_independence": {
                  "score": 0.4869,
                  "band_0_5": 2
                },
                "corroboration_convergence": {
                  "score": 0.257627,
                  "band_0_5": 1
                }
              },
              "chain_provenance_diagnostics": {
                "context_completeness": 1,
                "integrity_signal": 0,
                "lineage_quality": 0.8,
                "report_derived_ratio": 0,
                "artifact_traceability": 0,
                "provenance_quality": 0.51,
                "anchor_quality": 1,
                "chain_disclosure_quality": 0.4,
                "claim_anchor_alignment": 1,
                "artifact_proximity_hierarchy": 1,
                "artifact_proximity_tiers": {
                  "direct": 1,
                  "contextual": 0,
                  "local": 0,
                  "remote": 0
                }
              },
              "claim_support_coverage": 1,
              "six_c_vector": {
                "credibility": 0.122885,
                "corroboration": 0.262885,
                "coherence": 0.48,
                "confidence_discipline": 0.57,
                "compliance_mapping": 0.3
              }
            },
            {
              "claim_id": "C004",
              "evidence_count": 1,
              "source_count": 1,
              "unique_origin_count": 1,
              "source_unique_origin_count": 1,
              "evidence_weight_aggregate": 0.14651669499999997,
              "origin_cluster_weights": {
                "ORIG:src0001": 0.146517
              },
              "penalty_multiplier": 0.85,
              "penalties": [
                {
                  "name": "single_source",
                  "factor": 0.85
                }
              ],
              "final_score": 0.124539,
              "recovered_reference_count": 0,
              "data_contribution_score": 1,
              "data_contribution_multiplier": 1,
              "core_3c": {
                "chain_of_custody_provenance": {
                  "score": 0.044201,
                  "band_0_5": 0
                },
                "credibility_independence": {
                  "score": 0.4977,
                  "band_0_5": 2
                },
                "corroboration_convergence": {
                  "score": 0.280786,
                  "band_0_5": 1
                }
              },
              "chain_provenance_diagnostics": {
                "context_completeness": 1,
                "integrity_signal": 0,
                "lineage_quality": 0.8,
                "report_derived_ratio": 0,
                "artifact_traceability": 0,
                "provenance_quality": 0.51,
                "anchor_quality": 1,
                "chain_disclosure_quality": 0.8,
                "claim_anchor_alignment": 1,
                "artifact_proximity_hierarchy": 1,
                "artifact_proximity_tiers": {
                  "direct": 1,
                  "contextual": 0,
                  "local": 0,
                  "remote": 0
                }
              },
              "claim_support_coverage": 1,
              "six_c_vector": {
                "credibility": 0.146517,
                "corroboration": 0.286517,
                "coherence": 0.6,
                "confidence_discipline": 0.57,
                "compliance_mapping": 0.3
              }
            },
            {
              "claim_id": "C005",
              "evidence_count": 1,
              "source_count": 1,
              "unique_origin_count": 1,
              "source_unique_origin_count": 1,
              "evidence_weight_aggregate": 0.14651669499999997,
              "origin_cluster_weights": {
                "ORIG:src0001": 0.146517
              },
              "penalty_multiplier": 0.85,
              "penalties": [
                {
                  "name": "single_source",
                  "factor": 0.85
                }
              ],
              "final_score": 0.124539,
              "recovered_reference_count": 0,
              "data_contribution_score": 1,
              "data_contribution_multiplier": 1,
              "core_3c": {
                "chain_of_custody_provenance": {
                  "score": 0.044201,
                  "band_0_5": 0
                },
                "credibility_independence": {
                  "score": 0.4977,
                  "band_0_5": 2
                },
                "corroboration_convergence": {
                  "score": 0.280786,
                  "band_0_5": 1
                }
              },
              "chain_provenance_diagnostics": {
                "context_completeness": 1,
                "integrity_signal": 0,
                "lineage_quality": 0.8,
                "report_derived_ratio": 0,
                "artifact_traceability": 0,
                "provenance_quality": 0.51,
                "anchor_quality": 1,
                "chain_disclosure_quality": 0.8,
                "claim_anchor_alignment": 1,
                "artifact_proximity_hierarchy": 1,
                "artifact_proximity_tiers": {
                  "direct": 1,
                  "contextual": 0,
                  "local": 0,
                  "remote": 0
                }
              },
              "claim_support_coverage": 1,
              "six_c_vector": {
                "credibility": 0.146517,
                "corroboration": 0.286517,
                "coherence": 0.6,
                "confidence_discipline": 0.57,
                "compliance_mapping": 0.3
              }
            },
            {
              "claim_id": "C006",
              "evidence_count": 1,
              "source_count": 1,
              "unique_origin_count": 1,
              "source_unique_origin_count": 1,
              "evidence_weight_aggregate": 0.12288496999999998,
              "origin_cluster_weights": {
                "ORIG:src0001": 0.122885
              },
              "penalty_multiplier": 0.85,
              "penalties": [
                {
                  "name": "single_source",
                  "factor": 0.85
                }
              ],
              "final_score": 0.104452,
              "recovered_reference_count": 0,
              "data_contribution_score": 1,
              "data_contribution_multiplier": 1,
              "core_3c": {
                "chain_of_custody_provenance": {
                  "score": 0.041241,
                  "band_0_5": 0
                },
                "credibility_independence": {
                  "score": 0.4869,
                  "band_0_5": 2
                },
                "corroboration_convergence": {
                  "score": 0.257627,
                  "band_0_5": 1
                }
              },
              "chain_provenance_diagnostics": {
                "context_completeness": 1,
                "integrity_signal": 0,
                "lineage_quality": 0.8,
                "report_derived_ratio": 0,
                "artifact_traceability": 0,
                "provenance_quality": 0.51,
                "anchor_quality": 1,
                "chain_disclosure_quality": 0.4,
                "claim_anchor_alignment": 1,
                "artifact_proximity_hierarchy": 1,
                "artifact_proximity_tiers": {
                  "direct": 1,
                  "contextual": 0,
                  "local": 0,
                  "remote": 0
                }
              },
              "claim_support_coverage": 1,
              "six_c_vector": {
                "credibility": 0.122885,
                "corroboration": 0.262885,
                "coherence": 0.48,
                "confidence_discipline": 0.57,
                "compliance_mapping": 0.3
              }
            }
          ],
          "document": {
            "profile": "balanced",
            "headline_claim_id": "C001",
            "headline_vector": {
              "credibility": 0.114378,
              "corroboration": 0.415745,
              "coherence": 0.48,
              "confidence_discipline": 0.57,
              "compliance_mapping": 1
            },
            "coverage_vector_median": {
              "credibility": 0.129629,
              "corroboration": 0.286517,
              "coherence": 0.54,
              "confidence_discipline": 0.57,
              "compliance_mapping": 0.3
            },
            "overall_claim_score_mean": 0.142913,
            "overall_claim_score_geometric": 0.137324,
            "seriousness_gate": {
              "passed": true,
              "thresholds": {
                "overall_claim_score_mean_min": 0.1,
                "credibility_independence_median_min": 0.4,
                "corroboration_convergence_median_min": 0.2
              },
              "observed": {
                "overall_claim_score_mean": 0.142913,
                "credibility_independence_median": 0.4977,
                "corroboration_convergence_median": 0.280786
              }
            }
          }
        }
      },
      "full_icj_v3": {
        "report_type": "icj_score_report",
        "report_version": "v3",
        "document_scores": {
          "belief_weighted_0_100": 0.07,
          "grounding_avg_0_100": 56.68,
          "custody_avg_0_100": 38.75,
          "credibility_avg_0_100": 0,
          "corroboration_avg_0_100": 0,
          "confidence_avg_0_100": 64.95,
          "clarity_avg_0_100": 40.86,
          "citation_coverage_sources_0_1": 0,
          "sources_total": 1,
          "citations_total": 0
        },
        "claims": [
          {
            "claim_id": "C001",
            "allegation_gravity": "medium",
            "required_threshold_0_1": 0.7,
            "gravity_weight": 1.2,
            "claim_statement": {
              "anchor_id": "C001-A001",
              "text": "On Feb. 2, 2018, we published a blog detailing the use of an Adobe Flash zero-day vulnerability (CVE-2018-4878) by a suspected North Korean cyber espionage group that we now track as APT37 (Reaper). Recent examination of this group's activities by FireEye iSIGHT Intelligence reveals APT37 has expanded its operations in both scope and sophistication. APT37's toolset, which includes access to zero-day vulnerabilities and wiper malware, combined with heightened tensions in Northeast Asia and North Korea's penchant for norm breaking, means this group should be taken seriously."
            },
            "evidence": {
              "evidence_anchor_ids": [
                "P000-B03",
                "P000-B07",
                "P000-B12",
                "P000-B09",
                "P000-B04",
                "P000-B08",
                "P000-B06",
                "P000-B05"
              ],
              "claim_sources": []
            },
            "support_anchor_ids": [
              "P000-B03",
              "P000-B07",
              "P000-B12",
              "P000-B09",
              "P000-B04",
              "P000-B08",
              "P000-B06",
              "P000-B05"
            ],
            "sources_supporting_claim": [],
            "scores": {
              "grounding_0_100": 55.56,
              "custody_0_100": 33,
              "credibility_0_100": 0,
              "corroboration_0_100": 0,
              "confidence_0_100": 69.2,
              "clarity_0_100": 58.77,
              "evidence_weight_0_100": 21.01,
              "evidence_support_0_1": 0.1235,
              "belief_0_100": 0.1,
              "credibility_raw_0_100": 0,
              "corroboration_raw_0_100": 0
            },
            "score_details": {
              "grounding": {
                "evidence_anchor_count": 8,
                "evidence_citation_count": 0,
                "evidence_source_count": 0,
                "anchor_coverage": 1,
                "nondup": 0.8405,
                "evidence_marker_strength": 0.6875,
                "citation_coverage": 0,
                "source_coverage": 0
              },
              "custody": {
                "provenance": 0.1,
                "integrity": 0,
                "time_anchors": 0.6667,
                "artifact_identifiers": 1,
                "versioning": 0.5
              },
              "credibility": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_type_counts": {},
                "quality_mean": 0,
                "quality_top": 0,
                "source_diversity": 0,
                "domain_independence": 0,
                "single_source_penalty": 1,
                "has_high_cred_source": 0,
                "high_cred_threshold": 0.9,
                "claim_coverage_factor": 0,
                "credible_claims_count": 0,
                "claims_total": 6,
                "credible_claims_ratio": 0,
                "credible_claims_weighted_ratio": 0
              },
              "corroboration": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_weight_sum": 0,
                "excluded_source_ids": [],
                "source_type_counts": {},
                "source_count": 0,
                "unique_domain_count": 0,
                "domain_independence": 0,
                "multi_source_factor": 0,
                "domain_independence_effective": 0,
                "source_quantity": 0,
                "source_domain_independence": 0,
                "modality_diversity": 0,
                "modality_diversity_effective": 0,
                "crosscheck_language": 0,
                "crosscheck_language_effective": 0,
                "corroboration_method": 0,
                "claim_coverage_factor": 0,
                "corroborated_claims_count": 0,
                "claims_total": 6,
                "corroborated_claims_ratio": 0,
                "corroborated_claims_weighted_ratio": 0
              },
              "confidence": {
                "explicitness": 1,
                "uncertainty_transparency": 0.75,
                "stated_confidence": 0.9,
                "calibration": 0.2235
              },
              "clarity": {
                "act_specificity": 0.7833,
                "actor_specificity": 0.7833,
                "link_specificity": 0.3333,
                "act_claim": 0.6667,
                "act_evidence": 1,
                "actor_claim": 0.6667,
                "actor_evidence": 1,
                "link_claim": 0,
                "link_evidence": 0.8333,
                "state_actor_signal": 0.7,
                "organ_path_clarity": 0.615,
                "control_path_clarity": 0.239,
                "due_diligence_path_clarity": 0,
                "legal_path_max": 0.615,
                "legal_path_coverage": 0.3333,
                "state_claim_flag": 1,
                "state_link_evidence_flag": 1,
                "state_actor_gap_penalty": 0,
                "legal_path_gap_penalty": 0,
                "questions": {
                  "attribution_clarity": {
                    "question": "Is attribution to State X for attack Z clear?",
                    "score_0_1": 0.6483,
                    "answer": "partial"
                  },
                  "responsibility_mode_clarity": {
                    "question": "Is the state-responsibility mode clear (organ, control, or due diligence)?",
                    "score_0_1": 0.615,
                    "answer": "yes"
                  },
                  "due_diligence_clarity": {
                    "question": "Is state knowledge plus failure to prevent (due diligence) clear?",
                    "score_0_1": 0,
                    "answer": "no"
                  }
                },
                "responsibility_modes": {
                  "conducted_by_state_organs": {
                    "score_0_1": 0.615,
                    "answer": "yes"
                  },
                  "non_state_actors_under_state_control": {
                    "score_0_1": 0.239,
                    "answer": "no"
                  },
                  "state_due_diligence_failure": {
                    "score_0_1": 0,
                    "answer": "no"
                  }
                }
              }
            }
          },
          {
            "claim_id": "C002",
            "allegation_gravity": "medium",
            "required_threshold_0_1": 0.7,
            "gravity_weight": 1.2,
            "claim_statement": {
              "anchor_id": "C002-A001",
              "text": "In May 2017, APT37 used a bank liquidation letter as a spear phishing lure against a board member of a Middle Eastern financial company. The specially crafted email included an attachment containing exploit code for CVE-2017-0199, a vulnerability in Microsoft Office that had been disclosed just a month earlier. Once opened, the malicious document communicated with a compromised website, most likely to surreptitiously download and install a backdoor called SHUTTERSPEED (MDS: 7c2ebfc7960aac6f8d58b37a3f092a9c). The tool would enable APT37 to collect system information, take screenshots and download additional malicious files to the victim computer."
            },
            "evidence": {
              "evidence_anchor_ids": [
                "P000-B05",
                "P000-B12",
                "P000-B04",
                "P000-B08",
                "P000-B09",
                "P000-B10",
                "P000-B06",
                "P000-B07"
              ],
              "claim_sources": []
            },
            "support_anchor_ids": [
              "P000-B05",
              "P000-B12",
              "P000-B04",
              "P000-B08",
              "P000-B09",
              "P000-B10",
              "P000-B06",
              "P000-B07"
            ],
            "sources_supporting_claim": [],
            "scores": {
              "grounding_0_100": 56.75,
              "custody_0_100": 41,
              "credibility_0_100": 0,
              "corroboration_0_100": 0,
              "confidence_0_100": 67.61,
              "clarity_0_100": 29.7,
              "evidence_weight_0_100": 23.65,
              "evidence_support_0_1": 0.0702,
              "belief_0_100": 0.05,
              "credibility_raw_0_100": 0,
              "corroboration_raw_0_100": 0
            },
            "score_details": {
              "grounding": {
                "evidence_anchor_count": 8,
                "evidence_citation_count": 0,
                "evidence_source_count": 0,
                "anchor_coverage": 1,
                "nondup": 0.8846,
                "evidence_marker_strength": 0.7031,
                "citation_coverage": 0,
                "source_coverage": 0
              },
              "custody": {
                "provenance": 0.2,
                "integrity": 0,
                "time_anchors": 1,
                "artifact_identifiers": 1,
                "versioning": 0.5
              },
              "credibility": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_type_counts": {},
                "quality_mean": 0,
                "quality_top": 0,
                "source_diversity": 0,
                "domain_independence": 0,
                "single_source_penalty": 1,
                "has_high_cred_source": 0,
                "high_cred_threshold": 0.9,
                "claim_coverage_factor": 0,
                "credible_claims_count": 0,
                "claims_total": 6,
                "credible_claims_ratio": 0,
                "credible_claims_weighted_ratio": 0
              },
              "corroboration": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_weight_sum": 0,
                "excluded_source_ids": [],
                "source_type_counts": {},
                "source_count": 0,
                "unique_domain_count": 0,
                "domain_independence": 0,
                "multi_source_factor": 0,
                "domain_independence_effective": 0,
                "source_quantity": 0,
                "source_domain_independence": 0,
                "modality_diversity": 0,
                "modality_diversity_effective": 0,
                "crosscheck_language": 0,
                "crosscheck_language_effective": 0,
                "corroboration_method": 0,
                "claim_coverage_factor": 0,
                "corroborated_claims_count": 0,
                "claims_total": 6,
                "corroborated_claims_ratio": 0,
                "corroborated_claims_weighted_ratio": 0
              },
              "confidence": {
                "explicitness": 1,
                "uncertainty_transparency": 0.75,
                "stated_confidence": 0.9,
                "calibration": 0.1702
              },
              "clarity": {
                "act_specificity": 0.5667,
                "actor_specificity": 0.35,
                "link_specificity": 0.1333,
                "act_claim": 0.3333,
                "act_evidence": 1,
                "actor_claim": 0,
                "actor_evidence": 1,
                "link_claim": 0,
                "link_evidence": 0.3333,
                "state_actor_signal": 0.4,
                "organ_path_clarity": 0.37,
                "control_path_clarity": 0.128,
                "due_diligence_path_clarity": 0,
                "legal_path_max": 0.37,
                "legal_path_coverage": 0,
                "state_claim_flag": 0,
                "state_link_evidence_flag": 1,
                "state_actor_gap_penalty": 0,
                "legal_path_gap_penalty": 0,
                "questions": {
                  "attribution_clarity": {
                    "question": "Is attribution to State X for attack Z clear?",
                    "score_0_1": 0.3717,
                    "answer": "no"
                  },
                  "responsibility_mode_clarity": {
                    "question": "Is the state-responsibility mode clear (organ, control, or due diligence)?",
                    "score_0_1": 0.37,
                    "answer": "no"
                  },
                  "due_diligence_clarity": {
                    "question": "Is state knowledge plus failure to prevent (due diligence) clear?",
                    "score_0_1": 0,
                    "answer": "no"
                  }
                },
                "responsibility_modes": {
                  "conducted_by_state_organs": {
                    "score_0_1": 0.37,
                    "answer": "no"
                  },
                  "non_state_actors_under_state_control": {
                    "score_0_1": 0.128,
                    "answer": "no"
                  },
                  "state_due_diligence_failure": {
                    "score_0_1": 0,
                    "answer": "no"
                  }
                }
              }
            }
          },
          {
            "claim_id": "C003",
            "allegation_gravity": "medium",
            "required_threshold_0_1": 0.7,
            "gravity_weight": 1.2,
            "claim_statement": {
              "anchor_id": "C003-A001",
              "text": "Numerous campaigns have employed social engineering tactics tailored specifically to desired targets. Lures and websites of particular interest to South Korean organizations (e.g. reunification) are regularly leveraged in campaigns. Multiple South Korean websites were abused in strategic web compromises to deliver newer variants of KARAE and POORAIM malware. Identified sites included South Korean conservative media and a news site for North Korean refugees and defectors. In one instance, APT37 weaponized a video downloader application with KARAE malware that was indiscriminately distributed to South Korean victims through torrent websites."
            },
            "evidence": {
              "evidence_anchor_ids": [
                "P000-B06",
                "P000-B12",
                "P000-B09",
                "P000-B04",
                "P000-B08",
                "P000-B10",
                "P000-B05",
                "P000-B11"
              ],
              "claim_sources": []
            },
            "support_anchor_ids": [
              "P000-B06",
              "P000-B12",
              "P000-B09",
              "P000-B04",
              "P000-B08",
              "P000-B10",
              "P000-B05",
              "P000-B11"
            ],
            "sources_supporting_claim": [],
            "scores": {
              "grounding_0_100": 55.78,
              "custody_0_100": 41,
              "credibility_0_100": 0,
              "corroboration_0_100": 0,
              "confidence_0_100": 67.69,
              "clarity_0_100": 31.17,
              "evidence_weight_0_100": 23.46,
              "evidence_support_0_1": 0.0731,
              "belief_0_100": 0.05,
              "credibility_raw_0_100": 0,
              "corroboration_raw_0_100": 0
            },
            "score_details": {
              "grounding": {
                "evidence_anchor_count": 8,
                "evidence_citation_count": 0,
                "evidence_source_count": 0,
                "anchor_coverage": 1,
                "nondup": 0.8515,
                "evidence_marker_strength": 0.6875,
                "citation_coverage": 0,
                "source_coverage": 0
              },
              "custody": {
                "provenance": 0.2,
                "integrity": 0,
                "time_anchors": 1,
                "artifact_identifiers": 1,
                "versioning": 0.5
              },
              "credibility": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_type_counts": {},
                "quality_mean": 0,
                "quality_top": 0,
                "source_diversity": 0,
                "domain_independence": 0,
                "single_source_penalty": 1,
                "has_high_cred_source": 0,
                "high_cred_threshold": 0.9,
                "claim_coverage_factor": 0,
                "credible_claims_count": 0,
                "claims_total": 6,
                "credible_claims_ratio": 0,
                "credible_claims_weighted_ratio": 0
              },
              "corroboration": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_weight_sum": 0,
                "excluded_source_ids": [],
                "source_type_counts": {},
                "source_count": 0,
                "unique_domain_count": 0,
                "domain_independence": 0,
                "multi_source_factor": 0,
                "domain_independence_effective": 0,
                "source_quantity": 0,
                "source_domain_independence": 0,
                "modality_diversity": 0,
                "modality_diversity_effective": 0,
                "crosscheck_language": 0,
                "crosscheck_language_effective": 0,
                "corroboration_method": 0,
                "claim_coverage_factor": 0,
                "corroborated_claims_count": 0,
                "claims_total": 6,
                "corroborated_claims_ratio": 0,
                "corroborated_claims_weighted_ratio": 0
              },
              "confidence": {
                "explicitness": 1,
                "uncertainty_transparency": 0.75,
                "stated_confidence": 0.9,
                "calibration": 0.1731
              },
              "clarity": {
                "act_specificity": 0.5667,
                "actor_specificity": 0.35,
                "link_specificity": 0.2,
                "act_claim": 0.3333,
                "act_evidence": 1,
                "actor_claim": 0,
                "actor_evidence": 1,
                "link_claim": 0,
                "link_evidence": 0.5,
                "state_actor_signal": 0.4,
                "organ_path_clarity": 0.39,
                "control_path_clarity": 0.128,
                "due_diligence_path_clarity": 0,
                "legal_path_max": 0.39,
                "legal_path_coverage": 0,
                "state_claim_flag": 0,
                "state_link_evidence_flag": 1,
                "state_actor_gap_penalty": 0,
                "legal_path_gap_penalty": 0,
                "questions": {
                  "attribution_clarity": {
                    "question": "Is attribution to State X for attack Z clear?",
                    "score_0_1": 0.3917,
                    "answer": "no"
                  },
                  "responsibility_mode_clarity": {
                    "question": "Is the state-responsibility mode clear (organ, control, or due diligence)?",
                    "score_0_1": 0.39,
                    "answer": "no"
                  },
                  "due_diligence_clarity": {
                    "question": "Is state knowledge plus failure to prevent (due diligence) clear?",
                    "score_0_1": 0,
                    "answer": "no"
                  }
                },
                "responsibility_modes": {
                  "conducted_by_state_organs": {
                    "score_0_1": 0.39,
                    "answer": "no"
                  },
                  "non_state_actors_under_state_control": {
                    "score_0_1": 0.128,
                    "answer": "no"
                  },
                  "state_due_diligence_failure": {
                    "score_0_1": 0,
                    "answer": "no"
                  }
                }
              }
            }
          },
          {
            "claim_id": "C004",
            "allegation_gravity": "medium",
            "required_threshold_0_1": 0.7,
            "gravity_weight": 1.2,
            "claim_statement": {
              "anchor_id": "C004-A001",
              "text": "APT37 relies on compromised websites to host second stage malware. Small websites focused on subjects such as aromatherapy and scuba diving have been leveraged, and were most likely compromised opportunistically and made to host malicious payloads."
            },
            "evidence": {
              "evidence_anchor_ids": [
                "P000-B08",
                "P000-B12",
                "P000-B09",
                "P000-B05",
                "P000-B06",
                "P000-B04",
                "P000-B10",
                "P000-B11"
              ],
              "claim_sources": []
            },
            "support_anchor_ids": [
              "P000-B08",
              "P000-B12",
              "P000-B09",
              "P000-B05",
              "P000-B06",
              "P000-B04",
              "P000-B10",
              "P000-B11"
            ],
            "sources_supporting_claim": [],
            "scores": {
              "grounding_0_100": 57.64,
              "custody_0_100": 41,
              "credibility_0_100": 0,
              "corroboration_0_100": 0,
              "confidence_0_100": 67.73,
              "clarity_0_100": 31.17,
              "evidence_weight_0_100": 23.83,
              "evidence_support_0_1": 0.0743,
              "belief_0_100": 0.05,
              "credibility_raw_0_100": 0,
              "corroboration_raw_0_100": 0
            },
            "score_details": {
              "grounding": {
                "evidence_anchor_count": 8,
                "evidence_citation_count": 0,
                "evidence_source_count": 0,
                "anchor_coverage": 1,
                "nondup": 0.9445,
                "evidence_marker_strength": 0.6875,
                "citation_coverage": 0,
                "source_coverage": 0
              },
              "custody": {
                "provenance": 0.2,
                "integrity": 0,
                "time_anchors": 1,
                "artifact_identifiers": 1,
                "versioning": 0.5
              },
              "credibility": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_type_counts": {},
                "quality_mean": 0,
                "quality_top": 0,
                "source_diversity": 0,
                "domain_independence": 0,
                "single_source_penalty": 1,
                "has_high_cred_source": 0,
                "high_cred_threshold": 0.9,
                "claim_coverage_factor": 0,
                "credible_claims_count": 0,
                "claims_total": 6,
                "credible_claims_ratio": 0,
                "credible_claims_weighted_ratio": 0
              },
              "corroboration": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_weight_sum": 0,
                "excluded_source_ids": [],
                "source_type_counts": {},
                "source_count": 0,
                "unique_domain_count": 0,
                "domain_independence": 0,
                "multi_source_factor": 0,
                "domain_independence_effective": 0,
                "source_quantity": 0,
                "source_domain_independence": 0,
                "modality_diversity": 0,
                "modality_diversity_effective": 0,
                "crosscheck_language": 0,
                "crosscheck_language_effective": 0,
                "corroboration_method": 0,
                "claim_coverage_factor": 0,
                "corroborated_claims_count": 0,
                "claims_total": 6,
                "corroborated_claims_ratio": 0,
                "corroborated_claims_weighted_ratio": 0
              },
              "confidence": {
                "explicitness": 1,
                "uncertainty_transparency": 0.75,
                "stated_confidence": 0.9,
                "calibration": 0.1743
              },
              "clarity": {
                "act_specificity": 0.5667,
                "actor_specificity": 0.35,
                "link_specificity": 0.2,
                "act_claim": 0.3333,
                "act_evidence": 1,
                "actor_claim": 0,
                "actor_evidence": 1,
                "link_claim": 0,
                "link_evidence": 0.5,
                "state_actor_signal": 0.4,
                "organ_path_clarity": 0.39,
                "control_path_clarity": 0.128,
                "due_diligence_path_clarity": 0,
                "legal_path_max": 0.39,
                "legal_path_coverage": 0,
                "state_claim_flag": 0,
                "state_link_evidence_flag": 1,
                "state_actor_gap_penalty": 0,
                "legal_path_gap_penalty": 0,
                "questions": {
                  "attribution_clarity": {
                    "question": "Is attribution to State X for attack Z clear?",
                    "score_0_1": 0.3917,
                    "answer": "no"
                  },
                  "responsibility_mode_clarity": {
                    "question": "Is the state-responsibility mode clear (organ, control, or due diligence)?",
                    "score_0_1": 0.39,
                    "answer": "no"
                  },
                  "due_diligence_clarity": {
                    "question": "Is state knowledge plus failure to prevent (due diligence) clear?",
                    "score_0_1": 0,
                    "answer": "no"
                  }
                },
                "responsibility_modes": {
                  "conducted_by_state_organs": {
                    "score_0_1": 0.39,
                    "answer": "no"
                  },
                  "non_state_actors_under_state_control": {
                    "score_0_1": 0.128,
                    "answer": "no"
                  },
                  "state_due_diligence_failure": {
                    "score_0_1": 0,
                    "answer": "no"
                  }
                }
              }
            }
          },
          {
            "claim_id": "C005",
            "allegation_gravity": "medium",
            "required_threshold_0_1": 0.7,
            "gravity_weight": 1.2,
            "claim_statement": {
              "anchor_id": "C005-A001",
              "text": "We assess with high confidence that APT37 acts in support of the North Korean government and is primarily based in North Korea. This assessment is based on multiple factors, including APT37's targeting profile, insight into the group's malware development and probable links to a North Korean individual believed to be the developer of several of APT37's proprietary malware families:"
            },
            "evidence": {
              "evidence_anchor_ids": [
                "P000-B10",
                "P000-B04",
                "P000-B03",
                "P000-B12",
                "P000-B09",
                "P000-B05",
                "P000-B08",
                "P000-B07"
              ],
              "claim_sources": []
            },
            "support_anchor_ids": [
              "P000-B10",
              "P000-B04",
              "P000-B03",
              "P000-B12",
              "P000-B09",
              "P000-B05",
              "P000-B08",
              "P000-B07"
            ],
            "sources_supporting_claim": [],
            "scores": {
              "grounding_0_100": 56.52,
              "custody_0_100": 41,
              "credibility_0_100": 0,
              "corroboration_0_100": 0,
              "confidence_0_100": 69.23,
              "clarity_0_100": 52.7,
              "evidence_weight_0_100": 23.6,
              "evidence_support_0_1": 0.1244,
              "belief_0_100": 0.1,
              "credibility_raw_0_100": 0,
              "corroboration_raw_0_100": 0
            },
            "score_details": {
              "grounding": {
                "evidence_anchor_count": 8,
                "evidence_citation_count": 0,
                "evidence_source_count": 0,
                "anchor_coverage": 1,
                "nondup": 0.9043,
                "evidence_marker_strength": 0.6719,
                "citation_coverage": 0,
                "source_coverage": 0
              },
              "custody": {
                "provenance": 0.2,
                "integrity": 0,
                "time_anchors": 1,
                "artifact_identifiers": 1,
                "versioning": 0.5
              },
              "credibility": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_type_counts": {},
                "quality_mean": 0,
                "quality_top": 0,
                "source_diversity": 0,
                "domain_independence": 0,
                "single_source_penalty": 1,
                "has_high_cred_source": 0,
                "high_cred_threshold": 0.9,
                "claim_coverage_factor": 0,
                "credible_claims_count": 0,
                "claims_total": 6,
                "credible_claims_ratio": 0,
                "credible_claims_weighted_ratio": 0
              },
              "corroboration": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_weight_sum": 0,
                "excluded_source_ids": [],
                "source_type_counts": {},
                "source_count": 0,
                "unique_domain_count": 0,
                "domain_independence": 0,
                "multi_source_factor": 0,
                "domain_independence_effective": 0,
                "source_quantity": 0,
                "source_domain_independence": 0,
                "modality_diversity": 0,
                "modality_diversity_effective": 0,
                "crosscheck_language": 0,
                "crosscheck_language_effective": 0,
                "corroboration_method": 0,
                "claim_coverage_factor": 0,
                "corroborated_claims_count": 0,
                "claims_total": 6,
                "corroborated_claims_ratio": 0,
                "corroborated_claims_weighted_ratio": 0
              },
              "confidence": {
                "explicitness": 1,
                "uncertainty_transparency": 0.75,
                "stated_confidence": 0.9,
                "calibration": 0.2244
              },
              "clarity": {
                "act_specificity": 0.5667,
                "actor_specificity": 0.7833,
                "link_specificity": 0.3333,
                "act_claim": 0.3333,
                "act_evidence": 1,
                "actor_claim": 0.6667,
                "actor_evidence": 1,
                "link_claim": 0,
                "link_evidence": 0.8333,
                "state_actor_signal": 0.7,
                "organ_path_clarity": 0.55,
                "control_path_clarity": 0.239,
                "due_diligence_path_clarity": 0,
                "legal_path_max": 0.55,
                "legal_path_coverage": 0.3333,
                "state_claim_flag": 1,
                "state_link_evidence_flag": 1,
                "state_actor_gap_penalty": 0,
                "legal_path_gap_penalty": 0,
                "questions": {
                  "attribution_clarity": {
                    "question": "Is attribution to State X for attack Z clear?",
                    "score_0_1": 0.5617,
                    "answer": "partial"
                  },
                  "responsibility_mode_clarity": {
                    "question": "Is the state-responsibility mode clear (organ, control, or due diligence)?",
                    "score_0_1": 0.55,
                    "answer": "partial"
                  },
                  "due_diligence_clarity": {
                    "question": "Is state knowledge plus failure to prevent (due diligence) clear?",
                    "score_0_1": 0,
                    "answer": "no"
                  }
                },
                "responsibility_modes": {
                  "conducted_by_state_organs": {
                    "score_0_1": 0.55,
                    "answer": "partial"
                  },
                  "non_state_actors_under_state_control": {
                    "score_0_1": 0.239,
                    "answer": "no"
                  },
                  "state_due_diligence_failure": {
                    "score_0_1": 0,
                    "answer": "no"
                  }
                }
              }
            }
          },
          {
            "claim_id": "C006",
            "allegation_gravity": "medium",
            "required_threshold_0_1": 0.7,
            "gravity_weight": 1.2,
            "claim_statement": {
              "anchor_id": "C006-A001",
              "text": "Many of the compromised domains in the command and control infrastructure are linked to South Korean companies. Most of these domains host a fake webpage pertinent to targets. | FE_APT_Backdoor_SHUTTERSPEED"
            },
            "evidence": {
              "evidence_anchor_ids": [
                "P000-B12",
                "P000-B08",
                "P000-B09",
                "P000-B05",
                "P000-B06",
                "P000-B02",
                "P000-B04",
                "P000-B11"
              ],
              "claim_sources": []
            },
            "support_anchor_ids": [
              "P000-B12",
              "P000-B08",
              "P000-B09",
              "P000-B05",
              "P000-B06",
              "P000-B02",
              "P000-B04",
              "P000-B11"
            ],
            "sources_supporting_claim": [],
            "scores": {
              "grounding_0_100": 57.81,
              "custody_0_100": 35.5,
              "credibility_0_100": 0,
              "corroboration_0_100": 0,
              "confidence_0_100": 48.27,
              "clarity_0_100": 41.63,
              "evidence_weight_0_100": 22.21,
              "evidence_support_0_1": 0.0925,
              "belief_0_100": 0.07,
              "credibility_raw_0_100": 0,
              "corroboration_raw_0_100": 0
            },
            "score_details": {
              "grounding": {
                "evidence_anchor_count": 8,
                "evidence_citation_count": 0,
                "evidence_source_count": 0,
                "anchor_coverage": 1,
                "nondup": 0.9528,
                "evidence_marker_strength": 0.6875,
                "citation_coverage": 0,
                "source_coverage": 0
              },
              "custody": {
                "provenance": 0.1,
                "integrity": 0,
                "time_anchors": 0.8333,
                "artifact_identifiers": 1,
                "versioning": 0.5
              },
              "credibility": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_type_counts": {},
                "quality_mean": 0,
                "quality_top": 0,
                "source_diversity": 0,
                "domain_independence": 0,
                "single_source_penalty": 1,
                "has_high_cred_source": 0,
                "high_cred_threshold": 0.9,
                "claim_coverage_factor": 0,
                "credible_claims_count": 0,
                "claims_total": 6,
                "credible_claims_ratio": 0,
                "credible_claims_weighted_ratio": 0
              },
              "corroboration": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_weight_sum": 0,
                "excluded_source_ids": [],
                "source_type_counts": {},
                "source_count": 0,
                "unique_domain_count": 0,
                "domain_independence": 0,
                "multi_source_factor": 0,
                "domain_independence_effective": 0,
                "source_quantity": 0,
                "source_domain_independence": 0,
                "modality_diversity": 0,
                "modality_diversity_effective": 0,
                "crosscheck_language": 0,
                "crosscheck_language_effective": 0,
                "corroboration_method": 0,
                "claim_coverage_factor": 0,
                "corroborated_claims_count": 0,
                "claims_total": 6,
                "corroborated_claims_ratio": 0,
                "corroborated_claims_weighted_ratio": 0
              },
              "confidence": {
                "explicitness": 0.5,
                "uncertainty_transparency": 0.5,
                "stated_confidence": 0.65,
                "calibration": 0.4425
              },
              "clarity": {
                "act_specificity": 0.7833,
                "actor_specificity": 0.35,
                "link_specificity": 0.4,
                "act_claim": 0.6667,
                "act_evidence": 1,
                "actor_claim": 0,
                "actor_evidence": 1,
                "link_claim": 0.3333,
                "link_evidence": 0.5,
                "state_actor_signal": 0.4,
                "organ_path_clarity": 0.515,
                "control_path_clarity": 0.128,
                "due_diligence_path_clarity": 0,
                "legal_path_max": 0.515,
                "legal_path_coverage": 0,
                "state_claim_flag": 0,
                "state_link_evidence_flag": 1,
                "state_actor_gap_penalty": 0,
                "legal_path_gap_penalty": 0,
                "questions": {
                  "attribution_clarity": {
                    "question": "Is attribution to State X for attack Z clear?",
                    "score_0_1": 0.5383,
                    "answer": "partial"
                  },
                  "responsibility_mode_clarity": {
                    "question": "Is the state-responsibility mode clear (organ, control, or due diligence)?",
                    "score_0_1": 0.515,
                    "answer": "partial"
                  },
                  "due_diligence_clarity": {
                    "question": "Is state knowledge plus failure to prevent (due diligence) clear?",
                    "score_0_1": 0,
                    "answer": "no"
                  }
                },
                "responsibility_modes": {
                  "conducted_by_state_organs": {
                    "score_0_1": 0.515,
                    "answer": "partial"
                  },
                  "non_state_actors_under_state_control": {
                    "score_0_1": 0.128,
                    "answer": "no"
                  },
                  "state_due_diligence_failure": {
                    "score_0_1": 0,
                    "answer": "no"
                  }
                }
              }
            }
          }
        ]
      },
      "full_icj_v4": {
        "report_type": "icj_score_report",
        "report_version": "v4",
        "document_scores": {
          "belief_weighted_0_100": 0.06,
          "grounding_avg_0_100": 56.68,
          "custody_avg_0_100": 34.5,
          "credibility_avg_0_100": 0.0,
          "corroboration_avg_0_100": 0.0,
          "confidence_avg_0_100": 59.03,
          "clarity_avg_0_100": 37.11,
          "citation_coverage_sources_0_1": 0.0,
          "sources_total": 1,
          "citations_total": 0,
          "credibility_composite_avg_0_100": 0.0,
          "bootstrap_95ci": {
            "belief_weighted_0_100": {
              "mean": 0.06,
              "ci95_low": 0.05,
              "ci95_high": 0.07
            },
            "custody_avg_0_100": {
              "mean": 34.5,
              "ci95_low": 33.0,
              "ci95_high": 35.76
            },
            "credibility_avg_0_100": {
              "mean": 0.0,
              "ci95_low": 0.0,
              "ci95_high": 0.0
            },
            "corroboration_avg_0_100": {
              "mean": 0.0,
              "ci95_low": 0.0,
              "ci95_high": 0.0
            },
            "clarity_avg_0_100": {
              "mean": 37.11,
              "ci95_low": 31.96,
              "ci95_high": 42.67
            }
          }
        },
        "claims": [
          {
            "claim_id": "C001",
            "allegation_gravity": "medium",
            "required_threshold_0_1": 0.7,
            "gravity_weight": 1.2,
            "claim_statement": {
              "anchor_id": "C001-A001",
              "text": "On Feb. 2, 2018, we published a blog detailing the use of an Adobe Flash zero-day vulnerability (CVE-2018-4878) by a suspected North Korean cyber espionage group that we now track as APT37 (Reaper). Recent examination of this group's activities by FireEye iSIGHT Intelligence reveals APT37 has expanded its operations in both scope and sophistication. APT37's toolset, which includes access to zero-day vulnerabilities and wiper malware, combined with heightened tensions in Northeast Asia and North Korea's penchant for norm breaking, means this group should be taken seriously."
            },
            "evidence": {
              "evidence_anchor_ids": [
                "P000-B03",
                "P000-B07",
                "P000-B12",
                "P000-B09",
                "P000-B04",
                "P000-B08",
                "P000-B06",
                "P000-B05"
              ],
              "claim_sources": []
            },
            "support_anchor_ids": [
              "P000-B03",
              "P000-B07",
              "P000-B12",
              "P000-B09",
              "P000-B04",
              "P000-B08",
              "P000-B06",
              "P000-B05"
            ],
            "sources_supporting_claim": [],
            "scores": {
              "grounding_0_100": 55.56,
              "custody_0_100": 31.27,
              "credibility_0_100": 0.0,
              "corroboration_0_100": 0.0,
              "confidence_0_100": 61.31,
              "clarity_0_100": 47.62,
              "evidence_weight_0_100": 20.49,
              "evidence_support_0_1": 0.0976,
              "belief_0_100": 0.07,
              "credibility_raw_0_100": 0.0,
              "corroboration_raw_0_100": 0.0
            },
            "score_details": {
              "grounding": {
                "evidence_anchor_count": 8,
                "evidence_citation_count": 0,
                "evidence_source_count": 0,
                "anchor_coverage": 1.0,
                "nondup": 0.8405,
                "evidence_marker_strength": 0.6875,
                "citation_coverage": 0.0,
                "source_coverage": 0.0
              },
              "custody": {
                "provenance": 0.1,
                "integrity": 0.0,
                "time_anchors": 0.6667,
                "artifact_identifiers": 1.0,
                "versioning": 0.5
              },
              "credibility": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_type_counts": {},
                "quality_mean": 0.0,
                "quality_top": 0.0,
                "source_diversity": 0.0,
                "domain_independence": 0.0,
                "single_source_penalty": 1.0,
                "has_high_cred_source": 0,
                "high_cred_threshold": 0.9,
                "claim_coverage_factor": 0.0,
                "credible_claims_count": 0,
                "claims_total": 6,
                "credible_claims_ratio": 0.0,
                "credible_claims_weighted_ratio": 0.0
              },
              "corroboration": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_weight_sum": 0.0,
                "excluded_source_ids": [],
                "source_type_counts": {},
                "source_count": 0,
                "unique_domain_count": 0,
                "domain_independence": 0.0,
                "multi_source_factor": 0.0,
                "domain_independence_effective": 0.0,
                "source_quantity": 0.0,
                "source_domain_independence": 0.0,
                "modality_diversity": 0.0,
                "modality_diversity_effective": 0.0,
                "crosscheck_language": 0.0,
                "crosscheck_language_effective": 0.0,
                "corroboration_method": 0.0,
                "claim_coverage_factor": 0.0,
                "corroborated_claims_count": 0,
                "claims_total": 6,
                "corroborated_claims_ratio": 0.0,
                "corroborated_claims_weighted_ratio": 0.0
              },
              "confidence": {
                "explicitness": 1.0,
                "uncertainty_transparency": 0.75,
                "stated_confidence": 0.9,
                "calibration": 0.2235
              },
              "clarity": {
                "act_specificity": 0.7833,
                "actor_specificity": 0.7833,
                "link_specificity": 0.3333,
                "act_claim": 0.6667,
                "act_evidence": 1.0,
                "actor_claim": 0.6667,
                "actor_evidence": 1.0,
                "link_claim": 0.0,
                "link_evidence": 0.8333,
                "state_actor_signal": 0.7,
                "organ_path_clarity": 0.615,
                "control_path_clarity": 0.239,
                "due_diligence_path_clarity": 0.0,
                "legal_path_max": 0.615,
                "legal_path_coverage": 0.3333,
                "state_claim_flag": 1,
                "state_link_evidence_flag": 1,
                "state_actor_gap_penalty": 0.0,
                "legal_path_gap_penalty": 0.0,
                "questions": {
                  "attribution_clarity": {
                    "question": "Is attribution to State X for attack Z clear?",
                    "score_0_1": 0.6483,
                    "answer": "partial"
                  },
                  "responsibility_mode_clarity": {
                    "question": "Is the state-responsibility mode clear (organ, control, or due diligence)?",
                    "score_0_1": 0.615,
                    "answer": "yes"
                  },
                  "due_diligence_clarity": {
                    "question": "Is state knowledge plus failure to prevent (due diligence) clear?",
                    "score_0_1": 0.0,
                    "answer": "no"
                  }
                },
                "responsibility_modes": {
                  "conducted_by_state_organs": {
                    "score_0_1": 0.615,
                    "answer": "yes"
                  },
                  "non_state_actors_under_state_control": {
                    "score_0_1": 0.239,
                    "answer": "no"
                  },
                  "state_due_diligence_failure": {
                    "score_0_1": 0.0,
                    "answer": "no"
                  }
                }
              },
              "statistical_calibration_v4": {
                "reliability_factor": 0.6062,
                "effective_evidence_n": 4.0,
                "shrinkage_lambda": {
                  "custody": 0.6452,
                  "credibility": 0.5882,
                  "corroboration": 0.5556,
                  "clarity": 0.6897,
                  "confidence": 0.6897
                },
                "prior_scores_0_1": {
                  "custody": 0.3875,
                  "credibility": 0.0,
                  "corroboration": 0.0,
                  "clarity": 0.4086,
                  "confidence": 0.6495
                },
                "saturation_factors": {
                  "chain_quantity_score": 1.0,
                  "chain_saturation": 1.0,
                  "corroboration_source_quantity": 0.0,
                  "corroboration_saturation": 0.0,
                  "credibility_quality_gate": 0.0
                }
              }
            },
            "scores_raw_v3": {
              "grounding_0_100": 55.56,
              "custody_0_100": 33.0,
              "credibility_0_100": 0.0,
              "corroboration_0_100": 0.0,
              "confidence_0_100": 69.2,
              "clarity_0_100": 58.77,
              "evidence_weight_0_100": 21.01,
              "evidence_support_0_1": 0.1235,
              "belief_0_100": 0.1,
              "credibility_raw_0_100": 0.0,
              "corroboration_raw_0_100": 0.0
            }
          },
          {
            "claim_id": "C002",
            "allegation_gravity": "medium",
            "required_threshold_0_1": 0.7,
            "gravity_weight": 1.2,
            "claim_statement": {
              "anchor_id": "C002-A001",
              "text": "In May 2017, APT37 used a bank liquidation letter as a spear phishing lure against a board member of a Middle Eastern financial company. The specially crafted email included an attachment containing exploit code for CVE-2017-0199, a vulnerability in Microsoft Office that had been disclosed just a month earlier. Once opened, the malicious document communicated with a compromised website, most likely to surreptitiously download and install a backdoor called SHUTTERSPEED (MDS: 7c2ebfc7960aac6f8d58b37a3f092a9c). The tool would enable APT37 to collect system information, take screenshots and download additional malicious files to the victim computer."
            },
            "evidence": {
              "evidence_anchor_ids": [
                "P000-B05",
                "P000-B12",
                "P000-B04",
                "P000-B08",
                "P000-B09",
                "P000-B10",
                "P000-B06",
                "P000-B07"
              ],
              "claim_sources": []
            },
            "support_anchor_ids": [
              "P000-B05",
              "P000-B12",
              "P000-B04",
              "P000-B08",
              "P000-B09",
              "P000-B10",
              "P000-B06",
              "P000-B07"
            ],
            "sources_supporting_claim": [],
            "scores": {
              "grounding_0_100": 56.75,
              "custody_0_100": 35.76,
              "credibility_0_100": 0.0,
              "corroboration_0_100": 0.0,
              "confidence_0_100": 60.7,
              "clarity_0_100": 30.49,
              "evidence_weight_0_100": 22.08,
              "evidence_support_0_1": 0.0673,
              "belief_0_100": 0.05,
              "credibility_raw_0_100": 0.0,
              "corroboration_raw_0_100": 0.0
            },
            "score_details": {
              "grounding": {
                "evidence_anchor_count": 8,
                "evidence_citation_count": 0,
                "evidence_source_count": 0,
                "anchor_coverage": 1.0,
                "nondup": 0.8846,
                "evidence_marker_strength": 0.7031,
                "citation_coverage": 0.0,
                "source_coverage": 0.0
              },
              "custody": {
                "provenance": 0.2,
                "integrity": 0.0,
                "time_anchors": 1.0,
                "artifact_identifiers": 1.0,
                "versioning": 0.5
              },
              "credibility": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_type_counts": {},
                "quality_mean": 0.0,
                "quality_top": 0.0,
                "source_diversity": 0.0,
                "domain_independence": 0.0,
                "single_source_penalty": 1.0,
                "has_high_cred_source": 0,
                "high_cred_threshold": 0.9,
                "claim_coverage_factor": 0.0,
                "credible_claims_count": 0,
                "claims_total": 6,
                "credible_claims_ratio": 0.0,
                "credible_claims_weighted_ratio": 0.0
              },
              "corroboration": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_weight_sum": 0.0,
                "excluded_source_ids": [],
                "source_type_counts": {},
                "source_count": 0,
                "unique_domain_count": 0,
                "domain_independence": 0.0,
                "multi_source_factor": 0.0,
                "domain_independence_effective": 0.0,
                "source_quantity": 0.0,
                "source_domain_independence": 0.0,
                "modality_diversity": 0.0,
                "modality_diversity_effective": 0.0,
                "crosscheck_language": 0.0,
                "crosscheck_language_effective": 0.0,
                "corroboration_method": 0.0,
                "claim_coverage_factor": 0.0,
                "corroborated_claims_count": 0,
                "claims_total": 6,
                "corroborated_claims_ratio": 0.0,
                "corroborated_claims_weighted_ratio": 0.0
              },
              "confidence": {
                "explicitness": 1.0,
                "uncertainty_transparency": 0.75,
                "stated_confidence": 0.9,
                "calibration": 0.1702
              },
              "clarity": {
                "act_specificity": 0.5667,
                "actor_specificity": 0.35,
                "link_specificity": 0.1333,
                "act_claim": 0.3333,
                "act_evidence": 1.0,
                "actor_claim": 0.0,
                "actor_evidence": 1.0,
                "link_claim": 0.0,
                "link_evidence": 0.3333,
                "state_actor_signal": 0.4,
                "organ_path_clarity": 0.37,
                "control_path_clarity": 0.128,
                "due_diligence_path_clarity": 0.0,
                "legal_path_max": 0.37,
                "legal_path_coverage": 0.0,
                "state_claim_flag": 0,
                "state_link_evidence_flag": 1,
                "state_actor_gap_penalty": 0.0,
                "legal_path_gap_penalty": 0.0,
                "questions": {
                  "attribution_clarity": {
                    "question": "Is attribution to State X for attack Z clear?",
                    "score_0_1": 0.3717,
                    "answer": "no"
                  },
                  "responsibility_mode_clarity": {
                    "question": "Is the state-responsibility mode clear (organ, control, or due diligence)?",
                    "score_0_1": 0.37,
                    "answer": "no"
                  },
                  "due_diligence_clarity": {
                    "question": "Is state knowledge plus failure to prevent (due diligence) clear?",
                    "score_0_1": 0.0,
                    "answer": "no"
                  }
                },
                "responsibility_modes": {
                  "conducted_by_state_organs": {
                    "score_0_1": 0.37,
                    "answer": "no"
                  },
                  "non_state_actors_under_state_control": {
                    "score_0_1": 0.128,
                    "answer": "no"
                  },
                  "state_due_diligence_failure": {
                    "score_0_1": 0.0,
                    "answer": "no"
                  }
                }
              },
              "statistical_calibration_v4": {
                "reliability_factor": 0.6269,
                "effective_evidence_n": 4.0,
                "shrinkage_lambda": {
                  "custody": 0.6452,
                  "credibility": 0.5882,
                  "corroboration": 0.5556,
                  "clarity": 0.6897,
                  "confidence": 0.6897
                },
                "prior_scores_0_1": {
                  "custody": 0.3875,
                  "credibility": 0.0,
                  "corroboration": 0.0,
                  "clarity": 0.4086,
                  "confidence": 0.6495
                },
                "saturation_factors": {
                  "chain_quantity_score": 1.0,
                  "chain_saturation": 1.0,
                  "corroboration_source_quantity": 0.0,
                  "corroboration_saturation": 0.0,
                  "credibility_quality_gate": 0.0
                }
              }
            },
            "scores_raw_v3": {
              "grounding_0_100": 56.75,
              "custody_0_100": 41.0,
              "credibility_0_100": 0.0,
              "corroboration_0_100": 0.0,
              "confidence_0_100": 67.61,
              "clarity_0_100": 29.7,
              "evidence_weight_0_100": 23.65,
              "evidence_support_0_1": 0.0702,
              "belief_0_100": 0.05,
              "credibility_raw_0_100": 0.0,
              "corroboration_raw_0_100": 0.0
            }
          },
          {
            "claim_id": "C003",
            "allegation_gravity": "medium",
            "required_threshold_0_1": 0.7,
            "gravity_weight": 1.2,
            "claim_statement": {
              "anchor_id": "C003-A001",
              "text": "Numerous campaigns have employed social engineering tactics tailored specifically to desired targets. Lures and websites of particular interest to South Korean organizations (e.g. reunification) are regularly leveraged in campaigns. Multiple South Korean websites were abused in strategic web compromises to deliver newer variants of KARAE and POORAIM malware. Identified sites included South Korean conservative media and a news site for North Korean refugees and defectors. In one instance, APT37 weaponized a video downloader application with KARAE malware that was indiscriminately distributed to South Korean victims through torrent websites."
            },
            "evidence": {
              "evidence_anchor_ids": [
                "P000-B06",
                "P000-B12",
                "P000-B09",
                "P000-B04",
                "P000-B08",
                "P000-B10",
                "P000-B05",
                "P000-B11"
              ],
              "claim_sources": []
            },
            "support_anchor_ids": [
              "P000-B06",
              "P000-B12",
              "P000-B09",
              "P000-B04",
              "P000-B08",
              "P000-B10",
              "P000-B05",
              "P000-B11"
            ],
            "sources_supporting_claim": [],
            "scores": {
              "grounding_0_100": 55.78,
              "custody_0_100": 35.73,
              "credibility_0_100": 0.0,
              "corroboration_0_100": 0.0,
              "confidence_0_100": 60.7,
              "clarity_0_100": 31.35,
              "evidence_weight_0_100": 21.87,
              "evidence_support_0_1": 0.0686,
              "belief_0_100": 0.05,
              "credibility_raw_0_100": 0.0,
              "corroboration_raw_0_100": 0.0
            },
            "score_details": {
              "grounding": {
                "evidence_anchor_count": 8,
                "evidence_citation_count": 0,
                "evidence_source_count": 0,
                "anchor_coverage": 1.0,
                "nondup": 0.8515,
                "evidence_marker_strength": 0.6875,
                "citation_coverage": 0.0,
                "source_coverage": 0.0
              },
              "custody": {
                "provenance": 0.2,
                "integrity": 0.0,
                "time_anchors": 1.0,
                "artifact_identifiers": 1.0,
                "versioning": 0.5
              },
              "credibility": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_type_counts": {},
                "quality_mean": 0.0,
                "quality_top": 0.0,
                "source_diversity": 0.0,
                "domain_independence": 0.0,
                "single_source_penalty": 1.0,
                "has_high_cred_source": 0,
                "high_cred_threshold": 0.9,
                "claim_coverage_factor": 0.0,
                "credible_claims_count": 0,
                "claims_total": 6,
                "credible_claims_ratio": 0.0,
                "credible_claims_weighted_ratio": 0.0
              },
              "corroboration": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_weight_sum": 0.0,
                "excluded_source_ids": [],
                "source_type_counts": {},
                "source_count": 0,
                "unique_domain_count": 0,
                "domain_independence": 0.0,
                "multi_source_factor": 0.0,
                "domain_independence_effective": 0.0,
                "source_quantity": 0.0,
                "source_domain_independence": 0.0,
                "modality_diversity": 0.0,
                "modality_diversity_effective": 0.0,
                "crosscheck_language": 0.0,
                "crosscheck_language_effective": 0.0,
                "corroboration_method": 0.0,
                "claim_coverage_factor": 0.0,
                "corroborated_claims_count": 0,
                "claims_total": 6,
                "corroborated_claims_ratio": 0.0,
                "corroborated_claims_weighted_ratio": 0.0
              },
              "confidence": {
                "explicitness": 1.0,
                "uncertainty_transparency": 0.75,
                "stated_confidence": 0.9,
                "calibration": 0.1731
              },
              "clarity": {
                "act_specificity": 0.5667,
                "actor_specificity": 0.35,
                "link_specificity": 0.2,
                "act_claim": 0.3333,
                "act_evidence": 1.0,
                "actor_claim": 0.0,
                "actor_evidence": 1.0,
                "link_claim": 0.0,
                "link_evidence": 0.5,
                "state_actor_signal": 0.4,
                "organ_path_clarity": 0.39,
                "control_path_clarity": 0.128,
                "due_diligence_path_clarity": 0.0,
                "legal_path_max": 0.39,
                "legal_path_coverage": 0.0,
                "state_claim_flag": 0,
                "state_link_evidence_flag": 1,
                "state_actor_gap_penalty": 0.0,
                "legal_path_gap_penalty": 0.0,
                "questions": {
                  "attribution_clarity": {
                    "question": "Is attribution to State X for attack Z clear?",
                    "score_0_1": 0.3917,
                    "answer": "no"
                  },
                  "responsibility_mode_clarity": {
                    "question": "Is the state-responsibility mode clear (organ, control, or due diligence)?",
                    "score_0_1": 0.39,
                    "answer": "no"
                  },
                  "due_diligence_clarity": {
                    "question": "Is state knowledge plus failure to prevent (due diligence) clear?",
                    "score_0_1": 0.0,
                    "answer": "no"
                  }
                },
                "responsibility_modes": {
                  "conducted_by_state_organs": {
                    "score_0_1": 0.39,
                    "answer": "no"
                  },
                  "non_state_actors_under_state_control": {
                    "score_0_1": 0.128,
                    "answer": "no"
                  },
                  "state_due_diligence_failure": {
                    "score_0_1": 0.0,
                    "answer": "no"
                  }
                }
              },
              "statistical_calibration_v4": {
                "reliability_factor": 0.6242,
                "effective_evidence_n": 4.0,
                "shrinkage_lambda": {
                  "custody": 0.6452,
                  "credibility": 0.5882,
                  "corroboration": 0.5556,
                  "clarity": 0.6897,
                  "confidence": 0.6897
                },
                "prior_scores_0_1": {
                  "custody": 0.3875,
                  "credibility": 0.0,
                  "corroboration": 0.0,
                  "clarity": 0.4086,
                  "confidence": 0.6495
                },
                "saturation_factors": {
                  "chain_quantity_score": 1.0,
                  "chain_saturation": 1.0,
                  "corroboration_source_quantity": 0.0,
                  "corroboration_saturation": 0.0,
                  "credibility_quality_gate": 0.0
                }
              }
            },
            "scores_raw_v3": {
              "grounding_0_100": 55.78,
              "custody_0_100": 41.0,
              "credibility_0_100": 0.0,
              "corroboration_0_100": 0.0,
              "confidence_0_100": 67.69,
              "clarity_0_100": 31.17,
              "evidence_weight_0_100": 23.46,
              "evidence_support_0_1": 0.0731,
              "belief_0_100": 0.05,
              "credibility_raw_0_100": 0.0,
              "corroboration_raw_0_100": 0.0
            }
          },
          {
            "claim_id": "C004",
            "allegation_gravity": "medium",
            "required_threshold_0_1": 0.7,
            "gravity_weight": 1.2,
            "claim_statement": {
              "anchor_id": "C004-A001",
              "text": "APT37 relies on compromised websites to host second stage malware. Small websites focused on subjects such as aromatherapy and scuba diving have been leveraged, and were most likely compromised opportunistically and made to host malicious payloads."
            },
            "evidence": {
              "evidence_anchor_ids": [
                "P000-B08",
                "P000-B12",
                "P000-B09",
                "P000-B05",
                "P000-B06",
                "P000-B04",
                "P000-B10",
                "P000-B11"
              ],
              "claim_sources": []
            },
            "support_anchor_ids": [
              "P000-B08",
              "P000-B12",
              "P000-B09",
              "P000-B05",
              "P000-B06",
              "P000-B04",
              "P000-B10",
              "P000-B11"
            ],
            "sources_supporting_claim": [],
            "scores": {
              "grounding_0_100": 57.64,
              "custody_0_100": 35.79,
              "credibility_0_100": 0.0,
              "corroboration_0_100": 0.0,
              "confidence_0_100": 60.81,
              "clarity_0_100": 31.39,
              "evidence_weight_0_100": 22.27,
              "evidence_support_0_1": 0.0699,
              "belief_0_100": 0.05,
              "credibility_raw_0_100": 0.0,
              "corroboration_raw_0_100": 0.0
            },
            "score_details": {
              "grounding": {
                "evidence_anchor_count": 8,
                "evidence_citation_count": 0,
                "evidence_source_count": 0,
                "anchor_coverage": 1.0,
                "nondup": 0.9445,
                "evidence_marker_strength": 0.6875,
                "citation_coverage": 0.0,
                "source_coverage": 0.0
              },
              "custody": {
                "provenance": 0.2,
                "integrity": 0.0,
                "time_anchors": 1.0,
                "artifact_identifiers": 1.0,
                "versioning": 0.5
              },
              "credibility": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_type_counts": {},
                "quality_mean": 0.0,
                "quality_top": 0.0,
                "source_diversity": 0.0,
                "domain_independence": 0.0,
                "single_source_penalty": 1.0,
                "has_high_cred_source": 0,
                "high_cred_threshold": 0.9,
                "claim_coverage_factor": 0.0,
                "credible_claims_count": 0,
                "claims_total": 6,
                "credible_claims_ratio": 0.0,
                "credible_claims_weighted_ratio": 0.0
              },
              "corroboration": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_weight_sum": 0.0,
                "excluded_source_ids": [],
                "source_type_counts": {},
                "source_count": 0,
                "unique_domain_count": 0,
                "domain_independence": 0.0,
                "multi_source_factor": 0.0,
                "domain_independence_effective": 0.0,
                "source_quantity": 0.0,
                "source_domain_independence": 0.0,
                "modality_diversity": 0.0,
                "modality_diversity_effective": 0.0,
                "crosscheck_language": 0.0,
                "crosscheck_language_effective": 0.0,
                "corroboration_method": 0.0,
                "claim_coverage_factor": 0.0,
                "corroborated_claims_count": 0,
                "claims_total": 6,
                "corroborated_claims_ratio": 0.0,
                "corroborated_claims_weighted_ratio": 0.0
              },
              "confidence": {
                "explicitness": 1.0,
                "uncertainty_transparency": 0.75,
                "stated_confidence": 0.9,
                "calibration": 0.1743
              },
              "clarity": {
                "act_specificity": 0.5667,
                "actor_specificity": 0.35,
                "link_specificity": 0.2,
                "act_claim": 0.3333,
                "act_evidence": 1.0,
                "actor_claim": 0.0,
                "actor_evidence": 1.0,
                "link_claim": 0.0,
                "link_evidence": 0.5,
                "state_actor_signal": 0.4,
                "organ_path_clarity": 0.39,
                "control_path_clarity": 0.128,
                "due_diligence_path_clarity": 0.0,
                "legal_path_max": 0.39,
                "legal_path_coverage": 0.0,
                "state_claim_flag": 0,
                "state_link_evidence_flag": 1,
                "state_actor_gap_penalty": 0.0,
                "legal_path_gap_penalty": 0.0,
                "questions": {
                  "attribution_clarity": {
                    "question": "Is attribution to State X for attack Z clear?",
                    "score_0_1": 0.3917,
                    "answer": "no"
                  },
                  "responsibility_mode_clarity": {
                    "question": "Is the state-responsibility mode clear (organ, control, or due diligence)?",
                    "score_0_1": 0.39,
                    "answer": "no"
                  },
                  "due_diligence_clarity": {
                    "question": "Is state knowledge plus failure to prevent (due diligence) clear?",
                    "score_0_1": 0.0,
                    "answer": "no"
                  }
                },
                "responsibility_modes": {
                  "conducted_by_state_organs": {
                    "score_0_1": 0.39,
                    "answer": "no"
                  },
                  "non_state_actors_under_state_control": {
                    "score_0_1": 0.128,
                    "answer": "no"
                  },
                  "state_due_diligence_failure": {
                    "score_0_1": 0.0,
                    "answer": "no"
                  }
                }
              },
              "statistical_calibration_v4": {
                "reliability_factor": 0.6294,
                "effective_evidence_n": 4.0,
                "shrinkage_lambda": {
                  "custody": 0.6452,
                  "credibility": 0.5882,
                  "corroboration": 0.5556,
                  "clarity": 0.6897,
                  "confidence": 0.6897
                },
                "prior_scores_0_1": {
                  "custody": 0.3875,
                  "credibility": 0.0,
                  "corroboration": 0.0,
                  "clarity": 0.4086,
                  "confidence": 0.6495
                },
                "saturation_factors": {
                  "chain_quantity_score": 1.0,
                  "chain_saturation": 1.0,
                  "corroboration_source_quantity": 0.0,
                  "corroboration_saturation": 0.0,
                  "credibility_quality_gate": 0.0
                }
              }
            },
            "scores_raw_v3": {
              "grounding_0_100": 57.64,
              "custody_0_100": 41.0,
              "credibility_0_100": 0.0,
              "corroboration_0_100": 0.0,
              "confidence_0_100": 67.73,
              "clarity_0_100": 31.17,
              "evidence_weight_0_100": 23.83,
              "evidence_support_0_1": 0.0743,
              "belief_0_100": 0.05,
              "credibility_raw_0_100": 0.0,
              "corroboration_raw_0_100": 0.0
            }
          },
          {
            "claim_id": "C005",
            "allegation_gravity": "medium",
            "required_threshold_0_1": 0.7,
            "gravity_weight": 1.2,
            "claim_statement": {
              "anchor_id": "C005-A001",
              "text": "We assess with high confidence that APT37 acts in support of the North Korean government and is primarily based in North Korea. This assessment is based on multiple factors, including APT37's targeting profile, insight into the group's malware development and probable links to a North Korean individual believed to be the developer of several of APT37's proprietary malware families:"
            },
            "evidence": {
              "evidence_anchor_ids": [
                "P000-B10",
                "P000-B04",
                "P000-B03",
                "P000-B12",
                "P000-B09",
                "P000-B05",
                "P000-B08",
                "P000-B07"
              ],
              "claim_sources": []
            },
            "support_anchor_ids": [
              "P000-B10",
              "P000-B04",
              "P000-B03",
              "P000-B12",
              "P000-B09",
              "P000-B05",
              "P000-B08",
              "P000-B07"
            ],
            "sources_supporting_claim": [],
            "scores": {
              "grounding_0_100": 56.52,
              "custody_0_100": 35.75,
              "credibility_0_100": 0.0,
              "corroboration_0_100": 0.0,
              "confidence_0_100": 61.66,
              "clarity_0_100": 44.27,
              "evidence_weight_0_100": 22.03,
              "evidence_support_0_1": 0.0975,
              "belief_0_100": 0.07,
              "credibility_raw_0_100": 0.0,
              "corroboration_raw_0_100": 0.0
            },
            "score_details": {
              "grounding": {
                "evidence_anchor_count": 8,
                "evidence_citation_count": 0,
                "evidence_source_count": 0,
                "anchor_coverage": 1.0,
                "nondup": 0.9043,
                "evidence_marker_strength": 0.6719,
                "citation_coverage": 0.0,
                "source_coverage": 0.0
              },
              "custody": {
                "provenance": 0.2,
                "integrity": 0.0,
                "time_anchors": 1.0,
                "artifact_identifiers": 1.0,
                "versioning": 0.5
              },
              "credibility": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_type_counts": {},
                "quality_mean": 0.0,
                "quality_top": 0.0,
                "source_diversity": 0.0,
                "domain_independence": 0.0,
                "single_source_penalty": 1.0,
                "has_high_cred_source": 0,
                "high_cred_threshold": 0.9,
                "claim_coverage_factor": 0.0,
                "credible_claims_count": 0,
                "claims_total": 6,
                "credible_claims_ratio": 0.0,
                "credible_claims_weighted_ratio": 0.0
              },
              "corroboration": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_weight_sum": 0.0,
                "excluded_source_ids": [],
                "source_type_counts": {},
                "source_count": 0,
                "unique_domain_count": 0,
                "domain_independence": 0.0,
                "multi_source_factor": 0.0,
                "domain_independence_effective": 0.0,
                "source_quantity": 0.0,
                "source_domain_independence": 0.0,
                "modality_diversity": 0.0,
                "modality_diversity_effective": 0.0,
                "crosscheck_language": 0.0,
                "crosscheck_language_effective": 0.0,
                "corroboration_method": 0.0,
                "claim_coverage_factor": 0.0,
                "corroborated_claims_count": 0,
                "claims_total": 6,
                "corroborated_claims_ratio": 0.0,
                "corroborated_claims_weighted_ratio": 0.0
              },
              "confidence": {
                "explicitness": 1.0,
                "uncertainty_transparency": 0.75,
                "stated_confidence": 0.9,
                "calibration": 0.2244
              },
              "clarity": {
                "act_specificity": 0.5667,
                "actor_specificity": 0.7833,
                "link_specificity": 0.3333,
                "act_claim": 0.3333,
                "act_evidence": 1.0,
                "actor_claim": 0.6667,
                "actor_evidence": 1.0,
                "link_claim": 0.0,
                "link_evidence": 0.8333,
                "state_actor_signal": 0.7,
                "organ_path_clarity": 0.55,
                "control_path_clarity": 0.239,
                "due_diligence_path_clarity": 0.0,
                "legal_path_max": 0.55,
                "legal_path_coverage": 0.3333,
                "state_claim_flag": 1,
                "state_link_evidence_flag": 1,
                "state_actor_gap_penalty": 0.0,
                "legal_path_gap_penalty": 0.0,
                "questions": {
                  "attribution_clarity": {
                    "question": "Is attribution to State X for attack Z clear?",
                    "score_0_1": 0.5617,
                    "answer": "partial"
                  },
                  "responsibility_mode_clarity": {
                    "question": "Is the state-responsibility mode clear (organ, control, or due diligence)?",
                    "score_0_1": 0.55,
                    "answer": "partial"
                  },
                  "due_diligence_clarity": {
                    "question": "Is state knowledge plus failure to prevent (due diligence) clear?",
                    "score_0_1": 0.0,
                    "answer": "no"
                  }
                },
                "responsibility_modes": {
                  "conducted_by_state_organs": {
                    "score_0_1": 0.55,
                    "answer": "partial"
                  },
                  "non_state_actors_under_state_control": {
                    "score_0_1": 0.239,
                    "answer": "no"
                  },
                  "state_due_diligence_failure": {
                    "score_0_1": 0.0,
                    "answer": "no"
                  }
                }
              },
              "statistical_calibration_v4": {
                "reliability_factor": 0.6263,
                "effective_evidence_n": 4.0,
                "shrinkage_lambda": {
                  "custody": 0.6452,
                  "credibility": 0.5882,
                  "corroboration": 0.5556,
                  "clarity": 0.6897,
                  "confidence": 0.6897
                },
                "prior_scores_0_1": {
                  "custody": 0.3875,
                  "credibility": 0.0,
                  "corroboration": 0.0,
                  "clarity": 0.4086,
                  "confidence": 0.6495
                },
                "saturation_factors": {
                  "chain_quantity_score": 1.0,
                  "chain_saturation": 1.0,
                  "corroboration_source_quantity": 0.0,
                  "corroboration_saturation": 0.0,
                  "credibility_quality_gate": 0.0
                }
              }
            },
            "scores_raw_v3": {
              "grounding_0_100": 56.52,
              "custody_0_100": 41.0,
              "credibility_0_100": 0.0,
              "corroboration_0_100": 0.0,
              "confidence_0_100": 69.23,
              "clarity_0_100": 52.7,
              "evidence_weight_0_100": 23.6,
              "evidence_support_0_1": 0.1244,
              "belief_0_100": 0.1,
              "credibility_raw_0_100": 0.0,
              "corroboration_raw_0_100": 0.0
            }
          },
          {
            "claim_id": "C006",
            "allegation_gravity": "medium",
            "required_threshold_0_1": 0.7,
            "gravity_weight": 1.2,
            "claim_statement": {
              "anchor_id": "C006-A001",
              "text": "Many of the compromised domains in the command and control infrastructure are linked to South Korean companies. Most of these domains host a fake webpage pertinent to targets. | FE_APT_Backdoor_SHUTTERSPEED"
            },
            "evidence": {
              "evidence_anchor_ids": [
                "P000-B12",
                "P000-B08",
                "P000-B09",
                "P000-B05",
                "P000-B06",
                "P000-B02",
                "P000-B04",
                "P000-B11"
              ],
              "claim_sources": []
            },
            "support_anchor_ids": [
              "P000-B12",
              "P000-B08",
              "P000-B09",
              "P000-B05",
              "P000-B06",
              "P000-B02",
              "P000-B04",
              "P000-B11"
            ],
            "sources_supporting_claim": [],
            "scores": {
              "grounding_0_100": 57.81,
              "custody_0_100": 32.73,
              "credibility_0_100": 0.0,
              "corroboration_0_100": 0.0,
              "confidence_0_100": 49.01,
              "clarity_0_100": 37.56,
              "evidence_weight_0_100": 21.38,
              "evidence_support_0_1": 0.0803,
              "belief_0_100": 0.06,
              "credibility_raw_0_100": 0.0,
              "corroboration_raw_0_100": 0.0
            },
            "score_details": {
              "grounding": {
                "evidence_anchor_count": 8,
                "evidence_citation_count": 0,
                "evidence_source_count": 0,
                "anchor_coverage": 1.0,
                "nondup": 0.9528,
                "evidence_marker_strength": 0.6875,
                "citation_coverage": 0.0,
                "source_coverage": 0.0
              },
              "custody": {
                "provenance": 0.1,
                "integrity": 0.0,
                "time_anchors": 0.8333,
                "artifact_identifiers": 1.0,
                "versioning": 0.5
              },
              "credibility": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_type_counts": {},
                "quality_mean": 0.0,
                "quality_top": 0.0,
                "source_diversity": 0.0,
                "domain_independence": 0.0,
                "single_source_penalty": 1.0,
                "has_high_cred_source": 0,
                "high_cred_threshold": 0.9,
                "claim_coverage_factor": 0.0,
                "credible_claims_count": 0,
                "claims_total": 6,
                "credible_claims_ratio": 0.0,
                "credible_claims_weighted_ratio": 0.0
              },
              "corroboration": {
                "raw_source_count": 0,
                "eligible_source_count": 0,
                "excluded_sources_count": 0,
                "source_weight_sum": 0.0,
                "excluded_source_ids": [],
                "source_type_counts": {},
                "source_count": 0,
                "unique_domain_count": 0,
                "domain_independence": 0.0,
                "multi_source_factor": 0.0,
                "domain_independence_effective": 0.0,
                "source_quantity": 0.0,
                "source_domain_independence": 0.0,
                "modality_diversity": 0.0,
                "modality_diversity_effective": 0.0,
                "crosscheck_language": 0.0,
                "crosscheck_language_effective": 0.0,
                "corroboration_method": 0.0,
                "claim_coverage_factor": 0.0,
                "corroborated_claims_count": 0,
                "claims_total": 6,
                "corroborated_claims_ratio": 0.0,
                "corroborated_claims_weighted_ratio": 0.0
              },
              "confidence": {
                "explicitness": 0.5,
                "uncertainty_transparency": 0.5,
                "stated_confidence": 0.65,
                "calibration": 0.4425
              },
              "clarity": {
                "act_specificity": 0.7833,
                "actor_specificity": 0.35,
                "link_specificity": 0.4,
                "act_claim": 0.6667,
                "act_evidence": 1.0,
                "actor_claim": 0.0,
                "actor_evidence": 1.0,
                "link_claim": 0.3333,
                "link_evidence": 0.5,
                "state_actor_signal": 0.4,
                "organ_path_clarity": 0.515,
                "control_path_clarity": 0.128,
                "due_diligence_path_clarity": 0.0,
                "legal_path_max": 0.515,
                "legal_path_coverage": 0.0,
                "state_claim_flag": 0,
                "state_link_evidence_flag": 1,
                "state_actor_gap_penalty": 0.0,
                "legal_path_gap_penalty": 0.0,
                "questions": {
                  "attribution_clarity": {
                    "question": "Is attribution to State X for attack Z clear?",
                    "score_0_1": 0.5383,
                    "answer": "partial"
                  },
                  "responsibility_mode_clarity": {
                    "question": "Is the state-responsibility mode clear (organ, control, or due diligence)?",
                    "score_0_1": 0.515,
                    "answer": "partial"
                  },
                  "due_diligence_clarity": {
                    "question": "Is state knowledge plus failure to prevent (due diligence) clear?",
                    "score_0_1": 0.0,
                    "answer": "no"
                  }
                },
                "responsibility_modes": {
                  "conducted_by_state_organs": {
                    "score_0_1": 0.515,
                    "answer": "partial"
                  },
                  "non_state_actors_under_state_control": {
                    "score_0_1": 0.128,
                    "answer": "no"
                  },
                  "state_due_diligence_failure": {
                    "score_0_1": 0.0,
                    "answer": "no"
                  }
                }
              },
              "statistical_calibration_v4": {
                "reliability_factor": 0.6192,
                "effective_evidence_n": 4.0,
                "shrinkage_lambda": {
                  "custody": 0.6452,
                  "credibility": 0.5882,
                  "corroboration": 0.5556,
                  "clarity": 0.6897,
                  "confidence": 0.6897
                },
                "prior_scores_0_1": {
                  "custody": 0.3875,
                  "credibility": 0.0,
                  "corroboration": 0.0,
                  "clarity": 0.4086,
                  "confidence": 0.6495
                },
                "saturation_factors": {
                  "chain_quantity_score": 1.0,
                  "chain_saturation": 1.0,
                  "corroboration_source_quantity": 0.0,
                  "corroboration_saturation": 0.0,
                  "credibility_quality_gate": 0.0
                }
              }
            },
            "scores_raw_v3": {
              "grounding_0_100": 57.81,
              "custody_0_100": 35.5,
              "credibility_0_100": 0.0,
              "corroboration_0_100": 0.0,
              "confidence_0_100": 48.27,
              "clarity_0_100": 41.63,
              "evidence_weight_0_100": 22.21,
              "evidence_support_0_1": 0.0925,
              "belief_0_100": 0.07,
              "credibility_raw_0_100": 0.0,
              "corroboration_raw_0_100": 0.0
            }
          }
        ],
        "statistical_profile": "reliability_shrinkage_saturation_bootstrap"
      }
    },
    "validation_bundle": {
      "file": {
        "path": "/home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.validation_report.json",
        "exists": true,
        "size_bytes": 3983
      },
      "content": {
        "certification": "PASS",
        "overall_score": 92.09,
        "category_scores": {
          "schema": 100.0,
          "integrity": 100.0,
          "tables": 92.0,
          "citations": 100.0,
          "source_years": 100.0,
          "artifacts": 100.0,
          "chain_grounding": 100.0,
          "credibility_grounding": 82.0,
          "corroboration_grounding": 76.0
        },
        "summary_counts": {
          "pages": 1,
          "tables": 3,
          "citations": 0,
          "artifacts": 14,
          "claims": 6,
          "sources": 1
        },
        "hard_failures": [],
        "findings": [
          {
            "severity": "warning",
            "message": "C001: 1/1 support anchors duplicate claim statement text."
          },
          {
            "severity": "warning",
            "message": "C001: corroboration uses little anchor diversity across multiple components."
          },
          {
            "severity": "warning",
            "message": "C001: components missing non-duplicative support anchors: actor_identity, infrastructure_linkage."
          },
          {
            "severity": "warning",
            "message": "C002: 1/1 support anchors duplicate claim statement text."
          },
          {
            "severity": "warning",
            "message": "C002: corroboration uses little anchor diversity across multiple components."
          },
          {
            "severity": "warning",
            "message": "C002: components missing non-duplicative support anchors: command_and_control, timeline_linkage, victimology."
          },
          {
            "severity": "warning",
            "message": "C003: 1/1 support anchors duplicate claim statement text."
          },
          {
            "severity": "warning",
            "message": "C003: corroboration uses little anchor diversity across multiple components."
          },
          {
            "severity": "warning",
            "message": "C003: components missing non-duplicative support anchors: command_and_control, timeline_linkage, victimology."
          },
          {
            "severity": "warning",
            "message": "C004: 1/1 support anchors duplicate claim statement text."
          },
          {
            "severity": "warning",
            "message": "C004: corroboration uses little anchor diversity across multiple components."
          },
          {
            "severity": "warning",
            "message": "C004: components missing non-duplicative support anchors: command_and_control, timeline_linkage, victimology."
          },
          {
            "severity": "warning",
            "message": "C005: 1/1 support anchors duplicate claim statement text."
          },
          {
            "severity": "warning",
            "message": "C005: corroboration uses little anchor diversity across multiple components."
          },
          {
            "severity": "warning",
            "message": "C005: components missing non-duplicative support anchors: actor_identity, infrastructure_linkage."
          },
          {
            "severity": "warning",
            "message": "C006: 1/1 support anchors duplicate claim statement text."
          },
          {
            "severity": "warning",
            "message": "C006: corroboration uses little anchor diversity across multiple components."
          },
          {
            "severity": "warning",
            "message": "C006: components missing non-duplicative support anchors: command_and_control, timeline_linkage, victimology."
          }
        ]
      }
    },
    "runtime_libraries": {
      "python_version": "3.10.12",
      "platform": "Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35",
      "python_packages": {
        "openai": "2.9.0",
        "pypdf": "6.0.0",
        "pymupdf4llm": "0.2.9",
        "pymupdf": "1.26.7",
        "plotly": "5.18.0"
      },
      "viewer_node_packages": {
        "name": "threec-electron-viewer",
        "version": "0.1.0",
        "dependencies": {
          "update-electron-app": "^3.1.1"
        },
        "devDependencies": {
          "electron": "^31.7.7",
          "electron-builder": "^26.0.12"
        }
      }
    },
    "pipeline_methods": {
      "pdf_to_markdown_primary": "process_pdf_mistral_ocr.py (Mistral OCR/provider-backed conversion)",
      "pdf_to_markdown_fallback": "offline fallback via PyMuPDF4LLM when provider conversion fails or times out",
      "table_and_image_extraction": "stage1 markdown parse emits tables and figures/images with anchors",
      "artifact_extraction": "schema extraction stage emits artifact indices from text/tables/images",
      "reference_parsing": "citations and footnote-like references are parsed and linked to source registry",
      "institution_inference": "infer_source_institutions.py using gpt-5-mini (+ optional web fallback)"
    },
    "qa_protocol": {
      "agent_review_enabled": true,
      "human_sample_fraction": 0.1,
      "human_sample_observed_error_rate": 0.0,
      "note": "Human review is targeted and sampled; results reported for reviewed sample."
    },
    "methodology_reference_excerpt": "# Methodology\n\n## I. Methodological Position\n\nThis study treats cyber-attribution reporting as an evidentiary exercise rather than a narrative exercise. The central claim is that attribution assessments should be evaluated as structured arguments about State responsibility, not as standalone assertions of confidence. In consequence, the method asks, for each proposition advanced in a report, what evidentiary materials are relied upon, how those materials are connected to the proposition, and whether those connections can bear weight under adversarial scrutiny.\n\nThe analytical posture is therefore jurisprudential. It draws from recurring ICJ evidentiary practice: differential weighting of heterogeneous materials, caution toward single-origin or litigation-shaped records, and preference for convergent indications over repetition. The objective is not to replicate adjudication procedurally, but to translate judicially legible evidentiary logic into a reproducible scoring framework for cyber-attribution dossiers.\n\n## II. Corpus, Record Formation, and Constraints\n\nThe corpus consists of cybersecurity attribution reports in PDF form. Each report is transcribed into markdown and then transformed into a schema-constrained evidentiary record. The schema is strict by design. It separates claims, sources, artifacts, and evidence links into distinct objects and requires explicit anchors for evidentiary references.\n\nThis design choice has methodological significance. It prevents retrospective reconstruction of support through implicit model inference and compels the system to preserve an inspectable chain between proposition and proof. The method thus prioritizes evidentiary legibility over extraction breadth.\n\n## III. Procedural Architecture\n\nThe workflow proceeds in four phases. First, source documents are transcribed from PDF into markdown. Second, markdown is parsed into structured outputs containing metadata, source inventories, artifact inventories, and claim-level analytic blocks. Third, outputs undergo integrity checks. Fourth, validated outputs are scored under an ICJ-inspired weighting model.\n\nTwo procedural commitments govern all phases. The first is determinacy: where possible, deterministic transformations are preferred, and model-assisted components are constrained by schema and post-run verification. The second is persistence: all major intermediate and terminal artifacts are written to disk, so that any score can be reconstructed and contested from the underlying record.\n\n## IV. Integrity Controls as Admissibility Discipline\n\nNo score is produced absent structural integrity of the evidentiary record. In practical terms, runs are failed where identifier collisions occur, where artifact anchors are missing, where citation pathways cannot be traced, or where references resolve to nonexistent entities. These controls function analogously to admissibility discipline: they do not determine substantive truth, but they determine whether the record is fit to carry a substantive weighing exercise.\n\nThis stage is essential to avoid pseudo-precision. Without strict record integrity, downstream numerical outputs risk expressing parser convenience rather than evidentiary strength.\n\n## V. Evidentiary Weight Model\n\n### A. Item-Level Weight\n\nEach evidence item is evaluated along five bounded dimensions: independence, authentication/provenance, methodological soundness, procedural testing, and contemporaneity. Item-level probative force is computed multiplicatively. The multiplicative form is deliberate: strong performance on one dimension cannot fully compensate for critical weakness on another.\n\n### B. Corroboration and Anti-Circularity\n\nCorroboration is not treated as citation volume. Evidence is clustered by origin, origin-level contribution is aggregated with diminishing returns, and claim-level corroboration is then derived from convergence across origins. This implements an anti-circularity rule: repeated downstream reporting of one upstream source does not become independent support merely by repetition.\n\n### C. Core 3Cs\n\nThe principal outputs are the core 3Cs: Chain of Custody, Credibility, and Clarity. Corroboration is preserved as an explicit sub-calculation and audit surface, but it is integrated into the top-level Credibility axis rather than exposed as a separate top-level C.\n\nChain of Custody is modeled as claim-specific evidentiary handling quality, not as raw artifact quantity. In the current implementation, custody is computed from five normalized variables extracted from evidence text: provenance markers, integrity markers, temporal anchors, artifact identifiers, and versioning/update lineage. These dimensions are weighted and combined linearly, then bounded in `[0,1]`, so that the score remains auditable at claim level and cannot be inflated by a single indicator class.\n\nCredibility is modeled as a composite of source-quality support and corroborative convergence for each claim. The source-quality component is derived from source-type quality, the strongest source attached to the claim, source diversity, and domain independence, with a single-source penalty. Internal/auto sources and newspapers are excluded from credibility support. The model gives maximal weight to international institutions/judicial material and peer-reviewed academic material, intermediate weight to official government and NGO material, and lower weight to think-tank/other material.\n\nIn addition, credibility is calibrated at the document level by a weighted claim-coverage factor keyed to high-credibility source support. Let `Cred_raw_i` denote raw credibility for claim `i`, and let `w_i` denote claim gravity weight. Define a credibility-covered claim as one with at least one high-credibility source (`source_quality >= 0.90`). Then:\n\n`credibility_coverage_factor = ( w_i over credibility-covered claims) / ( w_i over all claims)`\n\nand:\n\n`Cred_i = Cred_raw_i  credibility_coverage_factor`\n\nThis ensures credibility is interpreted as weighted coverage across the claim set rather than isolated source quality in a small subset of claims.\n\nCorroboration is modeled as convergence constrained by support relevance. A claim with broad wording but narrow evidentiary support receives lower corroborative strength, even where artifact volume is high. In the current presentation model, corroboration is retained as a dedicated subscore and then merged into top-level credibility:\n\n`Credibility_top = 0.50  Credibility_source_quality + 0.50  Corroboration`\n\nClarity is modeled as legal attribution intelligibility. Operationally, it answers two linked questions: (i) whether attribution of attack `Z` to State `X` is clearly reasoned in the text, and (ii) whether the mode of responsibility is clear under state-responsibility doctrine. The scorer therefore checks not only actactorlink specificity, but also whether the report clearly indicates one of three legal pathways: attribution through state organs, attribution through non-state actors operating under state direction/control, or state omission/failure of due diligence (knowledge plus failure to prevent within jurisdiction).\n\nIn practice, the clarity panel and score details expose explicit question-level outputs:\n\n- Is attribution to State X given attack Z clear?\n- Was the states responsibility pathway clear: direct conduct by official organs, control/direction of non-state operators, or omission/due diligence failure in its territory?\n- Is it clear that the state knew of the activity and failed to prevent, investigate, or suppress it (due diligence)?\n\nIn addition, corroboration is calibrated at the document level by a claim-coverage factor so that corroboration is interpreted as proportionate coverage across the claim set, not absolute citation mass in isolated claims. Let `C_raw_i` denote raw corroboration for claim `i`, and let `w_i` denote the claim gravity weight. Define a corroborated claim as one for which `C_raw_i > 0`. Th..."
  },
  "sections": [
    {
      "id": "introduction",
      "title": "Introduction and Epistemic Framing",
      "html": "<section id=\"introduction\"><h2>Introduction and Epistemic Framing</h2><p>This methodology is presented as an evidentiary framework for cyber attribution where findings are contestable and subject to adversarial scrutiny. Its epistemic posture is jurisprudential: the framework treats attribution claims as premises in structured evidentiary arguments rather than as standalone assertions of confidence. This burden-sensitive orientation requires that analysts make explicit which materials support each proposition, how those materials are connected to the proposition, and what inferential steps are required before legal responsibility might be imputed. The approach therefore separates descriptive extraction from normative legal inference so that upstream uncertainties remain visible to downstream adjudication.</p></section><section id=\"scope_units\"><h2>Scope and Units of Analysis</h2><p>The primary unit of analysis is the evidentiary record derived from a single attribution report, identified in the raw dataset by path and identifier values. For the present implementation these identifiers include the transcribed source path and report identifier reported in the input metadata. Each report is decomposed into constituent claims, discrete artifacts, and explicit source references. Claims are weighted according to declared gravity and evidentiary anchoring; artifacts are cataloged with provenance markers; and source references are registered to permit later institution inference. Aggregation operates only after claim-level scoring to preserve localized uncertainty and to avoid conflating disparate inferential steps.</p></section><section id=\"data_ingestion\"><h2>Data Ingestion and Record Formation</h2><p>Documents are ingested from a corpus of PDF reports and associated metadata records. The pipeline records ingestion provenance and timestamps from the input metadata to ensure persistence and auditability; examples of such metadata include the internal report path and the generated-at timestamp. Deterministic transformations are preferred where possible to reduce parser-induced variance. Ingestion produces an immutable raw-markdown transcription that is the authoritative input for downstream structural parsing and integrity checks.</p></section><section id=\"pdf_to_markdown\"><h2>PDF-to-Markdown Conversion</h2><p>The conversion stage employs a primary provider-backed OCR/transcription method with an explicitly declared fallback process. Primary and fallback methods are recorded in the pipeline metadata to ensure reproducibility of any particular run. During conversion, page-level anchors for tables, figures, and footnotes are emitted into the markdown so that later artifact extraction can reference concrete document offsets rather than paraphrased summaries. This design supports burden-sensitive review because it preserves the original textual context required for legal argumentation.</p></section><section id=\"structural_parsing\"><h2>Structural Parsing and Schema Enforcement</h2><p>Transcribed markdown is parsed into a schema-constrained evidentiary record separating claims, sources, artifacts, and evidentiary links. The schema enforces explicit anchors for each reference and disallows implicit support chains. Structural integrity checks are applied at this stage and runs are failed where anchors are missing, identifier collisions occur, or citation pathways cannot be traced. These admissibility-style controls prevent pseudo-precision by ensuring that numerical outputs reflect inspectable linkages rather than parser convenience.</p></section><section id=\"artifact_extraction\"><h2>Artifact Extraction</h2><p>Artifact extraction indexes items referenced in text, tables, and images into an artifact register. Each artifact entry records provenance markers, temporal anchors, artifact identifiers, and any versioning information recoverable from the record. The extraction method is deliberately conservative: only artifacts explicitly anchored to the source document are admitted to the register. This constraint preserves the evidentiary distinction between what is asserted and what is demonstrably present in the corpus.</p></section><section id=\"reference_parsing\"><h2>Reference Parsing</h2><p>Citation and footnote-like references are parsed into a source registry that preserves original citation text and resolved identifiers when available. The registry records source type categories and a linkage to the artifact indices so that later scoring computations can disaggregate independent origin contributions from downstream repetition. Reference parsing therefore operationalizes the anti-circularity principle by making origin relationships explicit and machine-inspectable.</p></section><section id=\"institution_inference\"><h2>Institution Inference</h2><p>Institution inference maps parsed references to canonical institutional identities using a named-institution resolution process recorded in the pipeline metadata. The method privileges deterministic resolution paths, with an explicit optional web-fallback recorded when lexical matching is insufficient. Inferred institutions are assigned provisional quality markers to reflect uncertainty in resolution; those markers feed directly into the later credibility calculations so that source-identity ambiguity is reflected as diminished probative force rather than silently normalized away.</p></section><section id=\"claim_evidence_graph\"><h2>ClaimEvidence Graph Construction</h2><p>Claims and their supporting items are represented as a directed claimevidence graph in which nodes denote claims, artifacts, and sources, and edges encode explicit anchor relationships. This graph preserves the chain of evidentiary dependence and enables automated queries about independence, redundancy, and provenance. The graph model underwrites burden-sensitive adjudication because it makes explicit what must be proved for a claim to carry weight and which inferential steps depend on contested links.</p></section><section id=\"scoring_overview\"><h2>Scoring Framework Overview</h2><p>Scoring is performed at the item and claim levels under an ICJ-inspired weighting model recorded in the methodological reference. Item-level evaluations consider bounded dimensions such as independence, authentication/provenance, methodological soundness, procedural testing, and contemporaneity. Item scores combine multiplicatively to ensure that severe deficiencies in any single dimension constrain overall probative force. Claim-level outputs aggregate item-level scores with attention to origin clustering and diminishing returns to guard against artificial inflation by repetitive downstream reporting.</p></section><section id=\"chain_of_custody\"><h2>Chain of Custody Axis</h2><p>The Chain of Custody axis operationalizes evidentiary handling quality for each claim. It is computed from normalized variables extracted from the text: provenance markers, integrity markers, temporal anchors, artifact identifiers, and version/update lineage. These variables are combined in a bounded, auditable function so that custody is interpretable as claim-specific handling quality rather than raw artifact quantity, and so that custody can be independently assessed during adversarial review.</p></section><section id=\"credibility_corroboration\"><h2>Credibility and Corroboration</h2><p>Credibility integrates source-quality assessment and corroborative convergence. Source quality weights are applied according to specified taxonomies that privilege judicial and peer-reviewed materials while penalizing single-source dependence and self-reporting. Corroboration is modeled as constrained convergence: claims receive corroborative strength only from materially independent origins and only to the extent the supporting items are substantively relevant to the claim wording. Corroboration is retained as an auditable subscore and then merged into the top-level credibility axis using prespecified weights to preserve interpretability.</p></section><section id=\"clarity_axis\"><h2>Clarity Axis</h2><p>Clarity measures legal intelligibility: whether the report sets out a clear actactorlink specification and whether it maps that specification onto recognized modes of state responsibility. The clarity assessment explicitly queries whether attribution reasoning corresponds to direct state conduct, control/direction over non-state actors, or omission/due-diligence failure. Because clarity bears on legal utility rather than technical plausibility alone, it is computed separately",
      "model": "gpt-5-mini"
    },
    {
      "id": "scope_units",
      "title": "Scope, Units of Analysis, and Output Semantics",
      "html": "<h2>Introduction</h2>\n<p>This methodological chapter sets out the units of analysis and the interpretive semantics that govern an attribution scoring pipeline for cyber-incidents. It explains, in legal-academic register, how discrete objects extracted from a document are normalized, related, and scored; and it clarifies what each output score is intended to indicate and expressly not to indicate. The exposition is grounded in the supplied raw data excerpt and pipeline metadata, which serve as exemplar inputs for the described procedures rather than as case-specific findings. References to counts and example metric values that appear in the excerpt are used only to illustrate method behavior and calibration, not to substantiate or repeat substantive allegations contained in any specific report.</p>\n\n<h2>Data processing and extraction</h2>\n<p>Ingestion begins with a document and attendant metadata. The supplied excerpt demonstrates standard metadata fields (title, authoring entity, publication date and its anchor, version, and input format) and a filesystem source locator. The first stage performs content normalization and provenance capture: the document-level metadata are extracted verbatim and any inferred anchors (for example a publication date derived from a filename heuristic) are recorded with an explicit provenance tag. Structural parsing then segments the text into machine-readable units; in the exemplar dataset the pipeline recorded one sampled page, six sampled claims, one sampled source, and four sampled artifacts. Each segment is assigned a stable identifier and a custody record that records the transformation history (for example: original file  markdown conversion  structural parse). Artifact extraction isolates technical items such as binaries, network indicators, tables, and figures and associates them with the containing document and the structural location where they appeared.</p>\n\n<h2>References and institution inference</h2>\n<p>Reference parsing treats in-text citations, bibliographic entries, and implicit source mentions as structured source objects. The pipeline stores source-level metadata (such as a vendor report label and file path) separately from extracted claims. Where explicit affiliations or authoring entities are present in document metadata, those values are recorded; where affiliation must be inferred (for example from a filename or a byline line that lacks formal affiliation), the inference is recorded with a confidence flag and the heuristic used. The supplied raw_metadata shows an authoring entity field and an inferred publication-date anchor; the methodology requires that such inferences remain auditable by preserving the extraction method and verbatim anchor text. Institution inference therefore proceeds only on the basis of explicit metadata or on transparent heuristics, and always produces structured output that includes the provenance and a cautionary note that institutional association is an interpretive step rather than an evidentiary conclusion.</p>\n\n<h2>Scoring framework: units and semantics</h2>\n<p>Units of analysis are defined as follows. A claim is a discrete propositional statement extracted from the text whose truth or probative force the pipeline seeks to characterize. A source is an identifiable origin of information (for example a vendor report, a research article, or a dataset) and is represented separately from claims it contains. An artifact is a physical or digital object described in the document (for example a file hash, binary, network indicator, table, or figure) that may function as an evidence item. An evidence item is a parsed, named unit created from an artifact or a quoted passage that can be linked to one or more claims. Document-level denotes metrics and metadata that apply to the entire document as opposed to individual claims or artifacts.</p>\n<p>The scoring model produces multiple axes intended to describe different epistemic properties rather than to deliver a single definitive judgment. A grounding score quantifies the degree to which a claim is anchored in locally supplied evidence items; a custody score records the completeness and traceability of artifacts and transformation steps; a credibility axis expresses the assessed reliability of a source independent of the claims internal grounding; a corroboration metric captures whether independent sources provide concordant support; a clarity axis measures the semantic preciseness and extractability of the claim; and a belief or confidence score synthesizes these axes into a calibrated epistemic judgment. These outputs are probabilistic or ordinal summaries and must be read as indicators of evidentiary weight and provenance quality, not as legal determinations or final attribution verdicts. For example, a low credibility score does not by itself prove falsity; a high grounding score does not adjudicate motives or organizational responsibility. In the exemplar document_scores_v4, numeric summaries such as an average grounding value and a custody average are illustrative of how the pipeline aggregates per-claim and per-document measures for calibration and quality monitoring.</p>\n\n<h2>Validation and quality assurance</h2>\n<p>Validation comprises automated consistency checks, manual review, and bootstrap estimation of metric stability. Automated checks assert internal invariants (for example that every evidence item references a document-level custody record and that inferred metadata contain provenance tags). Manual adjudication samples parsed claims and artifacts against their source text to measure extraction precision and recall; the pipelines provided preview of claim-level scores is used to select stratified samples across the score distribution. Statistical resamplingillustrated in the raw output by the provided bootstrap confidence intervalsis used to estimate the stability of aggregated metrics and to detect overfitting to specific parsing idiosyncrasies. Quality assurance also imposes governance constraints: all inferences about institution or authorship are logged with the heuristic used, and outputs intended for downstream decision-making are accompanied by explicit caveats about what the scores do not establish (for example that they do not constitute legal proof of attribution). Periodic recalibration of the scoring functions is required when the composition of the input corpus changes; the exemplar pipeline_counts and claim_score_preview_v4 provide the kinds of summary statistics used to trigger such recalibrations.</p>\n\n<p>Taken together, these procedures create an auditable pipeline that separates extraction, inference, and scoring, preserves provenance at every transformation step, and frames scores as structured epistemic assessments rather than categorical assertions. Where specific numeric values from the provided excerpt are cited, they serve to illustrate aggregation and validation behaviors and are not invoked as factual findings about any actor or incident.</p>",
      "model": "gpt-5-mini"
    },
    {
      "id": "data_ingestion",
      "title": "Data Ingestion and Corpus Handling",
      "html": "<h1>Methodology: Data Ingestion and Corpus Handling for Cyber-Attribution Scoring</h1>\n<p>Introduction. This methodology describes the deterministic processes and reproducibility guarantees applied to the input corpus from raw PDF reports through intermediate artifacts that feed downstream attribution scoring. It is written to articulate the rationale and operational controls used to transform an input corpus into structured artifacts suitable for automated extraction, institutional inference, and quantitative scoring. The exposition emphasizes process invariants, artifact provenance, and verifiable metadata while avoiding any adjudication of the substantive claims contained in source documents.</p>\n\n<h2>Scope and Units of Analysis</h2>\n<p>The input corpus for this pipeline is one or more publisher artifacts in portable document format, together with derived machine-readable representations. Every unit of analysis is a discrete report file instantiated on disk and recorded in the system registry. For the file set treated here, the canonical report identifier and its record in the processing registry are preserved as immutable metadata elements. The working units include the original PDF, a primary JSON report manifest, a structured text extraction JSON, a validation report, and versioned scoring outputs; these units form the minimal reproducible bundle for any subsequent audit or reprocessing.</p>\n\n<h2>Data Ingestion</h2>\n<p>Report ingestion begins with the intake of the original PDF identified in the processing registry. The pipeline records absolute file paths, file existence flags, and size-in-bytes as first-order provenance properties. The storage paths used in the project context are recorded as persistent artifact names, for example the canonical PDF path and manifest paths. Deterministic file handling is achieved through strict naming conventions for intermediate artifacts, controlled directory layout, and immutably recorded artifact metadata. For each ingested file the pipeline emits a processing manifest that records the original path, a processing timestamp, the environment snapshot, and the set of produced artifacts. This manifest supports reproducibility by providing a complete mapping from an input corpus item to a set of downstream artifacts without reliance on external state.</p>\n\n<h2>PDF-to-Markdown and Text Extraction</h2>\n<p>The textual conversion stage is implemented with a primary provider-backed OCR and conversion routine and a deterministic offline fallback. The primary conversion is executed by a provider-backed script that produces a stage-one markdown artifact; when provider conversion fails the offline fallback executes via an alternative converter with equivalent deterministic settings. All conversion stages are parameterized with fixed options and are executed in a predictable order so that a given input PDF and a fixed runtime environment will yield the same intermediate markdown output. Tables and figures are extracted into anchored representations within the markdown during this stage, and the extraction stage emits a structured output JSON that indexes tables, figure anchors, and block-level text segments to enable later deterministic parsing.</p>\n\n<h2>Structural Parsing and Artifact Extraction</h2>\n<p>Structural parsing transforms the stage-one markdown and extraction JSON into typed artifact indices. The extraction schema defines artifact classes such as text span, table, figure, and code-like blocks; each artifact is assigned a stable identifier derived from the source file identity and a positional ordinal. Artifact indices are materialized into a machine-readable JSON that includes artifact offsets, anchor tokens, and contextual snippets. Deterministic behavior is enforced by using canonical tokenization rules, fixed header parsing precedence, and a documented block classification order. Artifact extraction additionally produces an artifacts registry that can be audited to trace every extracted element back to a location in the original PDF and the stage-one markdown.</p>\n\n<h2>Reference Parsing</h2>\n<p>References, citations, and footnote-like constructs are parsed using a rule-based parser that links reference tokens to a cross-document source registry. The parser emits a mapping from in-text citation anchors to resolved reference entries in the registry, and it records uncertainty metrics for ambiguous matches. Reference parsing is deterministic in that the same inputs, parser configuration, and environment yield the same linkage decisions. All reference mappings and their confidence metadata are persisted in the extraction artifacts so that downstream processes can re-evaluate or override automatic linkages in follow-up analyses.</p>\n\n<h2>Institution Inference</h2>\n<p>Institution inference is implemented as a staged process that first applies deterministic heuristics and gazetteer lookups and then augments those results with a supervised inference model when heuristic resolution is insufficient. The model-based step is performed with a fixed model version and parameters; optional external web lookups are recorded as separate ephemeral annotations so that they may be disabled to preserve strict determinism where required. The institution inference stage produces provenance annotations that reference the originating artifact identifiers and the precise textual evidence used for each inferred institution, enabling reviewers to trace every inferred association back to its textual basis.</p>\n\n<h2>ClaimEvidence Graph Construction</h2>\n<p>Extracted artifacts and parsed references are integrated into a claimevidence graph that encodes provenance edges, textual support spans, and cross-references. Each node in the graph corresponds to either a claim tokenization from the text or to an evidence artifact such as a table cell or figure caption. The graph construction uses fixed graph schema and deterministic ordering rules; node and edge identifiers are stable functions of the artifact registry and the token offsets. This graph provides the canonical linkage layer for scoring and allows reproducibility of any claim-level decision by replaying the deterministic graph construction process from the archived intermediate artifacts.</p>\n\n<h2>Scoring Overview</h2>\n<p>Scoring is performed against the claimevidence graph using a versioned scoring engine. All scoring runs are recorded as distinct output artifacts, and version identifiers are embedded in artifact filenames to preserve the chain of custody between scoring logic and scored outputs. Versioned scoring outputs are persisted alongside the extraction artifacts so that any score can be traced back to the exact extraction and inference inputs used to compute it. Metrics and confidence values are stored as structured JSON that references the exact graph node and edge identifiers that informed each scored item, enabling deterministic recomputation under the same runtime snapshot.</p>\n\n<h2>Chain of Custody and Provenance</h2>\n<p>Chain-of-custody is enforced by recording file system metadata and cryptographic checksums for the input PDF and all intermediate artifacts. The processing manifest captures environment details including runtime language version, operating platform, and installed package versions. Artifact existence flags and size-in-bytes are recorded as part of the manifest to support rapid integrity checks. The provenance trail therefore comprises the original file path, the transformation history with timestamps, checksum values, and the set of downstream",
      "model": "gpt-5-mini"
    },
    {
      "id": "pdf_to_markdown",
      "title": "PDF-to-Markdown Conversion",
      "html": "<h2>Introduction</h2>\n<p>This methodology chapter sets out the procedural and evidentiary framework used to convert source reports into structured inputs for cyber-attribution scoring. The account that follows is procedural and normative: it explains how original artefacts are treated, how text and tabular structures are extracted, how references and institutional assertions are inferred, and how those outputs feed a transparent scoring procedure. Methodological rationale is prioritized over substantive case findings; the objective is to render the pipeline reproducible and auditable while emphasising safeguards that preserve evidentiary integrity and permit independent validation.</p>\n\n<h2>Data Processing and Extraction</h2>\n<p>The pipeline ingests native report files and emits structured representations that form the basis for subsequent analytic stages. Primary inputs for the project instance described here included a portable document format (PDF) located at /home/pantera/projects/TEIA/annotarium/Reports/Fireeye_rpt_APT37(02-20-2018).pdf (size: 2,920,140 bytes) and an associated markdown output at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.md (size: 22,477 bytes). The broader pipeline produces and archives intermediate JSON artifacts, including report-level JSON, raw extraction outputs, validation metadata and scoring inputs; paths and sizes for these objects are recorded (for example, report JSON at /home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json, 520,983 bytes, and raw extraction at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.output.json, 316,434 bytes). These files serve as the auditable trail for provenance and reproducibility checks.</p>\n\n<h2>PDF-to-Markdown Conversion</h2>\n<p>The conversion of PDF to markdown constitutes a core resilient stage in the pipeline. The primary conversion path uses a provider-backed Mistral OCR process implemented in process_pdf_mistral_ocr.py. This Mistral-based processing is employed to capture textual content, layout cues and basic structural markers rapidly and with provider-accelerated recognition models. When the provider-backed conversion fails or exceeds operational time limits, the pipeline adopts an offline fallback posture: PyMuPDF4LLM is invoked as an alternative renderer and extractor. Presenting the offline fallback as a posture of resilience, rather than a conceptual departure, ensures that the logical model of extractiontransforming visual page elements into a linear, semantically annotated markdown representationremains consistent across modes. Both modes emit a primary stage-one markdown that encodes paragraphs, headings, inline artefact anchors, tables and figures. The pipeline archives both the original PDF and the derived markdown to preserve chain-of-custody and to allow reprocessing should extraction heuristics be revised. The conversion design records metrics about pages, tables and figures (in this instance the pipeline registered 1 page, 3 tables and 4 figures) to illustrate handling modes such as table-preservation fidelity and image anchoring, not to adjudicate content.</p>\n\n<h2>Structural Parsing and Artifact Extraction</h2>\n<p>After markdown synthesis, a structural parsing stage normalises the markdown into discrete units for downstream analysis. The stage identifies tabular structures, embedded figures and inline artefacts and emits an artefact index of extracted indicators and blobs. The artefact extraction schema produces indexable references for text extracts, table cells and images so that individual evidence items can be referenced by identifier throughout the scoring workflow. This separation of concernsdocument-level markdown and item-level artefact indicessupports differential validation, targeted re-extraction and selective manual review while preserving the original markdown as an auditable transformation layer.</p>\n\n<h2>References and Institution Inference</h2>\n<p>References, citations and footnote-like linkages are parsed from the markdown output and linked to a source registry. The reference-parsing component associates citation text with provenance anchors and record identifiers, enabling cross-indexing across documents and external registries. Institution inference is implemented in infer_source_institutions.py and leverages a probabilistic language model (gpt-5-mini) to propose candidate source institutions from the extracted text and contextual artefacts; an optional web fallback can be consulted to corroborate institutional metadata. The inference process maintains explicable provenance: candidate matches are recorded together with confidence markers and the textual segments that triggered the inference so that downstream reviewers can evaluate the degree to which an asserted institutional link depends on explicit textual citation versus inference heuristics.</p>\n\n<h2>ClaimEvidence Graph and Scoring Framework</h2>\n<p>The pipeline organises extracted claims and evidence into an explicit claimevidence graph that maps discrete assertions to their supporting artefacts and to the parsed references. The graph formalism supports traceable scoring because each score component is defined as a function over graph featuressuch as artefact type, reference quality, and cross-source corroborationrather than as an opaque aggregate. Scoring inputs and intermediate score reports are persisted (for example, score input at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.score_input_v3.json, 21,210 bytes; and score reports at /home/pantera/projects/TEIA/annotarium/outputs/scoring/fireeye_rpt_apt37_02_20_2018.icj_score_report.json, 30,870 bytes). The scoring framework distinguishes evidentiary axessuch as attribution plausibility, artefact integrity and corroboration densityand calibrates weights via validation sets rather than arbitrary heuristics. Rationale for weighting choices is preserved in accompanying metadata files so that the scoring process remains auditable and subject to revision.</p>\n\n<h2>Validation and Quality Assurance</h2>\n<p>Quality assurance is operationalised through multiple layers. The pipeline emits a validation report (for this run: /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.validation_report.json, 3,983 bytes) that records extraction success rates, fallback invocations and anomalies in structural parsing. Validation routines compare the provider-backed Mistral output with the offline PyMuPDF4LLM fallback to detect substantive divergences; differences are flagged for human review rather than automatically prioritised, consistent with the design principle that fallback behaviour",
      "model": "gpt-5-mini"
    },
    {
      "id": "structural_parsing",
      "title": "Structural Parsing of Text, Tables, and Figures",
      "html": "<h2>Introduction</h2>\n<p>This methodology chapter sets out the processes and rationale for structural parsing and downstream processing of vendor-origin cyberattribution reports within a reproducible scoring pipeline. The objective is to preserve provenance and enable independent auditability while transforming heterogeneous source artifacts (pages, text blocks, tables, figures, and embedded objects) into structured, machinereadable representations that feed reference inference and scoring modules. The description that follows is procedural and methodological; it explains why certain extraction choices are made and how anchor preservation and artifact linking support chainofcustody and verification, without advancing or interpreting substantive case findings contained in any particular report.</p>\n\n<h2>Data Processing and Extraction</h2>\n<p>The ingestion pipeline begins with acquisition of the original source file and associated auxiliary artifacts. Source acquisition is recorded with file system and checksum metadata and stored alongside an extraction manifest (for example, the pipeline stores references to both the original PDF and derived markdown and JSON extractions). Conversion from a paginated document to a structured intermediate is performed so that textual content, typographic structure, and object boundaries are explicitly represented. Structural parsing produces discrete text blocks (paragraphs and headings), table objects with celllevel coordinates, and figure/image objects with bounding boxes and any embedded captions. When optical character recognition is required, the OCR output is retained together with confidence scores and bounding coordinates to preserve a map between raw pixels and extracted text.</p>\n\n<p>Preserving anchors that reference the original document location for every extracted element is essential for auditability. An anchor is a stable identifier and a locator tuple (e.g., page index, bounding box, object id) that links an extracted item back to the exact position in the source. The pipeline attaches anchor metadata to document metadata fields and every extracted object; an example of such an anchor is a manually noted publication date anchor recorded with an anchor identifier and page location. Anchors enable third parties to reinspect the original artifact at the precise locus of extraction, to resolve ambiguity in context, and to validate that subsequent annotations, annotations revisions, or scoring decisions were applied to the intended fragment of the source. Without anchors, provenance claims are unverifiable because textual snippets and tabular values can be displaced or conflated during iterative processing.</p>\n\n<h2>References and Institution Inference</h2>\n<p>Reference parsing identifies and normalizes citations, named organizations, and institutionally relevant entities through multistage processing. The first stage performs surface extraction of intext citations, footnotes, headers, and contact blocks; the second stage applies normalization heuristics and authority resolution against curated registries and the documents own metadata. Anchor linkage is maintained for every reference so that the textual span that yielded the reference can be compared back to the original context. Institution inference leverages explicit references (for example, organization names appearing in author or source metadata) and implicit signals (such as recurring domains in command and control lists, or recurring contact patterns in artifacts). Rationale for each inference is recorded as structured evidence items with anchors to the source spans used, a categorical description of the signal type (named entity, domain, artifact header), and the transformation rules or external registries applied during normalization. This design permits forensic review of why a given institution label was suggested or scored, and the anchor trace supports contested corrections.</p>\n\n<h2>Scoring Framework</h2>\n<p>The scoring framework consumes structured evidence and applies calibrated rules that weight signal types according to provenance, extraction confidence, and corroboration. Structural elementstext blocks, tables, and figuresare treated distinctly because they vary in evidentiary semantics: narrative text often contains analyst interpretation, tables commonly contain enumerations and structured mappings, and figures may contain visual evidence or captured logs. Anchored evidence items carry both a qualitative tag (the element type) and quantitative attributes (extraction confidence, table cell coordinates, figure OCR confidence). Scores are computed by aggregating weighted signals across anchors, with explicit handling to avoid doublecounting when the same content is present in multiple representations (for example, a table rendered as an image and transcribed as text). The pipeline uses counts of structural elementssuch as the number of tables or figures extractedas operational diagnostics to guide sampling and manual review, not as substantive indicators of attribution. All scoring steps persist the contributing anchor identifiers so that any score can be decomposed back to the specific anchored items that produced it.</p>\n\n<h2>Validation and Quality Assurance</h2>\n<p>Quality assurance is implemented through automated validation checks, manual spot audits, and versioned artifacts. Automated checks include anchor integrity verification (ensuring every extracted item references a valid anchor and coordinate), schema validation of table and figure objects, and crossconsistency tests between different extraction outputs (for example, comparing the markdown extraction to the pagelevel JSON extraction). Manual audit trails use the stored anchors and payload paths to permit human reviewers to open the original artifact at the exact locus of the extraction and to confirm or correct parsed values. All validation activities and corrections are recorded in a change log that includes references to the extraction manifest and the file paths used in processing. The pipeline also outputs machinereadable validation reports and store locations for score artifacts and intermediate files so that independent reprocessing can reproduce the chain of custody. Paths and manifests to primary processing artifacts (for example, raw extraction JSON, markdown conversion, validation reports, and final score outputs) are retained and referenced so that every analytical decision is traceable back to a specific file and anchor. In summary, anchorpreserving structural extraction, explicit reference provenance, and persistent recording of processing artifacts together constitute the core measures that enable reproducible, auditable, and defensible attribution scoring.</p>",
      "model": "gpt-5-mini"
    },
    {
      "id": "artifact_extraction",
      "title": "Artifact Extraction and Technical Object Normalization",
      "html": "<h3>Introduction</h3>\n<p>This methodology chapter describes procedures for extracting artifacts across modalities and for normalizing technical objects to support provenance and custody evaluations within a cyber-attribution scoring pipeline. The presentation follows a structured roadmap that begins with definitions of scope and units of analysis and proceeds through ingestion, conversion, structural parsing, artifact extraction, reference linking, institutional inference, claimevidence graph construction, scoring, and validation. The account is written in legal-academic prose and emphasizes methodological rationale rather than adjudicating specific case findings.</p>\n\n<h3>Scope and Units</h3>\n<p>The unit of analysis is the discrete technical object or reference as it appears in source materials, hereafter termed a technical object. Technical objects include identifiers such as CVE entries, domains, email addresses, and cryptographic hashes, as well as structural elements such as table rows, figure captions, and inline citations. The provided raw metadata indicates artifact_type_counts that include six CVE entries, five domain strings, one email address, and two MD5 hashes; these counts are used to illustrate how modality coverage affects downstream normalization and provenance reasoning without implying any substantive conclusion about network activity or actor attribution.</p>\n\n<h3>Data Ingestion</h3>\n<p>Data ingestion begins with archival retrieval of primary documents and associated artifacts. The ingestion stage enforces file integrity checks, timestamps for receipt, and an initial metadata capture that records the source identifier and any accompanying registry references. This stage is designed to preserve chain-of-custody markers and to annotate modality labels so that downstream processing is aware whether a given technical object originated in main text, a table, an image caption, or a footnote.</p>\n\n<h3>PDF-to-Markdown Conversion</h3>\n<p>Conversion of binary source documents to text seeks to maximize fidelity to the original layout because modality and positional cues are material to provenance assessments. The pipeline_methods show a primary conversion via a provider-backed Mistral OCR wrapper (process_pdf_mistral_ocr.py) with an offline fallback using PyMuPDF4LLM when provider conversion fails or times out. The rationale for a dual-path approach is to reduce loss of modality-specific cues (for example, table delimiters and figure anchors) and to provide an auditable conversion log that records which converter and which configuration produced the markdown output.</p>\n\n<h3>Structural Parsing</h3>\n<p>Structural parsing operates on the markdown output to recover hierarchical constructs such as headings, tables, figures, captions, and footnotes. The pipelines stage1 markdown parse deliberately emits tables and figures with stable anchors to enable later cross-referencing between extracted technical objects and their visual or tabular context. Maintaining these anchors is essential to provenance because the same string token may bear different evidentiary weight depending on whether it appears in the main narrative or within a table footnote.</p>\n\n<h3>Artifact Extraction and Technical Object Normalization</h3>\n<p>Artifact extraction is performed by a schema extraction stage that identifies candidate technical objects across modalities using pattern detection and context heuristics. Recognized types in the raw preview include CVE identifiers (e.g., CVE-2018-4878), domain names (for example, daum.net and fireeye.com), an email address (info@fireeye.com), and MD5 hash values. Normalization translates parsed tokens into canonical representations: CVE entries are normalized to the standard CVE registry form, domain names are lowercased and punycode-decoded where applicable, email addresses are canonicalized following mailbox parsing rules, and cryptographic hashes are validated for length and character class. Normalization also records provenance metadata for each normalized technical object, including the originating file, conversion path, markdown anchor, and byte-offset when available. This combination of normalization plus preservation of modality and positional metadata enables comparative analyses across documents while preserving the chain-of-custody context required for rigorous provenance assessment.</p>\n\n<h3>Reference Parsing</h3>\n<p>Reference parsing links in-text citations and footnote-like references to a source registry. The pipelines reference_parsing stage extracts citation anchors and attempts to reconcile them with recorded sources; where explicit bibliographic identifiers are absent, the parser emits candidate links with confidence scores. The parser records both the matched registry entry and the matching rationale (for example, exact string match versus fuzzy title similarity). These linkages are retained as part of the technical objects provenance record to support later corroboration analyses.</p>\n\n<h3>Institution Inference</h3>\n<p>Institution inference augments explicit source metadata when institutional attributions are necessary for scoring or aggregation. The provided pipeline_methods indicate an infer_source_institutions.py step that leverages a language model (gpt-5-mini) with an optional web fallback. The methodology prescribes conservative heuristics: inferred institutions are accompanied by provenance notes that distinguish explicit self-identification from model-inferred associations and by confidence markers reflecting the inference source. This preserves an auditable trail and avoids conflating model-derived suggestions with primary-source claims.</p>\n\n<h3>ClaimEvidence Graph</h3>\n<p>Extracted and normalized technical objects are organized into a claimevidence graph that represents propositions (claims) and their supporting artifacts. Each graph edge encodes modality, normalized object identifier, source anchor, and a provenance tuple documenting conversion path, parser version, and the inference confidence. The graph design supports queryable provenance chains, enabling assessments that trace a claim back to the precise textual or tabular location where a technical object appeared.</p>\n\n<h3>Scoring Overview</h3>\n<p>Scoring operates along multiple axescustody, credibility, corroboration, clarity, and overall belief weightingand uses normalized technical objects and their provenance records as inputs. Document-level example metrics in the provided document_scores_v4, such as custody_avg_0_100 and grounding_avg_0_100, illustrate how modality coverage and conversion fidelity can materially influence score components. Methodologically, the normalization and provenance metadata are used to weight contributions from artifacts that originate in high-fidelity modalities (for instance, well-structured tables with preserved anchors) more heavily than from low-fidelity OCR outputs with ambiguous anchors.</p>\n\n<h3>Chain of Custody</h3>\n<p>Chain-of-custody procedures embed immutable metadata at ingestion and retention points to record custody transitions. For each technical object the system records the ingest timestamp, the conversion path (including whether the Mistral OCR or the PyMuPDF4LLM fallback was used), and the structural anchor that tied the object to its original location. These records are cryptographically hashed and appended to an audit log to support later validation inquiries and to demonstrate provenance continuity.</p>\n\n<h3>Credibility and Corroboration</h3>\n<p>Credibility assessment separates source-level credibility from artifact-level corroboration. Credibility scoring incorporates explicit citations parsed by the reference_parsing stage, institutional provenance from the institution_inference stage, and contextual signals such as modality and extraction confidence. Corroboration measures compare normalized technical objects across sources and modalities; for example, exact canonical matches across independent source anchors",
      "model": "gpt-5-mini"
    },
    {
      "id": "reference_parsing",
      "title": "Footnote and Reference Parsing",
      "html": "<section><h2>Introduction</h2><p>This methodology chapter describes procedures for transforming rhetorically framed citations and footnote constructs in source documents into an analyzable source graph suitable for cyberattribution scoring. The exposition is methodological and abstains from asserting any casespecific factual conclusions. It defines the purpose of reference parsing within a broader pipeline that ingests documents, extracts artifacts and claims, and produces structured inputs for downstream scoring and validation. The approach is designed to preserve evidentiary provenance, enable reproducible linkage between claims and supporting materials, and to provide clear audit trails for chainofcustody and quality assessment.</p></section><section><h2>Data processing and extraction</h2><p>Ingestion begins with document acquisition and deterministic conversion to a machinereadable representation. The available pipeline metadata indicates primary PDF conversion through a providerbacked OCR pipeline (process_pdf_mistral_ocr.py) with an offline fallback (PyMuPDF4LLM) to guard against provider timeouts. Structural parsing produces pages, tables, figures and embedded artifacts; pipeline counts in the supplied preview show one source with a single page, multiple tables and figures, four extracted artifacts and six textually identified claims. These counts inform resource allocation and sampling strategies for subsequent manual review but are not interpreted as substantive evidence themselves.</p><p>Structured extraction proceeds in stages. The first stage converts visual and typographic elements to a markdownlike intermediary that retains anchors for tables and figures. A subsequent schema extraction stage enumerates artifacts referenced by caption or inline mention and emits indices tying text spans to artifact identifiers. All conversions retain bounding metadata, original page offsets and the conversion method used to facilitate reproducibility and to support later chainofcustody checks.</p></section><section><h2>References and institution inference</h2><p>Reference parsing converts rhetorical citation tokensfootnote markers, parenthetical citations, numbered endnotes and inline source mentionsinto explicit links to registry entries in the source index. The pipeline component described as reference_parsing locates footnote markers in the intermediary representation, extracts adjacent citation text, and attempts canonicalization against the source registry. Canonicalization uses deterministic heuristics (title and year matching, entity name normalization) augmented by an institution_inference stage that applies a language model (infer_source_institutions.py with gpt5mini and optional web fallback) to resolve ambiguous attributions such as institutional authoring statements embedded in front matter or internal sections. The method records confidence scores, the mapping algorithm used, and any fallback evidence (for example, matches to known publication titles or institutional identifiers) to support later review.</p><p>These steps produce a directed bipartite graph between rhetorical citation nodes (the textual locations of footnote or inline citation) and source registry nodes. The graph explicitly records unresolved citation tokens as orphan nodes with provenance metadata so that missing or ambiguous references are visible to analysts rather than silently dropped. The resulting source graph therefore encodes citation linkage at the granularity of text span  source identifier  inferred institution, permitting queries for all claims that reference a given source or all sources cited by a particular claim.</p></section><section><h2>Scoring framework</h2><p>The scoring framework treats parsed references and inferred institutions as evidence attributes that feed credibility, corroboration and clarity axes. Each claim receives a structured evidence vector containing pointers to artifact indices, the set of linked sources from the source graph, and institutional inference metadata with confidence values. Scoring components operate on these vectors to produce intermediate scoreschainofcustody completeness, citation linkage confidence and corroborative corroboration countswhich are then aggregated with calibrated weights to form composite attribution scores. Aggregation calibration is performed on heldout examples and synthetic cases to ensure the system responds predictively to changes in citation density, the presence of primary versus secondary sources, and institution inference confidence.</p><p>Methodological rationale for separating citation linkage from institution inference is to avoid conflating surface textual citation patterns with the provenance of origin: a clear footnote may link to a document whose authorship is disputed; institution inference is therefore maintained as a distinct attribute rather than a derived certainty. Confidence propagation rules are explicit and conservative: low confidence in reference resolution limits the downstream weight of that evidence in composite scoring, and the system records the dependency path from a score back to the underlying reference nodes for auditability.</p></section><section><h2>Validation and quality assurance</h2><p>Validation is multitiered. Automated checks verify that every parsed footnote has been assigned either a resolved source identifier or an explicit unresolved tag; statistics on unresolved citations are tracked as quality metrics. Synthetic and seeded groundtruth documents exercise the reference_parsing and institution_inference components to measure precision and recall of citation linkage and correct institution assignment. Crossvalidation with manual review samples is used to calibrate confidence thresholds and to estimate humanmachine agreement rates. The pipeline records the conversion method (primary OCR or fallback) and uses that metadata to stratify error analyses, since OCR quality materially affects footnote and inline citation recognition.</p><p>Chainofcustody documentation accompanies every output: conversion timestamps, tool identifiers, and reviewer annotations are retained in the provenance record for each claim and each source graph edge. Periodic audits reprocess a random sample of documents endtoend to detect regressions in parsing behavior. Limitations of the approach, including dependence on OCR accuracy, model inference fallibility in institution assignment, and the reduced reliability of citation linkage in poorly formatted or scanned documents, are monitored and surfaced in metadata so that downstream consumers can apply appropriate caution in interpreting attribution scores.</p></section>",
      "model": "gpt-5-mini"
    },
    {
      "id": "institution_inference",
      "title": "Institution Inference and Source Typology",
      "html": "<section><h2>Introduction</h2><p>This methodology chapter defines procedures for institution inference and source typology within a cyber-attribution scoring pipeline. It sets out the conceptual basis for mapping extracted source artifacts to institutional classes, and for operationalizing those classes into quantitative and qualitative effects on downstream credibility weighting and corroboration eligibility. The treatment below is methodological and refrains from applying attributions to any particular incident; it instead describes how institutional signals are detected, classified, and used in scoring while referencing the pipeline artefacts and processing modalities present in the supplied raw data.</p></section><section><h2>Scope and Units of Analysis</h2><p>The primary units of analysis are source objects as emitted by the ingestion and parsing stages of the pipeline. In the provided raw data this is illustrated by a preview record (source_id: SRC0001, title: \"APT1 Executive Summary and Key Findings\", source_type: \"internal_document_section\", entity_name: \"Mandiant\", year: 2013). The approach treats each source object as a bounded evidentiary unit from which institutional signals, reference links, and artifact indices are extracted. Source typology focuses on origin, provenance, and intended audience, and is operationalized at the granularity of source objects and their referenced artifacts rather than at the level of individual tokens.</p></section><section><h2>Data Ingestion</h2><p>Ingestion comprises the reception of primary documents and their registration in a source registry. As indicated in the pipeline metadata, source_type_counts currently reports a single internal_document_section; this count is used illustratively to show how typology calibration begins with available source classes. The registry preserves metadata fields such as title, entity_name, publication_or_venue, year, and source_type to support subsequent institution inference and chain-of-custody records.</p></section><section><h2>PDF-to-Markdown Conversion</h2><p>The pipelines primary conversion step is performed with a provider-backed OCR and conversion routine identified as process_pdf_mistral_ocr.py, with an offline fallback via PyMuPDF4LLM when provider conversion fails or times out. These routines produce machine-readable markdown representations of documents. Methodologically, conversion is assessed against fidelity metrics for textual accuracy and structural preservation; conversion artifacts (e.g., OCR confidence scores, timeout events) are recorded as inputs to credibility weighting.</p></section><section><h2>Structural Parsing</h2><p>Structural parsing decomposes converted markdown into hierarchical elements: sections, headings, paragraphs, tables, and figures. The pipelines stage1 markdown parse emits tables and figures/images with anchors, enabling linkage of in-text citations to visual artifacts. Structural metadatasuch as section identifiers and anchor IDsis stored alongside original source identifiers to maintain traceability.</p></section><section><h2>Artifact Extraction</h2><p>Artifact extraction isolates discrete evidentiary items (for example network indicators, configuration snippets, or labeled figures) and assigns them schema-record indices. The pipelines artifact_extraction stage emits artifact indices that reference both the structural location and the original source object. Each artifact index includes provenance metadata sufficient for later chain-of-custody reconstruction.</p></section><section><h2>Reference Parsing</h2><p>Reference parsing identifies in-text citations, footnotes, and bibliographic references and links them to the source registry. The reference_parsing stage attempts canonicalization and dereferencing where possible; unresolved references are flagged and their resolution status recorded. These linkage edges form the initial graph topology used to propagate institutional signals across the corpus.</p></section><section><h2>Institution Inference</h2><p>Institution inference assigns institutional attributes to source objects using a hybrid model described in the pipeline metadata: infer_source_institutions.py leveraging a purpose-built LLM (gpt-5-mini) with an optional web fallback for external validation. Institutional attributes include publisher identity, organizational type (e.g., vendor, academic, independent researcher, government), and known relationships (e.g., affiliation with a parent organization). The module consumes structured metadata, textual cues from the parsed content, and, when available, external resolving evidence. All inferences are probabilistic and tagged with confidence scores and provenance rationales to support downstream interpretability.</p></section><section><h2>ClaimEvidence Graph Construction</h2><p>Following institution inference, the system constructs a claimevidence graph that links extracted claims and artifacts to source nodes and inferred institutions. Graph edges encode relations such as cites, asserts, and corroborates, and carry provenance metadata (source id, section id, artifact id, and parsing confidence). Institutional nodes are annotated with the inference confidence from the previous stage and with metadata taken from raw ingestion records, enabling both structural and institutional reasoning over claims.</p></section><section><h2>Scoring Overview</h2><p>The scoring framework integrates institutional attributes into a multi-axis credibility model. Institutional class is one axis among others (technical provenance, artifact fidelity, and corroboration count). Scores are computed by combining institution-derived priors with artifact-level quality metrics and graph-based corroboration signals. The framework treats institutional signals as soft priors rather than determinative labels, ensuring that empirical artifact quality can override institutional expectations where warranted.</p></section><section><h2>Chain of Custody</h2><p>Chain-of-custody records capture each transformation from original document ingestion through conversion, parsing, and artifact extraction. Each transformation step logs tool identifiers and versions as present in the runtime_libraries metadata (for example, python_version and key packages such as pypdf, pymupdf, and pymupdf4llm), along with timestamps and operator or automated-process identifiers. These records support reproducibility and enable targeted re-evaluation of institutional inferences when upstream transformations are questioned.</p></section><section><h2>Credibility and Corroboration</h2><p>Credibility weighting uses institution inference to modulate both individual source credibility and corroboration eligibility. Institutional class affects the prior assigned to a source: institutional priors adjust the weight of claims in the aggregation step and determine whether a source can serve as an independent corroborator. Corroboration eligibility rules treat sources within the same institutional family as non-independent unless explicit evidence of independent observation is present. All such eligibility decisions are annotated with the inference confidence from the institution_inference stage.</p></section><section><h2>Clarity Axis</h2><p>A clarity axis assesses how explicitly an institutional claim is articulated in the source text (for example explicit attribution statements, byline metadata, or corporate branding). Clarity scores influence both credibility weighting and the stringency of corroboration rules; higher clarity reduces reliance on external inference while lower clarity raises the evidentiary bar for corroboration.</p></section><section><h2>Aggregation and Calibration</h2><p>Aggregation synthesizes weighted claims across the claimevidence graph. Calibration procedures adjust institutional priors and combination rules using holdout datasets and simulated perturbations in source typologies. Calibration emphasizes robustness to misclassification of institutional type by favoring artifact-level signals when institutional inference confidence is low.</p></section><section><h2>Validation and Quality Assurance</h2><p>Validation includes unit checks of parsing fidelity, reconciliation of reference links, and spot audits of institution inference rationales. Quality-assurance procedures incorporate reproducibility tests using the recorded runtime_libraries and conversion logs, and measure score stability under alternate inference thresholds. All validation outcomes are logged and considered when updating scoring parameters.</p></section><section><h2>Limitations and Governance</h2><p>The methodology recognizes limits inherent to automated institution inference, including dependence on conversion fidelity and the potential for biased training signals in language models. Governance measures require human-in-the-loop review for high-impact decisions and mandate explicit documentation of inference confidence, chain-of-custody records, and any manual overrides. The procedure therefore combines algorithmic institution inference with formal audit trails and governance safeguards",
      "model": "gpt-5-mini"
    },
    {
      "id": "claim_evidence_graph",
      "title": "Claim-Evidence Graph Construction",
      "html": "<h2>Introduction</h2>\n<p>This methodology section defines the construction and governance of a claim-evidence graph used for cyber-attribution scoring. The claim-evidence graph is a directed, typed graph that connects authored claims to discrete evidence items, source descriptors, and technical artifacts with explicit anchor-level pointers. The primary objectives of the graph are to enable machine-actionable traceability from claim to evidence, to support reproducible chain-of-custody assessment, and to enforce anti-circularity constraints so that evidence derived from the evaluated report is not treated as independent corroboration. The description below is procedural and methodological; it uses metadata samples contained in the supplied raw data (for example, identifiers such as claim_id values C001C006, source records such as SRC0001, and artifact records such as ART00001ART00014) to illustrate structure and controls rather than to restate substantive findings.</p>\n\n<h2>Data processing and extraction</h2>\n<p>Input processing proceeds in predictable stages. First, documents and auxiliary data are ingested into a normalized repository that preserves both the original file and an extracted text representation. The supplied bundle records this normalization step with entries under normalized.artifacts and normalized.sources; each artifact record (for example ART00001) carries an artifact_type, value, origin, location tuple (page, block_id) and an extraction confidence score. The extraction pipeline separates tasks conceptually: conversion of binary documents to a text/markdown form; structural parsing into logical blocks (paragraphs, lists, tables); and extraction of technical artifacts (CVE identifiers, domain names, email addresses, cryptographic hashes). The raw_artifacts_preview included with the input (artifact types and example_values such as \"CVE-2018-4878\" and \"daum.net\") is used to calibrate entity-extraction models and to validate that the extractor recognizes the relevant modal classes for subsequent linking.</p>\n<p>Structural parsing yields discrete anchors: stable, addressable block identifiers that become nodes in the claim-evidence graph. For every artifact and for every claim anchor we persist the original location metadata (page, block_id) and the block text snippet. The normalized.artifacts entries in the provided scoring_bundle show the representation used in practice: artifact_id, artifact_type, value, location and extraction confidence. Anchor-level persistence supports fine-grained traceability and enables later human review to re-evaluate extraction correctness without re-processing the entire document.</p>\n\n<h2>References and institution inference</h2>\n<p>Reference parsing and source normalization are treated as first-class processes. Cited items discovered in text are canonicalized into source records and cross-referenced to publisher and author metadata where available. The provided raw_sources_preview contains a representative source entry (SRC0001: Mandiant APT1 Report) which illustrates the normalized source record schema: source_id, source_type, authoring_org, publisher, and publication date. Where explicit bibliographic metadata are absent, reference-disambiguation relies on URL and domain normalization; domain strings present in artifact lists (for example, \"fireeye.com\") are used as candidate organizational signals in institution inference.</p>\n<p>Institution inference applies a conservative, multi-evidence heuristic. Domain ownership, publisher metadata, and in-document claims about provenance are combined to produce an inferred institutional attribution with an associated confidence band. The system records both the inferred institution and the provenance of that inference (for example which artifact_ids and which source records supported the inference). To minimize circularity, any institution inference that relies solely on artifacts extracted from the evaluated report is flagged and treated as report-derived. Those flagged inferences are not permitted to contribute to cross-source corroboration metrics unless validated by an independent source record or external registry lookup.</p>\n\n<h2>Claim-evidence graph construction and traceability</h2>\n<p>The claim-evidence graph is constructed by creating a claim node for each parsed claim (claim_id entries such as C001C006) and linking claim nodes to evidence nodes via explicit relation edges. Evidence nodes correspond to normalized evidence_items (for example E-0001..E-0008 in the normalized bundle) and to atomic artifact nodes that represent extracted technical indicators (ART00001..ART00014). Each evidence node includes modality metadata (for example \"cve\", \"infrastructure\") and feature vectors used for probative weighting (the input contains features labeled I, A, M, P, T in the evidence_items records). Edges are typed to indicate the role of the evidence relative to the claim (for example supports, contextualizes, links_actor). All edges are annotated with anchor pointers (page, block_id) so that every claimevidence relationship can be traced back to the original block in the source document; this anchor-level traceability permits deterministic human verification and preserves evidentiary context.</p>\n<p>To preserve provenance fidelity the system stores an origin_id for every evidence node (for example ORIG:src0001 in the evidence_items) and maintains a mapping from evidence nodes to their source_ids and artifact_ids. During graph construction the pipeline enforces an invariant that every edge must reference at least one anchor and at least one origin. This invariant is foundational for chain-of-custody scoring because it prevents unanchored syntheses from being used as primary evidence and thereby preserves an auditable trail from claim to raw textual anchor.</p>\n\n<h2>Anti-circularity safeguards</h2>\n<p>Anti-circularity mechanisms are explicit and multi-layered. First, origin clustering is used to detect single-origin support: the scoring bundle contains origin_cluster_weights and unique_origin_count fields that the pipeline uses to mark claims with limited source independence. Second, the system distinguishes report-derived evidence from independently obtained artifacts by tracking recovered_reference_count and a report_derived_ratio; any evidence node with a report-derived flag is excluded from multi-source corroboration tallies unless an external reconciliation step validates it. Third, provenance validation uses",
      "model": "gpt-5-mini"
    },
    {
      "id": "scoring_overview",
      "title": "Scoring Framework Overview",
      "html": "<h2>Introduction</h2><p>This methodology section describes the scoring architecture used to translate extraction outputs into reproducible attribution scores. The objective is to articulate a transparent, auditable pipeline that moves from claim-level evidence anchors to document-level synthesis, while maintaining an explicit separation between raw extraction outputs and the inferential weighting that yields final scores. The exposition follows the pipeline manifests present in the supplied data (for example, the scoring_bundle and document_scores_v4 structures) and explains the rationale for each processing stage in general terms rather than restating substantive findings from the source document.</p><h2>Data processing and extraction</h2><p>Primary ingestion begins with a provider-backed PDF conversion to markdown, with an offline fallback, as recorded in the pipeline_methods. Structural parsing produces discrete text blocks, anchors, tables and figure references. From these anchors a schema-driven artifact extraction stage emits typed artifacts (artifact_type enumerations such as vulnerability identifiers, cryptographic hashes, domains and contact addresses) and attaches provenance metadata including extraction confidence and block anchors. Evidence items are then assembled by grouping anchors and artifacts under evidence identifiers; each evidence item records modality tags (for example, technical artifact, dataset or infrastructure), feature vectors (the dataset labels I, A, M, P, T are present in the evidence feature maps) and a computed probative_weight. Those extraction outputs (anchors, artifacts, evidence items and their raw feature scores) are treated as observable inputs to scoring and are retained without inferential interpretation in this stage so that downstream weighting can be independently examined and audited.</p><h2>References and institution inference</h2><p>Reference parsing and source registry linkage are performed after artifact extraction. The pipeline captures a normalized source registry entry for each declared source, including source_kind and authoring organization; the supplied pipeline_methods indicate an institution inference step that uses a model-backed heuristic process. Institution inference operates on the parsed citation metadata and the source registry to produce inferred authoring_org entries and to populate flags relevant to source characterization (for example, indicators of litigation preparation, stated conflicts, or whether a source is single). The distinction between extracted reference tokens and inferred institutional attributes is critical: parsed citations and anchors are deterministic outputs from the structural parser, whereas inferred institutional attributes are model-derived annotations used only in credibility- and corroboration-related axes of scoring and are recorded separately so they may be re-evaluated or replaced without re-running extraction.</p><h2>Scoring framework: claim-level axes to document-level synthesis</h2><p>Scoring is organized as claim-level vectors and a document-level aggregation. At the claim-level each claim is associated with a set of evidence items (evidence_ids) and per-evidence probative measures. The claim-level axes represented in the data include grounding (the extent and coverage of anchors and markers supporting the claim), custody (chain-of-custody and artifact identifier quality), credibility (source quality and independence), corroboration (multi-source and modality convergence), confidence (stated confidence and calibration), and clarity (granularity and specificity of actoractlink relationships). Numerically, these axes appear as per-claim scores (for example grounding_0_100, custody_0_100, clarity_0_100) and as component vectors (the six-c or similar vectors) that feed an evidence_weight or evidence_support term.</p><p>Inferential weighting is explicitly separated from extraction outputs. The pipeline treats extracted indicators (anchors, artifact types and evidence probative_weight) as inputs to a weighting model that implements shrinkage and calibration. The statistical_calibration_v4 block in the supplied data documents mechanisms for reliability adjustment (reliability_factor), effective sample sizing (effective_evidence_n), and shrinkage lambdas applied to axes (shrinkage_lambda for custody, credibility, corroboration, clarity, confidence). Penalty multipliers (for example a single_source penalty factor) are applied to claim-level aggregates to encode known structural risks; these appear as penalty_multiplier and penalties in the claim scoring entries. Claim-level final_score values therefore reflect the composition of evidence weight aggregates, applied penalties, and calibrated shrinkage rather than any additional re-interpretation of extraction artifacts.</p><p>Document-level synthesis aggregates claim-level vectors into summary statistics that are designed to preserve the multi-axis structure. Aggregation methods present in the data include mean and geometric means across claim scores (for example overall_claim_score_mean and overall_claim_score_geometric) and a headline claim selection that propagates a headline_vector to the document-level. Bootstrap-derived uncertainty estimates (bootstrap_95ci) are computed for selected aggregates to communicate sampling and model uncertainty at the document-level. Where gating rules are required (for example seriousness or minimum credibility thresholds) those conditions are evaluated against the aggregated vectors and are recorded in gate structures so that decisions are auditable and reproducible.</p><h2>Validation and quality assurance</h2><p>Quality assurance is performed at multiple points. Extraction QA validates anchor coverage, artifact traceability and anchor-to-claim alignment; chain_provenance_diagnostics fields record context_completeness, integrity_signal, lineage_quality and anchor_quality metrics that support automated checks. Scoring QA validates internal consistency by checking that evidence_weight_aggregate equals the sum of constituent probative",
      "model": "gpt-5-mini"
    },
    {
      "id": "chain_of_custody",
      "title": "Chain of Custody Axis",
      "html": "<h2>Introduction</h2>\n<p>This chapter defines the methodological approach used to evaluate chain-of-custody aspects of evidence that underlie cyberattribution scoring. The presentation is disciplinary and procedural rather than substantive: it explains how provenance, integrity, time anchors, artifact identifiers and versioning are operationalised, how those signals are combined into quantitative custody measures, and how quality controls and penalties are applied when signals are incomplete. Where appropriate, the method refers to diagnostic fields and aggregated metrics drawn from the supplied scoring bundle and document-level summaries to illustrate behaviour of the method, not to adjudicate particular factual claims.</p>\n\n<h2>Data processing and extraction</h2>\n<p>The pipeline begins with structured ingestion of the primary source objects and derived artifacts. Documentlevel metadata and the normalized artifact table serve as primary inputs: artifact rows record artifact_type, value, extraction origin and extraction confidence; evidence items connect anchors to claims and enumerate modalities (for example, \"cve\" or \"infrastructure\"). Structural parsing produces anchor identifiers and blocklevel locations that are preserved as the primary linkage between textual claim statements and supporting artifacts. Extraction confidence and explicit artifact identifiers (for example, hash values and CVE strings) are recorded and propagated through the pipeline so that downstream custody calculations can weight direct, verifiable artifacts more heavily than contextually inferred entities.</p>\n<p>To protect the chain of custody we differentiate three extraction classes. Direct artifacts are those with explicit identifiers and high extraction confidence (artifact entries with confidence=1 and types such as hash_md5 or cve); contextual artifacts are textually proximate mentions that lack a stable identifier; and remote or derived artifacts are external references that require followup retrieval to be validated. The pipeline tags each artifact with provenance metadata indicating the source record (source_id), the extraction method (extracted_from), and the anchor block(s) that justify claim linkage. The scoring inputs include counts and previews of artifacts (for example, counts by artifact_type and example values) to allow automated flagging of weakly identified chains (for example, absence of artifact identifiers or absence of hash values where they would be expected).</p>\n\n<h2>Reference parsing and institution inference</h2>\n<p>Reference parsing separates internal report anchors from external citations. The normalised source table records authoring_org and",
      "model": "gpt-5-mini"
    },
    {
      "id": "credibility_corroboration",
      "title": "Credibility Axis with Corroboration Subcomponent",
      "html": "<section>\n  <h3>Introduction</h3>\n  <p>This methodology describes the Credibility axis with a dedicated Corroboration subcomponent as applied to cyber-attribution scoring. The approach prioritizes transparency of inputs and reproducibility of transformations while avoiding substantive adjudication of contested factual claims. The protocol distinguishes between structural processing steps and inference rules: the former converts raw artifacts and anchors into machine-readable evidence items, and the latter evaluates provenance, source quality and inter-source relationships. The following exposition uses the provided input metadata to illustrate procedural behavior (for example, a single documented source with type classification \"internal_document_section\" and an authoring organization listed as Mandiant), but it intentionally refrains from drawing operational conclusions about specific incidents referenced in the underlying report.</p>\n\n  <h3>Data processing and extraction</h3>\n  <p>Source ingestion begins with normalized metadata and a source type inventory. In the supplied input the source_type_counts map indicates one unit classified as internal_document_section; this triggers an ingestion pathway optimized for publisher-supplied technical reports. Documents are parsed into structural anchors (pages and blocks) and artifacts are extracted with modality labels (for example, CVE identifiers, hashes, domains and email addresses). Each artifact record carries provenance flags and an extraction confidence score; the scoring pipeline uses these values to weight downstream evidence. Evidence items are formed by clustering related anchors and artifacts, assigning modality vectors and computing a probative weight that combines artifact-level confidence with contextual indicators (anchor coverage, marker strength and modality relevance). This pipeline deliberately separates extraction confidence from interpretive credibility so that noisy or machine-extracted artifacts do not automatically inflate attribution measures.</p>\n\n  <h3>References and institution inference</h3>\n  <p>Reference parsing recognizes explicit citations, author lists and publisher fields and synthesizes an institutional identity when available. The normalized source record in the input includes an authoring_org field (\"Mandiant\") which permits an institutional label to be associated with that origin. Institution inference leverages explicit publisher metadata first, then secondary signals such as domain names and email addresses contained among extracted artifacts. The inferred institution, its declared role (publisher, vendor, academic etc.) and any declared conflicts of interest are recorded in a source record. These records form the basis of a source hierarchy: primary-origin sources (first-party technical reports and datasets) are placed above secondary-derivative items (aggregated summaries, citations) in the hierarchy used for credibility scoring. The source hierarchy is therefore explicit, auditable and used to compute scope factors such as claim coverage and the effective independence of corroborating chains.</p>\n\n  <h3>Scoring framework: credibility, corroboration and claim coverage</h3>\n  <p>The Credibility axis is computed from three components: chain-of-custody and provenance fidelity, source-quality and independence, and corroboration-convergence across distinct origins. Chain-of-custody metrics assess artifact identifiers, time anchors and lineage disclosure to produce a custody score. Source-quality and independence combine declared publisher type, domain diversity and the presence of independent corroborating origins into a credibility_independence measure. Independence is operationalized as the lack of shared origin identifiers across supporting sources, penalizing claims that derive from a single origin even if that origin contains multiple anchors.</p>\n  <p>Corroboration is assessed by counting and weighting supporting sources and modalities, then applying diminishing returns via a saturation function. The corroboration metric rewards multi-origin, cross-modality support and explicit cross-language or cross-domain corroboration. Claim coverage scaling maps the fraction of a claim's anchors that are supported by eligible sources into a multiplicative coverage factor: claims with full anchor coverage receive full weight while partially covered claims are down-weighted proportionally. The supplied document scores illustrate the framework's behavior when only one origin is present: credibility and corroboration summary values may be driven to near-zero when eligible independent sources and explicit citations are absent, while grounding and custody measures remain calculable from artifact anchors and identifiers.</p>\n\n  <h3>Exclusion criteria for low-value source classes</h3>\n  <p>To preserve analytic integrity the pipeline applies explicit exclusion criteria that remove or de-prioritize low-value source classes. Examples of exclusion criteria include anonymous fora with unverifiable archives, social-media posts without persistent anchors, third-party summaries lacking original artefactual linkage, and automated feeds with insufficient provenance metadata. Excluded items are recorded and reported; they do not contribute to eligible_source_count used in credibility or corroboration computations but remain available for human review. The exclusion criteria are conservative: a source is excluded only when its provenance cannot be reconstructed to a minimum threshold required for independent corroboration or when its content lacks persistent anchors that can be traced to artifact identifiers.</p>\n\n  <h3>Validation and quality assurance</h3>\n  <p>Validation proceeds at two levels. First, technical validation verifies that extraction outputs are internally consistent: anchor coverage, artifact-to-evidence alignment and non-duplication heuristics are checked and reported in chain_provenance_diagnostics. Second, scoring validation uses a calibration process that applies prior distributions and shrinkage (reliability_factor and shrinkage_lambda parameters) to avoid overconfidence when evidence counts are small. Bootstrap intervals and effective evidence counts are computed to quantify uncertainty. Quality assurance also includes manual review gates for cases where exclusion criteria or single-source penalties materially alter claim-level inferences. All pipeline decisions, from source hierarchy placements to exclusion actions, are recorded to permit audit and independent replication.</p>\n</section>",
      "model": "gpt-5-mini"
    },
    {
      "id": "clarity_axis",
      "title": "Clarity Axis and State Responsibility Pathways",
      "html": "<h2>Introduction</h2>\n<p>This methodology chapter defines a transparent approach to scoring the clarity of attribution narratives that implicate state actors and to distinguishing between three legal responsibility pathways: conduct by state organs, responsibility for non-state actors operating under state control, and state responsibility based on failure to exercise due diligence. The approach is designed to be instrumented against structured outputs from an automated extraction pipeline and to be auditable against underlying provenance metadata. To illustrate method behavior, the supplied scoring snapshot reports an overall clarity mean (clarity_avg_0_100) of 37.11 with a bootstrap 95% confidence interval of approximately 31.9642.67; such summary statistics are used in the methodology discussion only to demonstrate calibration and uncertainty handling, not to substantively characterize particular allegations.</p>\n\n<h2>Data processing and extraction</h2>\n<p>The pipeline begins with document ingestion and automated structural parsing. Source metadata and document-level inputs (for example a reported claims_count of 6) are normalized into a canonical scoring bundle. Text-to-structure conversion yields identified artifacts (examples in the provided bundle include CVE identifiers, cryptographic hashes and domain strings) and evidence items that aggregate anchors and modalities. Each artifact and evidence item carries extraction confidence and feature vectors used downstream: the input dataset includes artifacts such as ART00001ART00014 with extraction confidences at or near 1.0 and evidence items E-0001 through E-0008 that record modality, anchors, and a computed probative_weight. Chain-of-custody signals are derived from provenance fields in the chain_provenance_diagnostics (for example context_completeness, integrity_signal, lineage_quality and anchor_quality), and these signals feed the custody score component. The methodology explicitly separates syntactic extraction (artifact recognition and anchor mapping) from semantic linking (assignment of evidence to claims) so that errors in OCR, parsing or anchor placement are isolated and traceable through artifact_proximity_tiers and anchor identifiers.</p>\n\n<h2>Reference parsing and institutional inference</h2>\n<p>Reference parsing converts document citations and source metadata into structured source objects (for example normalized.sources entries such as SRC0001 with authoring_org fields). The scoring inputs in the provided bundle show a single indexed source and zero recovered citations; operationally this triggers a single-source penalty at the claim level. Institution inference is conservative: it uses explicit source metadata (authoring_org, publisher, domain strings) and corroborating infrastructure artifacts (domains and email addresses extracted from the text) to infer institutional association candidates. The method records a binary state-actor signal and a state_claim_flag when language or metadata in the claim explicitly asserts a state nexus; these flags are operational signals for the clarity axis rather than standalone determinations of state responsibility. Because institution inference relies on metadata rather than substantive narrative assertions, the system applies graded heuristics (for example probabilistic weighting when artifact domains are tied to particular institutions) and records the provenance of each inference to permit human review and contestation.</p>\n\n<h2>Scoring framework and responsibility-pathway decomposition</h2>\n<p>The scoring model partitions evidentiary quality into orthogonal components: grounding (evidence-to-claim anchoring), custody (provenance and integrity), credibility (source quality and independence), corroboration (multi-source and modality convergence), confidence (expression and calibration of uncertainty), and clarity (the specificity of acts, actors and links). The clarity axis is decomposed further into act_specificity, actor_specificity and link_specificity and into legal-path subcomponents that map to responsibility pathways: organ_path_clarity, control_path_clarity and due_diligence_path_clarity. Each claim receives per-component scores in the unit interval and an explanatory questions object that reports binary or graded answers to readability questions such as whether attribution to a named state is clear and whether a responsibility mode is indicated. For example, the score_details structure supplies numeric values for organ_path_cl",
      "model": "gpt-5-mini"
    },
    {
      "id": "aggregation_calibration",
      "title": "Aggregation, Calibration, and Uncertainty",
      "html": "<h2>Introduction</h2>\n<p>This methodology chapter articulates the procedures for claim-to-document aggregation, weighting, calibration, and the treatment of uncertainty and dispersion in a cyberattribution scoring pipeline. The discussion is procedural and methodological: it draws on numerical artifacts provided in the raw scoring bundle to illustrate how aggregation and calibration operate, but it deliberately refrains from reporting or interpreting substantive case findings. The purpose is to make explicit the logic by which discrete evidence features and provenance signals are transformed into composite claim and document scores and how those aggregates are adjusted and reported with measures of statistical uncertainty.</p>\n\n<h2>Data processing and extraction</h2>\n<p>Input processing begins with ingestion of primary documents and technical artefacts that have been extracted from the native source. Structural parsing yields artifact records (for example, CVE identifiers, hashes, domains and email addresses) each annotated with an extraction confidence value. Anchors are preserved as pointers to the original structural blocks. The pipeline represents these items in a normalized evidence model in which evidence items carry modality and feature vectors (for instance I, A, M, P, T components) and a derived probative weight. In the supplied bundle, artifacts are recorded under the normalized artifact list and evidence items reference anchors and modalities; the evidence probative weights are then summed per claim to produce an evidence_weight_aggregate value used downstream. Chain tracing metadata such as provenance, integrity signals and anchor quality are retained to permit downstream custody diagnostics and to support saturation calculations when multiple anchors or artifacts are present for the same claim.</p>\n\n<h2>References and institution inference</h2>\n<p>Reference parsing distinguishes internal anchors (textual locations inside a single report) from external citations and unique origins. The pipeline records explicit citations and computes a sources_total and citations_total for each document. Where external authoring organizations are declared, the system captures authoring_org and origin_signature fields to support institutional inference. Institutional inference is operationalised to produce domain and origin clusters that inform credibility and corroboration axes without supplanting technical evidence: domain independence and eligible_source_count metrics are derived from the normalized sources table, and single_source conditions are detected (for example via a single source count value) and trigger predefined penalty rules that affect final claim-level aggregation.</p>\n\n<h2>Scoring framework: aggregation, weighting and calibration</h2>\n<p>Claim scoring proceeds by aggregating probative weights from constituent evidence items into a claim-level evidence weight (illustrated by the evidence_weight_aggregate field). Aggregation is not a simple sum: each evidence item contributes a feature-weighted score that is adjusted for modality, anchor proximity, and documented provenance quality. The pipeline computes core vectors (for example custody, credibility, corroboration) for each claim using subcomponents such as chain_of_custody_provenance, credibility_independence and corroboration_convergence. When originating sources are not independent or when a single origin supplies multiple items, an explicit single_source penalty multiplier (illustratively 0.85 in the provided bundle) is applied to reduce overconfidence arising from nonindependent support. The aggregated output is then transformed into a normalized final_score for the claim after penalties and data contribution multipliers are applied.</p>\n<p>Statistical calibration is layered onto the aggregated scores to control for smallN variability and overfitting. The pipeline computes an effective_evidence_n that represents the information content of the aggregated chain (for example, an effective_evidence_n of 4.0 in the sample), and derives a reliability_factor that moderates shrinkage toward prior expectations. Shrinkage is implemented via lambda parameters per axis (for example custody, credibility, corroboration, clarity and confidence), which weight the observed score against a prior_scores vector. This empirical Bayes style shrinkage reduces variance when evidence is scarce or heterogeneous while preserving signal where evidence is substantive. Saturation factors are also tracked to ensure that additional items from the same provenance cluster produce diminishing marginal increases to the corroboration and custody axes rather than linear growth.</p>\n<p>Measures of uncertainty and dispersion are produced at both the claim and document level. Bootstrap resampling is used to derive 95% confidence intervals for key document aggregates such as belief_weighted_0_100 and axis averages (as encoded in bootstrap_95ci in the supplied bundle). Those intervals quantify sampling and model uncertainty under the pipeline's resampling assumptions; dispersion diagnostics (for example variance of axis scores across claims and the spread of bootstrap replicates) inform interpretation thresholds and the construction of conservative reporting gates. For instance, when a document-level belief metric exhibits a narrow bootstrap interval around a low mean, the reporting logic treats the low belief as robust to sampling variability; conversely, wide intervals trigger explicit uncertainty language in the downstream product and, where relevant, additional calibration (increased shrinkage) to avoid overconfident presentation.</p>\n\n<h2>Validation and quality assurance</h2>\n<p>Quality assurance includes procedural validation of extraction confidences and systematic audits of provenance. Chain_provenance_diagnostics fields such as context_completeness, lineage_quality and anchor_quality are used to flag chains that require human review; artifacts with low extraction confidence are excluded or downweighted in automated aggregation. Scoring calibration is validated by backtesting shrinkage parameters and reliability_factors across a labeled corpus; bootstrap_95ci results are compared to empirical dispersion observed in hold",
      "model": "gpt-5-mini"
    },
    {
      "id": "validation_quality_assurance",
      "title": "Validation and Quality Assurance",
      "html": "<section><h2>Introduction</h2><p>This methodology chapter describes the validation and quality assurance approach applied to the automated and semi-automated components of the cyber-attribution scoring pipeline. The description is deliberately procedural and normative: it explains the purpose and mechanics of the validation architecture, the rationale for sampling and human intervention, and the interface between automated validation gates and targeted review. Where appropriate, provenance to raw pipeline artifacts and validation outputs is indicated to demonstrate traceability of the procedural assertions.</p></section><section><h2>Data Processing and Extraction</h2><p>Data entering the validation regime originates from structured extraction artifacts produced by the pipeline; representative inputs are recorded in the pipeline output and report artifacts (for example, the extraction bundle at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.output.json and the consolidated report at /home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json). The validation process consumes these JSON artifacts and executes a set of deterministic checks that evaluate schema conformance, table integrity, citation presence, artifact enumeration, and grounding consistency. The rationale for these checks is to ensure structural and referential soundness prior to any substantive scoring or downstream synthesis. The validation artifact at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.validation_report.json documents the outcomes of those automated checks and provides the primary input to the QA protocol.</p></section><section><h2>References and Institution Inference</h2><p>Reference parsing and institutional inference are treated as distinct but interoperable tasks. Reference parsing extracts anchors and citation metadata from the structural parse and the artifact extraction stage, while institution inference applies rule-based and probabilistic heuristics to map reference metadata to institutional identifiers. Methodologically, the pipeline separates entity normalization from attribution scoring to avoid circular dependencies: institution inference outputs are stored alongside origin artifacts and are subject to the same validation gates as other components. Traceability is preserved by recording the provenance of reference parsing and institution inferences within the extraction bundle and by linking those outputs back to the validation report referenced above.</p></section><section><h2>Scoring Framework</h2><p>The scoring framework is agnostic to source content and operates over validated structural and evidentiary primitives. Scores are computed from component-level indicators such as schema integrity, corroboration diversity, and artifact completeness. The validation outputs inform confidence weighting within the scoring model: for example, higher scores on schema and integrity checks reduce uncertainty margins, while warnings regarding corroboration diversity or missing non-duplicative anchors trigger conservative adjustments. This separation ensures that scoring adjustments respond to measurable quality attributes rather than to substantive case conclusions. The scoring outputs are persisted with provenance pointers to the validation and extraction JSON files (for instance, the score reports in /home/pantera/projects/TEIA/annotarium/outputs/scoring/), enabling auditors to reconcile numerical adjustments against the underlying validation evidence.</p></section><section><h2>Validation and Quality Assurance</h2><p>The QA architecture comprises layered automated validation gates followed by a hybrid review regimen that combines continuous agent review with targeted human sampling. Automated validation gates execute deterministic checks on incoming extraction artifacts; these checks include schema validation, integrity verification, table and artifact counts, presence of citations, and a set of heuristics that identify potential corroboration weakness or duplicated anchor text. The outputs of those gates are materialized in a machine-readable validation report (path: /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.validation_report.json) and carry an overall certification indicator and category-level scores used to triage subsequent review activity.</p><p>Operational QA proceeds with enabled agent review for continuous monitoring and deterministic remediation of trivial failures; agent review operates as the first human-adjacent tier and flags artifacts for escalation when automated heuristics cannot resolve an anomaly. In parallel, a targeted human review protocol samples a defined fraction of processed items to provide an empirical estimate of residual error rates. The configured sampling fraction for targeted human review is 10% (reported as human_sample_fraction = 0.1 in the QA metadata). Results from the sampled reviews are recorded; in the sampled set for this run the observed error rate was zero (human_sample_observed_error_rate = 0.0), which is reported as no observed errors in the reviewed sample. That sampling outcome is reported alongside the automated certification (the validation bundle indicates certification = \"PASS\" and an overall_score = 92.09) and category-level diagnostics that can include warnings about repeated anchor duplication or limited corroboration diversity.</p><p>Methodologically, this hybrid approach balances scale and rigor: validation gates provide reproducible, high-throughput assurance that prevents malformed inputs from propagating, while agent review and the 10% human review sample provide an empirical check on algorithmic assumptions and capture error modes that automated rules may miss. The QA protocol records provenance to the raw payloads and score artifacts (see raw_payload_paths and raw_payload_files entries) so that any subsequent re-audit can re-run validation gates and replicate the targeted human sampling. Together, these elements form a defensible chain of custody and quality assurance strategy designed to support reproducible, transparent cyber-attribution scoring without conflating methodological controls with substantive analytic judgments.</p></section>",
      "model": "gpt-5-mini"
    },
    {
      "id": "limitations_governance",
      "title": "Limitations, Governance, and Future Refinement",
      "html": "<h2>Introduction</h2>\n<p>This section articulates the methodological limitations, governance controls, and a principled refinement roadmap for a reproducible cyberattribution scoring workflow. The objective is to describe, at a methodological level, how instrument and process fragilities are identified, controlled, and iteratively improved without asserting or adjudicating any substantive factual claims from the underlying dossier. The discussion situates the limits of automated and schemadriven processing in relation to evidentiary legibility, explains governance measures adopted to preserve auditability, and proposes a staged refinement roadmap that balances determinacy, human oversight, and measured experimentation. Methodological emphasis is placed on transparency of assumptions, explicit delimitation of scope, and mechanisms for recording decisions so that subsequent reviewers can reconstruct how particular outputs were obtained from inputs.</p>\n\n<h2>Data Processing and Extraction</h2>\n<p>Data processing begins by converting source documents into a consistent internal representation that preserves original textual anchors and metadata. Extraction prioritizes explicit signals such as named entities, timestamps, hashes, and quoted assertions while recording extraction provenance at the field level. Where automated extraction produces lowconfidence outputs, those items are flagged for targeted human review rather than being promoted into downstream scoring without adjudication. Normalization routines are applied to harmonize formats and reduce spurious variability, and all transformation steps are logged to enable reproducibility. The rationale for these controls is to limit propagation of extraction errors into inferential stages and to maintain an auditable chain from raw input to derived artifacts.</p>\n\n<h2>References and Institution Inference</h2>\n<p>Inference of institutional associations from references and contextual mentions is treated as a probabilistic, modelassisted exercise constrained by external registries and verifiable metadata. The system records which external sources were consulted and the confidence assigned to any inferred linkage, while explicitly excluding inferences that rely solely on contextual implication without corroborating anchors. Institutional inference workflows incorporate mechanisms to surface ambiguous or contradictory signals to human analysts and to prevent automated aggregation from amplifying tenuous linkages. This conservativism is intended to reduce false positive associations and to ensure that institutional attributions remain provisional until validated through adjudicative review.</p>\n\n<h2>Scoring Framework</h2>\n<p>The scoring framework is claimcentric and provenanceaware: each claim carries a set of evidentiary anchors, a gravity weight that encodes its potential significance, and a confidence metric derived from extraction quality and source credibility. Aggregation across claims applies explicit propagation rules for uncertainty and documents the mathematical form of aggregation so that score derivation is transparent. Thresholds used to trigger particular handling paths, such as human escalation or public disclosure, are configurable and justified by sensitivity analyses. All score components are accompanied by machinereadable rationales that permit downstream audits and enable alternative aggregation strategies to be applied retroactively.</p>\n\n<h2>Validation and Quality Assurance</h2>\n<p>Validation combines systematic testing with ongoing quality assurance practices. Test suites exercise extraction and scoring pipelines using seeded inputs and synthetic cases designed to probe known failure modes and edge conditions. Continuous monitoring tracks metrics indicative of extraction drift, score stability, and source quality, and periodic independent audits verify that logs and provenance records suffice to reproduce analytic outcomes. Governance controls include documented escalation paths for disputed inferences, scheduled bias and robustness assessments, and a staged rollout plan for changes that could materially affect attribution outcomes. Together these measures aim to sustain confidence in methodological soundness while enabling iterative refinement.</p>",
      "model": "gpt-5-mini"
    }
  ],
  "html": "<article class=\"wiki-page\">\n<header><h1>Annotarium Methodology: APT1: Exposing One of China's Cyber Espionage Units</h1><div class=\"wiki-meta\">Generated at 2026-02-22T21:20:40Z</div></header>\n<nav class=\"wiki-toc\"><h2>Contents</h2><ol>\n<li><a href=\"#sec-introduction\">Introduction and Epistemic Framing</a></li><li><a href=\"#sec-scope_units\">Scope, Units of Analysis, and Output Semantics</a></li><li><a href=\"#sec-data_ingestion\">Data Ingestion and Corpus Handling</a></li><li><a href=\"#sec-pdf_to_markdown\">PDF-to-Markdown Conversion</a></li><li><a href=\"#sec-structural_parsing\">Structural Parsing of Text, Tables, and Figures</a></li><li><a href=\"#sec-artifact_extraction\">Artifact Extraction and Technical Object Normalization</a></li><li><a href=\"#sec-reference_parsing\">Footnote and Reference Parsing</a></li><li><a href=\"#sec-institution_inference\">Institution Inference and Source Typology</a></li><li><a href=\"#sec-claim_evidence_graph\">Claim-Evidence Graph Construction</a></li><li><a href=\"#sec-scoring_overview\">Scoring Framework Overview</a></li><li><a href=\"#sec-chain_of_custody\">Chain of Custody Axis</a></li><li><a href=\"#sec-credibility_corroboration\">Credibility Axis with Corroboration Subcomponent</a></li><li><a href=\"#sec-clarity_axis\">Clarity Axis and State Responsibility Pathways</a></li><li><a href=\"#sec-aggregation_calibration\">Aggregation, Calibration, and Uncertainty</a></li><li><a href=\"#sec-validation_quality_assurance\">Validation and Quality Assurance</a></li><li><a href=\"#sec-limitations_governance\">Limitations, Governance, and Future Refinement</a></li>\n</ol></nav>\n<section class=\"wiki-section\" id=\"sec-introduction\"><section id=\"introduction\"><h2>Introduction and Epistemic Framing</h2><p>This methodology is presented as an evidentiary framework for cyber attribution where findings are contestable and subject to adversarial scrutiny. Its epistemic posture is jurisprudential: the framework treats attribution claims as premises in structured evidentiary arguments rather than as standalone assertions of confidence. This burden-sensitive orientation requires that analysts make explicit which materials support each proposition, how those materials are connected to the proposition, and what inferential steps are required before legal responsibility might be imputed. The approach therefore separates descriptive extraction from normative legal inference so that upstream uncertainties remain visible to downstream adjudication.</p></section><section id=\"scope_units\"><h2>Scope and Units of Analysis</h2><p>The primary unit of analysis is the evidentiary record derived from a single attribution report, identified in the raw dataset by path and identifier values. For the present implementation these identifiers include the transcribed source path and report identifier reported in the input metadata. Each report is decomposed into constituent claims, discrete artifacts, and explicit source references. Claims are weighted according to declared gravity and evidentiary anchoring; artifacts are cataloged with provenance markers; and source references are registered to permit later institution inference. Aggregation operates only after claim-level scoring to preserve localized uncertainty and to avoid conflating disparate inferential steps.</p></section><section id=\"data_ingestion\"><h2>Data Ingestion and Record Formation</h2><p>Documents are ingested from a corpus of PDF reports and associated metadata records. The pipeline records ingestion provenance and timestamps from the input metadata to ensure persistence and auditability; examples of such metadata include the internal report path and the generated-at timestamp. Deterministic transformations are preferred where possible to reduce parser-induced variance. Ingestion produces an immutable raw-markdown transcription that is the authoritative input for downstream structural parsing and integrity checks.</p></section><section id=\"pdf_to_markdown\"><h2>PDF-to-Markdown Conversion</h2><p>The conversion stage employs a primary provider-backed OCR/transcription method with an explicitly declared fallback process. Primary and fallback methods are recorded in the pipeline metadata to ensure reproducibility of any particular run. During conversion, page-level anchors for tables, figures, and footnotes are emitted into the markdown so that later artifact extraction can reference concrete document offsets rather than paraphrased summaries. This design supports burden-sensitive review because it preserves the original textual context required for legal argumentation.</p></section><section id=\"structural_parsing\"><h2>Structural Parsing and Schema Enforcement</h2><p>Transcribed markdown is parsed into a schema-constrained evidentiary record separating claims, sources, artifacts, and evidentiary links. The schema enforces explicit anchors for each reference and disallows implicit support chains. Structural integrity checks are applied at this stage and runs are failed where anchors are missing, identifier collisions occur, or citation pathways cannot be traced. These admissibility-style controls prevent pseudo-precision by ensuring that numerical outputs reflect inspectable linkages rather than parser convenience.</p></section><section id=\"artifact_extraction\"><h2>Artifact Extraction</h2><p>Artifact extraction indexes items referenced in text, tables, and images into an artifact register. Each artifact entry records provenance markers, temporal anchors, artifact identifiers, and any versioning information recoverable from the record. The extraction method is deliberately conservative: only artifacts explicitly anchored to the source document are admitted to the register. This constraint preserves the evidentiary distinction between what is asserted and what is demonstrably present in the corpus.</p></section><section id=\"reference_parsing\"><h2>Reference Parsing</h2><p>Citation and footnote-like references are parsed into a source registry that preserves original citation text and resolved identifiers when available. The registry records source type categories and a linkage to the artifact indices so that later scoring computations can disaggregate independent origin contributions from downstream repetition. Reference parsing therefore operationalizes the anti-circularity principle by making origin relationships explicit and machine-inspectable.</p></section><section id=\"institution_inference\"><h2>Institution Inference</h2><p>Institution inference maps parsed references to canonical institutional identities using a named-institution resolution process recorded in the pipeline metadata. The method privileges deterministic resolution paths, with an explicit optional web-fallback recorded when lexical matching is insufficient. Inferred institutions are assigned provisional quality markers to reflect uncertainty in resolution; those markers feed directly into the later credibility calculations so that source-identity ambiguity is reflected as diminished probative force rather than silently normalized away.</p></section><section id=\"claim_evidence_graph\"><h2>ClaimEvidence Graph Construction</h2><p>Claims and their supporting items are represented as a directed claimevidence graph in which nodes denote claims, artifacts, and sources, and edges encode explicit anchor relationships. This graph preserves the chain of evidentiary dependence and enables automated queries about independence, redundancy, and provenance. The graph model underwrites burden-sensitive adjudication because it makes explicit what must be proved for a claim to carry weight and which inferential steps depend on contested links.</p></section><section id=\"scoring_overview\"><h2>Scoring Framework Overview</h2><p>Scoring is performed at the item and claim levels under an ICJ-inspired weighting model recorded in the methodological reference. Item-level evaluations consider bounded dimensions such as independence, authentication/provenance, methodological soundness, procedural testing, and contemporaneity. Item scores combine multiplicatively to ensure that severe deficiencies in any single dimension constrain overall probative force. Claim-level outputs aggregate item-level scores with attention to origin clustering and diminishing returns to guard against artificial inflation by repetitive downstream reporting.</p></section><section id=\"chain_of_custody\"><h2>Chain of Custody Axis</h2><p>The Chain of Custody axis operationalizes evidentiary handling quality for each claim. It is computed from normalized variables extracted from the text: provenance markers, integrity markers, temporal anchors, artifact identifiers, and version/update lineage. These variables are combined in a bounded, auditable function so that custody is interpretable as claim-specific handling quality rather than raw artifact quantity, and so that custody can be independently assessed during adversarial review.</p></section><section id=\"credibility_corroboration\"><h2>Credibility and Corroboration</h2><p>Credibility integrates source-quality assessment and corroborative convergence. Source quality weights are applied according to specified taxonomies that privilege judicial and peer-reviewed materials while penalizing single-source dependence and self-reporting. Corroboration is modeled as constrained convergence: claims receive corroborative strength only from materially independent origins and only to the extent the supporting items are substantively relevant to the claim wording. Corroboration is retained as an auditable subscore and then merged into the top-level credibility axis using prespecified weights to preserve interpretability.</p></section><section id=\"clarity_axis\"><h2>Clarity Axis</h2><p>Clarity measures legal intelligibility: whether the report sets out a clear actactorlink specification and whether it maps that specification onto recognized modes of state responsibility. The clarity assessment explicitly queries whether attribution reasoning corresponds to direct state conduct, control/direction over non-state actors, or omission/due-diligence failure. Because clarity bears on legal utility rather than technical plausibility alone, it is computed separately</section>\n<section class=\"wiki-section\" id=\"sec-scope_units\"><h2>Introduction</h2>\n<p>This methodological chapter sets out the units of analysis and the interpretive semantics that govern an attribution scoring pipeline for cyber-incidents. It explains, in legal-academic register, how discrete objects extracted from a document are normalized, related, and scored; and it clarifies what each output score is intended to indicate and expressly not to indicate. The exposition is grounded in the supplied raw data excerpt and pipeline metadata, which serve as exemplar inputs for the described procedures rather than as case-specific findings. References to counts and example metric values that appear in the excerpt are used only to illustrate method behavior and calibration, not to substantiate or repeat substantive allegations contained in any specific report.</p>\n\n<h2>Data processing and extraction</h2>\n<p>Ingestion begins with a document and attendant metadata. The supplied excerpt demonstrates standard metadata fields (title, authoring entity, publication date and its anchor, version, and input format) and a filesystem source locator. The first stage performs content normalization and provenance capture: the document-level metadata are extracted verbatim and any inferred anchors (for example a publication date derived from a filename heuristic) are recorded with an explicit provenance tag. Structural parsing then segments the text into machine-readable units; in the exemplar dataset the pipeline recorded one sampled page, six sampled claims, one sampled source, and four sampled artifacts. Each segment is assigned a stable identifier and a custody record that records the transformation history (for example: original file  markdown conversion  structural parse). Artifact extraction isolates technical items such as binaries, network indicators, tables, and figures and associates them with the containing document and the structural location where they appeared.</p>\n\n<h2>References and institution inference</h2>\n<p>Reference parsing treats in-text citations, bibliographic entries, and implicit source mentions as structured source objects. The pipeline stores source-level metadata (such as a vendor report label and file path) separately from extracted claims. Where explicit affiliations or authoring entities are present in document metadata, those values are recorded; where affiliation must be inferred (for example from a filename or a byline line that lacks formal affiliation), the inference is recorded with a confidence flag and the heuristic used. The supplied raw_metadata shows an authoring entity field and an inferred publication-date anchor; the methodology requires that such inferences remain auditable by preserving the extraction method and verbatim anchor text. Institution inference therefore proceeds only on the basis of explicit metadata or on transparent heuristics, and always produces structured output that includes the provenance and a cautionary note that institutional association is an interpretive step rather than an evidentiary conclusion.</p>\n\n<h2>Scoring framework: units and semantics</h2>\n<p>Units of analysis are defined as follows. A claim is a discrete propositional statement extracted from the text whose truth or probative force the pipeline seeks to characterize. A source is an identifiable origin of information (for example a vendor report, a research article, or a dataset) and is represented separately from claims it contains. An artifact is a physical or digital object described in the document (for example a file hash, binary, network indicator, table, or figure) that may function as an evidence item. An evidence item is a parsed, named unit created from an artifact or a quoted passage that can be linked to one or more claims. Document-level denotes metrics and metadata that apply to the entire document as opposed to individual claims or artifacts.</p>\n<p>The scoring model produces multiple axes intended to describe different epistemic properties rather than to deliver a single definitive judgment. A grounding score quantifies the degree to which a claim is anchored in locally supplied evidence items; a custody score records the completeness and traceability of artifacts and transformation steps; a credibility axis expresses the assessed reliability of a source independent of the claims internal grounding; a corroboration metric captures whether independent sources provide concordant support; a clarity axis measures the semantic preciseness and extractability of the claim; and a belief or confidence score synthesizes these axes into a calibrated epistemic judgment. These outputs are probabilistic or ordinal summaries and must be read as indicators of evidentiary weight and provenance quality, not as legal determinations or final attribution verdicts. For example, a low credibility score does not by itself prove falsity; a high grounding score does not adjudicate motives or organizational responsibility. In the exemplar document_scores_v4, numeric summaries such as an average grounding value and a custody average are illustrative of how the pipeline aggregates per-claim and per-document measures for calibration and quality monitoring.</p>\n\n<h2>Validation and quality assurance</h2>\n<p>Validation comprises automated consistency checks, manual review, and bootstrap estimation of metric stability. Automated checks assert internal invariants (for example that every evidence item references a document-level custody record and that inferred metadata contain provenance tags). Manual adjudication samples parsed claims and artifacts against their source text to measure extraction precision and recall; the pipelines provided preview of claim-level scores is used to select stratified samples across the score distribution. Statistical resamplingillustrated in the raw output by the provided bootstrap confidence intervalsis used to estimate the stability of aggregated metrics and to detect overfitting to specific parsing idiosyncrasies. Quality assurance also imposes governance constraints: all inferences about institution or authorship are logged with the heuristic used, and outputs intended for downstream decision-making are accompanied by explicit caveats about what the scores do not establish (for example that they do not constitute legal proof of attribution). Periodic recalibration of the scoring functions is required when the composition of the input corpus changes; the exemplar pipeline_counts and claim_score_preview_v4 provide the kinds of summary statistics used to trigger such recalibrations.</p>\n\n<p>Taken together, these procedures create an auditable pipeline that separates extraction, inference, and scoring, preserves provenance at every transformation step, and frames scores as structured epistemic assessments rather than categorical assertions. Where specific numeric values from the provided excerpt are cited, they serve to illustrate aggregation and validation behaviors and are not invoked as factual findings about any actor or incident.</p></section>\n<section class=\"wiki-section\" id=\"sec-data_ingestion\"><h1>Methodology: Data Ingestion and Corpus Handling for Cyber-Attribution Scoring</h1>\n<p>Introduction. This methodology describes the deterministic processes and reproducibility guarantees applied to the input corpus from raw PDF reports through intermediate artifacts that feed downstream attribution scoring. It is written to articulate the rationale and operational controls used to transform an input corpus into structured artifacts suitable for automated extraction, institutional inference, and quantitative scoring. The exposition emphasizes process invariants, artifact provenance, and verifiable metadata while avoiding any adjudication of the substantive claims contained in source documents.</p>\n\n<h2>Scope and Units of Analysis</h2>\n<p>The input corpus for this pipeline is one or more publisher artifacts in portable document format, together with derived machine-readable representations. Every unit of analysis is a discrete report file instantiated on disk and recorded in the system registry. For the file set treated here, the canonical report identifier and its record in the processing registry are preserved as immutable metadata elements. The working units include the original PDF, a primary JSON report manifest, a structured text extraction JSON, a validation report, and versioned scoring outputs; these units form the minimal reproducible bundle for any subsequent audit or reprocessing.</p>\n\n<h2>Data Ingestion</h2>\n<p>Report ingestion begins with the intake of the original PDF identified in the processing registry. The pipeline records absolute file paths, file existence flags, and size-in-bytes as first-order provenance properties. The storage paths used in the project context are recorded as persistent artifact names, for example the canonical PDF path and manifest paths. Deterministic file handling is achieved through strict naming conventions for intermediate artifacts, controlled directory layout, and immutably recorded artifact metadata. For each ingested file the pipeline emits a processing manifest that records the original path, a processing timestamp, the environment snapshot, and the set of produced artifacts. This manifest supports reproducibility by providing a complete mapping from an input corpus item to a set of downstream artifacts without reliance on external state.</p>\n\n<h2>PDF-to-Markdown and Text Extraction</h2>\n<p>The textual conversion stage is implemented with a primary provider-backed OCR and conversion routine and a deterministic offline fallback. The primary conversion is executed by a provider-backed script that produces a stage-one markdown artifact; when provider conversion fails the offline fallback executes via an alternative converter with equivalent deterministic settings. All conversion stages are parameterized with fixed options and are executed in a predictable order so that a given input PDF and a fixed runtime environment will yield the same intermediate markdown output. Tables and figures are extracted into anchored representations within the markdown during this stage, and the extraction stage emits a structured output JSON that indexes tables, figure anchors, and block-level text segments to enable later deterministic parsing.</p>\n\n<h2>Structural Parsing and Artifact Extraction</h2>\n<p>Structural parsing transforms the stage-one markdown and extraction JSON into typed artifact indices. The extraction schema defines artifact classes such as text span, table, figure, and code-like blocks; each artifact is assigned a stable identifier derived from the source file identity and a positional ordinal. Artifact indices are materialized into a machine-readable JSON that includes artifact offsets, anchor tokens, and contextual snippets. Deterministic behavior is enforced by using canonical tokenization rules, fixed header parsing precedence, and a documented block classification order. Artifact extraction additionally produces an artifacts registry that can be audited to trace every extracted element back to a location in the original PDF and the stage-one markdown.</p>\n\n<h2>Reference Parsing</h2>\n<p>References, citations, and footnote-like constructs are parsed using a rule-based parser that links reference tokens to a cross-document source registry. The parser emits a mapping from in-text citation anchors to resolved reference entries in the registry, and it records uncertainty metrics for ambiguous matches. Reference parsing is deterministic in that the same inputs, parser configuration, and environment yield the same linkage decisions. All reference mappings and their confidence metadata are persisted in the extraction artifacts so that downstream processes can re-evaluate or override automatic linkages in follow-up analyses.</p>\n\n<h2>Institution Inference</h2>\n<p>Institution inference is implemented as a staged process that first applies deterministic heuristics and gazetteer lookups and then augments those results with a supervised inference model when heuristic resolution is insufficient. The model-based step is performed with a fixed model version and parameters; optional external web lookups are recorded as separate ephemeral annotations so that they may be disabled to preserve strict determinism where required. The institution inference stage produces provenance annotations that reference the originating artifact identifiers and the precise textual evidence used for each inferred institution, enabling reviewers to trace every inferred association back to its textual basis.</p>\n\n<h2>ClaimEvidence Graph Construction</h2>\n<p>Extracted artifacts and parsed references are integrated into a claimevidence graph that encodes provenance edges, textual support spans, and cross-references. Each node in the graph corresponds to either a claim tokenization from the text or to an evidence artifact such as a table cell or figure caption. The graph construction uses fixed graph schema and deterministic ordering rules; node and edge identifiers are stable functions of the artifact registry and the token offsets. This graph provides the canonical linkage layer for scoring and allows reproducibility of any claim-level decision by replaying the deterministic graph construction process from the archived intermediate artifacts.</p>\n\n<h2>Scoring Overview</h2>\n<p>Scoring is performed against the claimevidence graph using a versioned scoring engine. All scoring runs are recorded as distinct output artifacts, and version identifiers are embedded in artifact filenames to preserve the chain of custody between scoring logic and scored outputs. Versioned scoring outputs are persisted alongside the extraction artifacts so that any score can be traced back to the exact extraction and inference inputs used to compute it. Metrics and confidence values are stored as structured JSON that references the exact graph node and edge identifiers that informed each scored item, enabling deterministic recomputation under the same runtime snapshot.</p>\n\n<h2>Chain of Custody and Provenance</h2>\n<p>Chain-of-custody is enforced by recording file system metadata and cryptographic checksums for the input PDF and all intermediate artifacts. The processing manifest captures environment details including runtime language version, operating platform, and installed package versions. Artifact existence flags and size-in-bytes are recorded as part of the manifest to support rapid integrity checks. The provenance trail therefore comprises the original file path, the transformation history with timestamps, checksum values, and the set of downstream</section>\n<section class=\"wiki-section\" id=\"sec-pdf_to_markdown\"><h2>Introduction</h2>\n<p>This methodology chapter sets out the procedural and evidentiary framework used to convert source reports into structured inputs for cyber-attribution scoring. The account that follows is procedural and normative: it explains how original artefacts are treated, how text and tabular structures are extracted, how references and institutional assertions are inferred, and how those outputs feed a transparent scoring procedure. Methodological rationale is prioritized over substantive case findings; the objective is to render the pipeline reproducible and auditable while emphasising safeguards that preserve evidentiary integrity and permit independent validation.</p>\n\n<h2>Data Processing and Extraction</h2>\n<p>The pipeline ingests native report files and emits structured representations that form the basis for subsequent analytic stages. Primary inputs for the project instance described here included a portable document format (PDF) located at /home/pantera/projects/TEIA/annotarium/Reports/Fireeye_rpt_APT37(02-20-2018).pdf (size: 2,920,140 bytes) and an associated markdown output at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.md (size: 22,477 bytes). The broader pipeline produces and archives intermediate JSON artifacts, including report-level JSON, raw extraction outputs, validation metadata and scoring inputs; paths and sizes for these objects are recorded (for example, report JSON at /home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json, 520,983 bytes, and raw extraction at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.output.json, 316,434 bytes). These files serve as the auditable trail for provenance and reproducibility checks.</p>\n\n<h2>PDF-to-Markdown Conversion</h2>\n<p>The conversion of PDF to markdown constitutes a core resilient stage in the pipeline. The primary conversion path uses a provider-backed Mistral OCR process implemented in process_pdf_mistral_ocr.py. This Mistral-based processing is employed to capture textual content, layout cues and basic structural markers rapidly and with provider-accelerated recognition models. When the provider-backed conversion fails or exceeds operational time limits, the pipeline adopts an offline fallback posture: PyMuPDF4LLM is invoked as an alternative renderer and extractor. Presenting the offline fallback as a posture of resilience, rather than a conceptual departure, ensures that the logical model of extractiontransforming visual page elements into a linear, semantically annotated markdown representationremains consistent across modes. Both modes emit a primary stage-one markdown that encodes paragraphs, headings, inline artefact anchors, tables and figures. The pipeline archives both the original PDF and the derived markdown to preserve chain-of-custody and to allow reprocessing should extraction heuristics be revised. The conversion design records metrics about pages, tables and figures (in this instance the pipeline registered 1 page, 3 tables and 4 figures) to illustrate handling modes such as table-preservation fidelity and image anchoring, not to adjudicate content.</p>\n\n<h2>Structural Parsing and Artifact Extraction</h2>\n<p>After markdown synthesis, a structural parsing stage normalises the markdown into discrete units for downstream analysis. The stage identifies tabular structures, embedded figures and inline artefacts and emits an artefact index of extracted indicators and blobs. The artefact extraction schema produces indexable references for text extracts, table cells and images so that individual evidence items can be referenced by identifier throughout the scoring workflow. This separation of concernsdocument-level markdown and item-level artefact indicessupports differential validation, targeted re-extraction and selective manual review while preserving the original markdown as an auditable transformation layer.</p>\n\n<h2>References and Institution Inference</h2>\n<p>References, citations and footnote-like linkages are parsed from the markdown output and linked to a source registry. The reference-parsing component associates citation text with provenance anchors and record identifiers, enabling cross-indexing across documents and external registries. Institution inference is implemented in infer_source_institutions.py and leverages a probabilistic language model (gpt-5-mini) to propose candidate source institutions from the extracted text and contextual artefacts; an optional web fallback can be consulted to corroborate institutional metadata. The inference process maintains explicable provenance: candidate matches are recorded together with confidence markers and the textual segments that triggered the inference so that downstream reviewers can evaluate the degree to which an asserted institutional link depends on explicit textual citation versus inference heuristics.</p>\n\n<h2>ClaimEvidence Graph and Scoring Framework</h2>\n<p>The pipeline organises extracted claims and evidence into an explicit claimevidence graph that maps discrete assertions to their supporting artefacts and to the parsed references. The graph formalism supports traceable scoring because each score component is defined as a function over graph featuressuch as artefact type, reference quality, and cross-source corroborationrather than as an opaque aggregate. Scoring inputs and intermediate score reports are persisted (for example, score input at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.score_input_v3.json, 21,210 bytes; and score reports at /home/pantera/projects/TEIA/annotarium/outputs/scoring/fireeye_rpt_apt37_02_20_2018.icj_score_report.json, 30,870 bytes). The scoring framework distinguishes evidentiary axessuch as attribution plausibility, artefact integrity and corroboration densityand calibrates weights via validation sets rather than arbitrary heuristics. Rationale for weighting choices is preserved in accompanying metadata files so that the scoring process remains auditable and subject to revision.</p>\n\n<h2>Validation and Quality Assurance</h2>\n<p>Quality assurance is operationalised through multiple layers. The pipeline emits a validation report (for this run: /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.validation_report.json, 3,983 bytes) that records extraction success rates, fallback invocations and anomalies in structural parsing. Validation routines compare the provider-backed Mistral output with the offline PyMuPDF4LLM fallback to detect substantive divergences; differences are flagged for human review rather than automatically prioritised, consistent with the design principle that fallback behaviour</section>\n<section class=\"wiki-section\" id=\"sec-structural_parsing\"><h2>Introduction</h2>\n<p>This methodology chapter sets out the processes and rationale for structural parsing and downstream processing of vendor-origin cyberattribution reports within a reproducible scoring pipeline. The objective is to preserve provenance and enable independent auditability while transforming heterogeneous source artifacts (pages, text blocks, tables, figures, and embedded objects) into structured, machinereadable representations that feed reference inference and scoring modules. The description that follows is procedural and methodological; it explains why certain extraction choices are made and how anchor preservation and artifact linking support chainofcustody and verification, without advancing or interpreting substantive case findings contained in any particular report.</p>\n\n<h2>Data Processing and Extraction</h2>\n<p>The ingestion pipeline begins with acquisition of the original source file and associated auxiliary artifacts. Source acquisition is recorded with file system and checksum metadata and stored alongside an extraction manifest (for example, the pipeline stores references to both the original PDF and derived markdown and JSON extractions). Conversion from a paginated document to a structured intermediate is performed so that textual content, typographic structure, and object boundaries are explicitly represented. Structural parsing produces discrete text blocks (paragraphs and headings), table objects with celllevel coordinates, and figure/image objects with bounding boxes and any embedded captions. When optical character recognition is required, the OCR output is retained together with confidence scores and bounding coordinates to preserve a map between raw pixels and extracted text.</p>\n\n<p>Preserving anchors that reference the original document location for every extracted element is essential for auditability. An anchor is a stable identifier and a locator tuple (e.g., page index, bounding box, object id) that links an extracted item back to the exact position in the source. The pipeline attaches anchor metadata to document metadata fields and every extracted object; an example of such an anchor is a manually noted publication date anchor recorded with an anchor identifier and page location. Anchors enable third parties to reinspect the original artifact at the precise locus of extraction, to resolve ambiguity in context, and to validate that subsequent annotations, annotations revisions, or scoring decisions were applied to the intended fragment of the source. Without anchors, provenance claims are unverifiable because textual snippets and tabular values can be displaced or conflated during iterative processing.</p>\n\n<h2>References and Institution Inference</h2>\n<p>Reference parsing identifies and normalizes citations, named organizations, and institutionally relevant entities through multistage processing. The first stage performs surface extraction of intext citations, footnotes, headers, and contact blocks; the second stage applies normalization heuristics and authority resolution against curated registries and the documents own metadata. Anchor linkage is maintained for every reference so that the textual span that yielded the reference can be compared back to the original context. Institution inference leverages explicit references (for example, organization names appearing in author or source metadata) and implicit signals (such as recurring domains in command and control lists, or recurring contact patterns in artifacts). Rationale for each inference is recorded as structured evidence items with anchors to the source spans used, a categorical description of the signal type (named entity, domain, artifact header), and the transformation rules or external registries applied during normalization. This design permits forensic review of why a given institution label was suggested or scored, and the anchor trace supports contested corrections.</p>\n\n<h2>Scoring Framework</h2>\n<p>The scoring framework consumes structured evidence and applies calibrated rules that weight signal types according to provenance, extraction confidence, and corroboration. Structural elementstext blocks, tables, and figuresare treated distinctly because they vary in evidentiary semantics: narrative text often contains analyst interpretation, tables commonly contain enumerations and structured mappings, and figures may contain visual evidence or captured logs. Anchored evidence items carry both a qualitative tag (the element type) and quantitative attributes (extraction confidence, table cell coordinates, figure OCR confidence). Scores are computed by aggregating weighted signals across anchors, with explicit handling to avoid doublecounting when the same content is present in multiple representations (for example, a table rendered as an image and transcribed as text). The pipeline uses counts of structural elementssuch as the number of tables or figures extractedas operational diagnostics to guide sampling and manual review, not as substantive indicators of attribution. All scoring steps persist the contributing anchor identifiers so that any score can be decomposed back to the specific anchored items that produced it.</p>\n\n<h2>Validation and Quality Assurance</h2>\n<p>Quality assurance is implemented through automated validation checks, manual spot audits, and versioned artifacts. Automated checks include anchor integrity verification (ensuring every extracted item references a valid anchor and coordinate), schema validation of table and figure objects, and crossconsistency tests between different extraction outputs (for example, comparing the markdown extraction to the pagelevel JSON extraction). Manual audit trails use the stored anchors and payload paths to permit human reviewers to open the original artifact at the exact locus of the extraction and to confirm or correct parsed values. All validation activities and corrections are recorded in a change log that includes references to the extraction manifest and the file paths used in processing. The pipeline also outputs machinereadable validation reports and store locations for score artifacts and intermediate files so that independent reprocessing can reproduce the chain of custody. Paths and manifests to primary processing artifacts (for example, raw extraction JSON, markdown conversion, validation reports, and final score outputs) are retained and referenced so that every analytical decision is traceable back to a specific file and anchor. In summary, anchorpreserving structural extraction, explicit reference provenance, and persistent recording of processing artifacts together constitute the core measures that enable reproducible, auditable, and defensible attribution scoring.</p></section>\n<section class=\"wiki-section\" id=\"sec-artifact_extraction\"><h3>Introduction</h3>\n<p>This methodology chapter describes procedures for extracting artifacts across modalities and for normalizing technical objects to support provenance and custody evaluations within a cyber-attribution scoring pipeline. The presentation follows a structured roadmap that begins with definitions of scope and units of analysis and proceeds through ingestion, conversion, structural parsing, artifact extraction, reference linking, institutional inference, claimevidence graph construction, scoring, and validation. The account is written in legal-academic prose and emphasizes methodological rationale rather than adjudicating specific case findings.</p>\n\n<h3>Scope and Units</h3>\n<p>The unit of analysis is the discrete technical object or reference as it appears in source materials, hereafter termed a technical object. Technical objects include identifiers such as CVE entries, domains, email addresses, and cryptographic hashes, as well as structural elements such as table rows, figure captions, and inline citations. The provided raw metadata indicates artifact_type_counts that include six CVE entries, five domain strings, one email address, and two MD5 hashes; these counts are used to illustrate how modality coverage affects downstream normalization and provenance reasoning without implying any substantive conclusion about network activity or actor attribution.</p>\n\n<h3>Data Ingestion</h3>\n<p>Data ingestion begins with archival retrieval of primary documents and associated artifacts. The ingestion stage enforces file integrity checks, timestamps for receipt, and an initial metadata capture that records the source identifier and any accompanying registry references. This stage is designed to preserve chain-of-custody markers and to annotate modality labels so that downstream processing is aware whether a given technical object originated in main text, a table, an image caption, or a footnote.</p>\n\n<h3>PDF-to-Markdown Conversion</h3>\n<p>Conversion of binary source documents to text seeks to maximize fidelity to the original layout because modality and positional cues are material to provenance assessments. The pipeline_methods show a primary conversion via a provider-backed Mistral OCR wrapper (process_pdf_mistral_ocr.py) with an offline fallback using PyMuPDF4LLM when provider conversion fails or times out. The rationale for a dual-path approach is to reduce loss of modality-specific cues (for example, table delimiters and figure anchors) and to provide an auditable conversion log that records which converter and which configuration produced the markdown output.</p>\n\n<h3>Structural Parsing</h3>\n<p>Structural parsing operates on the markdown output to recover hierarchical constructs such as headings, tables, figures, captions, and footnotes. The pipelines stage1 markdown parse deliberately emits tables and figures with stable anchors to enable later cross-referencing between extracted technical objects and their visual or tabular context. Maintaining these anchors is essential to provenance because the same string token may bear different evidentiary weight depending on whether it appears in the main narrative or within a table footnote.</p>\n\n<h3>Artifact Extraction and Technical Object Normalization</h3>\n<p>Artifact extraction is performed by a schema extraction stage that identifies candidate technical objects across modalities using pattern detection and context heuristics. Recognized types in the raw preview include CVE identifiers (e.g., CVE-2018-4878), domain names (for example, daum.net and fireeye.com), an email address (info@fireeye.com), and MD5 hash values. Normalization translates parsed tokens into canonical representations: CVE entries are normalized to the standard CVE registry form, domain names are lowercased and punycode-decoded where applicable, email addresses are canonicalized following mailbox parsing rules, and cryptographic hashes are validated for length and character class. Normalization also records provenance metadata for each normalized technical object, including the originating file, conversion path, markdown anchor, and byte-offset when available. This combination of normalization plus preservation of modality and positional metadata enables comparative analyses across documents while preserving the chain-of-custody context required for rigorous provenance assessment.</p>\n\n<h3>Reference Parsing</h3>\n<p>Reference parsing links in-text citations and footnote-like references to a source registry. The pipelines reference_parsing stage extracts citation anchors and attempts to reconcile them with recorded sources; where explicit bibliographic identifiers are absent, the parser emits candidate links with confidence scores. The parser records both the matched registry entry and the matching rationale (for example, exact string match versus fuzzy title similarity). These linkages are retained as part of the technical objects provenance record to support later corroboration analyses.</p>\n\n<h3>Institution Inference</h3>\n<p>Institution inference augments explicit source metadata when institutional attributions are necessary for scoring or aggregation. The provided pipeline_methods indicate an infer_source_institutions.py step that leverages a language model (gpt-5-mini) with an optional web fallback. The methodology prescribes conservative heuristics: inferred institutions are accompanied by provenance notes that distinguish explicit self-identification from model-inferred associations and by confidence markers reflecting the inference source. This preserves an auditable trail and avoids conflating model-derived suggestions with primary-source claims.</p>\n\n<h3>ClaimEvidence Graph</h3>\n<p>Extracted and normalized technical objects are organized into a claimevidence graph that represents propositions (claims) and their supporting artifacts. Each graph edge encodes modality, normalized object identifier, source anchor, and a provenance tuple documenting conversion path, parser version, and the inference confidence. The graph design supports queryable provenance chains, enabling assessments that trace a claim back to the precise textual or tabular location where a technical object appeared.</p>\n\n<h3>Scoring Overview</h3>\n<p>Scoring operates along multiple axescustody, credibility, corroboration, clarity, and overall belief weightingand uses normalized technical objects and their provenance records as inputs. Document-level example metrics in the provided document_scores_v4, such as custody_avg_0_100 and grounding_avg_0_100, illustrate how modality coverage and conversion fidelity can materially influence score components. Methodologically, the normalization and provenance metadata are used to weight contributions from artifacts that originate in high-fidelity modalities (for instance, well-structured tables with preserved anchors) more heavily than from low-fidelity OCR outputs with ambiguous anchors.</p>\n\n<h3>Chain of Custody</h3>\n<p>Chain-of-custody procedures embed immutable metadata at ingestion and retention points to record custody transitions. For each technical object the system records the ingest timestamp, the conversion path (including whether the Mistral OCR or the PyMuPDF4LLM fallback was used), and the structural anchor that tied the object to its original location. These records are cryptographically hashed and appended to an audit log to support later validation inquiries and to demonstrate provenance continuity.</p>\n\n<h3>Credibility and Corroboration</h3>\n<p>Credibility assessment separates source-level credibility from artifact-level corroboration. Credibility scoring incorporates explicit citations parsed by the reference_parsing stage, institutional provenance from the institution_inference stage, and contextual signals such as modality and extraction confidence. Corroboration measures compare normalized technical objects across sources and modalities; for example, exact canonical matches across independent source anchors</section>\n<section class=\"wiki-section\" id=\"sec-reference_parsing\"><section><h2>Introduction</h2><p>This methodology chapter describes procedures for transforming rhetorically framed citations and footnote constructs in source documents into an analyzable source graph suitable for cyberattribution scoring. The exposition is methodological and abstains from asserting any casespecific factual conclusions. It defines the purpose of reference parsing within a broader pipeline that ingests documents, extracts artifacts and claims, and produces structured inputs for downstream scoring and validation. The approach is designed to preserve evidentiary provenance, enable reproducible linkage between claims and supporting materials, and to provide clear audit trails for chainofcustody and quality assessment.</p></section><section><h2>Data processing and extraction</h2><p>Ingestion begins with document acquisition and deterministic conversion to a machinereadable representation. The available pipeline metadata indicates primary PDF conversion through a providerbacked OCR pipeline (process_pdf_mistral_ocr.py) with an offline fallback (PyMuPDF4LLM) to guard against provider timeouts. Structural parsing produces pages, tables, figures and embedded artifacts; pipeline counts in the supplied preview show one source with a single page, multiple tables and figures, four extracted artifacts and six textually identified claims. These counts inform resource allocation and sampling strategies for subsequent manual review but are not interpreted as substantive evidence themselves.</p><p>Structured extraction proceeds in stages. The first stage converts visual and typographic elements to a markdownlike intermediary that retains anchors for tables and figures. A subsequent schema extraction stage enumerates artifacts referenced by caption or inline mention and emits indices tying text spans to artifact identifiers. All conversions retain bounding metadata, original page offsets and the conversion method used to facilitate reproducibility and to support later chainofcustody checks.</p></section><section><h2>References and institution inference</h2><p>Reference parsing converts rhetorical citation tokensfootnote markers, parenthetical citations, numbered endnotes and inline source mentionsinto explicit links to registry entries in the source index. The pipeline component described as reference_parsing locates footnote markers in the intermediary representation, extracts adjacent citation text, and attempts canonicalization against the source registry. Canonicalization uses deterministic heuristics (title and year matching, entity name normalization) augmented by an institution_inference stage that applies a language model (infer_source_institutions.py with gpt5mini and optional web fallback) to resolve ambiguous attributions such as institutional authoring statements embedded in front matter or internal sections. The method records confidence scores, the mapping algorithm used, and any fallback evidence (for example, matches to known publication titles or institutional identifiers) to support later review.</p><p>These steps produce a directed bipartite graph between rhetorical citation nodes (the textual locations of footnote or inline citation) and source registry nodes. The graph explicitly records unresolved citation tokens as orphan nodes with provenance metadata so that missing or ambiguous references are visible to analysts rather than silently dropped. The resulting source graph therefore encodes citation linkage at the granularity of text span  source identifier  inferred institution, permitting queries for all claims that reference a given source or all sources cited by a particular claim.</p></section><section><h2>Scoring framework</h2><p>The scoring framework treats parsed references and inferred institutions as evidence attributes that feed credibility, corroboration and clarity axes. Each claim receives a structured evidence vector containing pointers to artifact indices, the set of linked sources from the source graph, and institutional inference metadata with confidence values. Scoring components operate on these vectors to produce intermediate scoreschainofcustody completeness, citation linkage confidence and corroborative corroboration countswhich are then aggregated with calibrated weights to form composite attribution scores. Aggregation calibration is performed on heldout examples and synthetic cases to ensure the system responds predictively to changes in citation density, the presence of primary versus secondary sources, and institution inference confidence.</p><p>Methodological rationale for separating citation linkage from institution inference is to avoid conflating surface textual citation patterns with the provenance of origin: a clear footnote may link to a document whose authorship is disputed; institution inference is therefore maintained as a distinct attribute rather than a derived certainty. Confidence propagation rules are explicit and conservative: low confidence in reference resolution limits the downstream weight of that evidence in composite scoring, and the system records the dependency path from a score back to the underlying reference nodes for auditability.</p></section><section><h2>Validation and quality assurance</h2><p>Validation is multitiered. Automated checks verify that every parsed footnote has been assigned either a resolved source identifier or an explicit unresolved tag; statistics on unresolved citations are tracked as quality metrics. Synthetic and seeded groundtruth documents exercise the reference_parsing and institution_inference components to measure precision and recall of citation linkage and correct institution assignment. Crossvalidation with manual review samples is used to calibrate confidence thresholds and to estimate humanmachine agreement rates. The pipeline records the conversion method (primary OCR or fallback) and uses that metadata to stratify error analyses, since OCR quality materially affects footnote and inline citation recognition.</p><p>Chainofcustody documentation accompanies every output: conversion timestamps, tool identifiers, and reviewer annotations are retained in the provenance record for each claim and each source graph edge. Periodic audits reprocess a random sample of documents endtoend to detect regressions in parsing behavior. Limitations of the approach, including dependence on OCR accuracy, model inference fallibility in institution assignment, and the reduced reliability of citation linkage in poorly formatted or scanned documents, are monitored and surfaced in metadata so that downstream consumers can apply appropriate caution in interpreting attribution scores.</p></section></section>\n<section class=\"wiki-section\" id=\"sec-institution_inference\"><section><h2>Introduction</h2><p>This methodology chapter defines procedures for institution inference and source typology within a cyber-attribution scoring pipeline. It sets out the conceptual basis for mapping extracted source artifacts to institutional classes, and for operationalizing those classes into quantitative and qualitative effects on downstream credibility weighting and corroboration eligibility. The treatment below is methodological and refrains from applying attributions to any particular incident; it instead describes how institutional signals are detected, classified, and used in scoring while referencing the pipeline artefacts and processing modalities present in the supplied raw data.</p></section><section><h2>Scope and Units of Analysis</h2><p>The primary units of analysis are source objects as emitted by the ingestion and parsing stages of the pipeline. In the provided raw data this is illustrated by a preview record (source_id: SRC0001, title: \"APT1 Executive Summary and Key Findings\", source_type: \"internal_document_section\", entity_name: \"Mandiant\", year: 2013). The approach treats each source object as a bounded evidentiary unit from which institutional signals, reference links, and artifact indices are extracted. Source typology focuses on origin, provenance, and intended audience, and is operationalized at the granularity of source objects and their referenced artifacts rather than at the level of individual tokens.</p></section><section><h2>Data Ingestion</h2><p>Ingestion comprises the reception of primary documents and their registration in a source registry. As indicated in the pipeline metadata, source_type_counts currently reports a single internal_document_section; this count is used illustratively to show how typology calibration begins with available source classes. The registry preserves metadata fields such as title, entity_name, publication_or_venue, year, and source_type to support subsequent institution inference and chain-of-custody records.</p></section><section><h2>PDF-to-Markdown Conversion</h2><p>The pipelines primary conversion step is performed with a provider-backed OCR and conversion routine identified as process_pdf_mistral_ocr.py, with an offline fallback via PyMuPDF4LLM when provider conversion fails or times out. These routines produce machine-readable markdown representations of documents. Methodologically, conversion is assessed against fidelity metrics for textual accuracy and structural preservation; conversion artifacts (e.g., OCR confidence scores, timeout events) are recorded as inputs to credibility weighting.</p></section><section><h2>Structural Parsing</h2><p>Structural parsing decomposes converted markdown into hierarchical elements: sections, headings, paragraphs, tables, and figures. The pipelines stage1 markdown parse emits tables and figures/images with anchors, enabling linkage of in-text citations to visual artifacts. Structural metadatasuch as section identifiers and anchor IDsis stored alongside original source identifiers to maintain traceability.</p></section><section><h2>Artifact Extraction</h2><p>Artifact extraction isolates discrete evidentiary items (for example network indicators, configuration snippets, or labeled figures) and assigns them schema-record indices. The pipelines artifact_extraction stage emits artifact indices that reference both the structural location and the original source object. Each artifact index includes provenance metadata sufficient for later chain-of-custody reconstruction.</p></section><section><h2>Reference Parsing</h2><p>Reference parsing identifies in-text citations, footnotes, and bibliographic references and links them to the source registry. The reference_parsing stage attempts canonicalization and dereferencing where possible; unresolved references are flagged and their resolution status recorded. These linkage edges form the initial graph topology used to propagate institutional signals across the corpus.</p></section><section><h2>Institution Inference</h2><p>Institution inference assigns institutional attributes to source objects using a hybrid model described in the pipeline metadata: infer_source_institutions.py leveraging a purpose-built LLM (gpt-5-mini) with an optional web fallback for external validation. Institutional attributes include publisher identity, organizational type (e.g., vendor, academic, independent researcher, government), and known relationships (e.g., affiliation with a parent organization). The module consumes structured metadata, textual cues from the parsed content, and, when available, external resolving evidence. All inferences are probabilistic and tagged with confidence scores and provenance rationales to support downstream interpretability.</p></section><section><h2>ClaimEvidence Graph Construction</h2><p>Following institution inference, the system constructs a claimevidence graph that links extracted claims and artifacts to source nodes and inferred institutions. Graph edges encode relations such as cites, asserts, and corroborates, and carry provenance metadata (source id, section id, artifact id, and parsing confidence). Institutional nodes are annotated with the inference confidence from the previous stage and with metadata taken from raw ingestion records, enabling both structural and institutional reasoning over claims.</p></section><section><h2>Scoring Overview</h2><p>The scoring framework integrates institutional attributes into a multi-axis credibility model. Institutional class is one axis among others (technical provenance, artifact fidelity, and corroboration count). Scores are computed by combining institution-derived priors with artifact-level quality metrics and graph-based corroboration signals. The framework treats institutional signals as soft priors rather than determinative labels, ensuring that empirical artifact quality can override institutional expectations where warranted.</p></section><section><h2>Chain of Custody</h2><p>Chain-of-custody records capture each transformation from original document ingestion through conversion, parsing, and artifact extraction. Each transformation step logs tool identifiers and versions as present in the runtime_libraries metadata (for example, python_version and key packages such as pypdf, pymupdf, and pymupdf4llm), along with timestamps and operator or automated-process identifiers. These records support reproducibility and enable targeted re-evaluation of institutional inferences when upstream transformations are questioned.</p></section><section><h2>Credibility and Corroboration</h2><p>Credibility weighting uses institution inference to modulate both individual source credibility and corroboration eligibility. Institutional class affects the prior assigned to a source: institutional priors adjust the weight of claims in the aggregation step and determine whether a source can serve as an independent corroborator. Corroboration eligibility rules treat sources within the same institutional family as non-independent unless explicit evidence of independent observation is present. All such eligibility decisions are annotated with the inference confidence from the institution_inference stage.</p></section><section><h2>Clarity Axis</h2><p>A clarity axis assesses how explicitly an institutional claim is articulated in the source text (for example explicit attribution statements, byline metadata, or corporate branding). Clarity scores influence both credibility weighting and the stringency of corroboration rules; higher clarity reduces reliance on external inference while lower clarity raises the evidentiary bar for corroboration.</p></section><section><h2>Aggregation and Calibration</h2><p>Aggregation synthesizes weighted claims across the claimevidence graph. Calibration procedures adjust institutional priors and combination rules using holdout datasets and simulated perturbations in source typologies. Calibration emphasizes robustness to misclassification of institutional type by favoring artifact-level signals when institutional inference confidence is low.</p></section><section><h2>Validation and Quality Assurance</h2><p>Validation includes unit checks of parsing fidelity, reconciliation of reference links, and spot audits of institution inference rationales. Quality-assurance procedures incorporate reproducibility tests using the recorded runtime_libraries and conversion logs, and measure score stability under alternate inference thresholds. All validation outcomes are logged and considered when updating scoring parameters.</p></section><section><h2>Limitations and Governance</h2><p>The methodology recognizes limits inherent to automated institution inference, including dependence on conversion fidelity and the potential for biased training signals in language models. Governance measures require human-in-the-loop review for high-impact decisions and mandate explicit documentation of inference confidence, chain-of-custody records, and any manual overrides. The procedure therefore combines algorithmic institution inference with formal audit trails and governance safeguards</section>\n<section class=\"wiki-section\" id=\"sec-claim_evidence_graph\"><h2>Introduction</h2>\n<p>This methodology section defines the construction and governance of a claim-evidence graph used for cyber-attribution scoring. The claim-evidence graph is a directed, typed graph that connects authored claims to discrete evidence items, source descriptors, and technical artifacts with explicit anchor-level pointers. The primary objectives of the graph are to enable machine-actionable traceability from claim to evidence, to support reproducible chain-of-custody assessment, and to enforce anti-circularity constraints so that evidence derived from the evaluated report is not treated as independent corroboration. The description below is procedural and methodological; it uses metadata samples contained in the supplied raw data (for example, identifiers such as claim_id values C001C006, source records such as SRC0001, and artifact records such as ART00001ART00014) to illustrate structure and controls rather than to restate substantive findings.</p>\n\n<h2>Data processing and extraction</h2>\n<p>Input processing proceeds in predictable stages. First, documents and auxiliary data are ingested into a normalized repository that preserves both the original file and an extracted text representation. The supplied bundle records this normalization step with entries under normalized.artifacts and normalized.sources; each artifact record (for example ART00001) carries an artifact_type, value, origin, location tuple (page, block_id) and an extraction confidence score. The extraction pipeline separates tasks conceptually: conversion of binary documents to a text/markdown form; structural parsing into logical blocks (paragraphs, lists, tables); and extraction of technical artifacts (CVE identifiers, domain names, email addresses, cryptographic hashes). The raw_artifacts_preview included with the input (artifact types and example_values such as \"CVE-2018-4878\" and \"daum.net\") is used to calibrate entity-extraction models and to validate that the extractor recognizes the relevant modal classes for subsequent linking.</p>\n<p>Structural parsing yields discrete anchors: stable, addressable block identifiers that become nodes in the claim-evidence graph. For every artifact and for every claim anchor we persist the original location metadata (page, block_id) and the block text snippet. The normalized.artifacts entries in the provided scoring_bundle show the representation used in practice: artifact_id, artifact_type, value, location and extraction confidence. Anchor-level persistence supports fine-grained traceability and enables later human review to re-evaluate extraction correctness without re-processing the entire document.</p>\n\n<h2>References and institution inference</h2>\n<p>Reference parsing and source normalization are treated as first-class processes. Cited items discovered in text are canonicalized into source records and cross-referenced to publisher and author metadata where available. The provided raw_sources_preview contains a representative source entry (SRC0001: Mandiant APT1 Report) which illustrates the normalized source record schema: source_id, source_type, authoring_org, publisher, and publication date. Where explicit bibliographic metadata are absent, reference-disambiguation relies on URL and domain normalization; domain strings present in artifact lists (for example, \"fireeye.com\") are used as candidate organizational signals in institution inference.</p>\n<p>Institution inference applies a conservative, multi-evidence heuristic. Domain ownership, publisher metadata, and in-document claims about provenance are combined to produce an inferred institutional attribution with an associated confidence band. The system records both the inferred institution and the provenance of that inference (for example which artifact_ids and which source records supported the inference). To minimize circularity, any institution inference that relies solely on artifacts extracted from the evaluated report is flagged and treated as report-derived. Those flagged inferences are not permitted to contribute to cross-source corroboration metrics unless validated by an independent source record or external registry lookup.</p>\n\n<h2>Claim-evidence graph construction and traceability</h2>\n<p>The claim-evidence graph is constructed by creating a claim node for each parsed claim (claim_id entries such as C001C006) and linking claim nodes to evidence nodes via explicit relation edges. Evidence nodes correspond to normalized evidence_items (for example E-0001..E-0008 in the normalized bundle) and to atomic artifact nodes that represent extracted technical indicators (ART00001..ART00014). Each evidence node includes modality metadata (for example \"cve\", \"infrastructure\") and feature vectors used for probative weighting (the input contains features labeled I, A, M, P, T in the evidence_items records). Edges are typed to indicate the role of the evidence relative to the claim (for example supports, contextualizes, links_actor). All edges are annotated with anchor pointers (page, block_id) so that every claimevidence relationship can be traced back to the original block in the source document; this anchor-level traceability permits deterministic human verification and preserves evidentiary context.</p>\n<p>To preserve provenance fidelity the system stores an origin_id for every evidence node (for example ORIG:src0001 in the evidence_items) and maintains a mapping from evidence nodes to their source_ids and artifact_ids. During graph construction the pipeline enforces an invariant that every edge must reference at least one anchor and at least one origin. This invariant is foundational for chain-of-custody scoring because it prevents unanchored syntheses from being used as primary evidence and thereby preserves an auditable trail from claim to raw textual anchor.</p>\n\n<h2>Anti-circularity safeguards</h2>\n<p>Anti-circularity mechanisms are explicit and multi-layered. First, origin clustering is used to detect single-origin support: the scoring bundle contains origin_cluster_weights and unique_origin_count fields that the pipeline uses to mark claims with limited source independence. Second, the system distinguishes report-derived evidence from independently obtained artifacts by tracking recovered_reference_count and a report_derived_ratio; any evidence node with a report-derived flag is excluded from multi-source corroboration tallies unless an external reconciliation step validates it. Third, provenance validation uses</section>\n<section class=\"wiki-section\" id=\"sec-scoring_overview\"><h2>Introduction</h2><p>This methodology section describes the scoring architecture used to translate extraction outputs into reproducible attribution scores. The objective is to articulate a transparent, auditable pipeline that moves from claim-level evidence anchors to document-level synthesis, while maintaining an explicit separation between raw extraction outputs and the inferential weighting that yields final scores. The exposition follows the pipeline manifests present in the supplied data (for example, the scoring_bundle and document_scores_v4 structures) and explains the rationale for each processing stage in general terms rather than restating substantive findings from the source document.</p><h2>Data processing and extraction</h2><p>Primary ingestion begins with a provider-backed PDF conversion to markdown, with an offline fallback, as recorded in the pipeline_methods. Structural parsing produces discrete text blocks, anchors, tables and figure references. From these anchors a schema-driven artifact extraction stage emits typed artifacts (artifact_type enumerations such as vulnerability identifiers, cryptographic hashes, domains and contact addresses) and attaches provenance metadata including extraction confidence and block anchors. Evidence items are then assembled by grouping anchors and artifacts under evidence identifiers; each evidence item records modality tags (for example, technical artifact, dataset or infrastructure), feature vectors (the dataset labels I, A, M, P, T are present in the evidence feature maps) and a computed probative_weight. Those extraction outputs (anchors, artifacts, evidence items and their raw feature scores) are treated as observable inputs to scoring and are retained without inferential interpretation in this stage so that downstream weighting can be independently examined and audited.</p><h2>References and institution inference</h2><p>Reference parsing and source registry linkage are performed after artifact extraction. The pipeline captures a normalized source registry entry for each declared source, including source_kind and authoring organization; the supplied pipeline_methods indicate an institution inference step that uses a model-backed heuristic process. Institution inference operates on the parsed citation metadata and the source registry to produce inferred authoring_org entries and to populate flags relevant to source characterization (for example, indicators of litigation preparation, stated conflicts, or whether a source is single). The distinction between extracted reference tokens and inferred institutional attributes is critical: parsed citations and anchors are deterministic outputs from the structural parser, whereas inferred institutional attributes are model-derived annotations used only in credibility- and corroboration-related axes of scoring and are recorded separately so they may be re-evaluated or replaced without re-running extraction.</p><h2>Scoring framework: claim-level axes to document-level synthesis</h2><p>Scoring is organized as claim-level vectors and a document-level aggregation. At the claim-level each claim is associated with a set of evidence items (evidence_ids) and per-evidence probative measures. The claim-level axes represented in the data include grounding (the extent and coverage of anchors and markers supporting the claim), custody (chain-of-custody and artifact identifier quality), credibility (source quality and independence), corroboration (multi-source and modality convergence), confidence (stated confidence and calibration), and clarity (granularity and specificity of actoractlink relationships). Numerically, these axes appear as per-claim scores (for example grounding_0_100, custody_0_100, clarity_0_100) and as component vectors (the six-c or similar vectors) that feed an evidence_weight or evidence_support term.</p><p>Inferential weighting is explicitly separated from extraction outputs. The pipeline treats extracted indicators (anchors, artifact types and evidence probative_weight) as inputs to a weighting model that implements shrinkage and calibration. The statistical_calibration_v4 block in the supplied data documents mechanisms for reliability adjustment (reliability_factor), effective sample sizing (effective_evidence_n), and shrinkage lambdas applied to axes (shrinkage_lambda for custody, credibility, corroboration, clarity, confidence). Penalty multipliers (for example a single_source penalty factor) are applied to claim-level aggregates to encode known structural risks; these appear as penalty_multiplier and penalties in the claim scoring entries. Claim-level final_score values therefore reflect the composition of evidence weight aggregates, applied penalties, and calibrated shrinkage rather than any additional re-interpretation of extraction artifacts.</p><p>Document-level synthesis aggregates claim-level vectors into summary statistics that are designed to preserve the multi-axis structure. Aggregation methods present in the data include mean and geometric means across claim scores (for example overall_claim_score_mean and overall_claim_score_geometric) and a headline claim selection that propagates a headline_vector to the document-level. Bootstrap-derived uncertainty estimates (bootstrap_95ci) are computed for selected aggregates to communicate sampling and model uncertainty at the document-level. Where gating rules are required (for example seriousness or minimum credibility thresholds) those conditions are evaluated against the aggregated vectors and are recorded in gate structures so that decisions are auditable and reproducible.</p><h2>Validation and quality assurance</h2><p>Quality assurance is performed at multiple points. Extraction QA validates anchor coverage, artifact traceability and anchor-to-claim alignment; chain_provenance_diagnostics fields record context_completeness, integrity_signal, lineage_quality and anchor_quality metrics that support automated checks. Scoring QA validates internal consistency by checking that evidence_weight_aggregate equals the sum of constituent probative</section>\n<section class=\"wiki-section\" id=\"sec-chain_of_custody\"><h2>Introduction</h2>\n<p>This chapter defines the methodological approach used to evaluate chain-of-custody aspects of evidence that underlie cyberattribution scoring. The presentation is disciplinary and procedural rather than substantive: it explains how provenance, integrity, time anchors, artifact identifiers and versioning are operationalised, how those signals are combined into quantitative custody measures, and how quality controls and penalties are applied when signals are incomplete. Where appropriate, the method refers to diagnostic fields and aggregated metrics drawn from the supplied scoring bundle and document-level summaries to illustrate behaviour of the method, not to adjudicate particular factual claims.</p>\n\n<h2>Data processing and extraction</h2>\n<p>The pipeline begins with structured ingestion of the primary source objects and derived artifacts. Documentlevel metadata and the normalized artifact table serve as primary inputs: artifact rows record artifact_type, value, extraction origin and extraction confidence; evidence items connect anchors to claims and enumerate modalities (for example, \"cve\" or \"infrastructure\"). Structural parsing produces anchor identifiers and blocklevel locations that are preserved as the primary linkage between textual claim statements and supporting artifacts. Extraction confidence and explicit artifact identifiers (for example, hash values and CVE strings) are recorded and propagated through the pipeline so that downstream custody calculations can weight direct, verifiable artifacts more heavily than contextually inferred entities.</p>\n<p>To protect the chain of custody we differentiate three extraction classes. Direct artifacts are those with explicit identifiers and high extraction confidence (artifact entries with confidence=1 and types such as hash_md5 or cve); contextual artifacts are textually proximate mentions that lack a stable identifier; and remote or derived artifacts are external references that require followup retrieval to be validated. The pipeline tags each artifact with provenance metadata indicating the source record (source_id), the extraction method (extracted_from), and the anchor block(s) that justify claim linkage. The scoring inputs include counts and previews of artifacts (for example, counts by artifact_type and example values) to allow automated flagging of weakly identified chains (for example, absence of artifact identifiers or absence of hash values where they would be expected).</p>\n\n<h2>Reference parsing and institution inference</h2>\n<p>Reference parsing separates internal report anchors from external citations. The normalised source table records authoring_org and</section>\n<section class=\"wiki-section\" id=\"sec-credibility_corroboration\"><section>\n  <h3>Introduction</h3>\n  <p>This methodology describes the Credibility axis with a dedicated Corroboration subcomponent as applied to cyber-attribution scoring. The approach prioritizes transparency of inputs and reproducibility of transformations while avoiding substantive adjudication of contested factual claims. The protocol distinguishes between structural processing steps and inference rules: the former converts raw artifacts and anchors into machine-readable evidence items, and the latter evaluates provenance, source quality and inter-source relationships. The following exposition uses the provided input metadata to illustrate procedural behavior (for example, a single documented source with type classification \"internal_document_section\" and an authoring organization listed as Mandiant), but it intentionally refrains from drawing operational conclusions about specific incidents referenced in the underlying report.</p>\n\n  <h3>Data processing and extraction</h3>\n  <p>Source ingestion begins with normalized metadata and a source type inventory. In the supplied input the source_type_counts map indicates one unit classified as internal_document_section; this triggers an ingestion pathway optimized for publisher-supplied technical reports. Documents are parsed into structural anchors (pages and blocks) and artifacts are extracted with modality labels (for example, CVE identifiers, hashes, domains and email addresses). Each artifact record carries provenance flags and an extraction confidence score; the scoring pipeline uses these values to weight downstream evidence. Evidence items are formed by clustering related anchors and artifacts, assigning modality vectors and computing a probative weight that combines artifact-level confidence with contextual indicators (anchor coverage, marker strength and modality relevance). This pipeline deliberately separates extraction confidence from interpretive credibility so that noisy or machine-extracted artifacts do not automatically inflate attribution measures.</p>\n\n  <h3>References and institution inference</h3>\n  <p>Reference parsing recognizes explicit citations, author lists and publisher fields and synthesizes an institutional identity when available. The normalized source record in the input includes an authoring_org field (\"Mandiant\") which permits an institutional label to be associated with that origin. Institution inference leverages explicit publisher metadata first, then secondary signals such as domain names and email addresses contained among extracted artifacts. The inferred institution, its declared role (publisher, vendor, academic etc.) and any declared conflicts of interest are recorded in a source record. These records form the basis of a source hierarchy: primary-origin sources (first-party technical reports and datasets) are placed above secondary-derivative items (aggregated summaries, citations) in the hierarchy used for credibility scoring. The source hierarchy is therefore explicit, auditable and used to compute scope factors such as claim coverage and the effective independence of corroborating chains.</p>\n\n  <h3>Scoring framework: credibility, corroboration and claim coverage</h3>\n  <p>The Credibility axis is computed from three components: chain-of-custody and provenance fidelity, source-quality and independence, and corroboration-convergence across distinct origins. Chain-of-custody metrics assess artifact identifiers, time anchors and lineage disclosure to produce a custody score. Source-quality and independence combine declared publisher type, domain diversity and the presence of independent corroborating origins into a credibility_independence measure. Independence is operationalized as the lack of shared origin identifiers across supporting sources, penalizing claims that derive from a single origin even if that origin contains multiple anchors.</p>\n  <p>Corroboration is assessed by counting and weighting supporting sources and modalities, then applying diminishing returns via a saturation function. The corroboration metric rewards multi-origin, cross-modality support and explicit cross-language or cross-domain corroboration. Claim coverage scaling maps the fraction of a claim's anchors that are supported by eligible sources into a multiplicative coverage factor: claims with full anchor coverage receive full weight while partially covered claims are down-weighted proportionally. The supplied document scores illustrate the framework's behavior when only one origin is present: credibility and corroboration summary values may be driven to near-zero when eligible independent sources and explicit citations are absent, while grounding and custody measures remain calculable from artifact anchors and identifiers.</p>\n\n  <h3>Exclusion criteria for low-value source classes</h3>\n  <p>To preserve analytic integrity the pipeline applies explicit exclusion criteria that remove or de-prioritize low-value source classes. Examples of exclusion criteria include anonymous fora with unverifiable archives, social-media posts without persistent anchors, third-party summaries lacking original artefactual linkage, and automated feeds with insufficient provenance metadata. Excluded items are recorded and reported; they do not contribute to eligible_source_count used in credibility or corroboration computations but remain available for human review. The exclusion criteria are conservative: a source is excluded only when its provenance cannot be reconstructed to a minimum threshold required for independent corroboration or when its content lacks persistent anchors that can be traced to artifact identifiers.</p>\n\n  <h3>Validation and quality assurance</h3>\n  <p>Validation proceeds at two levels. First, technical validation verifies that extraction outputs are internally consistent: anchor coverage, artifact-to-evidence alignment and non-duplication heuristics are checked and reported in chain_provenance_diagnostics. Second, scoring validation uses a calibration process that applies prior distributions and shrinkage (reliability_factor and shrinkage_lambda parameters) to avoid overconfidence when evidence counts are small. Bootstrap intervals and effective evidence counts are computed to quantify uncertainty. Quality assurance also includes manual review gates for cases where exclusion criteria or single-source penalties materially alter claim-level inferences. All pipeline decisions, from source hierarchy placements to exclusion actions, are recorded to permit audit and independent replication.</p>\n</section></section>\n<section class=\"wiki-section\" id=\"sec-clarity_axis\"><h2>Introduction</h2>\n<p>This methodology chapter defines a transparent approach to scoring the clarity of attribution narratives that implicate state actors and to distinguishing between three legal responsibility pathways: conduct by state organs, responsibility for non-state actors operating under state control, and state responsibility based on failure to exercise due diligence. The approach is designed to be instrumented against structured outputs from an automated extraction pipeline and to be auditable against underlying provenance metadata. To illustrate method behavior, the supplied scoring snapshot reports an overall clarity mean (clarity_avg_0_100) of 37.11 with a bootstrap 95% confidence interval of approximately 31.9642.67; such summary statistics are used in the methodology discussion only to demonstrate calibration and uncertainty handling, not to substantively characterize particular allegations.</p>\n\n<h2>Data processing and extraction</h2>\n<p>The pipeline begins with document ingestion and automated structural parsing. Source metadata and document-level inputs (for example a reported claims_count of 6) are normalized into a canonical scoring bundle. Text-to-structure conversion yields identified artifacts (examples in the provided bundle include CVE identifiers, cryptographic hashes and domain strings) and evidence items that aggregate anchors and modalities. Each artifact and evidence item carries extraction confidence and feature vectors used downstream: the input dataset includes artifacts such as ART00001ART00014 with extraction confidences at or near 1.0 and evidence items E-0001 through E-0008 that record modality, anchors, and a computed probative_weight. Chain-of-custody signals are derived from provenance fields in the chain_provenance_diagnostics (for example context_completeness, integrity_signal, lineage_quality and anchor_quality), and these signals feed the custody score component. The methodology explicitly separates syntactic extraction (artifact recognition and anchor mapping) from semantic linking (assignment of evidence to claims) so that errors in OCR, parsing or anchor placement are isolated and traceable through artifact_proximity_tiers and anchor identifiers.</p>\n\n<h2>Reference parsing and institutional inference</h2>\n<p>Reference parsing converts document citations and source metadata into structured source objects (for example normalized.sources entries such as SRC0001 with authoring_org fields). The scoring inputs in the provided bundle show a single indexed source and zero recovered citations; operationally this triggers a single-source penalty at the claim level. Institution inference is conservative: it uses explicit source metadata (authoring_org, publisher, domain strings) and corroborating infrastructure artifacts (domains and email addresses extracted from the text) to infer institutional association candidates. The method records a binary state-actor signal and a state_claim_flag when language or metadata in the claim explicitly asserts a state nexus; these flags are operational signals for the clarity axis rather than standalone determinations of state responsibility. Because institution inference relies on metadata rather than substantive narrative assertions, the system applies graded heuristics (for example probabilistic weighting when artifact domains are tied to particular institutions) and records the provenance of each inference to permit human review and contestation.</p>\n\n<h2>Scoring framework and responsibility-pathway decomposition</h2>\n<p>The scoring model partitions evidentiary quality into orthogonal components: grounding (evidence-to-claim anchoring), custody (provenance and integrity), credibility (source quality and independence), corroboration (multi-source and modality convergence), confidence (expression and calibration of uncertainty), and clarity (the specificity of acts, actors and links). The clarity axis is decomposed further into act_specificity, actor_specificity and link_specificity and into legal-path subcomponents that map to responsibility pathways: organ_path_clarity, control_path_clarity and due_diligence_path_clarity. Each claim receives per-component scores in the unit interval and an explanatory questions object that reports binary or graded answers to readability questions such as whether attribution to a named state is clear and whether a responsibility mode is indicated. For example, the score_details structure supplies numeric values for organ_path_cl</section>\n<section class=\"wiki-section\" id=\"sec-aggregation_calibration\"><h2>Introduction</h2>\n<p>This methodology chapter articulates the procedures for claim-to-document aggregation, weighting, calibration, and the treatment of uncertainty and dispersion in a cyberattribution scoring pipeline. The discussion is procedural and methodological: it draws on numerical artifacts provided in the raw scoring bundle to illustrate how aggregation and calibration operate, but it deliberately refrains from reporting or interpreting substantive case findings. The purpose is to make explicit the logic by which discrete evidence features and provenance signals are transformed into composite claim and document scores and how those aggregates are adjusted and reported with measures of statistical uncertainty.</p>\n\n<h2>Data processing and extraction</h2>\n<p>Input processing begins with ingestion of primary documents and technical artefacts that have been extracted from the native source. Structural parsing yields artifact records (for example, CVE identifiers, hashes, domains and email addresses) each annotated with an extraction confidence value. Anchors are preserved as pointers to the original structural blocks. The pipeline represents these items in a normalized evidence model in which evidence items carry modality and feature vectors (for instance I, A, M, P, T components) and a derived probative weight. In the supplied bundle, artifacts are recorded under the normalized artifact list and evidence items reference anchors and modalities; the evidence probative weights are then summed per claim to produce an evidence_weight_aggregate value used downstream. Chain tracing metadata such as provenance, integrity signals and anchor quality are retained to permit downstream custody diagnostics and to support saturation calculations when multiple anchors or artifacts are present for the same claim.</p>\n\n<h2>References and institution inference</h2>\n<p>Reference parsing distinguishes internal anchors (textual locations inside a single report) from external citations and unique origins. The pipeline records explicit citations and computes a sources_total and citations_total for each document. Where external authoring organizations are declared, the system captures authoring_org and origin_signature fields to support institutional inference. Institutional inference is operationalised to produce domain and origin clusters that inform credibility and corroboration axes without supplanting technical evidence: domain independence and eligible_source_count metrics are derived from the normalized sources table, and single_source conditions are detected (for example via a single source count value) and trigger predefined penalty rules that affect final claim-level aggregation.</p>\n\n<h2>Scoring framework: aggregation, weighting and calibration</h2>\n<p>Claim scoring proceeds by aggregating probative weights from constituent evidence items into a claim-level evidence weight (illustrated by the evidence_weight_aggregate field). Aggregation is not a simple sum: each evidence item contributes a feature-weighted score that is adjusted for modality, anchor proximity, and documented provenance quality. The pipeline computes core vectors (for example custody, credibility, corroboration) for each claim using subcomponents such as chain_of_custody_provenance, credibility_independence and corroboration_convergence. When originating sources are not independent or when a single origin supplies multiple items, an explicit single_source penalty multiplier (illustratively 0.85 in the provided bundle) is applied to reduce overconfidence arising from nonindependent support. The aggregated output is then transformed into a normalized final_score for the claim after penalties and data contribution multipliers are applied.</p>\n<p>Statistical calibration is layered onto the aggregated scores to control for smallN variability and overfitting. The pipeline computes an effective_evidence_n that represents the information content of the aggregated chain (for example, an effective_evidence_n of 4.0 in the sample), and derives a reliability_factor that moderates shrinkage toward prior expectations. Shrinkage is implemented via lambda parameters per axis (for example custody, credibility, corroboration, clarity and confidence), which weight the observed score against a prior_scores vector. This empirical Bayes style shrinkage reduces variance when evidence is scarce or heterogeneous while preserving signal where evidence is substantive. Saturation factors are also tracked to ensure that additional items from the same provenance cluster produce diminishing marginal increases to the corroboration and custody axes rather than linear growth.</p>\n<p>Measures of uncertainty and dispersion are produced at both the claim and document level. Bootstrap resampling is used to derive 95% confidence intervals for key document aggregates such as belief_weighted_0_100 and axis averages (as encoded in bootstrap_95ci in the supplied bundle). Those intervals quantify sampling and model uncertainty under the pipeline's resampling assumptions; dispersion diagnostics (for example variance of axis scores across claims and the spread of bootstrap replicates) inform interpretation thresholds and the construction of conservative reporting gates. For instance, when a document-level belief metric exhibits a narrow bootstrap interval around a low mean, the reporting logic treats the low belief as robust to sampling variability; conversely, wide intervals trigger explicit uncertainty language in the downstream product and, where relevant, additional calibration (increased shrinkage) to avoid overconfident presentation.</p>\n\n<h2>Validation and quality assurance</h2>\n<p>Quality assurance includes procedural validation of extraction confidences and systematic audits of provenance. Chain_provenance_diagnostics fields such as context_completeness, lineage_quality and anchor_quality are used to flag chains that require human review; artifacts with low extraction confidence are excluded or downweighted in automated aggregation. Scoring calibration is validated by backtesting shrinkage parameters and reliability_factors across a labeled corpus; bootstrap_95ci results are compared to empirical dispersion observed in hold</section>\n<section class=\"wiki-section\" id=\"sec-validation_quality_assurance\"><section><h2>Introduction</h2><p>This methodology chapter describes the validation and quality assurance approach applied to the automated and semi-automated components of the cyber-attribution scoring pipeline. The description is deliberately procedural and normative: it explains the purpose and mechanics of the validation architecture, the rationale for sampling and human intervention, and the interface between automated validation gates and targeted review. Where appropriate, provenance to raw pipeline artifacts and validation outputs is indicated to demonstrate traceability of the procedural assertions.</p></section><section><h2>Data Processing and Extraction</h2><p>Data entering the validation regime originates from structured extraction artifacts produced by the pipeline; representative inputs are recorded in the pipeline output and report artifacts (for example, the extraction bundle at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.output.json and the consolidated report at /home/pantera/projects/TEIA/annotarium/outputs/reports/fireeye_rpt_apt37_02_20_2018_report.json). The validation process consumes these JSON artifacts and executes a set of deterministic checks that evaluate schema conformance, table integrity, citation presence, artifact enumeration, and grounding consistency. The rationale for these checks is to ensure structural and referential soundness prior to any substantive scoring or downstream synthesis. The validation artifact at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.validation_report.json documents the outcomes of those automated checks and provides the primary input to the QA protocol.</p></section><section><h2>References and Institution Inference</h2><p>Reference parsing and institutional inference are treated as distinct but interoperable tasks. Reference parsing extracts anchors and citation metadata from the structural parse and the artifact extraction stage, while institution inference applies rule-based and probabilistic heuristics to map reference metadata to institutional identifiers. Methodologically, the pipeline separates entity normalization from attribution scoring to avoid circular dependencies: institution inference outputs are stored alongside origin artifacts and are subject to the same validation gates as other components. Traceability is preserved by recording the provenance of reference parsing and institution inferences within the extraction bundle and by linking those outputs back to the validation report referenced above.</p></section><section><h2>Scoring Framework</h2><p>The scoring framework is agnostic to source content and operates over validated structural and evidentiary primitives. Scores are computed from component-level indicators such as schema integrity, corroboration diversity, and artifact completeness. The validation outputs inform confidence weighting within the scoring model: for example, higher scores on schema and integrity checks reduce uncertainty margins, while warnings regarding corroboration diversity or missing non-duplicative anchors trigger conservative adjustments. This separation ensures that scoring adjustments respond to measurable quality attributes rather than to substantive case conclusions. The scoring outputs are persisted with provenance pointers to the validation and extraction JSON files (for instance, the score reports in /home/pantera/projects/TEIA/annotarium/outputs/scoring/), enabling auditors to reconcile numerical adjustments against the underlying validation evidence.</p></section><section><h2>Validation and Quality Assurance</h2><p>The QA architecture comprises layered automated validation gates followed by a hybrid review regimen that combines continuous agent review with targeted human sampling. Automated validation gates execute deterministic checks on incoming extraction artifacts; these checks include schema validation, integrity verification, table and artifact counts, presence of citations, and a set of heuristics that identify potential corroboration weakness or duplicated anchor text. The outputs of those gates are materialized in a machine-readable validation report (path: /home/pantera/projects/TEIA/annotarium/outputs/pipeline/fireeye_rpt_apt37_02_20_2018/fireeye_rpt_apt37_02_20_2018.validation_report.json) and carry an overall certification indicator and category-level scores used to triage subsequent review activity.</p><p>Operational QA proceeds with enabled agent review for continuous monitoring and deterministic remediation of trivial failures; agent review operates as the first human-adjacent tier and flags artifacts for escalation when automated heuristics cannot resolve an anomaly. In parallel, a targeted human review protocol samples a defined fraction of processed items to provide an empirical estimate of residual error rates. The configured sampling fraction for targeted human review is 10% (reported as human_sample_fraction = 0.1 in the QA metadata). Results from the sampled reviews are recorded; in the sampled set for this run the observed error rate was zero (human_sample_observed_error_rate = 0.0), which is reported as no observed errors in the reviewed sample. That sampling outcome is reported alongside the automated certification (the validation bundle indicates certification = \"PASS\" and an overall_score = 92.09) and category-level diagnostics that can include warnings about repeated anchor duplication or limited corroboration diversity.</p><p>Methodologically, this hybrid approach balances scale and rigor: validation gates provide reproducible, high-throughput assurance that prevents malformed inputs from propagating, while agent review and the 10% human review sample provide an empirical check on algorithmic assumptions and capture error modes that automated rules may miss. The QA protocol records provenance to the raw payloads and score artifacts (see raw_payload_paths and raw_payload_files entries) so that any subsequent re-audit can re-run validation gates and replicate the targeted human sampling. Together, these elements form a defensible chain of custody and quality assurance strategy designed to support reproducible, transparent cyber-attribution scoring without conflating methodological controls with substantive analytic judgments.</p></section></section>\n<section class=\"wiki-section\" id=\"sec-limitations_governance\"><h2>Introduction</h2>\n<p>This section articulates the methodological limitations, governance controls, and a principled refinement roadmap for a reproducible cyberattribution scoring workflow. The objective is to describe, at a methodological level, how instrument and process fragilities are identified, controlled, and iteratively improved without asserting or adjudicating any substantive factual claims from the underlying dossier. The discussion situates the limits of automated and schemadriven processing in relation to evidentiary legibility, explains governance measures adopted to preserve auditability, and proposes a staged refinement roadmap that balances determinacy, human oversight, and measured experimentation. Methodological emphasis is placed on transparency of assumptions, explicit delimitation of scope, and mechanisms for recording decisions so that subsequent reviewers can reconstruct how particular outputs were obtained from inputs.</p>\n\n<h2>Data Processing and Extraction</h2>\n<p>Data processing begins by converting source documents into a consistent internal representation that preserves original textual anchors and metadata. Extraction prioritizes explicit signals such as named entities, timestamps, hashes, and quoted assertions while recording extraction provenance at the field level. Where automated extraction produces lowconfidence outputs, those items are flagged for targeted human review rather than being promoted into downstream scoring without adjudication. Normalization routines are applied to harmonize formats and reduce spurious variability, and all transformation steps are logged to enable reproducibility. The rationale for these controls is to limit propagation of extraction errors into inferential stages and to maintain an auditable chain from raw input to derived artifacts.</p>\n\n<h2>References and Institution Inference</h2>\n<p>Inference of institutional associations from references and contextual mentions is treated as a probabilistic, modelassisted exercise constrained by external registries and verifiable metadata. The system records which external sources were consulted and the confidence assigned to any inferred linkage, while explicitly excluding inferences that rely solely on contextual implication without corroborating anchors. Institutional inference workflows incorporate mechanisms to surface ambiguous or contradictory signals to human analysts and to prevent automated aggregation from amplifying tenuous linkages. This conservativism is intended to reduce false positive associations and to ensure that institutional attributions remain provisional until validated through adjudicative review.</p>\n\n<h2>Scoring Framework</h2>\n<p>The scoring framework is claimcentric and provenanceaware: each claim carries a set of evidentiary anchors, a gravity weight that encodes its potential significance, and a confidence metric derived from extraction quality and source credibility. Aggregation across claims applies explicit propagation rules for uncertainty and documents the mathematical form of aggregation so that score derivation is transparent. Thresholds used to trigger particular handling paths, such as human escalation or public disclosure, are configurable and justified by sensitivity analyses. All score components are accompanied by machinereadable rationales that permit downstream audits and enable alternative aggregation strategies to be applied retroactively.</p>\n\n<h2>Validation and Quality Assurance</h2>\n<p>Validation combines systematic testing with ongoing quality assurance practices. Test suites exercise extraction and scoring pipelines using seeded inputs and synthetic cases designed to probe known failure modes and edge conditions. Continuous monitoring tracks metrics indicative of extraction drift, score stability, and source quality, and periodic independent audits verify that logs and provenance records suffice to reproduce analytic outcomes. Governance controls include documented escalation paths for disputed inferences, scheduled bias and robustness assessments, and a staged rollout plan for changes that could materially affect attribution outcomes. Together these measures aim to sustain confidence in methodological soundness while enabling iterative refinement.</p></section>\n</article>"
}
