<article class="wiki-page">
<header><h1>Annotarium Methodology: APT1: Exposing One of China's Cyber Espionage Units</h1><div class="wiki-meta">Generated at 2026-02-22T21:44:27Z</div></header>
<nav class="wiki-toc"><h2>Contents</h2><ol>
<li><a href="#sec-introduction">Introduction and Epistemic Framing</a></li><li><a href="#sec-scope_units">Scope, Units of Analysis, and Output Semantics</a></li><li><a href="#sec-data_ingestion">Data Ingestion and Corpus Handling</a></li><li><a href="#sec-pdf_to_markdown">PDF-to-Markdown Conversion</a></li><li><a href="#sec-structural_parsing">Structural Parsing of Text, Tables, and Figures</a></li><li><a href="#sec-artifact_extraction">Artifact Extraction and Technical Object Normalization</a></li><li><a href="#sec-reference_parsing">Footnote and Reference Parsing</a></li><li><a href="#sec-institution_inference">Institution Inference and Source Typology</a></li><li><a href="#sec-claim_evidence_graph">Claim-Evidence Graph Construction</a></li><li><a href="#sec-scoring_overview">Scoring Framework Overview</a></li><li><a href="#sec-chain_of_custody">Chain of Custody Axis</a></li><li><a href="#sec-credibility_corroboration">Credibility Axis with Corroboration Subcomponent</a></li><li><a href="#sec-clarity_axis">Clarity Axis and State Responsibility Pathways</a></li><li><a href="#sec-aggregation_calibration">Aggregation, Calibration, and Uncertainty</a></li><li><a href="#sec-validation_quality_assurance">Validation and Quality Assurance</a></li><li><a href="#sec-limitations_governance">Limitations, Governance, and Future Refinement</a></li>
</ol></nav>
<section class="wiki-section" id="sec-introduction"><section>
  <h2>Introduction and Epistemic Framing</h2>
  <p>This methodology is presented as an evidentiary framework for cyber attribution exercised under contestable conditions. Its epistemic posture is jurisprudential: the analyst treats each attribution proposition as an evidentiary claim subject to adversarial scrutiny rather than as a standalone declarative truth. The approach is explicitly burden-sensitive—higher inferential thresholds apply where legal attribution is implicated, and lower thresholds are tolerated for exploratory or investigatory claims—so that the same underlying materials can support different normative outcomes according to the burden of proof that is appropriate to the question at hand. Structured extraction therefore precedes legal inference: only when textual, forensic, and provenance signals have been transformed into auditable, schema-constrained records does the framework permit closure toward legal reasoning.</p>

  <h2>Data Processing and Extraction</h2>
  <p>The pipeline begins with deterministic transcription of source PDFs into machine-readable markdown, as reflected in the operational metadata (pdf_to_markdown_primary: process_pdf_mistral_ocr.py; fallback: PyMuPDF4LLM). This step is followed by structural parsing that separates metadata, claim objects, artifact indices, tables, and figure anchors. Artifact extraction produces a bounded index of artifacts with explicit anchors that preserve the text span or image location that motivated each artifact entry. The rationale for strict separation is evidentiary legibility: by preserving discrete artifact identifiers and anchors we prevent retrospective or implicit reconstructions of support that would otherwise render evidentiary weight opaque or contestable.</p>

  <p>Downstream, a claim–evidence graph is constructed. Claims are nodes with gravity weights; evidence items and source records are attached as distinct objects with explicit links. Chain-of-custody variables are extracted from artifact text where present: provenance markers, integrity markers, temporal anchors, artifact identifiers, and version lineage. These variables are normalized to bounded scales to permit auditable combination at the claim level.</p>

  <h2>Reference Parsing and Institution Inference</h2>
  <p>Reference parsing produces a source registry in which citations, footnotes, and inline attributions are resolved to canonical source records where possible. Table and image anchors emitted during markdown parsing are linked into the same registry so that non-textual artifacts participate in the evidentiary network. Institution inference is executed as a distinct stage (infer_source_institutions.py in the pipeline metadata, using gpt-5-mini with optional web fallback). Inference outputs include resolved institution identifiers, confidence scores, and provenance for the inference itself. The methodology prescribes exclusion rules for certain classes of support from top-tier credibility: internal/automatic telemetry and unverified news reports are flagged and excluded from maximal credibility contribution.</p>

  <h2>Scoring Framework</h2>
  <p>Scoring proceeds on three core axes—Chain of Custody, Credibility, and Clarity—together with explicit corroboration sub-calculations. Item-level probative force is evaluated along five bounded dimensions: independence, authentication/provenance, methodological soundness, procedural testing, and contemporaneity. These dimensions are combined multiplicatively so that critical failures in any single dimension constrain overall item force. Corroboration is computed by clustering evidence by origin and aggregating with diminishing returns to enforce anti-circularity: repeated downstream citations of a single upstream source do not inflate independent support.</p>

  <p>Credibility integrates source-quality metrics (source type, strongest source attached to a claim, source diversity, domain independence) with corroborative convergence. A document-level coverage calibration is applied so that credibility reflects weighted coverage across the claim set rather than isolated high-quality support for a small subset of claims. Clarity is operationalized as legal intelligibility: the score evaluates whether act–actor–link specificity and the proposed mode of responsibility are stated with sufficient precision to support state-responsibility reasoning.</p>

  <h2>Validation and Quality Assurance</h2>
  <p>Integrity controls are enforced at multiple gates. Runs fail if identifier collisions occur, artifact anchors are missing, citation pathways cannot be traced, or referenced entities resolve to nonexistent records. Deterministic transformations are preferred where possible and all intermediate artifacts are persisted to disk to permit reproducibility and contestation. Post-run verification includes schema validation</section>
<section class="wiki-section" id="sec-scope_units"><p>Introduction: This methodology section describes the framework and procedures used to produce attribution-relevant scores at the claim, source, artifact, evidence item, and document-level. The description that follows is methodological and normative: it explains the units of analysis, the data processing pipeline, the inference steps for references and institutional links, the scoring schema, and validation and quality-assurance measures. Where the supplied raw data excerpt is invoked, it is cited only to illustrate method behavior and calibration; no case-specific substantive findings from the underlying report are asserted or interpreted here.</p>

<p>Scope, Units of Analysis, and Output Semantics: The corpus of inputs is treated through five nested units of analysis. A claim denotes a discrete asserted proposition extracted from a document. A source denotes an originator or publication entity referenced or cited by a document; a source may itself be a document or an external repository. An artifact denotes a technical object (for example a file, hash, malware sample, log snippet, or code fragment) that is amenable to forensic analysis. An evidence item denotes any discrete datum, observation, or measurement that supports or undermines a claim; evidence items may be linked to one or more artifacts. The document-level unit denotes the encompassing report or file from which claims, sources, artifacts, and evidence items are derived and for which aggregate scores are produced.</p>

<p>Each output score is defined with explicit semantic bounds. Claim-level scores quantify properties of the extracted proposition: grounding measures the degree to which the claim is linked to identifiable evidence items and artifacts; clarity measures the linguistic and conceptual precision of the claim as extracted; custody measures the documented provenance and handling of the supporting evidence items; credibility measures the assessed trustworthiness of cited sources and provenance metadata; and belief (or confidence) synthesizes these axes into a posterior assessment of the claim’s inferential weight. Document-level scores aggregate across claims and their supporting evidence to reflect the overall state of evidence and provenance for a report without implying categorical truth of any individual allegation. Importantly, a high score on any axis does not constitute legal proof or a definitive attribution; it is an index that describes the quality of evidence and reporting practices according to the defined metrics.</p>

<p>Data Ingestion: Ingested items originate from file-based inputs and parsed metadata. For the supplied raw_data excerpt, the document_metadata fields (for example title: "APT1: Exposing One of China's Cyber Espionage Units", authoring_entity: "Mandiant", and inferred publication_date) and the input_format marker (markdown) are used to seed downstream parsing parameters. Pipeline-level counters such as pages, claims, sources, and artifacts are recorded at ingestion and are used to configure batching, resource allocation, and sampling for subsequent validation steps. These counts are also used to demonstrate how aggregation weights operate when calibrating document-level aggregates versus individual claim-level assessments.</p>

<p>PDF to Markdown and Structural Parsing: Where source files are native PDFs, a deterministic conversion to markdown or an intermediate structured representation is applied. Structural parsing identifies headings, numbered lists, captions, tables, and inline citations to delimit candidate claims and locate artifacts and evidence items. The parsing stage preserves location provenance (page and positional indices) so that custody and reproducibility can be assessed. In the supplied excerpt the input_format is marked as markdown and a single sample page was observed; this informs our expectation of structural density for automated claim extraction and the degree of manual review required.</p>

<p>Artifact Extraction: Artifact extraction isolates technical objects embedded in or referenced by the document. Extraction prioritizes machine-identifiable items such as hashes, filenames, network indicators, code fragments, and binary blobs. Each extracted artifact is assigned a unique identifier, a provenance record referencing the document-level location, and initial feature metadata (type, format, size). These artifact records become primary anchors for linking evidence items and for later corroboration across sources.</p>

<p>Reference Parsing: Reference parsing detects explicit citations, footnotes, URLs, and any textual pointers to external materials. The parser distinguishes between inline references that constitute primary provenance versus rhetorical or contextual mentions. Where no formal citation structure exists, heuristics augmented by manual review flag candidate sources. The supplied pipeline_counts showing a single source and zero formal citations in the excerpt illustrate how the parser treats sparse citation environments when generating source-linking hypotheses.</p>

<p>Institution Inference: Institution inference maps named entities and references to canonical institutional identifiers. This step uses conservative matching and a confidence score to capture ambiguity: when an authoring_entity appears (for example the document_metadata authoring_entity), it is recorded with its asserted role but accompanied by an inference confidence reflecting normalization uncertainty. Institution inference is explicitly separated from attribution judgments; it supports credibility scoring by enabling cross-document corroboration while preserving uncertainty about identity resolution.</p>

<p>Claim–Evidence Graph: Extracted claims, evidence items, artifacts, and sources are represented as nodes in a directed claim–evidence graph. Edges carry typed relationships (for example "supports", "refers-to", "derived-from", "cites") and include provenance metadata (document-level location, extraction method). This graph provides the structural substrate for grounding and corroboration metrics: grounding favors claims with short provenance paths to verifiable artifacts, while corroboration favors claims connected to independent sources and artifacts.</p>

<p>Scoring Overview: Scoring is decomposed into orthogonal axes designed to be interpretable and testable. Grounding measures structural linkage between claim and evidence items; custody records documented handling and exposure of artifacts; credibility evaluates source provenance and institutional inference; corroboration measures independent confirmatory links across sources or artifacts; clarity assesses extraction and linguistic precision; and belief synthesizes these axes via a transparent aggregation function. The supplied document_scores_v4 and claim_score_preview_v4 were used as calibration examples: for instance, a low coverage of citations in a supplied excerpt is treated as an indicator for reduced credibility and corroboration weights, which modifies aggregated document-level belief without implying substantive conclusions about any claim’s truth.</p>

<p>Chain of Custody: Chain-of-custody protocols are captured as structured provenance metadata attached to artifacts and evidence items. Each custody record includes source document identifiers, extraction timestamps, and handling annotations (manual review, transformation steps). Custody metadata directly influences the custody axis score and thereby affects downstream belief synthesis. The methodology mandates retention of immutable provenance logs sufficient to reproduce score computations and to support audit requests.</p>

<p>Credibility and Corroboration: Credibility assessment combines institution inference, source provenance, and citation coverage. Corroboration is measured by counting independent linkage patterns in the claim–evidence graph while accounting for shared-origin bias; links that converge on a single source or artifact receive diminishing marginal corroboration. Where the supplied excerpt shows zero formal citations and a single source, the methodology treats corroboration as limited and reflects that constraint in score computation without inferring substantive attribution.</p>

<p>Clarity Axis: The clarity axis operationalizes linguistic and extraction quality. It captures ambiguity, hedging, and</section>
<section class="wiki-section" id="sec-data_ingestion"><div><h2>Introduction</h2><p>This methodology section describes deterministic procedures for ingesting a single report into an analysis pipeline intended to support cyber‑attribution scoring. The objective is to record a reproducible chain from the original input corpus items through intermediate artefacts to score inputs and final outputs, preserving provenance metadata and enabling third‑party re‑execution. Where the term input corpus is used, it denotes the set of source documents accepted for processing (in this instance the specified PDF and its accompanying metadata), together with any ancillary files produced during automated extraction.</p></div><div><h2>Data Processing and Extraction</h2><p>The ingestion workflow begins with a canonical file placed under versioned storage and referenced by an immutable path and metadata record (for example, the report manifest entries that include the original PDF path and report metadata such as report_id and generated_at_utc). Processing proceeds via a primary conversion stage that converts PDFs to an intermediate textual representation using a provider‑backed OCR/markdown conversion (documented here as the pdf_to_markdown_primary method). A deterministic fallback conversion using a self‑hosted engine is employed when the primary provider fails or times out. Each conversion invocation writes a named intermediate artifact to a reproducible output location (for example, the pipeline markdown and raw_extraction JSON). To enable strict reproducibility, every produced artifact is accompanied by a cryptographic checksum, a recorded tool invocation manifest (tool name, version, command line or parameters, model identifiers where applicable), and a timestamped log capturing non‑idempotent events such as provider timeouts. This practice ensures that rerunning the same pipeline with the same input corpus and the same toolset versions will produce byte‑comparable intermediate artifacts when external non‑determinism is bounded or explicitly captured.</p></div><div><h2>References and Institution Inference</h2><p>Structural parsing and reference extraction are implemented as staged operations that operate on the intermediate markdown and parsed tables/images. The reference parsing stage identifies citations, footnotes and bibliographic markers and emits a linked registry that points back to documented source offsets in the intermediate text artifact. Institution inference is performed as a deterministic downstream task that consumes the parsed reference registry and artifact indices; a deployed inference tool (described as institution_inference) uses a named model with its version recorded in the execution manifest and an optional web‑backstop lookup when local evidence is insufficient. Outputs from institution inference are stored in a stable schema and referenced by path in the scoring input bundle. Rationale for this separation is to preserve an auditable mapping from textual evidence to inferred institutional attributions while avoiding conflation of extraction heuristics and inference heuristics; this supports independent reanalysis of either stage without repeating the other.</p></div><div><h2>Scoring Framework</h2><p>The scoring framework accepts a normalized score_input artifact that aggregates parsed claims, extracted artefacts, reference links and institution inference outputs. Scoring is implemented as a rule and evidence‑weight engine whose operations are fully specified in a versioned scoring schema. Each score run records the scoring tool version, parameterization, and a deterministic seed where stochastic components are used; these records are persisted alongside full score outputs. Multiple score output versions may exist for the same input corpus to reflect schema evolution; each versioned score file is retained with provenance metadata to permit longitudinal comparison. Counts and intermediate metric values are used only to illustrate algorithmic behavior and are emitted as part of the score bundle; they are not treated here as substantive assertions about the underlying report.</p></div><div><h2>Validation and Quality Assurance</h2><p>Quality assurance is achieved through a layered validation approach. First, schema validation ensures that every artifact—raw extraction, reference registry, inference output, and score bundle—conforms to a declared JSON schema with clearly specified required fields. Second, integrity checks verify that expected artifacts exist at prescribed paths and match their recorded checksums. Third, automated validation reports summarize parsing completeness, missing fields, and deviations from deterministic expectations; these are captured in a dedicated validation_report artifact. Fourth, an analyst review step is used to inspect nonconforming cases and to annotate provenance metadata where human judgment is required. All validation actions and adjudications are logged to preserve an auditable chain of custody that supports reproducibility and independent verification.</p></div><div><p>Across these stages, the methodological rationale emphasizes deterministic handling of files, explicit recording of tool and model versions, and persistence of intermediate artifacts and checksums so that the entire ingest→parse→infer→score pipeline can be reproduced from the declared input corpus and execution manifests. The documented file paths and artifact names are retained without alteration to support archival retrieval and independent re‑execution of the pipeline for verification or methodological research.</p></div></section>
<section class="wiki-section" id="sec-pdf_to_markdown"><section>
  <h2>Introduction</h2>
  <p>This methodology chapter describes the pipeline and procedural rationale used to convert archival report artifacts into structured inputs for a cyber‑attribution scoring system. The account emphasizes reproducibility, chain of custody, and methodological resilience rather than adjudicating substantive assertions contained in any particular report. Procedures are presented as generalizable stages—ingestion, conversion, structural parsing, reference linking and institutional inference, evidence graph construction, scoring, and validation—each grounded in the concrete processing artefacts produced by the pipeline to ensure traceability and auditability.</p>
</section>

<section>
  <h2>Data processing and extraction</h2>
  <p>The ingestion stage establishes an immutable raw payload registry and records provenance metadata for each input file. For the present workflow the registry contains the original portable document format and derived extraction artifacts (for example, the PDF at /home/pantera/projects/TEIA/annotarium/Reports/Mandiant - 2017 - APT28 At the Center of the Storm.pdf and the primary pipeline output at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/mandiant_2017_apt28_at_the_center_of_the_storm/mandiant_2017_apt28_at_the_center_of_the_storm.output.json). File-level metadata (size, existence, and recorded path) are captured and retained in the report manifest. A small set of counts is recorded as operational diagnostics—pages, claims, sources, artifacts and tabular elements—and is used only to illustrate expected pipeline behaviour (for example, a single‑page input or the presence of multiple tables) rather than to interpret substantive content.</p>

  <p>Structural parsing follows ingestion: stage‑1 markdown parsing emits a stable markdown rendering and anchors for tables and figures so that downstream processors can address items deterministically. The pipeline stores a markdown representation (noted in the raw payload as .../mandiant_2017_apt28_at_the_center_of_the_storm.md) and an artifact index produced during schema extraction; these derived artifacts form the canonical inputs to subsequent extraction modules.</p>
</section>

<section>
  <h2>PDF-to-Markdown conversion</h2>
  <p>The conversion from PDF to markdown is implemented with a two‑tier approach to ensure operational resilience. The primary conversion path leverages a Mistral‑based, provider‑backed OCR and conversion component (documented in the pipeline as process_pdf_mistral_ocr.py). When provider‑backed conversion completes, the output is normalized into a markdown representation; table rows and figure anchors are preserved to enable reliable addressing. For occasions where the provider conversion fails, times out, or otherwise becomes unavailable, the pipeline invokes an offline fallback conversion using a PyMuPDF4LLM based extractor. This offline fallback is explicitly maintained as a posture of resilience rather than a conceptual change to the conversion model: both paths aim to produce the same canonical markdown schema and artifact anchors so that downstream stages operate identically irrespective of which conversion engine executed.</p>

  <p>Operationally, the conversion stage writes the markdown output to a documented path (see the markdown entry in the raw payload paths) and records which engine produced the output in the manifest. This promotes reproducibility and makes the conversion decision auditable: analysts can verify whether the Mistral provider or the offline fallback produced the markdown and can re-run either converter if a quality‑assurance process requires it.</p>
</section>

<section>
  <h2>References and institution inference</h2>
  <p>Reference parsing extracts citations, footnote‑like references, and in‑text source markers from the normalized markdown and associated tables. The parser emits a linked registry of cited materials and ties those registry entries to the artifact indices created during schema extraction. Reference parsing is implemented to tolerate a range of citation styles and to preserve linkability to the original page anchors so that any extracted reference can be traced back to the original text span and its markdown anchor.</p>

  <p>Institution inference is separate but interdependent: an inference module (implemented in infer_source_institutions.py and documented to use a gpt‑5‑mini model with an optional web fallback) proposes canonical institutional attributions for extracted references and artifacts. The design treats such inferences as probabilistic annotations rather than dispositive judgments; every inferred institution is recorded with a provenance tag identifying the inference model, the input artifact, and any web evidence used. This enables later re‑evaluation, corrective updates, or replacement of the inference model without altering earlier structural extractions.</p>
</section>

<section>
  <h2>Scoring framework</h2>
  <p>The scoring framework consumes the artifact indices, reference registry, and institutional inferences to compute structured attribution scores. Scores are derived from a transparent rubric that maps evidence types, source reliability, and corroboration patterns onto numerical and categorical outputs. For operational transparency the pipeline persists scoring inputs (for example, the score_input_v3.json) and final score artifacts (for example, full score reports and versioned score outputs) in a versioned scoring directory so that each score can be recomputed from the recorded inputs and scoring policy version.</p>

  <p>Calibration metadata—including score version identifiers and scoring policy references—are embedded alongside each produced score file. This enables auditors to determine which policy rules produced a given score, to reproduce the calculation, and to compare outcomes across scoring policy revisions without ambiguity.</p>
</section>

<section>
  <h2>Validation and quality assurance</h2>
  <p>Validation is implemented as a multi‑layered process addressing both syntactic correctness and semantic plausibility. Syntactic checks verify file existence, schema conformance of the markdown and artifact indices, and the presence of expected fields in the scoring input. Semantic checks examine cross‑references between artifact indices and the reference registry, confirm that institution inferences include provenance tags, and validate that scoring inputs were generated from a coherent set of artifacts. The pipeline emits a formal validation report (for example, the validation_report_json path) that records pass/fail outcomes for each test and the file paths implicated in any failure.</p>

  <p>Quality assurance also includes reprocessing capability and provenance preservation. All intermediate artifacts and raw inputs are retained (see the manifest entries for raw_extraction</section>
<section class="wiki-section" id="sec-structural_parsing"><h2>Introduction</h2>
<p>This methodology describes a reproducible approach to extracting and structuring evidentiary content from vendor reports for the purposes of cyber-attribution scoring. The procedures are informed by the document-level metadata and pipeline diagnostics present in the supplied raw data, including the report title and provenance markers (for example, document_metadata.title and document_metadata.authoring_entity), recorded page and artifact counts, and available payload file paths. The methodological rationale focuses on preserving provenance and enabling auditability through anchor-aware structural representations rather than on the substantive content of any particular claim or attribution asserted within the source material.</p>

<h2>Data Processing and Extraction</h2>
<p>The ingestion pipeline begins with canonicalization of the source file and the capture of provenance records. For the supplied corpus, provenance fields such as document_metadata.publication_date_anchor (anchor_id P000-A999) and source_locator.source_value were used to register the origin and extraction context of the report. The pipeline records basic counts—pages, claims, sources, artifacts, tables, and figures—as metadata to guide subsequent parsing (for example, the pipeline_counts entry reporting one page, ten claims, three artifacts, and seven tables). These counts are not substantive findings but operational signals indicating where structural parsing effort must be concentrated.</p>

<p>Textual extraction is performed at the block level: contiguous runs of rendered text are captured as discrete text blocks with explicit anchors that indicate their original location (page index, section heading, and object identifier where available). Tables are extracted by preserving their two-dimensional grid structure: cell boundaries, row and column indices, header rows, and any merged cells are retained in the extraction representation. Figures and images are captured as distinct artifact objects with file references, bounding-box coordinates, and contextual caption text when present. Every extracted object—text block, table, or figure—is assigned an anchor token that resolves to the original source locator and finer-grain location information to facilitate deterministic traceability back to the source PDF or markdown file.</p>

<h2>References and Institution Inference</h2>
<p>Reference parsing and institution inference follow from structural parsing. Citations and reference strings are extracted from the anchored text blocks and normalized into a machine-readable reference index; when conventional citation markers are absent, contextual anchors (proximity of named entities to section headings or table captions) are used to disambiguate referents. Institution inference is framed as a provenance-linked attribution: candidate institutions or actor labels are derived from named-entity recognition operating on anchored text spans, and each inference is associated with a confidence value and the anchors of the underlying evidence. This anchor-centric linking ensures that any institutional inference can be audited by examining the exact text, table cell, or figure caption that produced the inference.</p>

<h2>Scoring Framework</h2>
<p>The scoring framework for attribution applies calibrated evidence classes to anchored extractions rather than to decontextualized text fragments. Evidence classes include direct technical artifacts (file hashes, IOC strings), structural evidence (tables enumerating incident records), and contextual assertions (narrative claims); each class has a defined set of admissibility criteria that reference the extraction anchors and the original provenance fields. Scoring therefore operates over the evidence graph composed of anchored nodes: tables contribute structured rows that can increment or weight scores according to schema-validated columns; figures contribute supporting artifacts whose existence and caption anchors increase evidentiary linkage; while text blocks supply assertions whose scope is delimited by their anchors. Use of counts from the pipeline_counts metadata (for example, the number of tables) is limited to tuning selection thresholds and illustrating how the framework treats structured versus unstructured inputs.</p>

<h2>Validation and Quality Assurance</h2>
<p>Quality assurance procedures emphasize anchor-preserving checks and chain-of-custody logging. Validation steps include verifying that each extracted object maps to a non-ambiguous anchor (page index and object id), that table cell coordinates reproduce the visual grid when rendered, and that figure artifacts link to their caption text. Automated validation reports (see payload path entries such as validation_report_json) are supplemented with manual spot checks: auditors reconcile a sample of extracted anchors against the original PDF or markdown representation using the recorded source_locator.path. All transformations and normalization steps record their inputs and outputs with anchor metadata so that any downstream score or institution inference can be re-evaluated against the primary source for auditability.</p>

<p>In summary, the methodology prioritizes structured, anchor-preserving extraction of text blocks, tables, and figures to ensure auditability and reproducibility. By binding every inference and score to explicit anchors and provenance records, the process enables third-party verification, constrained reanalysis, and documented chains of custody without relying on decontextualized summaries of the source material.</p></section>
<section class="wiki-section" id="sec-artifact_extraction"><h2>Introduction</h2>
<p>This methodology chapter describes procedures for artifact extraction and technical object normalization within a cyber-attribution scoring framework. The account that follows sets out the rationale and operational steps by which heterogeneous signals are ingested from a document corpus, converted into machine-actionable representations, and assessed against metrics that inform provenance and custody judgments. The presentation is intentionally methodological and refrains from adjudicating any particular attribution claim; rather, it establishes the treatment of artifacts and the evaluation logic that support downstream scoring.</p>

<h2>Scope and Units of Analysis</h2>
<p>The methodological scope encompasses discrete technical objects extracted from source documents across multiple modalities, including native text, tabular extractions, figures and images, and embedded payloads in document containers. Units of analysis are defined as normalized technical objects—canonicalized identifiers such as vulnerability labels, domain names, and electronic contact references—together with contextual metadata that describe their source span, document anchor, and extraction modality. The normalization of these technical objects is necessary to ensure consistent semantic indexing and to permit linkage across heterogeneous representations.</p>

<h2>Data Ingestion</h2>
<p>Ingestion begins with raw documents entering a multi-stage pipeline that preserves provenance metadata at each handoff. Source-level attributes include original file identifier, ingestion timestamp, processing branch, and the provider or fallback mechanism responsible for conversion. In the supplied raw data, artifact_type_counts and raw_artifacts_preview illustrate that the pipeline anticipates multiple artifact classes and maintains counts per class to assist in downstream coverage assessment; such counts function illustratively to shape extraction heuristics rather than to determine substantive claims.</p>

<h2>PDF-to-Markdown Conversion</h2>
<p>Conversion of binary and scanned documents to a text-centric intermediate format is achieved by a tiered approach. A provider-backed OCR and conversion path is the primary mechanism, with an explicit fallback to an offline engine when the primary conversion fails or times out. The use of both a provider-backed conversion step and a deterministic offline converter ensures that modality-specific artifacts—for example, text recovered from images, PDFs with complex layout, or embedded tables—are captured with a documented processing provenance tag that records which converter produced the text. This provenance tag is carried forward and informs confidence scoring within custody evaluations.</p>

<h2>Structural Parsing</h2>
<p>Following conversion, the intermediate markdown representation undergoes structural parsing to emit tables, figures, and anchor-aware fragments. Structural parsing isolates syntactic constructs such as citation blocks, footnotes, and tabular cells and appends location anchors for each fragment. These anchors enable precise mapping from a normalized technical object back to the original byte-range or image region, a necessary capability for later validation and chain-of-custody reconstruction.</p>

<h2>Artifact Extraction and Technical Object Normalization</h2>
<p>Artifact extraction operates on the outputs of structural parsing to identify occurrences of technical objects across modalities. Extraction combines pattern-based recognition with schema-driven post-processing to produce artifact indices that reference the normalized form of each object, the extraction modality (text, table cell, image-derived OCR), and the structural anchor. Normalization standardizes representations into canonical forms so that semantically equivalent items are merged: for example, token variants, superficial formatting differences, and OCR-induced character substitutions are reconciled against authoritative patterns. The normalization step thus converts heterogeneous mentions into a common namespace of technical objects that supports cross-document linkage, frequency analysis, and co-occurrence graph construction. By recording the modality of origin for each normalized object, analysts can assess modality-specific error modes and weigh contributions to provenance and custody judgments accordingly.</p>

<h2>Reference Parsing</h2>
<p>Reference parsing extracts citations, footnote-like references, and other bibliographic markers and links them to a source registry. The registry records the parsed reference string, match confidence, and any resolved canonical identifier. Reference parsing is designed to support assertions about external support and source lineage without presuming the truth of referenced content. It thus produces structured pointers that feed into corroboration routines and that are themselves subject to normalization and provenance tagging.</p>

<h2>Institution Inference</h2>
<p>Where explicit institutional identifiers are absent or ambiguous, an institution inference component synthesizes likely source affiliations using model-assisted heuristics and optional web-based resolution. The inference process attaches an evidential provenance trail to each inferred institution, documenting the inputs (document text spans, resolved references, or external lookups) and the model version used. Institution inference is therefore an inference with its own confidence attributes rather than an authoritative assertion, and these attributes are incorporated into subsequent credibility and custody computations.</p>

<h2>Claim–Evidence Graph Construction</h2>
<p>Normalized technical objects, parsed references, and inferred institutions are woven into a claim–evidence graph that represents assertions, supporting artifacts, and provenance edges. Nodes encode the normalized technical object and the modality-specific anchors; edges encode relation types such as ‘‘supports,’’ ‘‘cites,’’ or ‘‘co-occurs with,’’ each annotated with modality and extraction confidence. The graph facilitates automated traversals for corroboration queries and enables transparent inspection of how any given claim is assembled from source artifacts and structural parsing outputs.</p>

<h2>Scoring Overview</h2>
<p>The scoring framework leverages object-level provenance, modality-aware extraction confidence, and reference linkage to compute composite metrics for custody, corroboration, clarity, and overall confidence. Scores are calibrated to penalize single-modality dependence and to reward multi-modal, independently sourced corroboration. The document_scores_v4 values in the supplied metadata exemplify how custody and clarity dimensions are represented numerically to inform further validation; such metrics are used diagnostically to drive quality-assurance workflows rather than to substitute for human adjudication.</p>

<h2>Chain of Custody and Provenance Evaluation</h2>
<p>Chain-of-custody procedures document every automated and manual transformation that a technical object experiences from ingestion to scoring. Each normalized technical object record retains a provenance trail recording the converter used, parsing stage, structural anchor, and any post-hoc normalization corrections. This provenance record supports reproducibility and enables targeted re-extraction or contested-evidence review. Evaluations of custody employ modality-specific error models and the recorded converter provenance to estimate the likelihood that an extracted object faithfully represents the original source content.</p>

<h2>Credibility and Corroboration Procedures</h2>
<p>Credibility assessments combine source-level attributes, reference resolution outcomes, and corroboration topology from the claim–evidence graph. Corroboration is operationalized as independent support across orthogonal modalities or distinct source registries and is distinguished from mere repetition within a single document. Where reference parsing yields no resolvable external sources, credibility and corroboration dimensions are flagged for low support and for prioritized review. The scoring architecture therefore treats corroboration as a structural property of the evidence graph rather than as an a priori attribute of any individual artifact.</p>

<h2>Clarity Axis and Presentation</h2>
<p>Clarity is measured with respect to the explicitness of object annotations, the stability of normalization mappings, and the interpretability of provenance metadata. Objects with stable normalization and unambiguous anchoring score higher on the clarity axis, which in turn affects downstream weighting in composite metrics. Presentation of normalized technical objects to human reviewers emphasizes anchorable excerpts and the provenance trail to permit rapid assessment of extraction reliability.</p>

<h2>Aggregation and Calibration</h2>
<p>Aggregation rules for object-level scores follow explicated policies that account for modality heterogeneity, reference coverage, and inferred institution confidence. Calibration routines use bootstrap and resampling methods on held-out data to estimate confidence intervals for composite metrics; the supplied bootstrap_95ci elements illustrate the intended form of such uncertainty quantification. Calibration is iteratively updated as conversion or parsing models change, and model versioning is recorded in provenance metadata to preserve temporal comparability.</p>

<h2>Validation and Quality Assurance</</section>
<section class="wiki-section" id="sec-reference_parsing"><h3>Introduction</h3>
<p>This methodology chapter describes a structured approach to converting rhetorical citations and footnote-like references embedded in forensic and intelligence documents into an analyzable source graph suitable for cyber-attribution scoring. It is authored to align with legal-academic standards of transparency and reproducibility. The chapter draws on the pipeline metadata and source registry provided for the processed document set and explains why each step is needed to move from human-oriented citation practices to machine-actionable provenance and linkage.</p>

<h3>Scope and Units of Analysis</h3>
<p>The operational units of analysis are pages, claims, artifacts, citations, tables, and identified source records as represented in the pipeline registry. The supplied pipeline summary indicates discrete counts for these units and a parsed source preview that identifies indexed source records. These units determine the granularity at which footnote parsing and citation linkage operate: page- and claim-level anchoring for rhetorical statements, artifact-level extraction for material indicators, and source-record resolution for provenance.</p>

<h3>Data Ingestion</h3>
<p>Ingestion begins with raw document files and accompanying registry entries. The pipeline metadata documents the primary and fallback conversion tools used and enumerates the assets emitted by the ingestion stage. Ingestion outputs are preserved alongside original files and a manifest that records processing tool versions, timestamps, and any failures. This preserved provenance supports subsequent chain-of-custody and validation steps.</p>

<h3>PDF-to-Markdown Conversion and OCR</h3>
<p>The pipeline employs a primary OCR/conversion path and a defined fallback for resilience. Where provider-backed conversion is used, the conversion tool name and configuration are recorded; where provider conversion fails, an offline conversion is invoked. The conversion step must emit structured markdown with anchors for tables and figures so that downstream structural parsers can reliably locate in-text references and footnote markers within a stable coordinate system.</p>

<h3>Structural Parsing</h3>
<p>Structural parsing aligns the markdown representation with logical document structures such as headings, paragraphs, footnotes, endnotes, inline parenthetical citations, and tabular references. The parser annotates tokens that represent candidate citation markers and records their span and syntactic context. This step is crucial for distinguishing rhetorical usages of bracketed numbers or parenthetical text from true source citations, particularly in complex legal or technical prose.</p>

<h3>Artifact Extraction</h3>
<p>Artifact extraction produces indexed records for tables, figures, code snippets, hashes, URLs, and other observables. Each artifact index entry includes the artifact type, its originating page/span, and any proximate citation markers. By associating artifact entries with nearby citation tokens, the pipeline creates the initial edges that later become part of the claim–evidence graph.</p>

<h3>Reference and Footnote Parsing</h3>
<p>The reference parsing stage normalizes footnote markers and inline citations into canonical citation objects. Footnote markers are resolved by mapping superscript or bracketed indices to corresponding footnote text blocks and to any bibliographic entries in a reference section. Inline parentheticals and textual attributions are tokenized and classified by citation intent (attribution, supporting evidence, methodological note). Citation linkage then proceeds by matching these canonical citation objects against the source registry and the external bibliographic corpus using string normalization, DOI/URL extraction, and heuristics for publisher or agency name variants. The resulting mappings attach each rhetorical citation to a source record and record confidence scores and matching provenance. This canonicalization enables the construction of a navigable source graph where nodes represent source records, artifacts, and claims, and edges represent citation linkage and provenance. Throughout, the parser retains both the original footnote text and the normalized linkage to support human review and legal defensibility.</p>

<h3>Institution Inference</h3>
<p>Institution inference assigns institutional ownership or affiliation to source records when not explicitly stated. The pipeline records the inference model and fallbacks used and captures confidence measures for each inferred affiliation. Institutional inference is treated as an evidentiary transformation: inferred attributions are recorded separately from explicit publisher metadata, and provenance traces include the model version and any web-fallback resolution attempts. This separation preserves the distinction between asserted institutional authorship and algorithmically inferred institution labels.</p>

<h3>Claim–Evidence Graph Construction</h3>
<p>Claims extracted from the text are represented as nodes that link to artifact nodes and to canonicalized source nodes via citation linkage edges. Each edge is annotated with context (page, paragraph, sentence), citation type (footnote, inline, table caption), and a confidence score derived from parsing certainty and source-match quality. The topology of the resulting source graph supports queries that traverse from a claim to its supporting artifacts and to the original source records, thereby enabling systematic analysis of evidence provenance and multiplicity of independent attestations.</p>

<h3>Scoring Overview</h3>
<p>The scoring framework uses the claim–evidence graph to compute attribution-relevant metrics such as source independence, corroboration count, temporal sequencing of citations, and the strength of artifact linkage. Scores combine quantitative measures (e.g., number of distinct independent sources citing an artifact) with qualitative provenance flags (e.g., explicit footnote vs. inferred attribution). The design philosophy is to separate score components so that individual inputs and transformations can be audited and adjusted without opaque aggregation.</p>

<h3>Chain of Custody</h3>
<p>Every transformation from ingestion through final scoring records provenance metadata, including processing tool identifiers (for example, the named OCR conversion and the fallback engine), timestamps, and operator or system-generated signatures. Chain-of-custody records are bound to the source graph so that each edge and node can be traced to the exact document span and conversion instance that produced it.</p>

<h3>Credibility Corroboration</h3>
<p>Credibility assessment integrates source-level metadata (publisher, date, explicit authorship), institution inference outputs, and cross-source corroboration patterns. The methodology favors corroborative structure in the source graph—convergent independent linkage to the same artifact increases credibility metrics, while reliance on a single originating source or on inferred attributions without explicit citations reduces them. All credibility adjustments carry provenance and rationale tags for transparency.</p>

<h3>Clarity Axis</h3>
<p>The clarity axis documents how explicitly a claim is cited: whether a footnote directly identifies a source, a parenthetical hints at an attribution, or a table caption references an external report. Clarity scores are used both to weight edges in the source graph and to triage items for human review when automatic disambiguation yields low confidence.</p>

<h3>Aggregation and Calibration</h3>
<p>Aggregation rules define how individual edges and node metrics combine into system-level scores. Calibration relies on held-out validation data and simulated perturb</section>
<section class="wiki-section" id="sec-institution_inference"><section>
  <h3>Introduction</h3>
  <p>This methodology documents a structured approach to institution inference and source typology within a cyber-attribution scoring pipeline. It situates institution inference as a discrete analytic operation that informs downstream processes including credibility weighting and corroboration eligibility. The presentation below follows a fixed roadmap from scope and ingestion through validation and governance, describing methods, rationale, and the interaction of structural parsing with institution-level classification while avoiding exposition of any case-specific factual findings.</p>

  <h3>Scope and Units of Analysis</h3>
  <p>The unit of analysis is the ingestible source object as produced by document ingestion and structural parsing. A source object contains metadata fields (title, publication venue, year, entity_name), extracted artifacts (tables, figures, IOCs), and parsed references. In practice this pipeline may encounter heterogeneous source_type values; the previewed dataset illustrates a predominance of materials categorized as internal_document_section, which is treated as a discrete typology for classification and weighting purposes.</p>

  <h3>Data Ingestion</h3>
  <p>Ingestion preserves original file provenance and records of transformation. The pipeline records provider and fallback conversion methods, timestamps, and environment identifiers to enable chain-of-custody reasoning. Capturing the conversion method is necessary because conversion artifacts can influence downstream institution inference; for example, OCR-induced textual distortions are recorded alongside textual extracts to inform uncertainty modelling.</p>

  <h3>PDF-to-Markdown Conversion</h3>
  <p>The primary conversion step uses a provider-backed OCR and conversion script identified in the raw pipeline methods as process_pdf_mistral_ocr.py; an offline fallback (PyMuPDF4LLM) is invoked upon provider failure. The methodology treats both outputs as first-class but flags fallback runs so that subsequent parsers can apply conservative text-normalization thresholds when text quality is reduced. Metadata about conversion software versions and runtime environment are retained for reproducibility and for assessing systematic biases introduced by specific converters.</p>

  <h3>Structural Parsing</h3>
  <p>Structural parsing produces a hierarchical document model: sections, paragraphs, tables, figures, captions, and footnotes. Stage1 markdown parse emits anchors for tables and images; artifact indices are recorded in the schema extraction stage. This structure enables targeted institution inference by isolating authorial metadata fields and provenance markers commonly found in front-matter, acknowledgements, and reference lists.</p>

  <h3>Artifact Extraction</h3>
  <p>Artifact extraction synthesizes indices of embedded artifacts (e.g., indicators, code snippets, dataset references) and associates each artifact with its source section and bounding confidence. The methodology records artifact provenance down to the parser stage that identified it, facilitating retrospective review and allowing calibration of artifact-level trust based on extraction pathway and conversion quality.</p>

  <h3>Reference Parsing</h3>
  <p>References and footnote-like citations are parsed and linked to a source registry. The reference-parsing step attempts canonicalization of cited entities and venues, producing candidate mappings that feed institution inference. Unmatched references are retained as unresolved tokens with positional context so that manual adjudication or web-fallback resolution is possible without modifying the original extracted text.</p>

  <h3>Institution Inference</h3>
  <p>Institution inference is carried out by a dedicated component (infer_source_institutions.py), which in the present pipeline employs a large language model to propose canonical institutional identities and classes (e.g., vendor report, academic paper, government release, internal document). The model proposals are constrained by structured cues extracted during parsing (entity_name, publication_or_venue, year) and by heuristic rules that account for venue strings and common organizational name variants. The pipeline records whether the inference used only local data or whether an optional web-fallback was invoked, and it attaches a confidence score and provenance trace to each inferred institution to support later review.</p>

  <h3>Claim–Evidence Graph Construction</h3>
  <p>Parsed claims and extracted artifacts are represented as nodes in a claim–evidence graph with typed edges indicating evidentiary relationships (supports, contradicts, references, derived-from). Institution-level nodes are linked to their constituent source objects so that institution inference propagates up to claim clusters and informs aggregate assessments. The graph representation permits fine-grained queries about which institutional classes contribute to any claim and enables conditional scoring strategies based on institutional mix.</p>

  <h3>Scoring Overview</h3>
  <p>Credibility weighting integrates institution inference with textual and artifact-level quality signals. Institutional class serves as a prior in a Bayesian-inspired scoring model: different classes (as determined by source typology) instantiate differing prior credibility distributions that are then updated by artifact confidence, conversion reliability, and corroborative signals from independent sources. Credibility weighting is therefore a function of both institution-level priors and empirical quality indicators derived from parsing and artifact extraction.</p>

  <h3>Chain of Custody and Provenance</h3>
  <p>The methodology mandates explicit chain-of-custody recording: timestamps, conversion method identifiers, and environment metadata (for example, python_version and platform strings) are persisted. These records support reproducibility and permit exclusion or down-weighting of material when provenance gaps are detected, thereby protecting scoring integrity when institutional attribution relies on poorly tracked sources.</p>

  <h3>Credibility and Corroboration Eligibility</h3>
  <p>Corroboration eligibility is determined by a combination of institution class, the degree of independence between sources, and artifact-level extraction confidence. Explicit rules prevent circular corroboration when multiple items derive from the same institutional origin or from derivative republications. The eligibility model uses institution inference to identify likely shared lineage and then enforces corroboration constraints so that corroborative evidence must come from independent institutional classes or from differently rooted extraction paths to materially increase score confidence.</p>

  <h3>Clarity Axis</h3>
  <p>The clarity axis quantifies how directly a source states a claim versus inferring it indirectly. Institution inference informs expectations along this axis because some institutional classes typically express different genres of clarity (e.g., academic articles versus internal operational notes). The pipeline encodes these expectations into scoring priors while allowing observed text features to override them when evidence is strong.</p>

  <h3>Aggregation and Calibration</h3>
  <p>Aggregation combines per-claim assessments across the claim–evidence graph using weighted pooling where weights derive from credibility weighting and corroboration eligibility rules. Calibration uses held-out validation sets and simulated perturbations of conversion and parsing quality to adjust weight parameters so that aggregate scores remain stable under plausible document-processing noise.</p>

  <h3>Validation and Quality Assurance</h3>
  <p>Validation entails both synthetic tests and human review. Synthetic tests inject controlled OCR errors, reference perturbations, and</section>
<section class="wiki-section" id="sec-claim_evidence_graph"><p>Introduction: This methodological section describes construction of a claim-evidence graph for cyber-attribution scoring. The objective is to define a reproducible, auditable mapping between extracted claims and the evidence, sources, and artifacts that support them. The description below follows established forensic and legal-academic norms for evidentiary linkage, emphasises anchor-level traceability, and specifies protections against analytical feedback loops. The approach here is general and procedural; it is grounded in the input structures supplied with the report (for example, the provided claim identifiers and extracted artifact lists) but refrains from treating those inputs as substantive conclusions.</p><p>Data processing and extraction: Document ingestion begins with canonical extraction of structural elements and artifacts from the source corpus. In the supplied data package this step produced a claims list (raw_claims_preview entries C001–C010), a source registry (raw_sources_preview entry SRC0001), and artifact enumerations (raw_artifacts_preview showing domain, cve, and email examples such as fancybear.net and CVE-2015-1701 and info@fireeye.com). Extraction records preserve provenance metadata for each artifact (for example artifact identifiers ART00006–ART00014 in the scoring bundle) and anchor coordinates that reference the originating page and block (e.g., block identifiers used within evidence anchors such as ART00001–ART00014). All extraction outputs are retained as immutable records to support later chain-of-custody assessment and to enable automated reproducibility checks.</p><p>Reference linking and institution inference: References are normalized and indexed to permit multi-modal attribution of provenance. Source entities (for example SRC0001, with authoring_org Mandiant in the normalized inputs) are represented in a canonical source registry with fields for publisher, date_published, and domain. Institution inference combines explicit source metadata with contextual indicators (publisher strings, domain names, and internal author fields) to produce an inferred institutional record while flagging any inferences as such. These inferred institution records are never used as sole basis for attribution; they serve to contextualize evidence provenance in the claim-evidence graph and to populate the credibility dimension of scoring only after corroboration and chain-of-custody checks are satisfied.</p><p>Claim-evidence graph construction and traceability: The claim-evidence graph is a directed labeled graph whose nodes represent claims, evidence items, sources, and artifacts, and whose edges capture explicit relationships (e.g., "supported_by", "extracted_from", "anchored_at"). Each edge to an artifact or anchor carries an explicit anchor reference (page and block identifier) to provide anchor-level traceability. In the supplied scoring bundle, evidence items E-0001 through E-0010 illustrate this model: each evidence node records modalities, feature vectors (for example features I, A, M, P, T), origin identifiers (such as ORIG:src0001), and an explicit list of anchors. Where multiple claims reference the same evidence node the graph records the linkage but marks shared-origin dependencies for downstream anti-circularity controls.</p><p>Anti-circularity safeguards: To avoid confirmation by construction, the system enforces anti-circularity rules before any evidence contributes to claim scoring. These rules include origin clustering (identifying unique origin identifiers such as ORIG:src0001), single-source penalties (documented in scoring outputs), and a requirement that any claim reaching a defined evidence threshold must be supported by evidence chains that have at least one independent origin or an independent modality. The scoring bundle contains operational examples of these controls: origin_cluster_weights and penalties in claim scoring indicate how single-source dependency is detected and downweighted. All graph operations record lineage_quality and anchor_quality diagnostics to permit post hoc review of potential circular derivation.</p><p>Scoring framework overview: The graph supports multi-dimensional scoring where each claim accrues contributions along custody, credibility, corroboration, grounding, and clarity axes. Evidence nodes carry probative_weight values and modality tags; these feed into an aggregation procedure that accounts for unique origin counts, source diversity, and chain provenance diagnostics (for example context_completeness, provenance_quality, and anchor_quality found in the scoring bundle). Penalisation factors derived from anti-circularity checks (e.g., single_source penalties) are applied multiplicatively and recorded in claim-level diagnostics so that score composition is transparent and auditable.</p><p>Validation and quality assurance: Validation procedures operate at extraction, linkage, and aggregation stages. Extraction QA compares raw_artifacts_preview counts and example values to the normalized artifact set (ART00001–ART00014) and logs mismatches. Linkage QA verifies that every claim node has at least one explicit evidence edge and that each evidence edge references a concrete anchor (page/block). Aggregation QA re-computes scores under alternative shrinkage and reliability parameters (statistical_calibration_v4 fields) to identify sensitivity to single-origin dependencies. All validation outputs are retained with the graph to support external audit and to document the system's anti-circularity posture and traceability guarantees.</p><p>Concluding note: The constructed claim-evidence graph is therefore both a technical artifact and an evidentiary ledger: it provides traceability from claim statements to page-and-block anchors, encodes institution inferences with provenance flags, and embeds anti-circularity safeguards into the scoring pipeline. This methodological architecture is designed to be defensible in legal-academic review and to support transparent reproducibility of attribution-relevant assessments.</p></section>
<section class="wiki-section" id="sec-scoring_overview"><p><strong>Introduction</strong></p><p>This methodology describes a structured, auditable architecture for producing attribution-related scores from an input document and its extracted artifacts. The approach intentionally separates the mechanical outputs of automated extraction from the subsequent inferential weighting that produces human-interpretable scores. The workflow accepts document-level inputs (for example the provided document_scores_v4 and the normalized entries in scoring_bundle) and maps them into a repeatable set of claim-level assessments and an aggregated document-level synthesis. The following sections explain the data processing and extraction pipeline, the handling of references and institution inference, the scoring framework that transforms claim-level features into final metrics, and the validation and quality assurance measures that preserve traceability and statistical calibration. All methodological choices are justified in general terms and are grounded in the supplied raw data rather than in any particular substantive allegation.</p><p><strong>Data processing and extraction</strong></p><p>The pipeline begins with ingest and structural parsing of the source artifact. In the supplied bundle this is represented by the pipeline_methods and the normalized.artifacts and normalized.evidence_items collections. The conversion stage (pdf_to_markdown_primary and fallback paths) performs optical character recognition and produces anchor-tagged text blocks; structural parsing then emits tables, figures and anchored text regions. Artifact extraction enumerates discrete objects such as domains, CVE identifiers, email addresses and other tokens (for instance artifact entries ART00001–ART00014 in the normalized.artifacts set). Each artifact is recorded with provenance metadata (extraction confidence, block_id, and page anchor) so that chain-of-custody diagnostics can be computed without altering the raw extraction output. These extraction outputs are treated as immutable inputs for subsequent scoring steps: the extraction stage reports what was found and where, and the inferential components consume but do not retroactively change those outputs.</p><p><strong>References and institution inference</strong></p><p>Reference parsing converts footnotes, citations, and publisher metadata into a source registry (for example the normalized.sources entry SRC0001). The registry links evidence_items to source_ids and records authoring_org and date_published where available. Institution inference operates as a distinct layer that maps source identifiers and textual signals to inferred institutions; the supplied pipeline_methods notes an institution_inference stage. Methodologically, inference of an authoring institution is treated as a probabilistic annotation: the inference output augments the registry but remains a derivative field subject to explicit provenance and confidence metadata. Recovered or inferred references are recorded as recovered_reference_count or similar fields so that downstream scoring can apply single-source or multi-source logic while preserving the original citation coverage as found in the extraction outputs.</p><p><strong>Scoring framework</strong></p><p>The scoring architecture is hierarchical and modular. At the base are claim-level vectors computed from grounded evidence anchors. Each claim receives separate axis scores for grounding, custody, credibility, corroboration, confidence and clarity. These axes are represented in the supplied claim_score_preview_v4 and in scoring_bundle.full_icj_v4.claims[*].scores. Axis-level computations use quantified inputs that originate in extraction and reference parsing: evidence weight aggregates, anchor counts, provenance flags, and artifact-level feature vectors (for example features and probative_weight present in evidence_items). A claim-level belief score is produced by combining axis scores with penalty multipliers where applicable (for example the single_source penalty reflected in claim scoring). Crucially, the framework enforces a clear separation between extraction outputs (counts, anchors, artifacts, probative_weight) and inferential weighting (shrinkage, priors, penalty application). Inferential weighting occurs only after extraction-derived diagnostics (chain_provenance_diagnostics, anchor_coverage, evidence_marker_strength) are computed. Document-level synthesis then aggregates claim-level vectors into a headline_vector and overall_claim_score metrics; the supplied document-level outputs (document_scores_v4 and scoring_bundle.full_icj_v4.document) illustrate how per-claim vectors are summarized into a document-level profile while retaining per-claim provenance links for auditability.</p><p><strong>Validation and quality assurance</strong></p><p>Quality assurance operates at two orthogonal levels: traceability and statistical calibration. Traceability validation confirms that each claim score can be back-traced to one or more evidence_item anchors and to the original artifact anchors (for example the anchoring present in normalized.evidence_items and normalized.artifacts). Automated diagnostics produce chain_provenance_diagnostics such as context_completeness, lineage_quality, and anchor_quality to flag gaps. Statistical calibration applies shrinkage and reliability factors (see statistical_calibration_v4 and bootstrap_95ci in document_scores_v4) to moderate raw axis scores when evidence quantity or diversity is limited. Validation routines recompute key indicators (effective_evidence_n, reliability_factor, and confidence calibration) and emit a readiness_validation record; discrepancies between raw and calibrated metrics are logged and explained in human-readable diagnostics. Together these QA controls preserve the separation between observed extraction data and the inferential parameters that transform those data into scored judgments, and they enable reproducible re-processing of a document with modified priors or penalty rules without altering the recorded extraction artifacts.</p></section>
<section class="wiki-section" id="sec-chain_of_custody"><h2>Introduction</h2>
<p>This methodology chapter describes the chain‑of‑custody axis and associated procedures used to score evidentiary support for attribution claims in cyber‑incident reporting. The exposition below presents a reproducible approach for extracting provenance attributes from source material, assessing integrity indicators, anchoring items in time, resolving artifact identifiers and versioning, and applying quality controls and penalties. The description is grounded in the structured inputs available in the scoring bundle (for example: document_scores_v4.custody_avg_0_100 = 42.89; document_scores_v4.sources_total = 1; document_scores_v4.citation_coverage_sources_0_1 = 0.0), which are used to illustrate method behavior rather than to draw substantive conclusions about any particular claim.</p>

<h2>Data processing and artifact extraction</h2>
<p>Data processing begins with ingestion of normalized records from the scoring bundle. Raw artifact lists and counts (see raw_artifacts_preview with artifact_type entries such as "cve", "domain", and "email" and scoring_bundle.full_icj.normalized.artifacts with artifact_id and value fields) are parsed to generate an artifact inventory. Structural parsing records anchor locations (page and block identifiers) that link artifacts to evidence anchors; evidence_items in the normalized bundle provide anchors and modalities that are carried forward. During extraction we preserve explicit provenance metadata supplied by sources (the normalized.sources array and evidence_items.source_ids), and we attach extraction confidence scores reported in the bundle (artifact confidence and evidence probative_weight) to each artifact record to support downstream integrity assessments.</p>

<h2>References, provenance resolution and institutional inference</h2>
<p>References and origin resolution are performed by linking anchored evidence to the documented origin identifiers (for example evidence_items.origin_id values such as "ORIG:src0001") and by enumerating source-level attributes from normalized.sources. Provenance is operationalized as a multi‑component vector that includes declared origin_id, source_kind (vendor, government, etc.), and explicit citation coverage metrics (document_scores_v4.citation_coverage_sources_0_1). Institution inference is an evidence‑level function that aggregates authoring_org and publisher fields to derive an institutional identity score; that score is used in the credibility independence component but is reported separately from the chain metrics. Where explicit source linkage is missing (for example citations_total = 0), the method records a provenance gap and reduces the provenance subscore in a transparent and reproducible manner.</p>

<h2>Scoring framework for chain of custody</h2>
<p>The chain‑of‑custody axis is decomposed into five measurable variables: provenance, integrity, time anchors, artifact identifiers, and versioning. Provenance is scored from explicit origin metadata and anchor alignment; integrity is assessed from extraction confidence, presence of cryptographic or forensic integrity signals (if present), and internal consistency across anchors; time anchors are evaluated by the presence and granularity of temporal markers associated with evidence (scored as a time anchor completeness fraction); artifact identifiers are confirmed via syntactic and semantic validation against known registries (for example CVE format checks drawn from artifact values); and versioning is recorded where the report provides file or dataset version information. These subcomponents are combined with calibrated weights to produce a custody score per evidence item and per claim. The scoring pipeline applies shrinkage and reliability adjustments (see full_icj_v4.statistical_calibration_v4 entries such as shrinkage_lambda and reliability_factor) so that single or sparse chains are shrunk toward prior expectations (illustrated where per‑claim custody_0_100 values vary across scoring versions). Penalties are applied as multiplicative factors when structural weaknesses are detected; common penalties include single_source reduction (documented in scoring.claim_scores.penalties and penalty_multiplier fields) and chain_disclosure penalties when anchors lack disclosed lineage. The method documents each applied penalty and its factor so that final scores remain auditable.</p>

<h2>Validation and quality assurance</h2>
<p>Validation comprises automated and manual checks. Automated validation verifies schema conformance against the normalized bundle and confirms that artifact identifiers match expected patterns (for example CVE syntactic validation). Integrity signals and anchor completeness are cross‑checked against evidence_item anchors; where confidence or probative_weight is provided these are compared to expected distributions and flagged if anomalous. A quality assurance workflow samples claims across the distribution of custody scores (for example claim_score_preview_v4 entries show per‑claim custody values used to exercise boundary behavior) to confirm that penalties and shrinkage behave as designed. Bootstrap and reliability diagnostics (full_icj_v4.document.bootstrap_95ci and statistical_calibration_v4.reliability_factor) inform the degree of uncertainty assigned to aggregated custody measures. All validation steps, applied penalties, and transformations (including provenance resolution, integrity reassessment, time anchor normalization, identifier normalization and versioning reconciliation) are logged to allow forensic review and reproducibility. Limitations and data gaps detected during QA—such as absence of external citations (citations_total = 0) or single‑source dependence (sources_total = 1)—are explicitly recorded and propagate to custody and downstream aggregation as described above.</p></section>
<section class="wiki-section" id="sec-credibility_corroboration"><h2>Introduction</h2>
<p>This methodology chapter defines the Credibility Axis and its Corroboration subcomponent for cyber-attribution scoring. Its purpose is to translate provenance, source metadata, and evidence structure into reproducible quantitative and qualitative outputs that support adjudication decisions. The approach distinguishes between (a) a source hierarchy that ranks evidence providers by demonstrated provenance and institutional attributes, and (b) independence logic that assesses whether multiple items of evidence are functionally independent. It also codifies corroboration rules, a claim coverage scaling mechanism, and explicit exclusion criteria for low-value source classes. The exposition below is grounded in the supplied raw scoring inputs and diagnostic vectors and is presented at the methodological level rather than as commentary on any report’s substantive allegations.</p>

<h2>Data processing and extraction</h2>
<p>Raw inputs are ingested as structured metadata and extracted artifacts. The dataset provided indicates one internal document section (source_type_counts: {"internal_document_section": 1}) and a single normalized source entry (normalized.sources[0] with source_id "SRC0001" and authoring_org "Mandiant"). The ingestion pipeline first converts original documents to an analyzable text representation, records explicit anchors (for example block identifiers ART00001..ART00014), and produces parsed artifacts (domains, CVE identifiers, and contact emails). Artifacts are retained with extraction confidence values and anchor locations; for instance, artifacts include domains such as "nato-news.com" (ART00001) and CVE identifiers such as "CVE-2015-1701" (ART00006). Evidence objects are assembled from anchored artifacts and annotated with modality and feature vectors (the evidence_items include modalities labeled "infrastructure" and features I, A, M, P, T with representative numeric values and a computed probative_weight; e.g., E-0001 lists feature values and a probative_weight ~0.1229). Structural parsing records anchor quality and provenance indicators required for subsequent chain-of-custody scoring.</p>

<h2>References and institution inference</h2>
<p>Reference parsing attempts to recover explicit citations and external corroborating documents. In the supplied inputs the record of explicit citations is empty (document_scores_v4.citations_total = 0 and citation_coverage_sources = 0.0), so reference recovery is a necessary upstream step in corroboration scoring. Institution inference uses multiple metadata signals: authoring_org and publisher fields in normalized.sources, domain names extracted from artifacts, origin_signature tags, and explicit flags such as is_litigation_prepared or has_stated_conflict. These signals are used to assign each source to a tier within the source hierarchy. Primary-tier indicators include named authoring organizations and documented provenance anchors; lower tiers include unattributed web content or ephemeral social personas. Exclusion criteria are applied to classes with low intrinsic evidentiary value or high risk of circularity: for example, anonymous forum posts, single-use paste services without provenance anchors, and automated aggregator pages lacking origin signatures are treated as ineligible for contributing credibility weight. Exclusion criteria are explicit, applied prior to independence assessments, and recorded so that excluded_source_ids appear in corroboration diagnostics when applicable.</p>

<h2>Scoring framework: source hierarchy, independence logic, corroboration, and claim coverage scaling</h2>
<p>The Credibility Axis aggregates per-source quality and independence into a claim-level score. The source hierarchy maps inferred institutional tiers to quality priors that inform a source_quality term. Independence logic examines origin signatures, domain diversity, modality diversity, and whether sources derive from common upstream reporting chains. Where normalized inputs show a single origin (for example origin_cluster_weights dominated by "ORIG:src0001"), an explicit single_source penalty is invoked; in the supplied scoring bundle claim-level penalties include a single_source multiplier of 0.85 applied to final claim scores. Corroboration is operationalized as a composite of multi-source factor, domain independence, modality diversity, and cross-language crosschecks. The corroboration subcomponent calculates a source_weight_sum and a domain_independence_effective term; when eligible_source_count is zero the corroboration_raw and corroboration_avg collapse to zero, demonstrating the importance of multiple independent sources to raise this subscore.

Claim coverage scaling ensures that credibility contributions are apportioned by the fraction of claims a source explicitly supports. The pipeline computes a claim_coverage_factor per source (document-level and claim-level mappings are recorded), and the credible_claims_weighted_ratio is used to scale aggregate credibility. Chain-of-custody diagnostics (context_completeness, lineage_quality, anchor_quality) feed into a provenance multiplier that attenuates or amplifies credibility depending on anchor integrity. The method records intermediate vectors (e.g. core_3c components such as credibility_independence and corroboration_convergence) to ensure interpretability of the aggregated axis.</p>

<h2>Validation and quality assurance</h2>
<p>Validation combines deterministic gates with statistical calibration. Deterministic gates include the credibility quality gate and explicit exclusion criteria; for example, the pipeline will flag a report-level credibility composite of zero when no eligible sources or citations exist. Statistical calibration uses reliability factors, effective evidence counts, shrinkage (shrinkage_lambda per dimension) and bootstrap confidence intervals. The supplied bundle includes bootstrap_95ci for several axes and shrinkage_lambda terms used to pull noisy per-claim estimates toward empirically derived priors. Satur</section>
<section class="wiki-section" id="sec-clarity_axis"><section><h2>Introduction</h2><p>This methodology section describes the clarity axis and its operationalization for assessing attribution to state actors through three distinct state-responsibility pathways: actions by state organs, exercise of control over non-state actors, and state failure to take preventive measures (due diligence). The objective is methodological: to set out the data processing, evidence-linking, and scoring rules that permit transparent, reproducible measurement of how clearly a document supports a legal mode of state responsibility. The exposition emphasizes rationale and mechanics rather than factual findings from any particular report, and it is grounded in the data structures and statistical controls present in the supplied scoring bundle.</p></section><section><h2>Data processing and extraction</h2><p>Inputs are ingested as structured document objects containing three primary kinds of records: extracted artifacts (domains, CVEs, emails, and other identifiers), evidence items that reference artifacts and anchors, and source metadata describing provenance. The pipeline converts source files into a canonical internal representation; artifacts appear in the normalized.artifacts array while extracted evidence items are represented as evidence_items with associated anchors and modal features. Anchor-level linkage (for example, an evidence item referencing multiple block_ids) enables alignment between claim statements and observable text or technical artifacts. Processing computes upstream diagnostics such as anchor_coverage, evidence_anchor_count, and modality attribution, which subsequently feed the clarity computations described below. Integrity and provenance signals (for example provenance, time_anchors, and artifact_identifiers fields within custody diagnostics) are preserved and propagated so that clarity scores remain conditional on chain-of-custody measures rather than divorced from them.</p></section><section><h2>References and institution inference</h2><p>Reference parsing extracts structured source metadata (authoring_org, publisher, date_published, domain and other identifiers) and assigns each source an origin cluster. Institution inference proceeds from explicit metadata fields when present and from contextual anchors otherwise: when a normalized.sources entry contains authoring_org or publisher, that string is retained as the primary institutional identifier; where explicit fields are absent, domain and origin_signature are used to group references into a source cluster. The scoring pipeline records source counts, citation_coverage, and recovered_reference_count to support downstream credibility and corroboration assessments. Institutional inference data are also used to flag whether a claim contains a direct state claim (state_claim_flag) and whether link evidence for a state actor exists (state_link_evidence_flag), inputs that feed the clarity axis without asserting substantive conclusions about a particular allegation.</p></section><section><h2>Scoring framework for the clarity axis and state responsibility pathways</h2><p>The clarity axis decomposes the concept of ‘‘how clearly a document supports attribution and responsibility’’ into measurable subcomponents. At the highest level, clarity aggregates three topical dimensions that map to legal modes of state responsibility: (1) organ attribution (conducted by state organs), (2) control attribution (non-state actors under state control), and (3) due diligence failure (state knowledge and failure to prevent). Each claim is scored on core specificity dimensions—act_specificity, actor_specificity, and link_specificity—and on pathway-specific indicators: organ_path_clarity, control_path_clarity, and due_diligence_path_clarity. The organ and control path indicators capture the presence, character, and directness of evidence linking an act to a state organ or to a non-state actor whose conduct is subject to the state’s control. The due diligence indicator captures evidence that the state knew of operations (or the relevant risk) and failed to take reasonable preventive measures.</p><p>Operationally, the per-claim clarity score is a bounded composite (</section>
<section class="wiki-section" id="sec-aggregation_calibration"><section>
  <h2>Introduction</h2>
  <p>This methodology chapter defines the procedures used to aggregate claim-level evidence into document-level attribution scores, and to communicate the resulting calibration and uncertainty. The intent is methodological transparency: to show how discrete data objects extracted from a report are transformed, weighted, and combined into interpretable scores, and how statistical methods quantify dispersion and uncertainty. The exposition below refers to elements present in the supplied raw data (for example: normalized source metadata, extracted artifacts, evidence_items, claims, claim_scores, and the document-level statistical calibration outputs) in order to ground the procedures in concrete inputs without engaging in substantive case findings.</p>

  <h2>Data processing and extraction</h2>
  <p>Input processing begins with structural parsing of the document and the production of normalized data objects. In the supplied bundle these appear as a normalized sources array (including authoring_org and publisher fields), an artifacts list (domains, CVE identifiers, email addresses), and an evidence_items array (each with modalities, feature vectors and probative_weight). Claims are represented as discrete claim records with associated evidence_ids. The extraction pipeline treats each evidence_item as a unit composed of modalities (e.g., infrastructure, technical_artifact), anchor locations (page/block identifiers) and a numeric feature vector (I, A, M, P, T) together with a computed probative_weight. These granular fields are used to produce chain_provenance_diagnostics (anchor_quality, lineage_quality, artifact_proximity_tiers) that feed later scoring stages. Counts (for example the number of claims or evidence items) are used only to scale algorithmic behavior such as saturation and effective sample size, not to substantively summarize report assertions.</p>

  <h2>Reference parsing and institution inference</h2>
  <p>Reference parsing isolates explicit citations and implicit provenance signals. Source metadata (authoring_org, publisher, domain fields and flags such as is_litigation_prepared or has_stated_conflict) are normalized; where explicit citations are absent the parser records citation_coverage = 0 and flags single_source conditions. Institution inference proceeds from author and publisher metadata, domain artifacts, and contact artifacts (for example extracted email domains) to classify source type and institutional footing (private vendor, academic, government). This classification is intentionally coarse and probabilistic: it produces an institutional type label and a provenance score component feeding custody and credibility submodels, and it avoids asserting organizational policy or intent. Recovered_reference_count and citation_coverage indicators are retained as diagnostics for later quality checks and for weighting adjustments during aggregation.</p>

  <h2>Scoring framework, aggregation and calibration</h2>
  <p>The scoring architecture is hierarchical. At the lowest level individual evidence_items produce a probative contribution (probative_weight) computed from feature vectors and modality-specific heuristics. Those contributions are associated with a claim via evidence_ids. Claim-level scoring combines three core axes—chain-of-custody provenance, credibility independence, and corroboration convergence (the “3C”)—into a six-component vector (credibility, corroboration, coherence, confidence_discipline, compliance_mapping and related submetrics). The supplied claim_scores illustrate this composition: evidence_weight_aggregate values are multiplied by penalty multipliers (for example a single_source penalty factor) to yield a claim final_score. Aggregation across claims to a document-level summary uses a calibrated shrinkage model: per-axis prior_scores and shrinkage_lambda parameters (shrinkage_lambda for custody, credibility, corroboration, clarity and confidence) are applied together with a reliability_factor and an effective_evidence_n to produce regularized document scores. Saturation factors (chain_saturation, corroboration_saturation) limit the marginal effect of additional, non-independent evidence, and gravity_weight scales the impact of higher-severity allegations. This two-stage aggregation (evidence→claim→document) enables interpretable attribution summaries while avoiding overconfidence induced by redundant or single-origin evidence.</p>

  <h2>Uncertainty quantification and dispersion diagnostics</h2>
  <p>Uncertainty is quantified through bootstrap-derived confidence intervals and model-based shrinkage diagnostics. The data bundle includes bootstrap_95ci results for several document-level metrics (for example belief_weighted and custody_avg) that report mean and ci95_low/ci95_high; these provide a nonparametric dispersion estimate reflecting sampling variability and algorithmic sensitivity to evidence selection. Statistical calibration outputs (reliability_factor, effective_evidence_n, shrinkage_lambda and saturation_factors) document how raw scores are pulled toward prior expectations and how much weight the observed evidence carries. Together, bootstrap intervals and shrinkage diagnostics give both an empirical measure of dispersion and a model-informed calibration metric to guide interpretation under uncertainty.</p>

  <h2>Validation and quality assurance</h2>
  <p>Quality assurance is layered. Procedural checks include readiness_validation flags, anchor_coverage and claim_anchor_alignment diagnostics, and chain_provenance_diagnostics (context_completeness, integrity_signal, lineage_quality). Statistical QA leverages cross-sample bootstrap dispersion and the reliability_factor to detect low-information or overconfident aggregations. Credibility gates (for example a credibility_quality_gate and checks on citation_coverage) prevent aggregation from overstating confidence where independent corroboration is absent. Finally, the system records provenance at multiple levels (artifact_proximity_tiers, origin_cluster_weights) so that reviewers can trace document-level outputs back to annotated anchors and source records for independent audit and legal or technical review. These</section>
<section class="wiki-section" id="sec-validation_quality_assurance"><h2>Validation and Quality Assurance</h2>
<p>This section describes the validation gates and the quality assurance protocol applied to the extraction and scoring pipeline. The validation process is implemented as a sequence of automated validation gates that operate on structural, integrity, and grounding dimensions prior to any manual intervention. The gates evaluate schema conformance, structural table integrity, citation presence, artifact extraction, chain-of-custody linkage, and grounding of credibility and corroboration claims. The applied gates are reflected in the validation bundle metadata (validation_report.json) where the extraction attains a certification status and a composite quantitative score summarizing these dimensions. The validation gates are configured to produce deterministic pass/fail signalling for critical checks and graded warnings for ancillary issues; hard failures block progression, whereas warnings are logged for downstream triage.</p>
<p>Automated validation gates are complemented by an agent review layer. Agent review is an automated, rules-driven verification step that rechecks anchor and component linking, duplicate support detection, and table fallback handling against the pipeline’s canonical patterns. The context for this system is captured in the validation bundle and in the associated raw payload files (for example, the validation report at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/mandiant_2017_apt28_at_the_center_of_the_storm/mandiant_2017_apt28_at_the_center_of_the_storm.validation_report.json and the primary report JSON at /home/pantera/projects/TEIA/annotarium/outputs/reports/mandiant_2017_apt28_at_the_center_of_the_storm_report.json). Agent review is enabled as a formal stage in the qa_protocol and is designed to reduce noise presented to humans by resolving a large fraction of predictable, non-ambiguous anomalies.</p>
<p>Human review is targeted and sampled. In the present QA configuration a 10% human review sample is drawn from the set of extracted claim components and associated anchors; this is represented in the qa_protocol as a human_sample_fraction of 0.1. The sampled records undergo focused human inspection for linkage correctness, anchor diversity, and non-duplicative support across components. The human review outcome is reported as an observed error rate for the sampled subset. In the reviewed sample, there were no observed errors, reported as human_sample_observed_error_rate: 0.0 (i.e., no observed errors). The human review is explicitly confirmatory rather than exhaustive: it validates the combined effect of the automated validation gates and the agent review, and it informs calibration of warning thresholds and sampling strategies for subsequent runs.</p>
<p>Quality assurance governance dictates that warnings surfaced by automated gates be triaged according to severity. The validation bundle documents warning-level findings and their counts so that recurring patterns (for example, unstructured table fallbacks or duplicated support anchors) may be addressed at the extraction or annotation specification level. File paths to raw inputs and intermediate artifacts are retained for reproducibility and audit (see raw_payload_paths and raw_payload_files entries in the data contract). Together, the validation gates, agent review, and a 10% targeted human review sample with no observed errors form a layered QA architecture that balances automation scale with human judgment for provenance-sensitive attribution scoring.</p></section>
<section class="wiki-section" id="sec-limitations_governance"><p>Introduction: This section articulates limitations, governance, and a principled refinement roadmap for the attribution scoring methodology. It situates the discussion within the jurisprudential posture described in the methodology reference, which treats attribution reporting as an evidentiary exercise and privileges structured, auditable links between propositions and supporting materials. The purpose here is methodological: to constrain claims about system reliability, to identify known procedural boundaries, and to prescribe governance and iterative improvements rather than to rehearse substantive findings from any underlying report.</p>

<p>Scope and units: The unit of analysis for the scoring system is the claim-level evidentiary block extracted from a report, together with the associated artifacts and source descriptors. This operational choice enforces granularity in evaluation: scores are produced at the claim level and then aggregated with explicit weights. The approach derives from the methodology reference excerpt’s emphasis on claim-specific chains and persists all intermediate artifacts so that any derived score can be reconstructed from the stored inputs.</p>

<p>Data ingestion: Source PDFs are ingested deterministically where possible. The validation bundle indicates this operational environment by recording ingestion metadata and outcome indicators such as the certification flag and an overall validation score, which are used to gate downstream processing. The ingestion pipeline is instrumented to record runtime context (for example, the environment and library versions) to aid reproducibility and forensic review.</p>

<p>PDF-to-markdown transcription: Transcription from PDF to markdown is treated as a canonicalization step that must preserve provenance markers and locators. Transcription failures or unstructured fallbacks are surfaced as integrity warnings in the validation bundle; in the present record the validation output reports a small set of table fallbacks and duplicate-anchor warnings, which are treated as process-level exceptions that reduce the admissibility of affected anchors until remediated.</p>

<p>Structural parsing: The markdown is parsed into a constrained schema that separates claims, sources, artifacts, and anchors. Structural integrity checks enforce identifier uniqueness and anchor traceability; where these checks fail the run is failed or flagged for manual remediation. These integrity controls implement an admissibility discipline intended to prevent pseudo-precision in downstream numeric outputs.</p>

<p>Artifact extraction: Artifacts are extracted with explicit provenance fields (e.g., artifact identifiers, timestamps, and any embedded cryptographic markers where available). The extraction schema records artifact-level attributes used later for multiplicative item-level weightings such as contemporaneity and authentication/provenance.</p>

<p>Reference parsing: References and citations are parsed into a source inventory so that source-type quality can be assigned. The validation bundle’s coverage metrics and citation totals feed a document-level calibration so that credibility is sensitive to the presence or absence of high-quality external sources rather than sheer citation volume.</p>

<p>Institution inference: Institution inference is a constrained, explainable mapping from source descriptors to institutional quality categories (e.g., international institution, peer-reviewed academic, government, NGO, think tank). The inference process uses deterministic heuristics supplemented by human-review flags where ambiguity remains, and every mapping is recorded in the evidentiary record to allow contestation.</p>

<p>Claim–evidence graph: Claims and artifacts are linked in a directed, auditable graph. Edges record anchor text and locators so that the chain of reasoning is inspectable. The graph is the principal audit surface for anti-circularity checks: origin clustering and diminution of returns for repeated downstream reporting are computed with reference to graph provenance.</p>

<p>Scoring overview: The scoring framework implements the three core axes described in the methodology reference: Chain of Custody, Credibility (including corroboration), and Clarity. Item-level probative force is computed along bounded dimensions—independence, provenance/authentication, methodological soundness, procedural testing, and contemporaneity—and combined multiplicatively to reflect that a critical deficiency on one axis cannot be fully offset by strengths on another.</p>

<p>Chain of custody: Custody scores are claim-specific and derived from a small set of normalized indicators extracted from artifact fields: provenance markers, integrity markers, temporal anchors, artifact identifiers, and versioning lineage. These are combined linearly then bounded so that the custody score remains auditable and resistant to inflation by single-indicator concentrations.</p>

<p>Credibility and corroboration: Credibility is a composite of source-quality support and corroborative convergence. Source quality is derived from institution inference and source diversity, with penalties for single-source dependence and for reliance on internal or low-quality outlets. Corroboration is measured as constrained convergence rather than citation volume: evidence sharing a common upstream origin receives diminishing returns, and corroboration for broad claims with narrow relevance is down-weighted.</p>

<p>Clarity axis: The clarity axis operationalizes legal intelligibility. It examines whether attribution logic links act–actor–link with sufficient specificity and whether one of the recognized responsibility pathways (state organs, state direction/control of non-state actors, or omission/due diligence failure) is clearly articulated. This axis is intentionally legalistic to ensure that numeric outputs remain interpretable against doctrinal standards.</p>

<p>Aggregation and calibration: Aggregation calibrates claim-level outputs by weighted claim importance and by document-level coverage of high-quality sources. Credibility and corroboration are both subject to claim-coverage factors so that a small set of high-scoring claims cannot wholly dominate a document-level assessment absent proportional coverage.</p>

<p>Validation and quality assurance: The system produces an explicit validation bundle that records structural integrity outcomes, score-level diagnostic metrics, and runtime provenance. In the supplied record the validation bundle reports a PASS certification alongside warnings that identify repeated-anchor duplicates and limited anchor diversity; such warnings are treated as actionable quality deficiencies, not as substantive adjudications. Bootstrap confidence intervals and other diagnostic statistics are retained to express uncertainty in aggregated indices.</p>

<p>Limitations, governance, and refinement roadmap: Limitations are intrinsic to the pipeline and arise from (i) source material quality and availability; (ii) transcription and parsing fidelity; (iii) residual institution inference ambiguity; and (iv) the philosophical choice to privilege auditable legibility over extraction breadth. Governance controls mitigate these limitations by enforcing deterministic transforms where feasible, requiring persistence of intermediate artifacts, and gating scoring on structural integrity checks. A principled refinement roadmap emphasizes three priorities. First, enhance extraction fidelity through targeted OCR and table-reconstruction improvements and formalize manual remediation workflows for identified fallbacks. Second, reduce institution-inference ambiguity by expanding deterministic name-resolution tables and</section>
</article>