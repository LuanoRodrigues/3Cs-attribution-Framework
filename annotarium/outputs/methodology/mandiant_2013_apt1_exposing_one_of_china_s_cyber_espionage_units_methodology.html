<article class="wiki-page">
<header><h1>Annotarium Methodology: APT1: Exposing One of China's Cyber Espionage Units</h1><div class="wiki-meta">Generated at 2026-02-22T21:30:28Z</div></header>
<nav class="wiki-toc"><h2>Contents</h2><ol>
<li><a href="#sec-introduction">Introduction and Epistemic Framing</a></li><li><a href="#sec-scope_units">Scope, Units of Analysis, and Output Semantics</a></li><li><a href="#sec-data_ingestion">Data Ingestion and Corpus Handling</a></li><li><a href="#sec-pdf_to_markdown">PDF-to-Markdown Conversion</a></li><li><a href="#sec-structural_parsing">Structural Parsing of Text, Tables, and Figures</a></li><li><a href="#sec-artifact_extraction">Artifact Extraction and Technical Object Normalization</a></li><li><a href="#sec-reference_parsing">Footnote and Reference Parsing</a></li><li><a href="#sec-institution_inference">Institution Inference and Source Typology</a></li><li><a href="#sec-claim_evidence_graph">Claim-Evidence Graph Construction</a></li><li><a href="#sec-scoring_overview">Scoring Framework Overview</a></li><li><a href="#sec-chain_of_custody">Chain of Custody Axis</a></li><li><a href="#sec-credibility_corroboration">Credibility Axis with Corroboration Subcomponent</a></li><li><a href="#sec-clarity_axis">Clarity Axis and State Responsibility Pathways</a></li><li><a href="#sec-aggregation_calibration">Aggregation, Calibration, and Uncertainty</a></li><li><a href="#sec-validation_quality_assurance">Validation and Quality Assurance</a></li><li><a href="#sec-limitations_governance">Limitations, Governance, and Future Refinement</a></li>
</ol></nav>
<section class="wiki-section" id="sec-introduction"><h2>Introduction and Epistemic Framing</h2>
<p>This methodology is presented as an evidentiary framework for attribution in contested cyber incidents. Its epistemic posture is jurisprudential: attribution is treated as an argument built from heterogeneous materials rather than as a standalone declarative claim. The framework is burden-sensitive and designed to make contestable inferential steps explicit. It therefore insists that extraction of structure from source documents precede any legal or policy-level inference so that attribution conclusions can be inspected, challenged, and reconstructed from the same evidentiary record.</p>

<p>The procedures described below were applied to the source record identified in the processing run (document title: "APT1: Exposing One of China's Cyber Espionage Units"; stored at the path recorded in the run metadata), and follow the methodological commitments set out in the pipeline documentation. These commitments emphasize determinacy, persistence, and admissibility-like integrity checks so that downstream numerical outputs do not express parser convenience rather than probative force. The framing therefore combines epistemic humility with an evidentiary discipline intended to render contestable judgments auditable.</p>

<h2>Scope and Units of Analysis</h2>
<p>The unit of analysis is the structured evidentiary record. The schema decomposes a report into distinct objects: claims (proposition-level analytic blocks), source records (bibliographic and provenance metadata), artifacts (files, code snippets, images, tables), and evidence links that explicitly anchor portions of the text to artifacts and sources. Separating these units prevents implicit model inference from masquerading as documentary support and preserves an inspectable chain between proposition and proof. The schema keys and pipeline stages are recorded as part of the run metadata so any score can be reconstituted from intermediate artifacts.</p>

<h2>Data Ingestion and Transcription</h2>
<p>Ingestion begins with the primary conversion of PDF to machine-readable markdown using the primary transcription tool indicated in the pipeline methods. A provider-backed OCR/transcription process is used first, with an offline fallback employed when provider conversion fails or times out. The transcription stage preserves textual structure, pagination anchors, and markings for tables and figures so that downstream parsing has accessible positional anchors. The run metadata records the timestamp and path of the original PDF and the transcript, to ensure persistence and reproducibility.</p>

<h2>PDF-to-Markdown and Structural Parsing</h2>
<p>Transcribed markdown is parsed deterministically into schema-constrained objects. Structural parsing separates headings, claim-level passages, footnotes, tables, and embedded media and emits a source registry. Determinacy is enforced where possible; model-assisted components are bounded by schema constraints and undergo explicit post-run verification. Integrity checks at this stage flag missing anchors, identifier collisions, or unresolvable references. A run that fails structural integrity checks is marked as failed and is not scored, reflecting an admissibility discipline analogous to judicial practice.</p>

<h2>Artifact Extraction and Reference Parsing</h2>
<p>The artifact extraction stage emits an index of artifacts referenced in text, tables, and figures. Each artifact entry records provenance markers, temporal anchors, artifact identifiers, and any explicit versioning or integrity statements found in the record. Citations and footnote-like references are parsed and linked to entries in the source registry. These links are preserved as first-order objects so that the evidentiary pathway from claim to source to artifact remains auditable and explicit.</p>

<h2>Institution Inference and Source Attribution</h2>
<p>Institution inference is performed with a constrained, auditable model-assisted procedure: an inference script identifies and normalizes source institutions using a lightweight language model pass supplemented by optional web-based lookup where necessary. Every inferred institution is recorded with the inference provenance and confidence metadata so that institution-level assignments remain contestable. The procedure emphasizes verifiability: an inferred institution is treated as a provisional node in the source registry until corroborated by independent bibliographic or archival evidence.</p></section>
<section class="wiki-section" id="sec-scope_units"><h2>Introduction</h2>
<p>This methodological chapter sets out the scope, units of analysis, and output semantics for a reproducible cyber-attribution scoring pipeline. The objective is to describe, at a conceptual and operational level, what the system treats as analyzable objects, how scores are computed and combined, and what the resulting numerical and categorical outputs are intended to represent and not to represent. The exposition is grounded in the provided raw data excerpt and pipeline summaries, which serve only as illustrations of method behavior rather than as sources of substantive factual claims about any particular case. For example, the supplied metadata and pipeline counts (document metadata fields, pages = 1, claims = 10, sources = 10, artifacts = 6) are used to illustrate how unit counts enter downstream quality checks and aggregation stages.</p>

<h2>Units of Analysis</h2>
<p>The pipeline distinguishes five primary units of analysis: claim, source, artifact, evidence item, and document. A claim is an asserted proposition contained in one or more documents; it is the principal semantic atom whose support and believability are evaluated. A source denotes an originator or publisher of information (for example, a vendor report, academic article, or public advisory) and is an attribute of one or more documents; source-level attributes inform credibility and custody assessments. An artifact is a discrete technical object extracted from a document such as a sample binary, a network indicator, a filename string, or a table cell that encodes a technical indicator; artifacts are the principal carriers of technical provenance used to link documents and incidents. An evidence item is a parsed, normalized extraction that links an artifact to a claim or links a document passage to a claim; evidence items are the edges in the claim–evidence graph. The document-level unit aggregates the above: it is the whole file or record (with associated metadata such as title, authoring entity, and publication date) from which claims, sources, artifacts, and evidence items are derived.</p>
<p>These units are hierarchical and relational. Evidence items connect artifacts and textual passages to claims; sources and document-level metadata qualify the custody and provenance of evidence items; artifacts provide technical anchors that can be algorithmically matched across documents to produce corroboration signals. Explicit separation of these units permits independent quality assessment (for example, treating artifact extraction accuracy separately from natural language claim extraction) and supports transparent chain-of-custody reasoning.</p>

<h2>Data Processing and Extraction</h2>
<p>The pipeline implements staged processing that begins with document ingestion and proceeds through structural parsing, artifact extraction, and evidence-item construction. Initial ingestion records document-level metadata (for example, title and source locator fields present in the raw_data_excerpt) and counts of extracted objects (the supplied pipeline_counts values such as pages, claims, sources, and artifacts illustrate expected intermediary yields). Structural parsing converts file formats into a normalized intermediate representation amenable to span-level annotation; when a document is supplied as markdown or PDF-derived markdown, the parser retains block boundaries and inline references to enable later mapping of evidence items to concrete document locations. Artifact extraction applies deterministic and heuristic extractors to harvest indicators and structured content (tables, figures) and to create normalized artifact records. Reference parsing and evidence-item assembly join extracted artifacts and textual claim spans to produce per-claim evidence sets; the resulting claim–evidence graph is the computational substrate for subsequent scoring. All extraction steps record provenance metadata and extraction confidence scores to feed custody computations.</p>

<h2>References and Institution Inference</h2>
<p>Reference parsing identifies bibliographic citations, inline pointers, and named sources within documents and normalizes them to a canonical source record when possible. Institution inference is an auxiliary inference layer that maps source records and authoring-entity strings (for example, the document_metadata.authoring_entity value in the raw_data_excerpt) to institutional identifiers used in downstream credibility and custody models. This mapping is intentionally conservative: it separates explicit provenance (the publisher string present in document-level metadata) from inferred institutional ties derived from heuristics or external registries, and records inference confidence. Institutional labels influence but do not deterministically set credibility priors; the pipeline records both the explicit source string and any inferred institution so that downstream users can audit and override automated inferences.</p>

<h2>Scoring Framework and Output Semantics</h2>
<p>The scoring framework computes metrics at multiple granularities: evidence-item level, claim level, and document-level aggregates. Key dimensions include grounding, custody, credibility, corroboration, clarity,</section>
<section class="wiki-section" id="sec-data_ingestion"><p>Introduction. This methodology section describes procedures for ingesting a report from the input corpus through to intermediate artifacts, and the deterministic controls instituted to support reproducibility. The account that follows is framed as a general methodological description grounded in the processing metadata and artifact registry associated with the input report identified by report_id: mandiant_2013_apt1_exposing_one_of_china_s_cyber_espionage_units_report and document_title: "APT1: Exposing One of China's Cyber Espionage Units". The provenance record for the ingestion task is anchored by the generated_at_utc timestamp and the canonical report_path within the project workbench; such identifiers are carried forward as part of the chain-of-custody for the input corpus.</p>

<p>Scope and units of analysis. The unit of analysis is the digital report file as presented in the input corpus, together with any structured extractions and derived artifacts produced by deterministic pipeline stages. All subsequent references to files and artifacts use the input corpus identifiers recorded in the report manifest so that every processing step resolves to a specific file path and size byte count. This approach keeps the scope limited to the documented pipeline transforms and prevents conflation of processing metadata with substantive subject-matter conclusions.</p>

<p>Data ingestion, deterministic file handling, and reproducibility guarantees. Ingestion proceeds from the original PDF file at the canonical PDF path through a sequence of versioned tools and emission points. Each stage records an explicit artifact path (for example, the initial report JSON, raw extraction JSON, and validation report JSON) and the runtime environment used to produce it (including Python version and key package versions). The presence of file-system level attributes such as the exact path and size in bytes, together with stable tool versioning, enable reproducible retrieval of the same bytes and reconstruction of the processing chain. Deterministic handling is enforced by (1) preserving the original input artifact at the documented pdf path within the project workspace, (2) emitting intermediate artifacts to fixed, predictable locations (for example the raw_extraction and report_json paths), and (3) recording the pipeline method identifiers and runtime library versions that produced each artifact. These controls collectively create a reproducible mapping from the input corpus to derived artifacts: given the same input bytes, the same tool versions, and unchanged pipeline configuration, the pipeline will produce the same intermediate JSON outputs that are referenced here.</p>

<p>PDF-to-markdown conversion and structural parsing. The pipeline employs an explicit primary conversion stage and a documented fallback. The primary conversion is a provider-backed Mistral OCR conversion as indicated by the pipeline_methods entry, while the fallback is an offline conversion using a PyMuPDF4LLM-based routine. Both conversions are parameterized and versioned; their deterministic behavior is supported by configuration capture and by writing conversion outputs to named markdown and extraction artifacts. Structural parsing follows conversion: the markdown parse emits tables, figure anchors, and a document tree that is captured in the raw_extraction artifact. By design, structural parsing emits a machine-readable representation that contains token positions, table coordinates, and figure/image anchors so that downstream processes can reference stable offsets rather than opaque page numbers, thereby increasing reproducibility across conversion runs.</p>

<p>Artifact extraction and reference parsing. Subsequent stages perform schema-driven artifact extraction from text, tables, and figures and produce artifact indices that are recorded in the extraction JSON. Reference parsing is performed by a dedicated stage that identifies citations, footnote-like references, and other inter-document pointers, links them to a source registry, and generates a validation report describing parse coverage and unresolved references. The architecture records both the raw extraction artifact and the validation_report_json so that auditors can verify which references were successfully canonicalized and which required manual adjudication. These artifacts form the basis for claim-to-evidence linking and for later credibility assessment.</p>

<p>Institution inference and source attribution procedures. Institutional inference is performed in a separate, documented stage that uses a probabilistic model assisted by a named language model (noted in the pipeline_methods as gpt-5-mini) with an optional web-fallback for external verification. The inference stage is explicitly decoupled from the extraction stage and emits both inferred institution labels and the confidence metadata supporting each inference. These outputs are recorded alongside extraction artifacts so that institution-level assertions are traceable to specific input passages, extracted artifacts, and the model version that produced them.</p>

<p>Claim–evidence graph construction. Extracted artifacts, parsed references, and institution inferences are integrated into a claim–evidence graph. Nodes represent extracted artifacts, referenced sources, and inferred institutions; edges capture provenance relationships (for example: extracted-from, cited-by, inferred-from). Every node and edge in the graph references the originating artifact path and byte-offsets where applicable, enabling deterministic reconstruction of the graph from the stored extraction artifacts. The graph is designed to be queryable and verifiable using only the recorded artifacts and toolchain versions, supporting reproducibility of downstream analyses.</p>

<p>Scoring overview and aggregation calibration. Scoring is implemented in staged, auditable modules that consume the claim–evidence graph and the validated extraction artifacts. Each scoring stage records its inputs and outputs to a versioned score artifact path (for example the full_scores and full_scores_v3 paths). When multiple score variants exist the pipeline records which variant and which score_input snapshot (for example score_input_v3) were used. Aggregation is calibrated through explicit aggregation rules and weighting schemes stored alongside the score output so that a subsequent auditor can recompute scores deterministically from the same inputs and configuration.</p>

<p>Chain-of-custody and documentation. Chain-of-custody is maintained by persisting the original PDF, the intermediate extraction artifacts, the validation report, and scored outputs at predictable locations and with recorded file metadata. Each artifact is associated with the runtime environment that produced it, thereby enabling the recreation of the processing environment and ensuring reproducibility of results derivable from the input corpus.</p>

<p>Credibility corroboration and clarity axis. Credibility assessment is conducted by comparing independent evidence paths within the claim–evidence graph and by annotating the degree of corroboration between artifacts. The clarity axis records the syntactic and semantic quality of extractions (for example whether a table was parsed as a structured object or remained unstructured text). These quality annotations are persisted in the validation artifacts so that they inform the final scoring stages and can be re-evaluated in repeated runs.</p>

<p>Validation, quality assurance, and limitations. Quality assurance is implemented through automated validation reports and manual review checkpoints. The validation_report</section>
<section class="wiki-section" id="sec-pdf_to_markdown"><p><strong>Introduction.</strong> This methodology chapter describes the pipeline and justificatory design for converting source documents into structured representations for cyber‑attribution scoring. The exposition emphasizes methodological rationale and procedural resilience rather than asserting or reinterpreting substantive findings contained in any particular report. Where specific pipeline artifacts are cited, their provenance and storage locations are referenced to support reproducibility and auditability.</p>

<p><strong>Scope and units of analysis.</strong> The unit of analysis for the pipeline is the individual report document and its derived artefacts: page images, markdown segments, tabular extractions, figures, cited or internal references, and indexed artifacts used in claim–evidence graphs. For the dataset instance documented here, pipeline meta‑counts used to illustrate method operation indicate a single input document with one page unit recorded for certain stages, ten enumerated claims, ten sources, six extracted artifacts, zero detected inline citation tokens at a particular parsing stage, nineteen tables, and twenty‑nine figures. These counts are provided to illustrate how components direct processing flow rather than to summarize substantive content.</p>

<p><strong>Data ingestion and traceability.</strong> Ingested inputs and derivative outputs are captured with explicit file system paths and size metadata to preserve chain of custody and enable later validation. Primary assets referenced in the pipeline include the original PDF at /home/pantera/projects/TEIA/annotarium/Reports/Mandiant - 2013 - APT1 Exposing One of China's Cyber Espionage Units.pdf (exists: true; size_bytes: 6797621), a generated markdown at /home/pantera/projects/TEIA/annotarium/outputs/pipeline/mandiant_2013_apt1_exposing_one_of_china_s_cyber_espionage_units/mandiant_2013_apt1_exposing_one_of_china_s_cyber_espionage_units.md (exists: true; size_bytes: 146723), and multiple intermediate and output JSON artifacts including report JSON, raw extraction JSON, validation report JSON, score inputs, and scoring reports. These file references are recorded in the pipeline manifest and are cited subsequently to ground each methodological assertion in tangible artifacts.</p>

<p><strong>PDF-to-Markdown conversion.</strong> The primary conversion posture relies on a Mistral‑backed provider conversion implemented by process_pdf_mistral_ocr.py. This Mistral‑based processing performs optical character recognition and layout-aware conversion into markdown, producing an initial, structured text representation suitable for downstream NLP and structural parsing. To preserve operational continuity when external provider conversion is unavailable or times out, the pipeline adopts an offline fallback posture implemented via PyMuPDF4LLM. The offline fallback is explicitly framed as resilience: it preserves the intended conversion semantics and downstream interface contracts rather than representing a conceptual change to the representation model. Both primary and fallback paths emit markdown with anchors for tables and figures so that subsequent stages can reference locations within the document deterministically.</p>

<p><strong>Structural parsing and enrichment.</strong> After markdown generation, a stage1 markdown parse identifies and annotates structural elements: headings, paragraphs, table blocks, figure anchors, and inline artifacts. Table and image extraction is conducted as part of this stage so that tabular data and figures are preserved with contextual anchors. Structural parsing enforces contract rules for anchor naming, positional metadata, and adjacency tokens to ensure artifacts retain linkage to their original page and byte offsets as recorded in the ingestion metadata. This design supports downstream provenance queries and selective re‑rendering.</p>

<p><strong>Artifact extraction schema.</strong> The artifact extraction stage produces a schema of indexed artifacts derived from text, tables, and images. The stage emits artifact indices describing artifact type, location anchors, extracted metadata (for example, table headers and figure captions when present), and cross‑references to the markdown anchor. The artifact index format is designed for idempotence: repeated extraction runs produce consistent identifiers when inputs are unchanged, enabling deterministic referencing for evidence aggregation and scoring. This stage in the pipeline is referenced in the manifest as the artifact_extraction output stage and contributed to the documented count of six artifacts for the illustrative run.</p>

<p><strong>Reference parsing and registry linking.</strong> Citation and footnote‑style reference parsing operates on the structural parse to detect in‑text citations, bibliographic entries, and other referential tokens. The reference parsing component normalizes extracted citations into a local source registry and links in‑text mentions to registry entries. The parser captures ambiguity flags and confidence scores for linkages, ensuring that downstream users of the claim–evidence graph can filter by linkage confidence. The pipeline records the output of this stage to the validation manifest and uses it to populate the sources count that we report for process illustration.</p>

<p><strong>Institution inference and provenance augmentation.</strong> To infer probable source institutions associated with sections of text or artifacts, the pipeline uses infer_source_institutions.py, which relies on a gpt‑5‑mini model for probabilistic inference and may optionally perform a controlled web fallback when model confidence is below a preset threshold. The institution inference component emits candidate institution labels with associated confidence metrics and provenance tokens that point back to the supporting text spans or artifacts. The use of gpt‑5‑mini is included to provide contextual language reasoning for institution attribution while the optional web fallback supplies corroborative public evidence when required by governance settings. All inferences are logged and retained for auditing rather than being treated as final determinations.</p>

<p><strong>Claim–evidence graph construction.</strong> Claims identified during parsing are represented as nodes in a claim–evidence graph, with edges linking claims to supporting artifacts, references, and inferred institutions. Each claim node records provenance pointers to the markdown anchors, artifact indices, and reference registry entries. Where multiple artifacts support the same claim, the graph encodes the nature of support (direct quotation, paraphrase, derived measurement, etc.) and attaches confidence metadata from prior stages to enable weighted aggregation. This graph style supports transparent tracing from final scoring back to original byte ranges in the source file.</p>

<p><strong>Scoring framework overview.</strong> The scoring framework aggregates claim‑level evidence into higher‑level attribution scores via calibrated operators that combine reliability of source, artifact integrity, reference linkage confidence, and institution inference confidence. Scoring inputs are captured in a discrete JSON input artifact (documented in the manifest as score_input_v3.json) that records the claim–evidence graph snapshot and the per‑node confidence vectors. The aggregation model treats the per‑component confidences as tunable parameters; their setting is governed by pre‑specified calibration procedures rather than ad hoc judgment, and scoring outputs are written to discrete score reports for downstream validation and legal review. Example score artifacts in the pipeline include icj_score_report.json and icj_score_report_v3.json, which are retained as immutable artifacts for audit.</p>

<p><strong>Chain of custody and immutability.</strong></section>
<section class="wiki-section" id="sec-structural_parsing"><section>
  <h2>Introduction</h2>
  <p>This methodology chapter describes the procedures used to parse and preserve structural elements of source documents for a reproducible cyber‑attribution scoring pipeline. The approach is designed to ensure that every extracted datum can be traced to an explicit location in an archival source, supporting later auditability and independent review. The description below is grounded in the supplied document metadata (for example, the title "APT1: Exposing One of China's Cyber Espionage Units" and the authoring_entity noted as "Mandiant"), and in pipeline inventory figures provided with the extraction (sample counts indicating one page sampled, ten claims, ten sources, six artifacts, nineteen tables, and twenty‑nine figures). These inventory items are used illustratively to show how structural parsing operates in a heterogeneous report that contains prose, tabular material, and illustrative figures.</p>

  <h2>Data processing and extraction</h2>
  <p>The ingestion pipeline begins with content acquisition from the file system and canonicalization into a working format suitable for downstream parsing. The provenance of the source is recorded using the provided source_locator and raw_payload_paths; for example, the original PDF and corresponding markdown extraction paths are recorded alongside intermediate JSON outputs. The pipeline records the input format and versioning metadata so that conversions (for example, pdf → markdown) are reversible and auditable. Text extraction routines break page content into discrete text blocks based on layout cues (columns, headings, paragraph runs) and assign each block a persistent anchor token that encodes its origin (document id, page index, and object id where available). For pages where machine extraction produces ambiguous layout regions, the system retains both the raw extraction artifacts and a normalized block representation, and records the extraction_method and any manual_description used to resolve ambiguities.</p>

  <h2>Structural parsing of text, tables, and figures</h2>
  <p>Structural parsing treats text blocks, tables, and figures as first‑class objects. Tables are captured as both raw cell grids and as semantic table objects with header inference, cell spans, and coordinate metadata; figures and images are stored with descriptive captions and an image object id. Each extracted object is annotated with an anchor: a stable identifier that references the exact extraction location (for example, the publication_date_anchor P000-A999 style token in the supplied metadata). Anchors are essential because they provide a deterministic link between downstream assertions and the exact source element from which they were derived. Anchor‑preserving structure supports chain of custody, permits selective re‑extraction, and enables auditors to reconcile any scoring decision back to the original byte range or rendered page region. Maintaining anchors also allows the system to record transformations (such as redactions or OCR corrections) as staged artifacts tied to the original anchor rather than destroying provenance information.</p>

  <h2>References and institution inference</h2>
  <p>Reference parsing separates in‑text citations, bibliographic references, and implicit institutional signals (for example, organizational names that appear in headers or metadata) and links them to anchors in the primary text or tables. Where explicit citation metadata is absent (the supplied pipeline_counts show zero citations in the sample), the parser extracts candidate referents from structured fields and from anchored text blocks that commonly indicate origin (title blocks, bylines, and publication metadata). Institution inference algorithms combine lexical matching, heuristic normalization, and anchored co‑occurrence analysis across text, tables, and embedded figures to propose institutional attributions with confidence scores. All inferred institutions are output with the anchors that supported the inference so that any downstream analyst may review the exact snippet or table cell that produced the inference.</p>

  <h2>Scoring framework</h2>
  <p>The scoring framework ingests anchored evidence units (text blocks, table cells, figure captions, and artifacts) and maps them to claim features used in attribution scoring. Feature extraction is anchor‑aware: each feature value records its originating anchor and extraction method so that feature provenance is preserved. Weights are assigned to evidence types according to a documented rubric that balances source credibility, specificity, and independence; for example, structured data in tables may yield different weightings from narrative claims or from images. Redactions and missing data are explicitly encoded as anchored events so that absence of evidence can be modeled. The framework is designed to be modular so that alternative weighting schemes can be applied and re‑scored without altering the underlying anchored evidence corpus.</p>

  <h2>Validation and quality assurance</h2>
  <p>Validation procedures operate at two levels. First, automated verification checks ensure the internal consistency of anchors and the referential integrity between derived objects and raw extraction outputs (for instance, cross‑verifying the raw_extraction JSON and the markdown outputs referenced in raw_payload_paths). Second, human review validates ambiguous or high‑impact anchors and corrects extraction errors; all manual interventions are logged and appended to the anchor’s transformation history. A dedicated validation_report (recorded in the pipeline</section>
<section class="wiki-section" id="sec-artifact_extraction"><section><h2>Introduction</h2><p>This methodology describes procedures for artifact extraction across modalities, normalization of technical objects, and the consequent implications for custody and provenance evaluation within a cyber-attribution scoring workflow. The approach is intended to be modality-aware, auditable, and calibrated to support downstream scoring axes such as custody, credibility, corroboration and clarity. The description below remains methodological: it explains rationale, processing steps and quality controls without making or repeating case-specific factual attributions from the underlying corpus.</p></section><section><h2>Data Processing and Extraction</h2><p>Data ingestion begins with provider-backed and fallback conversion of source documents into a common text-plus-structure representation. In the supplied pipeline, primary conversion is via a provider-backed Mistral OCR conversion (process_pdf_mistral_ocr.py), with an offline fallback implemented through a PyMuPDF4LLM path when provider conversion fails or times out. Structural parsing produces a markdown-like intermediate that preserves tables and figures with anchors, and a subsequent schema extraction stage emits indexed artifact records derived from text, tables and embedded images. This staged approach separates OCR/format conversion from structural parsing and artifact extraction so that provenance metadata can be recorded for each processing boundary.</p><p>Artifact inventories observed in sample outputs illustrate the diversity of modalities and the necessity of normalization: the preview counts indicate 115 domain tokens, 31 URLs, 22 IP-like tokens, 15 email-like tokens, five file-name tokens and two MD5 hash tokens. Modalities are treated explicitly as distinct extraction channels—rendered text, parsed hyperlinks, table cells, image OCR overlays and embedded metadata—each with extraction metadata that identifies source document, conversion method, bounding offsets and extraction timestamp. Where the raw extraction shows noisy or concatenated values (for example, merged URL and email text observed in source previews), the pipeline tags such cases for targeted normalization and human review to avoid propagation of spurious technical objects into analytic layers.</p></section><section><h2>References and Institution Inference</h2><p>Reference parsing is implemented as a discrete stage that recognizes citations, footnote-like references and explicit source attributions, links them to a source registry, and retains pointers to the originating structural element. The registry preserves the original reference token, normalized citation form, resolved target (when resolvable), and a provenance chain pointing to the document and conversion method. Institution inference proceeds after reference linkage; the supplied pipeline uses an infer_source_institutions.py component backed by a probabilistic model (gpt-5-mini) with optional web-fallback resolution. Because institution inference is an inferential operation rather than a direct extraction, the output is stored with calibrated confidence values and the provenance of the inference itself (model version, prompt template, and any web evidence used). All inferred institutional attributions are therefore first-class provenance objects and are not treated as incontrovertible facts in later scoring without corroboration.</p></section><section><h2>Scoring Framework</h2><p>The scoring framework operates on multiple axes—custody, provenance, credibility, corroboration, clarity and overall belief weighting—each computed from artifact-level and document-level inputs. Artifact normalization and technical object canonicalization are foundational because they determine whether items from different documents are recognized as the same technical object and therefore whether corroboration can be established. Normalization procedures include Unicode normalization, percent-decoding and canonicalization of URLs, punycode handling for internationalized domains, lowercasing and trimming of domain tokens, CIDR-aware normalization of IP ranges, canonical representation of cryptographic hashes (normalized hex, lowercase), and structured parsing of email tokens to separate display names from addresses. File-name normalization applies Unicode normalization and percent-decoding while preserving original byte offsets to maintain provenance.</p><p>Scoring combines deterministic signals (e.g., presence of a normalized hash or exact file-name match) with probabilistic signals (e.g., confidence of OCR conversion, institution-inference confidence). Document-level metrics such as custody_avg and grounding_avg (sample values in the supplied diagnostics include custody_avg ~52.58 and grounding_avg ~62.52) are used to calibrate artifact-level priors. Aggregation is performed by constructing a claim–evidence graph that links asserted claims to the set of normalized technical objects and their source references; graph edges carry provenance metadata and a custody score component. Belief weighting is computed with bootstrap-derived uncertainty estimates where feasible, and all intermediate scores retain confidence intervals so downstream consumers can apply policy thresholds appropriate to legal or operational contexts.</p></section><section><h2>Validation and Quality Assurance</h2><p>Quality assurance is multi-layered and designed to protect the integrity of provenance and custody assertions. First, conversion and parsing are monitored with deterministic validations: checks for idempotent normalization, round-trip URL reconstruction, and hash-signature verification of extracted artifact payloads where binary artifacts are available. Second, sampling-based manual review targets error-prone modalities revealed in diagnostics—OCR of figures and table cells, concatenated tokens in textual extractions, and ambiguous email strings—so that normalization heuristics and model prompts can be iteratively tuned. Third, provenance logging records the complete processing chain for every technical object, including original byte offsets, conversion method (primary or fallback), model/version</section>
<section class="wiki-section" id="sec-reference_parsing"><section><h2>Introduction</h2><p>This methodology chapter describes the structured approach to transforming rhetorically embedded references—footnote entries, inline citations, and bibliography items—into an analyzable source graph that supports reproducible cyber‑attribution scoring. The exposition focuses on procedural design and quality controls rather than on results or assertions from any particular report. The methods draw on the project’s ingestion outputs and tooling registry and are intended to ensure traceability from textual citation to canonical source node while preserving a defensible chain of custody for evidentiary artifacts.</p></section>

<section><h2>Scope and Analytical Units</h2><p>The unit of analysis for reference parsing is the citation instance: a discrete textual token or footnote that names, abbreviates, or links to an external source. Upstream units include pages, claims, artifacts (for example attachments or images), tables and figures; these units are enumerated in the ingestion summary used to calibrate parsing throughput. In the present pipeline preview, examples include a small set of pages and a larger complement of tabular and figure elements, which informs expected variance in citation density per page. All downstream graph nodes—document, page, claim, artifact, and source—are explicitly typed to permit provenance queries across the source graph.</p></section>

<section><h2>Data Ingestion and Extraction</h2><p>Document acquisition and initial parsing adopt a staged approach to preserve original layout and to capture contextual metadata. The pipeline records conversion provenance and method identifiers for each processing step. Where a primary provider conversion is used, the configuration records the primary converter name; when provider conversion fails or is unsuitable, a fallback stage performs conversion with alternative tooling. Ingestion captures and retains source metadata such as declared title, year, venue, and any available URL or identifier to facilitate later resolution and disambiguation of referents.</p></section>

<section><h2>PDF-to-Markdown Conversion and Text Normalization</h2><p>Text conversion is executed with an explicit primary-to-fallback flow so that conversion artifacts can be attributed to a conversion method during validation. The conversion stage emits a structured markdown-like representation that preserves anchors for tables and figures and demarcates footnote regions. The pipeline implementation records the converter identifiers and versions to the processing log so that any downstream anomaly can be traced to the conversion stage for reprocessing or revalidation.</p></section>

<section><h2>Structural Parsing</h2><p>Structural parsing decomposes documents into a type‑annotated parse tree that includes header blocks, paragraphs, footnotes, endnotes, captions, tables, and inline citations. The parser normalizes variant citation syntaxes (numeric brackets, author-date forms, free-text footnotes) to a canonical citation token. Each canonical token is linked to its origin (page number, paragraph offset, and byte range) so that the source graph can retain both semantic linkage and precise textual provenance for later forensic review.</p></section>

<section><h2>Artifact Extraction</h2><p>Artifacts such as embedded files, extracted tables, and figures are indexed as first‑class objects. The artifact index records extraction method, mime type, and any OCR or structural confidence scores. Where artifacts include captions that contain citation fragments, those fragments are incorporated into the same canonical citation token space used for footnotes to enable consistent citation linkage across text and artifact modalities.</p></section>

<section><h2>Reference and Footnote Parsing</h2><p>The reference parsing component identifies footnote markers and bibliographic entries, normalizes their surface forms, and resolves them to registry candidates. The parser performs syntactic normalization to remove typographic variability, and then applies a staged resolution sequence that uses exact match on declared identifiers (for example URLs or DOIs), bibliographic field matching, and fuzzy name matching when identifiers are absent. Footnote text that contains descriptive provenance (contract awards, organizational bios, or press reporting) is tokenized and attached to the candidate resolution records so that the explanatory context is preserved in the source graph. The parser also annotates ambiguous referents rather than forcing a best‑guess resolution, enabling downstream analytic stages to apply conservative weighting when ambiguity persists. This stage implements explicit handling for multi‑reference footnotes (where a single marker cites multiple sources) and for intra‑document cross‑references so that the resulting citation linkage supports both one‑to‑many and many‑to‑one relationships.</p></section>

<section><h2>Institution Inference and Source Attribution</h2><p>Institution inference operates on resolved source records to determine the most likely institutional owner or publisher for each source. The procedure combines automated heuristics with a model‑assisted inference engine; the recorded pipeline indicates that an inference module is used together with an optional web fallback for verification. The inference step produces a ranked set of institution hypotheses with associated confidence scores and records the inference method and any external queries performed. The aim is to enable institution‑level aggregation in the source graph while preserving uncertainty so that institutional affiliations are not over‑asserted in scoring computations.</p></section>

<section><h2>Claim–Evidence Graph Construction</h2><p>Parsed citations and inferred institutions are integrated into a claim–evidence graph in which claim nodes are linked to source nodes and artifact nodes via explicit provenance edges. Citation linkage is represented as typed edges that capture the nature of the citation (supporting, conflicting, background, or methodological reference) where such distinctions are textually evident. The graph schema captures the origin offsets for each linkage so reviewers may reproduce the linkage by inspecting the original document at the stated location. The resulting source graph supports queries that compute degrees of corroboration, identify clustering of institutional ties, and trace the propagation of specific claims across multiple documents.</p></section>

<section><h2>Scoring Framework Overview</h2><p>The scoring framework leverages the claim–evidence graph to compute attribution‑relevant metrics. Scores are composed from modular sub‑scores such as source reliability, temporal alignment, artifact integrity, and citation linkage strength. Importantly</section>
<section class="wiki-section" id="sec-institution_inference"><h2>Introduction</h2>
<p>This section documents the methodological approach to institution inference and source typology within a cyber‑attribution scoring pipeline. It situates institution inference as a traceable, reproducible transformation that maps parsed source metadata and contextual indicators onto an institution class schema, and it explains how that mapping interacts with downstream credibility weighting and corroboration eligibility. The rationale emphasizes transparency, repeatability, and defensibility: institution inference must be auditable against parsed source artifacts and robust to heterogenous input formats produced by the data ingestion pipeline.</p>

<h2>Scope and Units</h2>
<p>The operational units for institution inference are discrete source records as they emerge from structural parsing and reference parsing steps. Each record includes origin metadata (for example entity_name and publication_or_venue), declared source_type (such as government, press_media, academic, ngo, internal_document_section, or other), and any parsed bibliographic or contextual identifiers. The method treats these fields as primary signals and treats derived attributes (for example inferred organizational hierarchy or geographic locus) as secondary, requiring explicit provenance tracing back to the original parsed tokens.</p>

<h2>Data Ingestion</h2>
<p>Input data arrive through a multi‑stage pipeline that converts heterogeneous documents into structured records. The pipeline stages referenced here include OCR and provider‑backed PDF conversion, fallback parsing, and subsequent structural extraction. To preserve provenance, the pipeline attaches processing metadata to each source record documenting the conversion path and any fallbacks applied. This provenance is a mandatory input to institution inference, allowing analysts to assess whether inference results may have been affected by conversion artifacts.</p>

<h2>PDF-to-Markdown Conversion</h2>
<p>Conversion to machine‑readable text is performed using primary and fallback methods. The methodology records which conversion method produced the canonical text for each source record. Where provider conversion (for example a Mistral OCR process) fails and a local fallback is used, the inference process flags increased uncertainty in tokenization and referenced entity extraction; this uncertainty is carried forward into credibility weighting as an evidence quality modifier.</p>

<h2>Structural Parsing</h2>
<p>Structural parsing identifies discrete document components (titles, headings, footnotes, tables, figures, and bibliographic references) and emits anchors that are used in downstream institution inference. The approach requires that any inferred institutional claim cite the specific structural anchor from which entity strings were extracted, enabling reviewers to verify or challenge the instantiation of institution labels.</p>

<h2>Artifact Extraction</h2>
<p>Artifact extraction isolates technical indicators, contract references, figure captions, and other artifacts that can corroborate institutional affiliations. Artifacts are indexed and linked back to source records. The inference algorithm draws secondary signals from artifact co‑occurrence patterns (for example, a contract number appearing alongside a corporate name) but mandates that primary labels be tied to explicit metadata strings whenever available.</p>

<h2>Reference Parsing</h2>
<p>Reference parsing resolves citations and footnote references to entries in the source registry. Parsed references provide relational edges between documents and known institutional entities. The system distinguishes explicit self‑identification (for example an author line or corporate heading) from implicit association (for example a contract award notice mentioning an enterprise) and records that distinction in the provenance metadata attached to each inferred institution.</p>

<h2>Institution Inference and Source Typology</h2>
<p>Institution inference applies a typology mapping that groups sources into institutional classes (for example government, press, commercial/private sector, academic, non‑governmental organization, internal/corporate report, and uncategorized/other). The mapping uses a ruleset combining deterministic string matches, metadata heuristics (such as venue names and URL domains), and probabilistic entity‑matching signals derived from parsed artifacts. Each inference produces a structured output containing the chosen institution label, supporting evidence anchors, a confidence score that reflects both token match strength and conversion provenance, and a flag indicating whether the inferred class was derived from primary self‑identified metadata or inferred from contextual artifacts.</p>

<p>The source typology directly governs two downstream behaviors. First, credibility weighting: institutional class informs an initial credibility prior that is then modulated by evidence quality indicators and corroboration. For example, an academic publication and an internal document section may receive differing priors; these priors are explicit parameters in the scoring model rather than opaque adjustments. Second, corroboration eligibility: some institutional classes are eligible to corroborate certain classes of claims only when corroboration is supported by independent lines of evidence. The eligibility rules encode domain policy (for example requiring at least one independent government or academic confirmation to elevate confidence for high‑impact attribution claims) and are applied deterministically to the provenance‑annotated evidence graph.</p>

<h2>Claim–Evidence Graph</h2>
<p>Inferred institutions are represented as nodes within a claim–evidence graph that links claims, artifacts, and source records. Institution nodes are annotated with typology, confidence, and provenance anchors. Graph traversal algorithms use these annotations to identify independent corroborating paths and to compute redundancy and diversity metrics that feed into credibility weighting.</p>

<h2>Scoring Overview</h2>
<p>The scoring framework composes three orthogonal dimensions: source institutional prior (derived from the typology), evidence quality (derived from processing provenance and artifact extraction), and corroboration strength (derived from graph‑based path analysis</section>
<section class="wiki-section" id="sec-claim_evidence_graph"><h2>Introduction</h2>
<p>This methodology describes a systematic approach to constructing a claim-evidence graph for cyber-attribution scoring. The objective is to represent claims, sources, and artifacts as explicit graph nodes and to prescribe deterministic transformations that preserve anchor-level traceability and resist analytic circularity. The method is grounded in the supplied structured inputs: a claims inventory (claims C001–C010), a source registry (sources SRC0001–SRC0010), an artifact corpus (examples ART00001, ART00002, ART00084, ART00086, ART00063), and a set of pre-aggregated evidence items (E-0001 through E-0010). Where numerical summaries appear in the raw data (for example artifact counts by type or per-claim evidence_count), they are used solely to illustrate pipeline behavior and to parameterize quality-control thresholds rather than to substantively summarize report findings.</p>

<h2>Data processing and extraction</h2>
<p>Textual and structured inputs are normalized into three canonical tables: claims, sources, and artifacts. Structural parsing leverages the anchor tokens present in the source-level citations (for example citation anchors CIT0001–CIT0009 and support_anchor_ids such as P000-B02) to retain page- and block-level provenance. Artifact extraction produces typed artifact records (domain, url, email, ip, file_name, hash_md5) as in the ART00001–ART00190 series; extraction confidence scores from the raw_artifacts_preview are attached to each artifact record. During ingestion the system records chain metadata such as origin_id clusters (for example ORIG:src0001+src0002+src0003+src0006), unique_origin_count, and recovered_reference_count to support traceability and downstream diagnostics.</p>

<h2>References and institution inference</h2>
<p>Reference parsing reconciles bibliography and inline citations to canonical source identifiers. Each source record is annotated with a source_kind and authoring organization when available (for example SRC0001: vendor; SRC0002: government; SRC0003: media). Institution inference applies a conservative, evidence-first mapping that associates organizational entities only when explicit anchors or artifact provenance support the mapping. For example, mapping decisions use: (a) direct anchors that tie artifacts into a named source or organizational description (anchored citations such as those listed in each claim's citations array), (b) artifact proximity heuristics (artifact_proximity_tiers and artifact_proximity_hierarchy), and (c) chain provenance diagnostics (context_completeness, anchor_quality). All inferred institutional links are recorded with provenance and a confidence band so that every asserted institution-to-claim edge can be traced back to the original anchor or artifact and re-examined.</p>

<h2>Claim-evidence graph construction and scoring framework</h2>
<p>The claim-evidence graph models claims as nodes linked to evidence items and to artifact and source nodes. Each evidence item (E-0001..E-0010) carries modalities, features (I, A, M, P, T), anchors, and a computed probative_weight; these fields are preserved as edge attributes. The scoring framework computes per-claim scores from three core dimensions (the "core 3C"): chain_of_custody_provenance, credibility_independence, and corroboration_convergence. Intermediate diagnostics—such as artifact_traceability, lineage_quality, and anchor_quality—feed into custody and grounding sub-scores. Aggregation uses weighted sums with penalties where anti-circularity checks detect reuse of report-derived material: origin clustering (origin_id), source_unique_origin_count, and recovered_reference_count are used to detect and discount evidence that lacks independent provenance. The system implements explicit anti-circularity safeguards by (1) tagging evidence that is report-derived, (2) preventing a claim's own derived artifacts from increasing that claim's independent corroboration score, and (3) applying a single_source_penalty when eligible_source_count is low. Statistical calibration (shrinkage and bootstrap routines documented in full_icj_v4) provides reliability adjustments and confidence intervals for aggregated axes such as custody, clarity, and corroboration.</p>

<h2>Validation, quality assurance, and governance</h2>
<p>Validation employs deterministic unit checks and empirical bootstrap diagnostics. Unit checks confirm that every claim node has at least one traceable anchor (claim_anchor_alignment) and that every evidence-to-artifact link preserves the original artifact location metadata (page and block). Quality assurance reports include chain_provenance_diagnostics and claim_support_coverage measures drawn from the scoring_bundle to surface gaps such as low provenance integrity or missing source diversity. Where the statistical_profile indicates limited effective_evidence_n or reliability_factor below configured thresholds, the methodology prescribes human review gates. All transformations and calibration steps are logged to enable re-computation and external audit; these logs are the basis for traceability and for demonstrating that anti-circularity safeguards were applied consistently.</p></section>
<section class="wiki-section" id="sec-scoring_overview"><section>
  <h2>Introduction</h2>
  <p>This methodology describes a reproducible scoring architecture for cyber-attribution assessments. It distinguishes (a) the structured extraction of textual and technical artefacts from a source document and (b) the inferential procedures that convert those extraction outputs into quantitative axes at the claim-level and a synthesized document-level judgement. The text that follows is grounded in the supplied scoring bundle and extraction outputs (for example: the document-level metrics recorded under document_scores_v4 and the per-claim score previews in claim_score_preview_v4) and focuses on methodological rationale rather than substantive case findings.</p>

  <h2>Data processing and extraction</h2>
  <p>Our pipeline separates conversion, structural parsing, and artefact extraction as distinct stages. The conversion stage produces a machine-readable representation of the source (the pipeline represented by pdf_to_markdown_primary and its fallback), from which structural parsing isolates anchors and evidence anchor identifiers. Artefact extraction then produces typed items (domains, URLs, IPs, hashes, emails, filenames) with provenance metadata; these are exemplified in the supplied artifacts array (artifact entries such as ART00001..ART00190). Evidence assembly groups artefacts into evidence_items with modality tags and feature vectors (for example the evidence_items entries E-0001..E-0010 that include modalities, feature components such as I/A/M/P/T, and a computed probative_weight). The methodological separation ensures that extraction outputs remain auditable: each artefact and evidence item records source_ids, artifact_ids and anchor block_ids so that chain-of-custody diagnostics can be computed independently of later inferential weighting.</p>

  <h2>References and institution inference</h2>
  <p>Reference parsing is treated as a provenance layer: citations and footnotes are normalized to a source registry. In the supplied bundle this registry is visible under normalized.sources (entries SRC0001..SRC0010) and citation coverage figures (for example citation_coverage_sources_0_1 = 0.9). Institution inference uses parsed source metadata and the supplemental institution_inference routine to map source strings to institutional identifiers and types (vendor, government, media, academic, unknown). The inference output is consumed by credibility scoring but remains logically distinct from the raw extraction outputs: the parser provides a canonical list of sources and anchors which downstream scoring modules reference when computing source-type distributions and domain-independence metrics.</p>

  <h2>Scoring framework: from claim-level axes to document-level synthesis</h2>
  <p>At the claim-level we evaluate a multidimensional vector of axes that capture distinct epistemic concerns. In this implementation each claim is assigned grounding, custody, credibility, corroboration, confidence, and clarity components (the scoring_bundle.claim_scores and full_icj_v4.claims show per-claim grounding_0_100, custody_0_100, credibility_raw_0_100, corroboration_raw_0_100, confidence_0_100 and clarity_0_100). Evidence items feed claim-level aggregates via evidence_weight_aggregate and origin_cluster_weights; for example individual evidence probative_weight values (visible in evidence_items) are aggregated into an evidence_weight_aggregate per claim and form the primary data contribution to the claim-level credibility axis.</p>
  <p>Crucially, inferential weighting is separated from extraction outputs. Extraction establishes what is present and its provenance (anchors, artifact identifiers, source ids). Inferential weighting applies models and heuristics — calibrated shrinkage, single-source penalties, modality and domain-independence adjustments, and gravity-weighting for allegation seriousness — to those extracted features. The supplied data illustrates this separation: raw extraction counts (sources_total = 10, citations_total = 17, claims_count = 10) are distinct from inferential outputs such as claim final_score and belief_0_100 (examples in claim_score_preview_v4 and scoring_bundle.scoring.claim_scores). Statistical calibration parameters (statistical_calibration_v4, shrinkage_lambda values) formalize how claim-level axes are regularized toward prior expectations before document-level synthesis.</p>
  <p>Document-level synthesis composes claim-level vectors into headline and coverage vectors and then into scalar summaries (for example document_scores_v4.belief_weighted_0_100 and grounding_avg_0_100). Aggregation applies a combination of weighted averaging and gate conditions: the seriousness_gate is evaluated on aggregated metrics (for instance overall_claim_score_mean and median credibility_independence) to determine whether the document passes pre-defined thresholds for downstream actions. All aggregation steps preserve linkages back to evidence items and sources to support auditability and sensitivity analysis.</p>

  <h2>Validation and quality assurance</h2>
  <p>QA operates on two axes: structural validation of extraction outputs and statistical validation of inferential models. Structural checks confirm that anchors, artifact identifiers and source mappings are present and that evidence_items reference artifact_ids and source_ids (this is verifiable against the normalized.artifacts and normalized.sources tables). Chain-of-custody diagnostics (chain_provenance_diagnostics fields in claim-level scoring) report measures such as artifact_traceability and anchor_quality to flag incomplete provenance. Statistical QA uses bootstrap and shrinkage diagnostics (examples: the bootstrap_95ci block in document_scores_v4 and shrinkage_lambda values in statistical_calibration_v4) to quantify estimator uncertainty and to calibrate reliability_factor and effective_evidence_n for each claim and for the document aggregate.</p>
  <p>End-to-end reproducibility is enforced by preserving intermediate outputs (markdown conversion artifacts, parsed anchors, artifact indices, evidence groups, and the staging of statistical calibration), by recording pipeline methods, and by supporting targeted reweighting or reanalysis if new sources or corrected provenance arrive. The framework thereby enables transparent tracing from an evidence artifact to its contribution to claim-level axes and to the final document-level score, while keeping extraction and inferential weighting as separable, auditable stages.</p>
</section></section>
<section class="wiki-section" id="sec-chain_of_custody"><section>
  <h2>Introduction</h2>
  <p>This methodology chapter defines the chain‑of‑custody axis and the procedures by which document‑level artifacts and derived evidence are processed, scored, and quality‑assured for cyber‑attribution analysis. The intent is to describe, in general methodological terms, how custody‑related variables—provenance, integrity, time anchors, artifact identifiers, and versioning—are operationalized within an automated evidence pipeline and how quality controls and penalties are applied to limit over‑interpretation. The description below is grounded in the supplied reporting package fields (for example, document_scores_v4, raw_artifacts_preview, the artifacts list ART00001–ART00190, evidence_items E‑0001–E‑0010, and claims C001–C010) and explains how those inputs inform custody scoring without asserting substantive findings about any particular case.</p>

  <h2>Data processing and artifact extraction</h2>
  <p>Ingestion begins with structured extractions and a lightweight content inventory. The system ingests the provided raw_artifacts_preview (for example, domain_count = 115, ip_count = 22, url_count = 31, file_name_count = 5, hash_md5_count = 2) and the detailed artifacts table (ART00001 … ART00190). Each artifact record includes an artifact_type, canonicalized value, an extraction location, an extracted_from tag, and an extractor confidence score. These fields are used to create an artifact registry keyed by artifact identifiers (the artifact_id values) to ensure traceable references during downstream linking to evidence and claims.</p>
  <p>Structural parsing first normalizes extractable fields (domains, URLs, IPs, emails, file names, hashes) and records canonical forms alongside extraction metadata. A provenance ledger entry is created for each artifact that records origin (source document or anchor), the extractor identity, the extraction timestamp, and any normalization steps. Time anchors are ingested when present (publication dates in normalized source records and any explicit timestamps in artifact locations) and stored in the ledger to enable temporal validation of claimed links between artifacts and events. The artifact registry therefore provides the unique artifact identifiers required for subsequent chain‑of‑custody scoring.</p>

  <h2>Reference parsing and institution inference</h2>
  <p>Reference parsing consumes the normalized source list (for example, scoring_bundle.full_icj.normalized.sources SRC0001–SRC0010), citation anchors, and the evidence_items table (E‑0001 to E‑0010). For each evidence_item the pipeline reconstructs an origin signature (e.g., ORIG:src0001+src0002+…) and maps artifact_ids to source anchors. Institution inference uses explicit authoring_org entries and domain signals to produce institution candidate tags; this is treated as a probabilistic label rather than a deterministic assignment when the source metadata are incomplete. All inferred institutional attributions are retained with provenance metadata indicating which fields (authoring_org, domain, anchor) supported the inference and with a confidence value derived from source completeness.</p>
  <p>Reference parsing also computes source coverage metrics used in corroboration and credibility calculations. For example, the normalized bundle records citation_coverage_sources_0_1 = 0.9 and sources_total = 10; such summary metrics inform policy gates (e.g., whether single‑source penalties apply) but do not replace artifact‑level provenance checks.</p>

  <h2>Scoring framework and chain‑of‑custody variables</h2>
  <p>Chain‑of‑custody scoring is implemented as a modular, explainable vector of variables that map to observables in the ingestion output. The primary custody variables are defined and measured as follows: provenance is a quantitative index of source traceability and disclosure completeness (for example, derived from chain_provenance_diagnostics.provenance_quality and provenance counts per evidence_item); integrity measures the presence of independent artifact identifiers and signals of tampering or redaction (the integrity component is drawn from extracted integrity flags and explicit integrity scores where available); time anchors measure whether artifacts and claims are temporally grounded (presence/absence of document or artifact timestamps influences a time anchors subscore); artifact identifiers evaluate whether artifacts are uniquely and stably identified (artifact_id presence, hash values, and stable URLs feed this component); versioning captures explicit version metadata or internal evidence of document revision history and is assigned when version identifiers or multiple dated anchors are present (the scoring bundle stores versioning partial scores, for example versioning = 0.25 in custody breakdowns for some claims, which illustrates how incomplete version metadata reduces the versioning component).</p>
  <p>These components are combined into a custody composite score with calibrated weights. The scoring code also computes subordinate diagnostics such as artifact_traceability, anchor_quality, and artifact_proximity_tiers (direct/contextual/local/remote) that permit fine‑grained audit of why custody rose or fell for a given claim. The bundle shows claim‑level custody_0_100 values (for illustration, custody_avg_0_100 = 52.58 in document_scores_v4) and per‑claim custody components in core_3c.chain_of_custody_provenance; these are used to demonstrate the mapping between observed metadata and the custody composite.</p>
  <p>Penalties are applied when custody or corroboration prerequisites are unmet. The system enforces explicit penalties for single‑source dependence (single_source_penalty appears in credibility calculations) and for missing integrity or provenance signals. Penalty multipliers (for example penalty_multiplier = 1 in the claim_scores table when no penalty was triggered) are applied multiplicatively to evidence weights; a non‑unity multiplier reduces the final evidence weight and is recorded in the claim scoring record so the derivation is auditable. Penalties are governed by policy thresholds (e.g., required minimum provenance quality, minimal integrity indicator present, or minimal time anchors coverage) and an override log must record any manual adjustments.</p</section>
<section class="wiki-section" id="sec-credibility_corroboration"><section>
  <h2>Introduction</h2>
  <p>This methodology describes a reproducible, auditable approach to scoring attribution-relevant assertions along a credibility axis augmented by a corroboration subcomponent. The purpose is to make explicit the provenance, processing, and inferential rules that transform heterogeneous inputs—such as internal vendor reports, government statements, media articles, academic papers, nonstandard web resources, and NGO publications—into calibrated numeric indicators used in downstream aggregation. The method is designed to be data-driven and defensible: it operates on structured metadata and extracted artifacts, leverages explicit provenance anchors, and applies transparent weighting rules. Throughout the chapter we cite structural elements from the ingest bundle (for example, source counts and metadata fields) to illustrate how the rules operate; we do not reiterate, evaluate, or adjudicate the substantive content of any underlying allegation or claim.</p>

  <h2>Data processing and extraction</h2>
  <p>Inputs are first normalized into a canonical schema capturing source identifier, declared source type, authoring organization, publication date, URL or file identifier, and extracted artifacts. Source-type counts supplied with the intake (for example: internal_document_section: 1, government: 2, press_media: 1, academic: 3, other: 2, ngo: 1) guide initial expectations about the mix of media and institutional forms present. Textual and non-textual items are parsed for structured artifacts (domains, IP ranges, emails, file identifiers, hashes) and for citation anchors that link specific artifacts to claim statements. Extraction is conservative: artifact records retain page and block anchors and a confidence score so that subsequent provenance computations can penalize ambiguous or poorly localized extractions. Where multiple files or anchors describe the same artifact, records are de-duplicated but retain an origin signature list so that the system can account both for repeated reporting and for true independent corroboration.</p>

  <h2>References and institution inference</h2>
  <p>After artifact extraction, references are parsed to resolve canonical source identities and to infer institutional attributes. The normalization step maps raw source identifiers (for example SRC0001–SRC0010) to source_kind tags such as "vendor", "government", "media", "academic", "unknown", and "ngo" based on publisher and domain fields. Institution inference uses explicit publisher metadata where available and heuristics for domain-to-organization mapping where it is not; these mappings are recorded with provenance so any inference can be reviewed. The pipeline distinguishes single-source entries (is_single_source flags) from material corroborated across multiple origins. The design treats institutional metadata as evidence about the potential reliability and independence of a source, not as a substitute for artifact-level provenance; that is, an authoritative institutional label elevates a source's prior credibility only when the artifact-level chain of custody and anchoring meet minimum standards.</p>

  <h2>Scoring framework</h2>
  <p>The scoring framework operationalizes five conceptual elements into numerical sub-scores that feed the credibility axis: chain-of-custody provenance, credibility (quality and diversity of sources), corroboration (convergence across independent origins and modalities), custody diagnostics (artifact integrity and traceability), and claim coverage scaling. Source hierarchy is implemented by ranking source kinds into ordered bands that determine base quality priors; for example, peer-reviewed academic publications or verifiable government records receive higher base priors than unattributed web pages. Independence logic treats each origin signature as an atomic evidence origin; origins that share an identical origin_signature (for example ORIG:src0001+src0002+...) are clustered and down-weighted to avoid double-counting. Corroboration is computed from the set of eligible, independent origins and from modality diversity: corroboration increases when multiple, domain-independent sources and different evidence modalities (technical artifacts, documentary references, public records) support the same claim. Claim coverage scaling adjusts corroboration contribution by the fraction of claim elements addressed by supporting anchors; the pipeline computes a claim_coverage_factor so that partial support (e.g., support for actor identity but not for intent) receives commensurately scaled corroboration credit rather than full credit. Exclusion criteria are explicit: low-value source classes such as single-page scraped aggregator domains, automated content farms, and unverifiable personal blogs are placed in an exclusion bucket unless they provide unique artifacts with independent anchors and verifiable artifact identifiers; exclusion criteria are recorded as both a categorical tag and as a list of excluded_source_ids so that excluded material remains discoverable but does not inflate metrics such as eligible_source_count or source_weight_sum.</p>

  <h2>Validation and quality assurance</h2>
  <p>Quality assurance proceeds at three levels. First, extraction-level QA verifies that artifacts have non-null anchors and confidence scores; records failing anchor or artifact traceability checks are marked and excluded from custody-sensitive computations. Second, provenance QA inspects origin clusters and ensures that de-duplication did not conflate truly independent reports; the system reports diagnostics such as context_completeness and provenance_quality so operators can review chains where shrinkage or penalties were applied. Third, statistical calibration applies shrinkage and bootstrap diagnostics to stabilize sub-scores when evidence counts are small; the intake includes aggregate diagnostics (for example sources_total = 10 and citation_coverage_sources = 0.9) that the calibration routine uses to set reliability factors and effective evidence n. All calibration and exclusion decisions are logged so that any auditor can reconstruct the scoring path from raw source metadata and artifact anchors to the final corroboration and credibility numerics.</p>

  <p>This methodology foregrounds transparency: rules for the "source hierarchy", computational "independence", the mechanics of "corroboration", the operation of "claim coverage" scaling, and the "exclusion criteria" for low-value source classes are deterministic and applied to the same canonical fields that are retained in the processing bundle. Those who apply or review the method should consult the normalized source list, artifact anchors, and origin_signature fields to validate that the implemented operations correspond to the intended inferential behavior.</p>
</section></section>
<section class="wiki-section" id="sec-clarity_axis"><section>
  <h2>Introduction</h2>
  <p>This methodology subsection defines the clarity axis and explicates how it is operationalized for assessing pathways of state responsibility: actions by state organs, effective control over non-state actors, and failures of due diligence. The account that follows is methodological and normative; it describes the variables, diagnostic criteria, and aggregation steps used to convert heterogeneous documentary and technical inputs into interpretable clarity scores. The description is grounded in the supplied scoring bundle and document-level metrics (for example, the document-level clarity average and claim-level clarity components appearing in the bundle) to show how the algorithm consumes evidence without reiterating or adjudicating substantive allegations contained in the source material.</p>

  <h2>Data processing and artifact extraction</h2>
  <p>Input data are ingested from parsed structural elements, extracted artifacts, and annotated citations in the scoring bundle. The pipeline first canonicalizes artifacts (URLs, domains, IPs, file names, hashes, emails) and then links them to evidence items and to claim anchors. The provided artifact inventory (for example, the artifacts list and evidence_items array in the scoring bundle) supplies provenance markers and modality labels that feed the clarity calculations. Extraction preserves provenance chains and timestamps used in the custody diagnostics; these provenance signals are retained as metadata and influence the weighting applied to link-specific clarity. Parsing produces per-claim evidence anchor sets which are used to compute measures such as evidence_anchor_count and evidence_support; these quantities inform the denominators of clarity subcomponents rather than serving as binary determinations of responsibility.</p>

  <h2>Reference parsing and institutional inference</h2>
  <p>Reference parsing separates authoring entities, publisher metadata, and institutional names from body text and citation anchors. Where institutional names are present, the system normalizes them to canonical institution identifiers and records their role (authoring_org, publisher). Institution inference is conservative: it flags candidate organizational relationships and records signal strengths rather than asserting institutional responsibility. Those normalized references and institution indicators become inputs to actor-specific clarity submetrics (actor_specificity and state_actor_signal) and to cross-source linkage checks that contribute to control-path assessments. The scoring bundle contains explicit source metadata and origin signatures that the parser uses to derive a source-coverage profile; these parsed references are therefore central to the later legal-path scoring that distinguishes between claimed state actor roles and evidence of support or control over non-state actors.</p>

  <h2>Clarity scoring framework for state responsibility pathways</h2>
  <p>The clarity axis is decomposed into three legal-responsibility pathways that correspond to distinct doctrinal modes: (1) conducted by state organs (state organs), (2) non-state actors under state control (control over non-state actors), and (3) state knowledge plus failure to prevent (due diligence). For each claim the system computes act_specificity, actor_specificity, and link_specificity and then maps these to pathway-specific subscores: organ_path_clarity, control_path_clarity, and due_diligence_path_clarity. Each subscore is the product of three factors: evidential linkage (quality and proximity of anchors linking an actor to an act), institutional signal (normalized presence and quality of references to state entities or institutional identifiers), and legal-path coverage (whether the necessary elements for the legal mode are represented among anchors). The framework treats the three pathways as non-exclusive and reports both the numeric subscore and an interpretive band (for example, a scalar in [0,1] and a categorical assessment such as "yes", "partial", or "no"). The scoring logic is deliberately modular so that organ-based signals (strong actor specificity combined with institutionally authoritative references) raise organ_path_clarity, while dispersed references, indicia of command-and-control infrastructure, or documented patterns of interaction increase control_path_clarity. Due diligence is assessed by the presence of evidence indicating state knowledge or capability to prevent plus indicators of omission; because such evidence is often indirect, due_diligence_path_clarity typically receives greater shrinkage and conservative calibration.</p>

  <h2>Aggregation, calibration and statistical shrinkage</h2>
  <p>Per-claim pathway subscores are aggregated to the document level using reliability-weighted averaging with shrinkage toward empirically derived priors. The document-level clarity average (reported in the bundle) is the shrinkage-adjusted mean of claim clarity scores. Calibration parameters (reliability_factor, shrinkage_lambda) are estimated from the effective evidence count and are used to temper pathway estimates when evidence quantity or provenance quality is limited. This ensures that claims with few anchors or with weak custody signals do not produce overconfident pathway declarations. Where modality diversity and independent source coverage are present, the aggregation increases effective weight for control-path or organ-path signals; where only single-source or single-modality evidence exists, the calibration reduces the pathway score accordingly.</p>

  <h2>Chain of custody, credibility and corroboration controls</h2>
  <p>The clarity axis is computed after chain-of-custody and credibility diagnostics. Chain provenance diagnostics and custody scores modulate link-specific contributions to organ_path_clarity and control_path_clarity by discounting anchors with missing provenance or weak artifact traceability. Credibility and corroboration indices inform whether multiple independent lines of evidence support the same legal pathway; the system records both raw corroboration measures and a corroboration_convergence adjusted factor that increases confidence in pathway inference only when domain independence and modality diversity criteria are satisfied.</p>

  <h2>Validation and quality assurance</h2>
  <p>Quality assurance comprises unit-level checks, cross-claim consistency tests, and bootstrapped uncertainty estimation. Unit tests validate that (a) artifact-to-evidence links preserve provenance; (b) parsed institutional names map to canonical identifiers; and (c) pathway subscore computations correctly apply custody and credibility shrinkage. The pipeline also produces bootstrap 95% confidence intervals for document-level vectors to surface statistical uncertainty. Human review gates are prescribed for high-consequence pathway outputs; whenever organ_path_clarity, control_path_clarity, or due_diligence_path_clarity exceed operational thresholds, the system emits diagnostics (anchor lists, provenance summaries, and the contributing evidence_ids)</section>
<section class="wiki-section" id="sec-aggregation_calibration"><p>Introduction: This methodological section describes procedures for claim-to-document aggregation, weighting, calibration, and uncertainty quantification used in cyber-attribution scoring. The exposition is intentionally methodological: it illustrates how structured fields from the scoring bundle and document-level diagnostics are used to move from extracted artifacts and evidence to calibrated claim scores and document-level summaries. The presentation references items in the supplied raw data (for example: document_scores_v4, scoring_bundle.claims and evidence records) to ground methodological choices without advancing or repeating substantive allegations contained in those records.</p>

<p>Data processing and evidence extraction: Raw inputs are the parsed artifacts and anchors produced during structural parsing and artifact extraction. In the supplied bundle these appear as artifact records (scoring_bundle.normalized.artifacts) and evidence items (scoring_bundle.normalized.artifacts and scoring_bundle.normalized.evidence_items). Each evidence item contains an evidence_kind, a list of artifact_ids, source_ids, feature vectors (for example I, A, M, P, T) and a derived probative_weight. Methodologically, artifact extraction yields atomic tokens (domains, urls, ip, hashes, emails) that are then associated with anchors in the document. Evidence items are constructed by grouping such artifacts with provenance chains and modalities; the evidence-level features are used to compute an evidence_weight_aggregate which subsequently feeds claim-level aggregation.</p>

<p>Reference parsing and institutional inference: The system parses citations and normalized source metadata (scoring_bundle.normalized.sources) to compute source-level attributes such as source_kind, authoring_org, and indicator flags (is_single_source, has_stated_conflict). Institutional inference uses those fields to assess domain independence and source diversity. These assessments drive credibility adjustments: for example, single_source_penalty and source_type_counts are computed from source metadata and applied as multiplicative modifiers in the credibility axis. Recovered reference counts and source coverage diagnostics are retained in the claim record to support transparency and downstream review.</p>

<p>Aggregation and weighting framework: Claim-to-document aggregation proceeds in stages. First, evidence-level scores (probative_weight, features like I/A/M/P/T) are combined to produce an evidence_weight_aggregate for each claim, as shown by evidence_weight_aggregate fields in scoring_bundle.scoring.claim_scores. The claim-level core axes (custody/chain_of_custody_provenance, credibility_independence, corroboration_convergence and related chain_provenance_diagnostics) are computed by aggregating evidence weights while respecting provenance hierarchies (origin clusters and unique_origin_count). Aggregation strategy is explicit: evidence contributions are weighted by probative_weight and adjusted for reuse across claims using nondup and origin_cluster_weights to avoid double-counting. Where the scoring bundle includes penalties (single_source_penalty, excluded_sources_count), those are applied as multiplicative reductions to the credibility axis, not to the raw evidence weights, so that provenance-based penalties remain separable from technical probative signals.</p>

<p>Calibration: Calibration is applied after raw aggregation to temper overconfidence from small or biased evidence sets. The system-level statistical calibration components in full_icj_v4 (statistical_calibration_v4) illustrate the approach: a reliability_factor and an effective_evidence_n are estimated for each claim and used to compute shrinkage_lambda values per axis (examples: shrinkage_lambda.custody, .credibility, .clarity). Shrinkage mixes the observed axis score with prior_scores_0_1 (priors derived from corpus-level expectations) according to the estimated lambda, thereby implementing empirical Bayes–style shrinkage. Saturation factors and credibility_quality_gate values limit the influence of very large or repetitive chains (chain_saturation, corroboration_saturation). The calibration stage thus produces adjusted axis scores (for example custody_0_100 calibrated) that are reported alongside raw scores for auditability.</p>

<p>Uncertainty quantification and dispersion diagnostics: The methodology quantifies uncertainty through multiple complementary mechanisms. Parametric calibration produces point estimates and shrinkage-adjusted axes; nonparametric bootstrap diagnostics supply interval estimates and dispersion diagnostics. The supplied document_scores_v4.bootstrap_95ci block demonstrates bootstrap-derived 95% intervals for selected aggregates (for example belief_weighted and custody_avg). These intervals and their widths are used as dispersion diagnostics: a narrow CI indicates low dispersion and greater stability of the aggregated estimate; a wide CI signals high dispersion and calls for cautious interpretation. Additional uncertainty signals are retained at claim level (confidence calibration, stated_confidence and uncertainty_transparency) and at evidence level (integrity_signal, provenance_quality). Effective reporting surfaces both central estimates and dispersion measures so downstream users can interrogate both point belief and uncertainty.</p>

<p>Validation and quality assurance: Quality assurance comprises automated diagnostics and human review triggers. Automated checks include provenance completeness (chain_provenance_diagnostics.context_completeness), anchor quality, artifact_traceability, and saturation gates (corroboration_source_quantity). Bootstrap-based stability tests flag claims with high dispersion for secondary review. A separate credibility audit evaluates source_type_counts and quality_top against credibility_quality_gate thresholds. Finally, all calibrated scores are retained with their raw inputs (evidence anchors, origin_cluster_weights, and prior_scores) to enable reproducibility and legal-academic review.</p>

<p>Concluding methodological remarks: The described pipeline separates aggregation, calibration, and uncertainty quantification so that each stage is inspectable and defensible. Aggregation combines evidence into claim axis scores with explicit provenance-aware weighting; calibration shrinks those scores toward prior expectations using reliability and effective sample size estimates; uncertainty is characterized using bootstrap intervals and dispersion diagnostics. Together these elements provide a transparent, auditable framework that reports both point estimates and the dispersion and uncertainty around them.</p></section>
<section class="wiki-section" id="sec-validation_quality_assurance"><section id="validation_quality_assurance"><h2>Validation and Quality Assurance</h2><p>This section describes the validation gates and quality assurance protocol applied to the extraction, parsing, and scoring pipeline. The objective is to ensure that automated processing yields outputs that meet predefined structural and evidentiary standards, and that targeted human oversight is applied where the automation indicates uncertainty or where risk thresholds are exceeded. Validation artifacts generated by the pipeline are retained as part of the chain of custody to enable repeatability and independent audit; representative artifacts referenced in this protocol include the validation report and the raw payloads produced by the extraction and scoring stages.</p><p>Automated validation gates operate at multiple points in the pipeline. Schema validation is applied immediately after ingestion to confirm that extracted documents conform to the expected structural contract; integrity checks verify file presence and byte-level stability of retained artifacts; table and artifact reconciliation routines confirm that tabular extractions map to page-level source locations; and citation and grounding checks assess the presence and format of declared sources. These automated gates produce a validation bundle that records categorical pass/fail outcomes and numeric metrics. For illustration of method behavior, the validation bundle associated with the processed artifact reports a certification value and an overall_score (for example, an overall_score of 99.5284 in the recorded bundle), together with category-level metrics (such as schema, integrity, and citations). Such numeric outcomes inform whether an automated gate is considered passed, whether reprocessing is required, or whether the item is elevated for agent review.</p><p>The operational QA protocol comprises two complementary review layers: continuous agent review and sampled human review. Agent review is an automated or semi-automated supervisory layer that monitors gate outcomes, examines exception logs, and applies deterministic remediation or escalation rules. Where agent review cannot conclusively resolve an exception, a targeted human review is triggered. Human review is deliberately sampled and targeted to provide high-value verification; the current protocol samples a 10% human review fraction (human_sample_fraction = 0.1) of eligible items based on stratified sampling across sources, claim types, and artifact criticality. In the sampled tranche reported alongside these procedures the human review observed no observed errors (human_sample_observed_error_rate = 0.0), and those results were recorded in the validation bundle for transparency.</p><p>When a validation gate fails, the pipeline records the failure in the validation report and routes the item through the escalation path. The escalation path first invokes agent review to attempt automatic correction, logging any transformations applied; if the agent cannot correct the issue within deterministic bounds, the item is routed to human review for targeted adjudication and remediation. All decisions, timestamps, reviewer identifiers (or agent process identifiers), and retained source artifacts (for example, the source PDF and intermediate JSONs identified in the raw payload paths) are preserved to maintain chain of custody and to support subsequent independent verification.</p><p>Ongoing quality assurance includes periodic revalidation of retained outputs whenever the extraction or scoring logic changes, calibration of gate thresholds informed by historical error rates, and archival of validation reports and raw payloads to support legal and academic reproducibility. The protocol explicitly documents the locations of retained</section>
<section class="wiki-section" id="sec-limitations_governance"><div>
  <h3>Introduction</h3>
  <p>This methodological subsection articulates the principal limitations, governance controls, and a principled refinement roadmap for a reproducible cyber‑attribution scoring pipeline. The purpose is analytic: to explicate where the current implementation deliberately constrains inference, to describe the institutional and technical controls that govern score generation, and to set out a programmatic pathway for iterative improvement. In keeping with the jurisprudential posture of the broader methodology, the emphasis is on evidentiary legibility, auditable transformations, and calibrated uncertainty rather than on assertive claims about particular incidents.</p>

  <h3>Scope and Units of Analysis</h3>
  <p>The unit of analysis is the evidentiary record constructed from a single PDF report and its derived objects: claims, sources, artifacts, and explicit evidence links. Limitations arise from this unit choice: bounding analysis to report-level records supports tractability and chain-of-custody reasoning but can under-represent external corroborative context not cited within the focal document. This scoped posture intentionally prioritizes reproducible inspection of what the author supplied over opportunistic supplementation from external datasets.</p>

  <h3>Data Ingestion</h3>
  <p>Data ingestion is governed by deterministic extraction where feasible and model-assisted transduction where necessary. The validation record indicates a successful ingestion certification (validation bundle status marked PASS) and a high structural integrity signal (an overall_score of 99.5284 in the supplied validation artifact). Those figures illustrate that ingestion controls are effective at preventing malformed records from proceeding to scoring, but they do not eliminate modality-specific errors introduced earlier in the pipeline (for example, OCR artefacts or layout misassignment).</p>

  <h3>PDF-to-Markdown Transcription</h3>
  <p>The PDF-to-markdown stage is an explicit transformation point where uncertainty can enter the record. Transcription errors, paragraph boundary misassignments, and table-to-markdown conversion issues are possible and detectable through schema checks. The current governance posture requires persistence of intermediate artifacts so that any contested transcription can be replayed and reprocessed; this persistence supports non-repudiable audit trails but does not by itself guarantee semantic correctness of every transcribed token.</p>

  <h3>Structural Parsing</h3>
  <p>Structural parsing maps transcribed text into schema-constrained objects. The high category scores reported for schema and integrity in the validation bundle demonstrate the effectiveness of schema enforcement at catching identifier collisions and missing anchors. Nevertheless, structural correctness is a necessary but not sufficient condition for inferential reliability: correctly-formed anchors may still reference ambiguous textual statements, and parsing heuristics may misattribute scope when authors conflate multiple propositions in a single paragraph.</p>

  <h3>Artifact Extraction</h3>
  <p>Artifact extraction isolates machine and human artifacts cited by the report. Limitations here include heterogeneity in artifact provenance reporting and inconsistent authoring practices across reports. The pipeline records artifact counts (the example validation shows 190 artifacts) to illustrate workload and surface potential clustering of origin; such counts are used only diagnostically to guide quality assurance and are not treated as direct evidentiary weight.</p>

  <h3>Reference Parsing</h3>
  <p>Reference parsing converts bibliographic citations and in-text attributions into mapped source objects. The validation bundle reflects citation coverage (citation_coverage_sources_0_1 = 0.9), indicating most sources are captured, yet source parsing remains vulnerable to nonstandard citation styles and implicit references. Governance mitigations include explicit failure modes when citation pathways cannot be traced and conservative exclusion of ambiguous references from credibility support.</p>

  <h3>Institution Inference</h3>
  <p>Institution inference assigns qualitative source-type labels used in downstream credibility weighting. This step is constrained by an explicit source taxonomy and exclusion rules (for example, internal/auto sources are excluded from credibility support). A limitation is taxonomy brittleness: misclassification of a single source type can materially affect aggregated credibility. The refinement roadmap therefore prioritizes expansion and validation of the taxonomy against externally curated authority lists.</p>

  <h3>Claim–Evidence Graph</h3>
  <p>Claims are connected to evidence items through directed, auditable links forming a claim–evidence graph. This structure facilitates chain-of-custody reasoning and anti-circularity checks, but graph completeness depends on the fidelity of earlier parsing steps. The method treats missing anchors or unresolved links as integrity failures; such conservative governance reduces false precision but increases the risk of false negatives when source authors omit explicit anchors.</p>

  <</section>
</article>