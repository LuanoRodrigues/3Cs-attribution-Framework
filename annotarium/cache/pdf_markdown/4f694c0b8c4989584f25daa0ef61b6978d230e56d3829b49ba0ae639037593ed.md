{
  "full_text": "## **Chapter 8**\n# **Cyber Attribution: An Argumentation-Based** **Approach**\n\n**Paulo Shakarian, Gerardo I. Simari, Geoffrey Moores and Simon Parsons**\n\n\n**Abstract** Attributing a cyber-operation through the use of multiple pieces of\ntechnical evidence (i.e., malware reverse-engineering and source tracking) and conventional intelligence sources (i.e., human or signals intelligence) is a difficult\nproblem not only due to the effort required to obtain evidence, but the ease with\nwhich an adversary can plant false evidence. In this paper, we introduce a formal reasoning system called the InCA (Intelligent Cyber Attribution) framework\nthat is designed to aid an analyst in the attribution of a cyber-operation even when\nthe available information is conflicting and/or uncertain. Our approach combines\nargumentation-based reasoning, logic programming, and probabilistic models to not\nonly attribute an operation but also explain to the analyst why the system reaches its\nconclusions.\n\n\n**8.1** **Introduction**\n\n\nAn important issue in cyber-warfare is the puzzle of determining who was responsible\nfor a given cyber-operation—be it an incident of attack, reconnaissance, or information theft. This is known as the “attribution problem” (Shakarian et al. 2013).\nThe difficulty of this problem stems not only from the amount of effort required to\nfind forensic clues but also the ease with which an attacker can plant false clues to\n\n\nP. Shakarian (�)\nArizona Sate University, Tempe, AZ, USA\ne-mail: shak@asu.edu\n\n\nG. I. Simari\nDepartment of Computer Science and Engineering, Universidad Nacional del Sur,\nBahía Blanca, Argentina\ne-mail: gis@cs.uns.edu.ar\n\n\nG. Moores\nDepartment of Electrical Engineering and Computer Science, U.S. Military Academy,\nWest Point, NY, USA\ne-mail: geoffrey.moores@usma.edu\n\n\nS. Parsons\nDepartment of Computer Science, University of Liverpool, Liverpool, UK\ne-mail: s.d.parsons@liverpool.ac.uk\n\n© Springer International Publishing Switzerland 2015 151\nS. Jajodia et al. (eds.), _Cyber Warfare,_ Advances in Information Security 56,\nDOI 10.1007/978-3-319-14039-1_8\n\n\n152 P. Shakarian et al.\n\n\nmislead security personnel. Further, while techniques such as forensics and reverseengineering (Altheide 2011), source tracking (Thonnard et al. 2010), honeypots\n(Spitzner 2003), and sinkholing 2010 are commonly employed to find evidence that\ncan lead to attribution, it is unclear how this evidence is to be combined and reasoned\nabout. In a military setting, such evidence is augmented with normal intelligence collection, such as human intelligence (HUMINT), signals intelligence (SIGINT) and\nother means—this adds additional complications to the task of attributing a given\noperation. Essentially, cyber-attribution is a highly-technical intelligence analysis\nproblem where an analyst must consider a variety of sources, each with its associated level of confidence, to provide a decision maker (e.g., a military commander)\ninsight into who conducted a given operation.\nAs it is well known that people’s ability to conduct intelligence analysis is limited\n(Heuer 1999), and due to the highly technical nature of many cyber evidencegathering techniques, an automated reasoning system would be best suited for the\ntask. Such a system must be able to accomplish several goals, among which we\ndistinguish the following main capabilities:\n\n\n1. Reason about evidence in a formal, principled manner, i.e., relying on strong\nmathematical foundations.\n2. Consider evidence for cyber attribution associated with some level of probabilistic\nuncertainty.\n3. Consider logical rules that allow for the system to draw conclusions based on\ncertain pieces of evidence and iteratively apply such rules.\n4. Consider pieces of information that may not be compatible with each other, decide\nwhich information is most relevant, and express why.\n5. Attribute a given cyber-operation based on the above-described features and provide the analyst with the ability to understand how the system arrived at that\nconclusion.\n\n\nIn this paper we present the InCA (Intelligent Cyber Attribution) framework, which\nmeets all of the above qualities. Our approach relies on several techniques from\nthe artificial intelligence community, including argumentation, logic programming,\nand probabilistic reasoning. We first outline the underlying mathematical framework and provide a running example based on real-world cases of cyber-attribution\n(cf. Sect. 8.2); then, in Sects. 8.3 and 8.4, we formally present InCA and attribution\nqueries, respectively. Finally, we discuss conclusions and future work in Sect. 8.6.\n\n\n**8.2** **Two Kinds of Models**\n\n\nOur approach relies on _two separate models of the world_ . The first, called the _**environ-**_\n_**mentalmodel**_ (EM)isusedtodescribethebackgroundknowledgeandisprobabilistic\nin nature. The second one, called the _**analytical model**_ (AM) is used to analyze\ncompeting hypotheses that can account for a given phenomenon (in this case, a\ncyber-operation). The EM _must be consistent_ —this simply means that there must\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 153\n\n\nEM AM\n“Malware X was compiled on a system “Malware X was compiled on a system\nusing the English language.” English-speaking country Y.”\n“Malware W and malware X were created “Malware W and malware X are related.”\nin a similar coding sytle.”\n“Country Y and country Z are currently “Country Y has a motive to launch a\nat war.” cybre-attack against country Z.”\n“Country Y has a significant investment “Country Y has the capability to conduct\nin math-science-engineering (MSE) education.” a cyber-attack.”\n\n\n**Fig. 8.1** Example observations—EM vs. AM\n\n\nexist a probability distribution over the possible states of the world that satisfies all\nof the constraints in the model, as well as the axioms of probability theory. On the\ncontrary, the AM will allow for contradictory information as the system must have\nthe capability to reason about competing explanations for a given cyber-operation.\nIn general, the EM contains knowledge such as evidence, intelligence reporting, or\nknowledge about actors, software, and systems. TheAM, on the other hand, contains\nideas the analyst concludes based on the information in the EM. Figure 8.1 gives\nsome examples of the types of information in the two models. Note that an analyst\n(or automated system) could assign a probability to statements in the EM column\nwhereas statements in the AM column can be true or false depending on a certain\ncombination (or several possible combinations) of statements from the EM. We now\nformally describe these two models as well as a technique for _annotating_ knowledge\nin the AM with information from the EM—these annotations specify the conditions\nunder which the various statements in the AM can potentially be true.\nBefore describing the two models in detail, we first introduce the language used\nto describe them. Variable and constant symbols represent items such as computer\nsystems, types of cyber operations, actors (e.g., nation states, hacking groups), and\nother technical and/or intelligence information. The set of all variable symbols is\ndenoted with **V**, and the set of all constants is denoted with **C** . For our framework, we\nshallrequiretwosubsetsof **C**, **C** _act_ and **C** _ops_, thatspecifytheactorsthatcouldconduct\ncyber-operations and the operations themselves, respectively. In the examples in this\npaper, we will use capital letters to represent variables (e.g., _X_, _Y_, _Z_ ). The constants\nin **C** _act_ and **C** _ops_ that we use in the running example are specified in the following\nexample.\n\n\n_Example 1_ The following (fictitious) actors and cyber-operations will be used in\nour examples:\n\n\n**C** _act_ = { _baja_, _krasnovia_, _mojave_ } (8.1)\n\n**C** _ops_ = { _worm123_ } (8.2)\n\n\n                     The next component in the model is a set of predicate symbols. These constructs\ncan accept zero or more variables or constants as arguments, and map to either\n\n\n154 P. Shakarian et al.\n\n\n**P** _EM_ : _origIP_ ( _M,_ _X_ ) Malware _M_ originated from an IP address belonging to actor _X_ .\n_malwInOp_ ( _M,_ _O_ ) Malware _M_ was used in cyber-operation _O_ .\n_mwHint_ ( _M,_ _X_ ) Malware _M_ contained a hint that it was created by actor _X_ .\n_compilLang_ ( _M,C_ ) Malware _M_ was compiled in a system that used language _C_ .\n_nativLang_ ( _X,C_ ) Language _C_ is the native language of actor _X_ .\n_inLgConf_ ( _X,_ _X_ _[′]_ ) Actors _X_ and _X_ _[′]_ are in a larger conflict with each other.\n_mseTT_ ( _X,_ _N_ ) There are at least _N_ number of top-tier math-science-engineering\nuniversities in country _X_ .\n_infGovSys_ ( _X,_ _M_ ) Systems belonging to actor _X_ were infected with malware _M_ .\n_cybCapAge_ ( _X,_ _N_ ) Actor _X_ has had a cyber-warfare capability for _N_ years or less.\n_govCybLab_ ( _X_ ) Actor _X_ has a government cyber-security lab.\n\n\n**P** _AM_ : _condOp_ ( _X,_ _O_ ) Actor _X_ conducted cyber-operation _O_ .\n_evidOf_ ( _X,_ _O_ ) There is evidence that actor _X_ conducted cyber-operation _O_ .\n_motiv_ ( _X,_ _X_ _[′]_ ) Actor _X_ had a motive to launch a cyber-attack against actor _X_ _[′]_ .\n_isCap_ ( _X,_ _O_ ) Actor _X_ is capable of conducting cyber-operation _O_ .\n_tgt_ ( _X,_ _O_ ) Actor _X_ was the target of cyber-operation _O_ .\n_hasMseInvest_ ( _X_ ) Actor _X_ has a significant investment in math-science-engineering\neducation.\n_expCw_ ( _X_ ) Actor _X_ has experience in conducting cyber-operations.\n\n\n**Fig. 8.2** Predicate definitions for the environment and analytical models in the running example\n\n\n_true_ or false. _Note that the EM and AM use separate sets of predicate symbols_ however, they can share variables and constants. The sets of predicates for the\nEM and AM are denoted with **P** _EM_, **P** _AM_, respectively. In InCA, we require **P** _AM_\nto include the binary predicate _condOp_ ( _X_, _Y_ ), where _X_ is an actor and _Y_ is a cyberoperation. Intuitively, this means that actor _X_ conducted operation _Y_ . For instance,\n_condOp_ ( _baja_, _worm123_ )istrueif _baja_ wasresponsibleforcyber-operation _worm123_ .\nA sample set of predicate symbols for the analysis of a cyber attack between two\nstates over contention of a particular industry is shown in Fig. 8.2; these will be used\nin examples throughout the paper.\nA construct formed with a predicate and constants as arguments is known as a\n_ground atom_ (we shall often deal with ground atoms). The sets of all ground atoms\nfor EM and AM are denoted with **G** _EM_ and **G** _AM_, respectively.\n\n\n_Example 2_ The following are examples of ground atoms over the predicates given\nin Fig. 8.2.\n\n\n**G** _EM_ : _origIP_ ( _mw123sam1_, _krasnovia_ ),\n\n\n_mwHint_ ( _mw123sam1_, _krasnovia_ ),\n\n\n_inLgConf_ ( _krasnovia_, _baja_ ),\n\n\n_mseTT_ ( _krasnovia_, 2) _._\n\n\n**G** _AM_ : _evidOf_ ( _mojave_, _worm123_ ),\n\n\n_motiv_ ( _baja_, _krasnovia_ ),\n\n\n_expCw_ ( _baja_ ),\n\n\n_tgt_ ( _krasnovia_, _worm123_ ) _._                \n\n8 Cyber Attribution: An Argumentation-Based Approach 155\n\n\nFor a given set of ground atoms, a _world_ is a subset of the atoms that are considered\nto be true (ground atoms not in the world are false). Hence, there are 2 [|] **[G]** _[EM]_ [|] possible worlds in the EM and 2 [|] **[G]** _[AM]_ [|] worlds in the AM, denoted with _WEM_ and _WAM_,\nrespectively.\nClearly, even a moderate number of ground atoms can yield an enormous number\nof worlds to explore. One way to reduce the number of worlds is to include _integrity_\n_constraints_, which allow us to eliminate certain worlds from consideration—they\nsimplyarenotpossibleinthesettingbeingmodeled. Ourprincipleintegrityconstraint\nwill be of the form:\n\n\noneOf( _A_ [′] )\n\n\nwhere _A_ [′] is a subset of ground atoms. Intuitively, this says that any world where\nmore than one of the atoms from set _A_ [′] appear is invalid. Let **IC** _EM_ and **IC** _AM_ be the\nsets of integrity constraints for the EM and AM, respectively, and the sets of worlds\nthat conform to these constraints be _WEM_ ( **IC** _EM_ ), _WAM_ ( **IC** _AM_ ), respectively.\nAtoms can also be combined into formulas using standard logical connectives:\nconjunction ( _and_ ), disjunction ( _or_ ), and negation ( _not_ ). These are written using the\nsymbols ∧, ∨, ¬, respectively. We say a world ( _w_ ) _satisfies_ a formula ( _f_ ), written\n_w_ |= _f_, based on the following inductive definition:\n\n\n- if _f_ is a single atom, then _w_ |= _f_ iff _f_ ∈ _w_ ;\n\n- if _f_ = ¬ _f_ [′] then _w_ |= _f_ iff _w_ ̸|= _f_ [′] ;\n\n- if _f_ = _f_ [′] ∧ _f_ [′′] then _w_ |= _f_ iff _w_ |= _f_ [′] and _w_ |= _f_ [′′] ; and\n\n- if _f_ = _f_ [′] ∨ _f_ [′′] then _w_ |= _f_ iff _w_ |= _f_ [′] or _w_ |= _f_ [′′] .\n\n\nWe use the notation _f ormulaEM_, _f ormulaAM_ to denote the set of all possible\n(ground) formulas in the EM andAM, respectively.Also, note that we use the notation\n⊤, ⊥ to represent tautologies (formulas that are true in all worlds) and contradictions\n(formulas that are false in all worlds), respectively.\n\n\n_**8.2.1**_ _**Environmental Model**_\n\n\nIn this section we describe the first of the two models, namely the EM or environmental model. This model is largely based on the probabilistic logic of (Nilsson 1986),\nwhich we now briefly review.\nFirst, we define a _probabilistic formula_ that consists of a formula _f_ over\natoms from **G** _EM_, a real number _p_ in the interval [0, 1], and an error tolerance\n_ε_ ∈ [0, min( _p_, 1 − _p_ )]. A probabilistic formula is written as: _f_ : _p_ ± _ε_ . Intuitively,\nthis statement is interpreted as “formula _f_ is true with probability between _p_ - _ε_\nand _p_ + _ε_ ”—note that we make no statement about the probability distribution over\nthis interval. The uncertainty regarding the probability values stems from the fact\nthat certain assumptions (such as probabilistic independence) may not be suitable in\nthe environment being modeled.\n\n\n156 P. Shakarian et al.\n\n\n_Example 3_ To continue our running example, consider the following set _ΠEM_ :\n\n\n_f_ 1 = _govCybLab_ ( _baja_ ) : 0 _._ 8 ± 0 _._ 1\n\n_f_ 2 = _cybCapAge_ ( _baja_, 5) : 0 _._ 2 ± 0 _._ 1\n\n_f_ 3 = _mseTT_ ( _baja_, 2) : 0 _._ 8 ± 0 _._ 1\n\n_f_ 4 = _mwHint_ ( _mw123sam1_, _mojave_ ) ∧ _compilLang_ ( _worm123_, _english_ ) : 0 _._ 7 ± 0 _._ 2\n\n_f_ 5 = _malwInOp_ ( _mw123sam1_, _worm123_ )\n\n\n∧ _malwareRel_ ( _mw123sam1_, _mw123sam2_ )\n\n\n∧ _mwHint_ ( _mw123sam2_, _mojave_ ) : 0 _._ 6 ± 0 _._ 1\n\n\n_f_ 6 = _inLgConf_ ( _baja_, _krasnovia_ ) ∨¬ _cooper_ ( _baja_, _krasnovia_ ) : 0 _._ 9 ± 0 _._ 1\n\n_f_ 7 = _origIP_ ( _mw123sam1_, _baja_ ) : 1 ± 0\n\n\nThroughout other examples in the rest of the paper, we will make use of the subset\n_ΠEM_ [′] [= {] _[f]_ [1][,] _[ f]_ [2][,] _[ f]_ [3][}][.] We now consider a probability distribution _Pr_ over the set _WEM_ ( **IC** _EM_ ). We\nsay that _Pr satisfies_ probabilistic formula _f_ : _p_ ± _ε_ iff the following holds:\n_p_ - _ε_ ≤ [�] _w_ ∈ _WEM_ ( **IC** _EM_ ) _[Pr]_ [(] _[w]_ [)][ ≤] _[p]_ [ +] _[ ε.]_ [ A set] _[ Π][EM]_ [ of probabilistic formulas is]\ncalled a _knowledge base_ . We say that a probability distribution over _WEM_ ( **IC** _EM_ )\n_satisfies ΠEM_ if and only if it satisfies all probabilistic formulas in _ΠEM_ .\nIt is possible to create probabilistic knowledge bases for which there is no\nsatisfying probability distribution. The following is a simple example of this:\n\n\n_condOp_ ( _krasnovia_, _worm123_ ) ∨ _condOp_ ( _baja_, _worm123_ ) : 0 _._ 4 ± 0;\n\n\n_condOp_ ( _krasnovia_, _worm123_ ) ∧ _condOp_ ( _baja_, _worm123_ ) : 0 _._ 6 ± 0 _._ 1 _._\n\n\nFormulas and knowledge bases of this sort are _inconsistent_ . In this paper, we assume\nthat information is properly extracted from a set of historic data and hence consistent;\n(recall that inconsistent information can only be handled in the AM, not the EM). A\nconsistent knowledge base could also be obtained as a result of curation by experts,\nsuch that all inconsistencies were removed—see (Khuller et al. 2007; Shakarian et\nal. 2011) for algorithms for learning rules of this type.\nThe main kind of query that we require for the probabilistic model is the _maximum_\n_entailment_ problem: given a knowledge base _ΠEM_ and a (non-probabilistic) formula\n_q_, identify _p_, _ε_ such that all valid probability distributions _Pr_ that satisfy _ΠEM_ also\nsatisfy _q_ : _p_ ± _ε_, and there does not exist _p_ [′], _ε_ [′] s.t. [ _p_ - _ε_, _p_ + _ε_ ] ⊃ [ _p_ [′] - _ε_ [′], _p_ [′] + _ε_ [′] ],\nwhere all probability distributions _Pr_ that satisfy _ΠEM_ also satisfy _q_ : _p_ [′] ± _ε_ [′] . That\nis, given _q_, can we determine the probability (with maximum tolerance) of statement\n_q_ given the information in _ΠEM_ ? The approach adopted in (Nilsson et al. 1986) to\nsolve this problem works as follows. First, we must solve the linear program defined\nnext.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 157\n\n\n**Definition 1** ( **EM-LP-MIN)** Given a knowledge base _ΠEM_ and a formula _q_ :\n\n\n- create a variable _xi_ for each _wi_ ∈ _WEM_ ( **IC** _EM_ );\n\n- for each _fj_ : _pj_ ± _εj_ ∈ _ΠEM_, create constraint:\n\n      _pj_          - _εj_ ≤ _xi_ ≤ _pj_ + _εj_ ;\n\n_wi_ ∈ _WEM_ ( **IC** _EM_ ) _s.t.wi_ |= _fj_\n\n\n- finally, we also have a constraint:\n\n      \n_xi_ = 1 _._\n_wi_ ∈ _WEM_ ( **IC** _EM_ )\n\n\nThe objective is to minimize the function:\n\n      \n_xi._\n_wi_ ∈ _WEM_ ( **IC** _EM_ ) _s.t.wi_ |= _q_\n\n\nWe use the notation EP-LP-MIN( _ΠEM_, _q_ ) to refer to the value of the objective\nfunction in the solution to the EM-LP-MIN constraints.\nLet _ℓ_ be the result of the process described in Definition 1. The next step is to\nsolve the linear program a second time, but instead maximizing the objective function\n(we shall refer to this as EM-LP-MAX)—let _u_ be the result of this operation. In\n(Nilsson 1986), it is shown that _ε_ = _u_ −2 _ℓ_ and _p_ = _ℓ_ + _ε_ is the solution to the\n\nmaximum entailment problem. We note that although the above linear program has\nan exponential number of variables in the worst case (i.e., no integrity constraints),\nthe presence of constraints has the potential to greatly reduce this space. Further,\nthere are also good heuristics (cf. Khuller et al. 2007; Simari et al. 2012) that have\nbeen shown to provide highly accurate approximations with a reduced-size linear\nprogram.\n\n_Example 4_ Consider KB _ΠEM_ [′] [from Example][ 3][ and a set of ground atoms restricted]\nto those that appear in that program. Hence, we have:\n\n\n_w_ 1 = { _govCybLab_ ( _baja_ ), _cybCapAge_ ( _baja_, 5), _mseTT_ ( _baja_, 2)}\n\n_w_ 2 = { _govCybLab_ ( _baja_ ), _cybCapAge_ ( _baja_, 5)}\n\n_w_ 3 = { _govCybLab_ ( _baja_ ), _mseTT_ ( _baja_, 2)}\n\n_w_ 4 = { _cybCapAge_ ( _baja_, 5), _mseTT_ ( _baja_, 2)}\n\n_w_ 5 = { _cybCapAge_ ( _baja_, 5)}\n\n_w_ 6 = { _govCybLab_ ( _baja_ )}\n\n_w_ 7 = { _mseTT_ ( _baja_, 2)}\n\n_w_ 8 = ∅\n\n\nand suppose we wish to compute the probability for formula:\n\n\n_q_ = _govCybLab_ ( _baja_ ) ∨ _mseTT_ ( _baja_, 2) _._\n\n\n158 P. Shakarian et al.\n\n\nFor each formula in _ΠEM_ we have a constraint, and for each world above we have a\nvariable. An objective function is created based on the worlds that satisfy the query\nformula (here, worlds _w_ 1 − _w_ 4, _w_ 6, _w_ 7). Hence, EP-LP-MIN( _ΠEM_ [′] [,] _[ q]_ [) can be written]\nas follows:\n\n\nmax _x_ 1 + _x_ 2 + _x_ 3 + _x_ 4 + _x_ 6 + _x_ 7 _w.r.t._ :\n\n0 _._ 7 ≤ _x_ 1 + _x_ 2 + _x_ 3 + _x_ 6 ≤ 0 _._ 9\n\n0 _._ 1 ≤ _x_ 1 + _x_ 2 + _x_ 4 + _x_ 5 ≤ 0 _._ 3\n\n0 _._ 8 ≤ _x_ 1 + _x_ 3 + _x_ 4 + _x_ 7 ≤ 1\n\n_x_ 1 + _x_ 2 + _x_ 3 + _x_ 4 + _x_ 5 + _x_ 6 + _x_ 7 + _x_ 8 = 1\n\nWe can now solve EP-LP-MAX( _ΠEM_ [′] [,] _[ q]_ [) and][ EP-LP-MIN][(] _[Π]_ _EM_ [′] [,] _[ q]_ [) to get solution]\n0 _._ 9 ± 0 _._ 1. \n\n_**8.2.2**_ _**Analytical Model**_\n\n\nFor the analytical model (AM), we choose a structured argumentation framework\n(Rahwan et al. 2009) due to several characteristics that make such frameworks highly\napplicable to cyber-warfare domains. Unlike the EM, which describes probabilistic\ninformation about the state of the real world, the AM must allow for competing\nideas—it _must be able to represent contradictory information_ . The algorithmic\napproach allows for the creation of _arguments_ based on the AM that may “compete” with each other to describe who conducted a given cyber-operation. In this\ncompetition—known as a _dialectical process_ —one argument may defeat another\nbased on a _comparison criterion_ that determines the prevailing argument. Resulting\nfrom this process, the InCA framework will determine arguments that are _war-_\n_ranted_ (those that are not _defeated_ by other arguments) thereby providing a suitable\nexplanation for a given cyber-operation.\nThe transparency provided by the system can allow analysts to identify potentially\nincorrect input information and fine-tune the models or, alternatively, collect more\ninformation. In short, argumentation-based reasoning has been studied as a natural\nway to manage a set of inconsistent information—it is the way humans settle disputes. As we will see, another desirable characteristic of (structured) argumentation\nframeworks is that, once a conclusion is reached, we are left with an explanation of\nhow we arrived at it and information about why a given argument is warranted; this\nis very important information for analysts to have. In this section, we recall some\npreliminaries of the underlying argumentation framework used, and then introduce\nthe analytical model (AM).\n\n\n**Defeasible Logic Programming with Presumptions**\n\n\nDeLP with Presumptions (PreDeLP) (Martinez et al. 2012) is a formalism combining Logic Programming with Defeasible Argumentation. We now briefly recall\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 159\n\n\n**Fig. 8.3** A ground argumentation framework\n\n\nthe basics of PreDeLP; we refer the reader to (García and Simari 2004; Martinez\net al. 2012) for the complete presentation. The formalism contains several different\nconstructs: facts, presumptions, strict rules, and defeasible rules. Facts are statements\nabout the analysis that can always be considered to be true, while presumptions are\nstatements that may or may not be true. Strict rules specify logical consequences of\na set of facts or presumptions (similar to an implication, though not the same) that\nmust always occur, while defeasible rules specify logical consequences that may be\nassumed to be true when no contradicting information is present. These constructs\nare used in the construction of _arguments_, and are part of a PreDeLP program, which\nis a set of facts, strict rules, presumptions, and defeasible rules. Formally, we use\nthe notation _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) to denote a PreDeLP program, where _Ω_ is the set\nof strict rules, _Θ_ is the set of facts, _Δ_ is the set of defeasible rules, and _Φ_ is the set\nof presumptions. In Fig. 8.3, we provide an example _ΠAM_ . We now describe each of\nthese constructs in detail.\n\n\n**Facts** ( _Θ_ ) are ground literals representing atomic information or its negation, using\nstrong negation “¬”. Note that all of the literals in our framework must be formed\nwith a predicate from the set **P** _AM_ . Note that information in this form cannot be\ncontradicted.\n\n\n**Strict Rules** ( _Ω_ ) represent non-defeasible cause-and-effect information that resembles a material implication (though the semantics is different since the contrapositive\n\n\n160 P. Shakarian et al.\n\n\ndoes not hold) and are of the form _L_ 0 ←− _L_ 1, _. . ._, _Ln_, where _L_ 0 is a ground literal\nand { _Li_ } _i>_ 0 is a set of ground literals.\n\n\n**Presumptions** ( _Φ_ ) are ground literals of the same form as facts, except that they\nare not taken as being true but rather defeasible, which means that they can be\ncontradicted. Presumptions are denoted in the same manner as facts, except that the\nsymbol –≺ is added. While any literal can be used as a presumption in InCA, we\nspecifically require all literals created with the predicate _condOp_ to be defeasible.\n\n\n**Defeasible Rules** ( _Δ_ ) represent tentative knowledge that can be used if nothing\ncan be posed against it. Just as presumptions are the defeasible counterpart of facts,\ndefeasible rules are the defeasible counterpart of strict rules. They are of the form\n_L_ 0 –≺ _L_ 1, _. . ._, _Ln_, where _L_ 0 is a ground literal and { _Li_ } _i>_ 0 is a set of ground literals.\nNote that with both strict and defeasible rules, _strong negation_ is allowed in the head\nof rules, and hence may be used to represent contradictory knowledge.\nWe note that strict rules and facts are necessary constructs as they may not be true\nin all environmental conditions. We shall discuss this further in the next section with\nthe introduction of an annotation function.\nEven though the above constructs are ground, we allow for schematic versions\nwith variables that are used to represent sets of ground rules. We denote variables\nwith strings starting with an uppercase letter; Fig. 8.4 shows a non-ground example.\nWhen a cyber-operation occurs, InCA must derive arguments as to who could\nhave potentially conducted the action. Informally, an argument for a particular actor\n_x_ conducting cyber-operation _y_ is a consistent subset of the analytical model that\nentails the atom _condOp_ ( _x_, _y_ ). If the argument contains only strict rules and facts,\nthen it is _factual_ . If it contains presumptions or defeasible rules, then it _defeasibly_\n_derives_ that actor _x_ conducted operation _y_ .\nDerivation follows the same mechanism of Logic Programming (Lloyd 1987).\nSince rule heads can contain strong negation, it is possible to defeasibly derive contradictory literals from a program. For the treatment of contradictory knowledge,\nPreDeLP incorporates a defeasible argumentation formalism that allows the identification of the pieces of knowledge that are in conflict, and through the previously\nmentioned _dialectical process_ decides which information prevails as warranted.\nThis dialectical process involves the construction and evaluation of arguments\nthat either support or interfere with a given query, building a _dialectical tree_ in the\nprocess. Formally, we have:\n\n\n**Definition 2 (Argument)** An _argument A_, _L_ ⟩ for a literal _L_ is a pair of the literal\nand a (possibly empty) set of the EM ( _A_ ⊆ _ΠAM_ ) that provides a minimal proof for _L_\nmeeting the requirements: (1) _L_ is defeasibly derived from _A_, (2) _Ω_ ∪ _Θ_ ∪ _A_ is not\ncontradictory, and (3) _A_ is a minimal subset of _Δ_ ∪ _Φ_ satisfying 1 and 2, denoted\n⟨ _A_, _L_ ⟩.\nLiteral _L_ is called the _conclusion_ supported by the argument, and _A_ is the _support_\nof the argument. An argument ⟨ _B_, _L_ ⟩ is a _subargument_ of ⟨ _A_, _L_ [′] ⟩ iff _B_ ⊆ _A_ . An\nargument ⟨ _A_, _L_ ⟩ is _presumptive_ iff _A_ ∩ _Φ_ is not empty. We will also use _Ω_ ( _A_ ) =\n_A_ ∩ _Ω_, _Θ_ ( _A_ ) = _A_ ∩ _Θ_, _Δ_ ( _A_ ) = _A_ ∩ _Δ_, and _Φ_ ( _A_ ) = _A_ ∩ _Φ_ .\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 161\n\n\n**Fig. 8.4** A non-ground argumentation framework\n\n\n**Fig. 8.5** Example ground arguments from Fig. 8.3\n\n\nNote that our definition differs slightly from that of (Simari and Loui 1992) where\nDeLP is introduced, as we include strict rules and facts as part of the argument. The\nreason for this will become clear in Sect. 8.3. Arguments for our scenario are shown\nin the following example.\n\n\n_Example 5_ Figure 8.5 shows example arguments based on the knowledge base from\nFig. 8.3. Note that the following relationship exists:\n\n\n⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ is a sub-argument of\n\n\n⟨ _A_ 2, _condOp_ ( _baja_, _worm123_ )⟩ and\n\n\n⟨ _A_ 3, _condOp_ ( _baja_, _worm123_ )⟩ _._         \n\nGiven argument ⟨ _A_ 1, _L_ 1⟩, counter-arguments are arguments that contradict it.\nArgument ⟨ _A_ 2, _L_ 2⟩ _counterargues_ or _attacks_ ⟨ _A_ 1, _L_ 1⟩ literal _L_ [′] iff there exists a\nsubargument ⟨ _A_, _L_ [′′] ⟩ of ⟨ _A_ 1, _L_ 1⟩ s.t. set _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _Θ_ ( _A_ 1) ∪ _Θ_ ( _A_ 2) ∪\n{ _L_ 2, _L_ [′′] } is contradictory.\n\n\n_Example 6_ Consider the arguments from Example 5. The following are some of\nthe attack relationships between them: _A_ 1, _A_ 2, _A_ 3, and _A_ 4 all attack _A_ 6; _A_ 5 attacks\n_A_ 7; and _A_ 7 attacks _A_ 2. \n\n162 P. Shakarian et al.\n\n\nA _proper defeater_ of an argument ⟨ _A_, _L_ ⟩ is a counter-argument that—by some\ncriterion—is considered to be better than ⟨ _A_, _L_ ⟩; if the two are incomparable according to this criterion, the counterargument is said to be a _blocking_ defeater. An\nimportantcharacteristicofPreDeLPisthattheargumentcomparisoncriterionismodular, and thus the most appropriate criterion for the domain that is being represented\ncan be selected; the default criterion used in classical defeasible logic programming (from which PreDeLP is derived) is _generalized specificity_ (Stolzenburg et al.\n2003), though an extension of this criterion is required for arguments using presumptions (Martinez et al. 2012). We briefly recall this criterion next—the first\ndefinition is for generalized specificity, which is subsequently used in the definition\nof presumption-enabled specificity.\n\n\n**Definition 3** Let _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) be a PreDeLP program and let _F_ be the\nset of all literals that have a defeasible derivation from _ΠAM_ . An argument ⟨ _A_ 1, _L_ 1⟩\nis _preferred to_ ⟨ _A_ 2, _L_ 2⟩, denoted with _A_ 1 ≻ _PS A_ 2 iff the two following conditions\nhold:\n\n\n1. For all _H_ ⊆ _F_, _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ is non-contradictory: if there is a derivation\nfor _L_ 1 from _Ω_ ( _A_ 2) ∪ _Ω_ ( _A_ 1) ∪ _Δ_ ( _A_ 1) ∪ _H_, and there is no derivation for _L_ 1\nfrom _Ω_ ( _A_ 1)∪ _Ω_ ( _A_ 2)∪ _H_, then there is a derivation for _L_ 2 from _Ω_ ( _A_ 1)∪ _Ω_ ( _A_ 2)∪\n_Δ_ ( _A_ 2) ∪ _H_ .\n2. There is at least one set _H_ [′] ⊆ _F_, _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] is non-contradictory,\nsuch that there is a derivation for _L_ 2 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] ∪ _Δ_ ( _A_ 2), there\nis no derivation for _L_ 2 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′], and there is no derivation for\n_L_ 1 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] ∪ _Δ_ ( _A_ 1).\n\n\nIntuitively, the principle of specificity says that, in the presence of two conflicting\nlines of argument about a proposition, the one that uses more of the available information is more convincing.A classic example involves a bird, Tweety, and arguments\nstating that it both flies (because it is a bird) and doesn’t fly (because it is a penguin).\nThe latter argument uses more information about Tweety—it is more specific—and\nis thus the stronger of the two.\n\n\n**Definition 4** (Martinez et al. 2012) Let _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) be a PreDeLP program. An argument ⟨ _A_ 1, _L_ 1⟩ is _preferred to_ ⟨ _A_ 2, _L_ 2⟩, denoted with _A_ 1 ≻ _A_ 2 iff any\nof the following conditions hold:\n\n\n1. ⟨ _A_ 1, _L_ 1⟩ and ⟨ _A_ 2, _L_ 2⟩ are both factual arguments and ⟨ _A_ 1, _L_ 1⟩≻ _PS_ ⟨ _A_ 2, _L_ 2⟩.\n2. ⟨ _A_ 1, _L_ 1⟩ is a factual argument and ⟨ _A_ 2, _L_ 2⟩ is a presumptive argument.\n3. ⟨ _A_ 1, _L_ 1⟩ and ⟨ _A_ 2, _L_ 2⟩ are presumptive arguments, and\na) ¬( _Φ_ ( _A_ 1) ⊆ _Φ_ ( _A_ 2)), or\nb) _Φ_ ( _A_ 1) = _Φ_ ( _A_ 2) and ⟨ _A_ 1, _L_ 1⟩≻ _PS_ ⟨ _A_ 2, _L_ 2⟩.\n\n\nGenerally, if _A_, _B_ are arguments with rules _X_ and _Y_, resp., and _X_ ⊂ _Y_, then _A_ is\nstronger than _B_ . This also holds when _A_ and _B_ use presumptions _P_ 1 and _P_ 2, resp.,\nand _P_ 1 ⊂ _P_ 2.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 163\n\n\n_Example 7_ The following are relationships between arguments from Example 5,\nbased on Definitions 3 and 4:\n\n\n_A_ 1 and _A_ 6 are incomparable (blocking defeaters);\n\n_A_ 6 ≻ _A_ 2, and thus _A_ 6 defeats _A_ 2;\n\n_A_ 6 ≻ _A_ 3, and thus _A_ 6 defeats _A_ 3;\n\n_A_ 6 ≻ _A_ 4, and thus _A_ 6 defeats _A_ 4;\n\n_A_ 5 and _A_ 7 are incomparable (blocking defeaters) _._     \n\nA sequence of arguments called an _argumentation line_ thus arises from this attack relation, where each argument defeats its predecessor. To avoid undesirable\nsequences, that may represent circular or fallacious argumentation lines, in DeLP\nan _argumentation line_ is _acceptable_ if it satisfies certain constraints (see García and\nSimari 2004). A literal _L_ is _warranted_ if there exists a non-defeated argument _A_\nsupporting _L_ .\nClearly, there can be more than one defeater for a particular argument ⟨ _A_, _L_ ⟩.\nTherefore, many acceptable argumentation lines could arise from ⟨ _A_, _L_ ⟩, leading\nto a tree structure. The tree is built from the set of all argumentation lines rooted\nin the initial argument. In a dialectical tree, every node (except the root) represents\na defeater of its parent, and leaves correspond to undefeated arguments. Each path\nfrom the root to a leaf corresponds to a different acceptable argumentation line. A\ndialectical tree provides a structure for considering all the possible acceptable argumentation lines that can be generated for deciding whether an argument is defeated.\nWe call this tree _dialectical_ because it represents an exhaustive dialectical analysis\n(in the sense of providing reasons for and against a position) for the argument in its\nroot. For argument ⟨ _A_, _L_ ⟩, we denote its dialectical tree with _T_ (⟨ _A_, _L_ ⟩).\nGiven a literal _L_ and an argument ⟨ _A_, _L_ ⟩, in order to decide whether or not a literal\n_L_ is warranted, every node in the dialectical tree _T_ (⟨ _A_, _L_ ⟩) is recursively marked as\n“D” ( _defeated_ ) or “U” ( _undefeated_ ), obtaining a marked dialectical tree _T_ [∗] (⟨ _A_, _L_ ⟩)\nwhere:\n\n\n- All leaves in _T_ [∗] (⟨ _A_, _L_ ⟩) are marked as “U”s, and\n\n- Let ⟨ _B_, _q_ ⟩ be an inner node of _T_ [∗] (⟨ _A_, _L_ ⟩). Then, ⟨ _B_, _q_ ⟩ will be marked as “U” iff\nevery child of ⟨ _B_, _q_ ⟩ is marked as “D”. Node ⟨ _B_, _q_ ⟩ will be marked as “D” iff it\nhas at least a child marked as “U”.\n\n\nGiven argument ⟨ _A_, _L_ ⟩ over _ΠAM_, if the root of _T_ [∗] (⟨ _A_, _L_ ⟩) is marked “U”, then\n_T_ [∗] (⟨ _A_, _h_ ⟩) _warrants L_ and that _L_ is _warranted_ from _ΠAM_ . (Warranted arguments\ncorrespond to those in the grounded extension of a Dung argumentation system\n(Dung 1995)).\nWe can then extend the idea of a dialectical tree to a _dialectical forest_ . For a\ngiven literal _L_, a dialectical forest _F_ ( _L_ ) consists of the set of dialectical trees for all\narguments for _L_ . We shall denote a marked dialectical forest, the set of all marked\ndialectical trees for arguments for _L_, as _F_ [∗] ( _L_ ). Hence, for a literal _L_, we say it is\n_warranted_ if there is at least one argument for that literal in the dialectical forest\n\n\n164 P. Shakarian et al.\n\n\n**Fig. 8.6** Example annotation function\n\n\n_F_ [∗] ( _L_ ) that is labeled “U”, _not warranted_ if there is at least one argument for literal\n¬ _L_ in the forest _F_ [∗] (¬ _L_ ) that is labeled “U”, and _undecided_ otherwise.\n\n\n**8.3** **The InCA Framework**\n\n\nHaving defined our environmental and analytical models ( _ΠEM_, _ΠAM_ respectively),\nwe now define how the two relate, which allows us to complete the definition of our\nInCA framework.\nThe key intuition here is that given a _ΠAM_, every element of _Ω_ ∪ _ΘΔ_ ∪ _Φ_\nmight only hold in certain worlds in the set _WEM_ —that is, worlds specified by the\nenvironment model. As formulas over the environmental atoms in set **G** _EM_ specify\nsubsets of _WEM_ (i.e., the worlds that satisfy them), we can use these formulas to\nidentify the conditions under which a component of _Ω_ ∪ _ΘΔ_ ∪ _Φ can be_ true.\nRecall that we use the notation _formulaEM_ to denote the set of all possible formulas\nover **G** _EM_ . Therefore, it makes sense to associate elements of _Ω_ ∪ _Θ_ ∪ _Φ_ with a\nformula from _formulaEM_ . In doing so, we can in turn compute the probabilities\nof subsets of _Ω_ ∪ _ΘΔ_ ∪ _Φ_ using the information contained in _ΠEM_, which we\nshall describe shortly. We first introduce the notion of _annotation function_, which\nassociates elements of _Ω_ ∪ _Θ_ ∪ _Φ_ with elements of _formulaEM_ .\nWe also note that, by using the annotation function (see Fig. 8.6), we may have\ncertain statements that appear as both facts and presumptions (likewise for strict and\ndefeasible rules). However, these constructs would have different annotations, and\nthus be applicable in different worlds. Suppose we added the following presumptions\nto our running example:\n\n\n_φ_ 3 = _evidOf_ ( _X_, _O_ ) **–** ≺, and\n_φ_ 4 = _motiv_ ( _X_, _X_ [′] ) **–** ≺.\n\n\nNote that these presumptions are constructed using the same formulas as facts\n_θ_ 1, _θ_ 2.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 165\n\n\nSuppose we extend _af_ as follows:\n\n\n_af_ ( _φ_ 3) = _malwInOp_ ( _M_, _O_ ) ∧ _malwareRel_ ( _M_, _M_ [′] ) ∧ _mwHint_ ( _M_ [′], _X_ )\n\n_af_ ( _φ_ 4) = _inLgConf_ ( _Y_, _X_ [′] ) ∧ _cooper_ ( _X_, _Y_ )\n\n\nSo, for instance, unlike _θ_ 1, _φ_ 3 can potentially be true in any world of the form:\n\n\n{ _malwInOp_ ( _M_, _O_ ), _malwareRel_ ( _M_, _M_ [′] ), _mwHint_ ( _M_ [′], _X_ )}\n\n\nwhile _θ_ 1 cannot be considered in any those worlds.\nWith the annotation function, we now have all the components to formally define\nan InCA framework.\n\n\n**Definition 5 (InCA Framework)** Given environmental model _ΠEM_, analytical\nmodel _ΠAM_, and annotation function _af_, _I_ = ( _ΠEM_, _ΠAM_, _af_ ) is an **InCA framework** .\nGiventhesetupdescribedabove, weconsidera _world-based_ approach—thedefeat\nrelationship among arguments will depend on the current state of the world (based\non the EM). Hence, we now define the status of an argument with respect to a given\nworld.\n\n\n**Definition 6 (Validity)** Given InCA framework _I_ = ( _ΠEM_, _ΠAM_, _af_ ), argument\n⟨ _A_, _L_ ⟩ is valid w.r.t. world _w_ ∈ _WEM_ iff ∀ _c_ ∈ _A_, _w_ |= _af_ ( _c_ ).\nIn other words, an argument is valid with respect to _w_ if the rules, facts, and\npresumptions in that argument are present in _w_ —the argument can then be built\nfrom information that is available in that world. In this paper, we extend the notion\nof validity to argumentation lines, dialectical trees, and dialectical forests in the\nexpected way (an argumentation line is valid w.r.t. _w_ iff all arguments that comprise\nthat line are valid w.r.t. _w_ ).\n\n\n_Example 8_ Consider worlds _w_ 1, _. . ._, _w_ 8 from Example 4 along with the argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ from Example 5. This argument is valid in worlds\n_w_ 1— _w_ 4, _w_ 6, and _w_ 7. We now extend the idea of a dialectical tree w.r.t. worlds—so, for a given world\n_w_ ∈ _WEM_, the dialectical (resp., marked dialectical) tree induced by _w_ is denoted\nby _Tw_ ⟨ _A_, _L_ ⟩ (resp., _Tw_ [∗][⟨] _[A]_ [,] _[ L]_ [⟩][). We require that all arguments and defeaters in these]\ntrees to be valid with respect to _w_ . Likewise, we extend the notion of dialectical\nforests in the same manner (denoted with _Fw_ ( _L_ ) and _Fw_ [∗][(] _[L]_ [), respectively). Based on]\nthese concepts we introduce the notion of _warranting scenario_ .\n\n\n**Definition 7 (Warranting Scenario)** Let _I_ = ( _ΠEM_, _ΠAM_, _af_ ) be an InCA framework and _L_ be a ground literal over **G** _AM_ ; a world _w_ ∈ _WEM_ is said to be a _warranting_\n_scenario_ for _L_ (denoted _w_ ⊢war _L_ ) iff there is a dialectical forest _Fw_ [∗][(] _[L]_ [) in which] _[ L]_\nis warranted and _Fw_ [∗][(] _[L]_ [) is valid w.r.t] _[ w]_ [.]\n\n\n_Example 9_ Following from Example 8, argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ is\nwarranted in worlds _w_ 3, _w_ 6, and _w_ 7. \n\n166 P. Shakarian et al.\n\n\nHence, the set of worlds in the EM where a literal _L_ in the AM _must_ be true is\nexactly the set of warranting scenarios—these are the “necessary” worlds, denoted:\n\n\n_nec_ ( _L_ ) = { _w_ ∈ _WEM_ | ( _w_ ⊢war _L_ )} _._\n\n\nNow, the set of worlds in the EM where AM literal _L can_ be true is the following—\nthese are the “possible” worlds, denoted:\n\n\n_poss_ ( _L_ ) = { _w_ ∈ _WEM_ | _w_ ̸⊢war ¬ _L_ } _._\n\n\nThe following example illustrates these concepts.\n\n\n_Example 10_ Following from Example 8:\n\n\n_nec_ ( _isCap_ ( _baja_, _worm123_ )) = { _w_ 3, _w_ 6, _w_ 7} and\n\n\n_poss_ ( _isCap_ ( _baja_, _worm123_ )) = { _w_ 1, _w_ 2, _w_ 3, _w_ 4, _w_ 6, _w_ 7} _._     \n\nHence, for a given InCA framework _I_, if we are given a probability distribution\n_Pr_ over the worlds in the EM, then we can compute an upper and lower bound on\nthe probability of literal _L_ (denoted **P** _L_, _Pr_, _I_ ) as follows:\n\n\n      _ℓL_, _Pr_, _I_ = _Pr_ ( _w_ ),\n\n_w_ ∈ _nec_ ( _L_ )\n\n\n\nand\n\n\n\n\n  _uL_, _Pr_, _I_ = _Pr_ ( _w_ ),\n\n_w_ ∈ _poss_ ( _L_ )\n\n\n_ℓL_, _Pr_, _I_ ≤ **P** _L_, _Pr_, _I_ ≤ _uL_, _Pr_, _I._\n\n\n\nNow let us consider the computation of probability bounds on a literal when we\nare given a knowledge base _ΠEM_ in the environmental model, which is specified in\n_I_, instead of a probability distribution over all worlds. For a given world _w_ ∈ _WEM_,\nlet _f or_ ( _w_ ) = _(_ [�] _a_ ∈ _w_ _[a)]_ [ ∧] _[(]_ [ �] _a /_ ∈ _w_ [¬] _[a)]_ [—that is, a formula that is satisfied only by]\nworld _w_ . Now we can determine the upper and lower bounds on the probability of a\nliteral w.r.t. _ΠEM_ (denoted **P** _L_, _I_ ) as follows:\n\n\n\n_ℓL_, _I_ = EP-LP-MIN\n\n\n_uL_, _I_ = EP-LP-MAX\n\n\n\n⎛ ⎞\n\n  ⎝ _ΠEM_, _f or_ ( _w_ )⎠,\n\n_w_ ∈ _nec_ ( _L_ )\n\n\n⎛ ⎞\n\n\n\n\n  ⎝ _ΠEM_, _f or_ ( _w_ )⎠,\n\n_w_ ∈ _poss_ ( _L_ )\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 167\n\n\nand\n\n\n_ℓL_, _I_ ≤ **P** _L_, _I_ ≤ _uL_, _I._\n\n\nHence, we have:\n\n\n\n\n   **P** _L_, _I_ = _ℓL_, _I_ + _[u][L]_ [,] _[I]_ [−] _[ℓ][L]_ [,] _[I]_\n\n2\n\n\n\n\n\n[−] _[ℓ][L]_ [,] _[I]_\n± _[u][L]_ [,] _[I]_ _._\n\n2\n\n\n\n_Example 11_ Following from Example 8, argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩,\nwe can compute **P** _isCap_ ( _baja_, _worm123_ ), _I_ (where _I_ = ( _ΠEM_ [′] [,] _[ Π][AM]_ [,] _[ af]_ [)). Note that for]\nthe upper bound, the linear program we need to set up is as in Example 4. For the\nlower bound, the objective function changes to: min _x_ 3 + _x_ 6 + _x_ 7. From these linear\nconstraints, we obtain: **P** _isCap_ ( _baja_, _worm123_ ), _I_ = 0 _._ 75 ± 0 _._ 25 _._ \n\n**8.4** **Attribution Queries**\n\n\nWe now have the necessary elements required to formally define the kind of queries\nthat correspond to the attribution problems studied in this paper.\n\n\n**Definition 8** Let _I_ = ( _ΠEM_, _ΠAM_, _af_ ) be an InCA framework, _S_ ⊆ **C** _act_ (the set of\n“suspects”), _O_ ∈ **C** _ops_ (the “operation”), and _E_ ⊆ **G** _EM_ (the “evidence”). An actor\nA ∈ _S_ is said to be a _most probable suspect_ iff there does not exist A [′] ∈ _S_ such that\n**P** _condOp_ ( **A** [′], _O_ ), _I_ ′ _>_ **P** _condOp_ ( _A_, _O_ ) _I_ ′ where _I_ [′] = ( _ΠEM_ ∪ _ΠE_, _ΠAM_, _af_ [′] ) with _ΠE_ defined\nas [�] _c_ ∈ _E_ [{] _[c]_ [ : 1][ ±][ 0][}][.]\nGiven the above definition, we refer to _Q_ = ( _I_, _S_, _O_, _E_ ) as an _attribution query_,\nand A as an _answer_ to _Q_ . We note that in the above definition, the items of evidence\nare added to the environmental model with a probability of 1 ± 0. While in general\nthis may be the case, there are often instances in analysis of a cyber-operation where\nthe evidence may be true with some degree of uncertainty. Allowing for probabilistic\nevidence is a simple extension to Definition 8 that does not cause any changes to the\nresults of this paper.\nTo understand how uncertain evidence can be present in a cyber-security scenario,\nconsiderthefollowing. InSymantec’sinitialanalysisoftheStuxnetworm, theyfound\nthe routine designed to attack the S7-417 logic controller was incomplete, and hence\nwould not function (Falliere et al. 2011). However, industrial control system expert\nRalph Langner claimed that the incomplete code would run provided a missing\ndata block is generated, which he thought was possible (Langner et al. 2011). In\nthis case, though the code was incomplete, there was clearly uncertainty regarding\nits usability. This situation provides a real-world example of the need to compare\narguments—in this case, in the worlds where both arguments are valid, Langner’s\nargument would likely defeat Symantec’s by generalized specificity (the outcome,\nof course, will depend on the exact formalization of the two). Note that Langner was\n\n\n168 P. Shakarian et al.\n\n\nlater vindicated by the discovery of an older sample, Stuxnet 0.5, which generated\nthe data block. [1]\n\nInCA also allows for a variety of relevant scenarios to the attribution problem.\nFor instance, we can easily allow for the modeling of non-state actors by extending\nthe available constants—for example, traditional groups such as Hezbollah, which\nhas previously wielded its cyber-warfare capabilities in operations against Israel\n(Shakarian et al. 2013). Likewise, the InCA can also be used to model cooperation\namong different actors in performing an attack, including the relationship between\nnon-state actors and nation-states, such as the potential connection between Iran and\nmilitants stealing UAV feeds in Iraq, or the much-hypothesized relationship between\nhacktivist youth groups and the Russian government (Shakarian et al. 2013).Another\naspect that can be modeled is deception where, for instance, an actor may leave false\nclues in a piece of malware to lead an analyst to believe a third party conducted\nthe operation. Such a deception scenario can be easily created by adding additional\nrules in the AM that allow for the creation of such counter-arguments. Another type\nof deception that could occur include attacks being launched from a system not in\nthe responsible party’s area, but under their control (e.g., see Shadows in the Cloud\n2010). Again, modeling who controls a given system can be easily accomplished in\nour framework, and doing so would simply entail extending an argumentation line.\nFurther, campaigns of cyber-operations can also be modeled, as well as relationships\namong malware and/or attacks (as detailed in APT1 2013).\nAs with all of these abilities, InCA provides the analyst the means to model a\ncomplex situation in cyber-warfare but saves him from carrying out the reasoning\nassociated with such a situation. Additionally, InCA results are constructive, so an\nanalyst can “trace-back” results to better understand how the system arrived at a\ngiven conclusion.\n\n\n**8.5** **Open Questions**\n\n\nIn this section we review some major areas of research to address to move InCA\ntoward a deployed system.\n\n\n_**8.5.1**_ _**Rule Learning**_\n\n\nThe InCA framework depends on logical rules and statements as part of the input,\nthough there are existing bodies of work we can leverage (decision tree rule learning,\ninductive logic programming, etc.) there are some specific challenges with regard to\nInCA that we must account for, specifically:\n\n\n1 http://www.symantec.com/connect/blogs/stuxnet-05-disrupting-uranium-processing-natanz.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 169\n\n\n- Quickly learning probabilistic rules from data received as an input stream\n\n- Learning of the annotation function\n\n- Identification of the diagnosticity of new additions to the knowledgebase\n\n- Learning rules that combine multiple, disparate sources (i.e. malware analysis\nand PCAP files, for instance)\n\n\n_**8.5.2**_ _**Belief Revision**_\n\n\nEven though we allow for inconsistencies in the AM portion of the model, inconsistency can arise even with a consistent EM. In a companion paper, (Shakarian et al.\n2014) we introduce the following notion of consistency.\n\n\n**Definition 9** InCA program _I_ = ( _ΠEM_, _ΠAM_, _af_ ), with _ΠAM_ = ⟨ _Θ_, _Ω_, _Φ_, _Δ_ ⟩, is\n_Type II consistent_ iff: given any probability distribution _Pr_ that satisfies _ΠEM_, if\nthere exists a world _w_ ∈ _WEM_ such that [�] _x_ ∈ _Θ_ ∪ _Ω_ | _w_ |= _af_ ( _x_ ) [{] _[x]_ [}][ is inconsistent, then we]\nhave _Pr_ ( _w_ ) = 0.\nThus, any EM world in which the set of associated facts and strict rules are\ninconsistent (we refer to this as “classical consistency”) must always be assigned a\nzero probability. The intuition is as follows: any subset of facts and strict rules are\nthought to be true under certain circumstances—these circumstances are determined\nthrough the annotation function and can be expressed as sets of EM worlds. Suppose\nthere is a world where two contradictory facts can both be considered to be true (based\non the annotation function). If this occurs, then there must not exist a probability\ndistribution that satisfies the program _ΠEM_ that assigns such a world a non-zero\nprobability, as this world leads to an inconsistency.\nWhile we have studied this theoretically (Shakarian et al. 2014), several important\nchallenges remain: How do different belief revision methods affect the results of\nattribution queries? In particular, can we develop tractable algorithms for belief\nrevision in the InCA framework? Further, finding efficient methods for re-computing\nattribution queries following a belief revision operation is a related concern for future\nwork.\n\n\n_**8.5.3**_ _**Temporal Reasoning**_\n\n\nCyber-security data often has an inherent temporal component (in particular, PCAP\nfiles, system logs, and traditional intelligence). One way to represent this type of\ninformation in InCA is by replacing the EM with a probabilistic temporal logic (i.e.\nHansson and Jonsson 1994; Dekhtyar et al. 1999; Shakarian et al. 2011; Shakarian\nand Simari 2012). However, even though this would be a relatively straightforward\nadjustment to the framework, it leads to several interesting questions, specifically:\n\n\n170 P. Shakarian et al.\n\n\n- Can we identify hacking groups responsible for a series of incidents over a period\nof time (a cyber campaign)?\n\n- Can we identify the group responsible for a campaign if it is not known a priori?\n\n- Can we differentiate between multiple campaigns conducted by multiple culprits\nin time-series data?\n\n\n_**8.5.4**_ _**Abductive Inference Queries**_\n\n\nWe may often have a case where more than one culprit is attributed to the same\ncyber-attack with nearly the same probabilities. In this case, can we identify certain\nevidence that, if found, can lead us to better differentiate among the potential culprits?\nIn the intelligence community, this is often referred as identifying _intelligence gaps_ .\nWe can also frame this as an abductive inference problem (Reggia and Peng 1990).\nThis type of problems leads to several interesting challenges:\n\n\n- Can we identify all pieces of diagnostic evidence that would satisfy an important\nintelligence gap?\n\n- Can we identify diagnostic evidence under constraints (i.e., taking into account\nlimitations on the type of evidence that can be collected)?\n\n- In the case where a culprit is attributed with a high probability, can we identify\nevidence that can falsify the finding?\n\n\n**8.6** **Conclusions**\n\n\nIn this paper we introduced InCA, a new framework that allows the modeling of various cyber-warfare/cyber-security scenarios in order to help answer the attribution\nquestion by means of a combination of probabilistic modeling and argumentative\nreasoning. This is the first framework, to our knowledge, that addresses the attribution problem while allowing for multiple pieces of evidence from different sources,\nincluding traditional (non-cyber) forms of intelligence such as human intelligence.\nFurther, our framework is the first to extend Defeasible Logic Programming with\nprobabilistic information. Currently, we are implementing InCA along with the\nassociated algorithms and heuristics to answer these queries.\n\n\n**Acknowledgments** This work was supported by UK EPSRC grant EP/J008346/1—“PrOQAW”,\nERC grant 246858—“DIADEM”, by NSF grant #1117761, by the National Security Agency under\nthe Science of Security Lablet grant (SoSL), Army Research Office project 2GDATXR042, and\nDARPA project R.0004972.001.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 171\n\n\n**References**\n\n\nShadows in the Cloud: Investigating Cyber Espionage 2.0. Tech. rep., Information Warfare Monitor\nand Shadowserver Foundation (2010)\nAPT1: Exposing one of China’s cyber espionage units. Mandiant (tech. report) (2013)\nAltheide, C.: Digital Forensics with Open Source Tools. Syngress (2011)\nDekhtyar,A., Dekhtyar, M.I., Subrahmanian, V.S.: Temporal probabilistic logic programs. In: ICLP\n1999, pp. 109–123. The MIT Press, Cambridge, MA, USA (1999)\nDung, P.M.: On the acceptability of arguments and its fundamental role in nonmonotonic reasoning,\nlogic programming and _n_ -person games. Artif. Intell. **77**, pp. 321–357 (1995)\nFalliere, N., Murchu, L.O., Chien, E.: W32.Stuxnet Dossier Version 1.4. Symantec Corporation\n(2011)\nGarcía, A.J., Simari, G.R.: Defeasible logic programming: An argumentative approach. TPLP\n**4** (1–2), 95–138 (2004)\nHansson, H., Jonsson, B.: A logic for reasoning about time and probability. Formal Aspects of\nComputing **6**, 512–535 (1994)\nHeuer, R.J.: Psychology of Intelligence Analysis. Center for the Study of Intelligence (1999)\nKhuller, S., Martinez, M.V., Nau, D.S., Sliva, A., Simari, G.I., Subrahmanian, V.S.: Computing\nmost probable worlds of action probabilistic logic programs: scalable estimation for 10 [30,000]\n\nworlds. AMAI **51(2–4)**, 295–331 (2007)\nLangner, R.: Matching Langner Stuxnet analysis and Symantic dossier update. Langner Communications GmbH (2011)\nLloyd, J.W.: Foundations of Logic Programming, 2nd Edition. Springer (1987)\nMartinez, M.V., García, A.J., Simari, G.R.: On the use of presumptions in structured defeasible\nreasoning. In: Proc. of COMMA, pp. 185–196 (2012)\nNilsson, N.J.: Probabilistic logic. Artif. Intell. **28** (1), 71–87 (1986)\nRahwan, I., Simari, G.R.: Argumentation in Artificial Intelligence. Springer (2009)\nReggia, J.A., Peng,Y.:Abductive inference models for diagnostic problem-solving. Springer-Verlag\nNew York, Inc., New York, NY, USA (1990)\nShakarian, P., Parker, A., Simari, G.I., Subrahmanian, V.S.: Annotated probabilistic temporal logic.\nTOCL **12** (2), 14 (2011)\nShakarian, P., Simari, G.I., Subrahmanian, V.S.: Annotated probabilistic temporal logic: Approximate fixpoint implementation. ACM Trans. Comput. Log. **13** (2), 13 (2012)\nShakarian, P., Shakarian, J., Ruef, A.: Introduction to Cyber-Warfare: A Multidisciplinary\nApproach. Syngress (2013)\nShakarian, P., Simari, G.I., Falappa, M.A.: Belief revision in structured probabilistic argumentation.\nIn: Proceedings of FoIKS, pp. 324–343 (2014)\nSimari, G.R., Loui, R.P.: A mathematical treatment of defeasible reasoning and its implementation.\nArtif. Intell. **53** (2-3), 125–157 (1992)\nSimari, G.I., Martinez, M.V., Sliva, A., Subrahmanian, V.S.: Focused most probable world\ncomputations in probabilistic logic programs. AMAI **64** (2–3), 113–143 (2012)\nSpitzner, L.: Honeypots: Catching the Insider Threat. In: Proc. ofACSAC 2003, pp. 170–179. IEEE\nComputer Society (2003)\nStolzenburg, F., García, A., Chesñevar, C.I., Simari, G.R.: Computing Generalized Specificity.\nJournal of Non-Classical Logics **13** (1), 87–113 (2003)\nThonnard, O., Mees, W., Dacier, M.: On a multicriteria clustering approach for attack attribution.\nSIGKDD Explorations **12** (1), 11–20 (2010)",
  "flat_text": "## **Chapter 8**\n# **Cyber Attribution: An Argumentation-Based** **Approach**\n\n**Paulo Shakarian, Gerardo I. Simari, Geoffrey Moores and Simon Parsons**\n\n\n**Abstract** Attributing a cyber-operation through the use of multiple pieces of\ntechnical evidence (i.e., malware reverse-engineering and source tracking) and conventional intelligence sources (i.e., human or signals intelligence) is a difficult\nproblem not only due to the effort required to obtain evidence, but the ease with\nwhich an adversary can plant false evidence. In this paper, we introduce a formal reasoning system called the InCA (Intelligent Cyber Attribution) framework\nthat is designed to aid an analyst in the attribution of a cyber-operation even when\nthe available information is conflicting and/or uncertain. Our approach combines\nargumentation-based reasoning, logic programming, and probabilistic models to not\nonly attribute an operation but also explain to the analyst why the system reaches its\nconclusions.\n\n\n**8.1** **Introduction**\n\n\nAn important issue in cyber-warfare is the puzzle of determining who was responsible\nfor a given cyber-operation—be it an incident of attack, reconnaissance, or information theft. This is known as the “attribution problem” (Shakarian et al. 2013).\nThe difficulty of this problem stems not only from the amount of effort required to\nfind forensic clues but also the ease with which an attacker can plant false clues to\n\n\nP. Shakarian (�)\nArizona Sate University, Tempe, AZ, USA\ne-mail: shak@asu.edu\n\n\nG. I. Simari\nDepartment of Computer Science and Engineering, Universidad Nacional del Sur,\nBahía Blanca, Argentina\ne-mail: gis@cs.uns.edu.ar\n\n\nG. Moores\nDepartment of Electrical Engineering and Computer Science, U.S. Military Academy,\nWest Point, NY, USA\ne-mail: geoffrey.moores@usma.edu\n\n\nS. Parsons\nDepartment of Computer Science, University of Liverpool, Liverpool, UK\ne-mail: s.d.parsons@liverpool.ac.uk\n\n© Springer International Publishing Switzerland 2015 151\nS. Jajodia et al. (eds.), _Cyber Warfare,_ Advances in Information Security 56,\nDOI 10.1007/978-3-319-14039-1_8\n\n\n152 P. Shakarian et al.\n\n\nmislead security personnel. Further, while techniques such as forensics and reverseengineering (Altheide 2011), source tracking (Thonnard et al. 2010), honeypots\n(Spitzner 2003), and sinkholing 2010 are commonly employed to find evidence that\ncan lead to attribution, it is unclear how this evidence is to be combined and reasoned\nabout. In a military setting, such evidence is augmented with normal intelligence collection, such as human intelligence (HUMINT), signals intelligence (SIGINT) and\nother means—this adds additional complications to the task of attributing a given\noperation. Essentially, cyber-attribution is a highly-technical intelligence analysis\nproblem where an analyst must consider a variety of sources, each with its associated level of confidence, to provide a decision maker (e.g., a military commander)\ninsight into who conducted a given operation.\nAs it is well known that people’s ability to conduct intelligence analysis is limited\n(Heuer 1999), and due to the highly technical nature of many cyber evidencegathering techniques, an automated reasoning system would be best suited for the\ntask. Such a system must be able to accomplish several goals, among which we\ndistinguish the following main capabilities:\n\n\n1. Reason about evidence in a formal, principled manner, i.e., relying on strong\nmathematical foundations.\n2. Consider evidence for cyber attribution associated with some level of probabilistic\nuncertainty.\n3. Consider logical rules that allow for the system to draw conclusions based on\ncertain pieces of evidence and iteratively apply such rules.\n4. Consider pieces of information that may not be compatible with each other, decide\nwhich information is most relevant, and express why.\n5. Attribute a given cyber-operation based on the above-described features and provide the analyst with the ability to understand how the system arrived at that\nconclusion.\n\n\nIn this paper we present the InCA (Intelligent Cyber Attribution) framework, which\nmeets all of the above qualities. Our approach relies on several techniques from\nthe artificial intelligence community, including argumentation, logic programming,\nand probabilistic reasoning. We first outline the underlying mathematical framework and provide a running example based on real-world cases of cyber-attribution\n(cf. Sect. 8.2); then, in Sects. 8.3 and 8.4, we formally present InCA and attribution\nqueries, respectively. Finally, we discuss conclusions and future work in Sect. 8.6.\n\n\n**8.2** **Two Kinds of Models**\n\n\nOur approach relies on _two separate models of the world_ . The first, called the _**environ-**_\n_**mentalmodel**_ (EM)isusedtodescribethebackgroundknowledgeandisprobabilistic\nin nature. The second one, called the _**analytical model**_ (AM) is used to analyze\ncompeting hypotheses that can account for a given phenomenon (in this case, a\ncyber-operation). The EM _must be consistent_ —this simply means that there must\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 153\n\n\nEM AM\n“Malware X was compiled on a system “Malware X was compiled on a system\nusing the English language.” English-speaking country Y.”\n“Malware W and malware X were created “Malware W and malware X are related.”\nin a similar coding sytle.”\n“Country Y and country Z are currently “Country Y has a motive to launch a\nat war.” cybre-attack against country Z.”\n“Country Y has a significant investment “Country Y has the capability to conduct\nin math-science-engineering (MSE) education.” a cyber-attack.”\n\n\n**Fig. 8.1** Example observations—EM vs. AM\n\n\nexist a probability distribution over the possible states of the world that satisfies all\nof the constraints in the model, as well as the axioms of probability theory. On the\ncontrary, the AM will allow for contradictory information as the system must have\nthe capability to reason about competing explanations for a given cyber-operation.\nIn general, the EM contains knowledge such as evidence, intelligence reporting, or\nknowledge about actors, software, and systems. TheAM, on the other hand, contains\nideas the analyst concludes based on the information in the EM. Figure 8.1 gives\nsome examples of the types of information in the two models. Note that an analyst\n(or automated system) could assign a probability to statements in the EM column\nwhereas statements in the AM column can be true or false depending on a certain\ncombination (or several possible combinations) of statements from the EM. We now\nformally describe these two models as well as a technique for _annotating_ knowledge\nin the AM with information from the EM—these annotations specify the conditions\nunder which the various statements in the AM can potentially be true.\nBefore describing the two models in detail, we first introduce the language used\nto describe them. Variable and constant symbols represent items such as computer\nsystems, types of cyber operations, actors (e.g., nation states, hacking groups), and\nother technical and/or intelligence information. The set of all variable symbols is\ndenoted with **V**, and the set of all constants is denoted with **C** . For our framework, we\nshallrequiretwosubsetsof **C**, **C** _act_ and **C** _ops_, thatspecifytheactorsthatcouldconduct\ncyber-operations and the operations themselves, respectively. In the examples in this\npaper, we will use capital letters to represent variables (e.g., _X_, _Y_, _Z_ ). The constants\nin **C** _act_ and **C** _ops_ that we use in the running example are specified in the following\nexample.\n\n\n_Example 1_ The following (fictitious) actors and cyber-operations will be used in\nour examples:\n\n\n**C** _act_ = { _baja_, _krasnovia_, _mojave_ } (8.1)\n\n**C** _ops_ = { _worm123_ } (8.2)\n\n\n                     The next component in the model is a set of predicate symbols. These constructs\ncan accept zero or more variables or constants as arguments, and map to either\n\n\n154 P. Shakarian et al.\n\n\n**P** _EM_ : _origIP_ ( _M,_ _X_ ) Malware _M_ originated from an IP address belonging to actor _X_ .\n_malwInOp_ ( _M,_ _O_ ) Malware _M_ was used in cyber-operation _O_ .\n_mwHint_ ( _M,_ _X_ ) Malware _M_ contained a hint that it was created by actor _X_ .\n_compilLang_ ( _M,C_ ) Malware _M_ was compiled in a system that used language _C_ .\n_nativLang_ ( _X,C_ ) Language _C_ is the native language of actor _X_ .\n_inLgConf_ ( _X,_ _X_ _[′]_ ) Actors _X_ and _X_ _[′]_ are in a larger conflict with each other.\n_mseTT_ ( _X,_ _N_ ) There are at least _N_ number of top-tier math-science-engineering\nuniversities in country _X_ .\n_infGovSys_ ( _X,_ _M_ ) Systems belonging to actor _X_ were infected with malware _M_ .\n_cybCapAge_ ( _X,_ _N_ ) Actor _X_ has had a cyber-warfare capability for _N_ years or less.\n_govCybLab_ ( _X_ ) Actor _X_ has a government cyber-security lab.\n\n\n**P** _AM_ : _condOp_ ( _X,_ _O_ ) Actor _X_ conducted cyber-operation _O_ .\n_evidOf_ ( _X,_ _O_ ) There is evidence that actor _X_ conducted cyber-operation _O_ .\n_motiv_ ( _X,_ _X_ _[′]_ ) Actor _X_ had a motive to launch a cyber-attack against actor _X_ _[′]_ .\n_isCap_ ( _X,_ _O_ ) Actor _X_ is capable of conducting cyber-operation _O_ .\n_tgt_ ( _X,_ _O_ ) Actor _X_ was the target of cyber-operation _O_ .\n_hasMseInvest_ ( _X_ ) Actor _X_ has a significant investment in math-science-engineering\neducation.\n_expCw_ ( _X_ ) Actor _X_ has experience in conducting cyber-operations.\n\n\n**Fig. 8.2** Predicate definitions for the environment and analytical models in the running example\n\n\n_true_ or false. _Note that the EM and AM use separate sets of predicate symbols_ however, they can share variables and constants. The sets of predicates for the\nEM and AM are denoted with **P** _EM_, **P** _AM_, respectively. In InCA, we require **P** _AM_\nto include the binary predicate _condOp_ ( _X_, _Y_ ), where _X_ is an actor and _Y_ is a cyberoperation. Intuitively, this means that actor _X_ conducted operation _Y_ . For instance,\n_condOp_ ( _baja_, _worm123_ )istrueif _baja_ wasresponsibleforcyber-operation _worm123_ .\nA sample set of predicate symbols for the analysis of a cyber attack between two\nstates over contention of a particular industry is shown in Fig. 8.2; these will be used\nin examples throughout the paper.\nA construct formed with a predicate and constants as arguments is known as a\n_ground atom_ (we shall often deal with ground atoms). The sets of all ground atoms\nfor EM and AM are denoted with **G** _EM_ and **G** _AM_, respectively.\n\n\n_Example 2_ The following are examples of ground atoms over the predicates given\nin Fig. 8.2.\n\n\n**G** _EM_ : _origIP_ ( _mw123sam1_, _krasnovia_ ),\n\n\n_mwHint_ ( _mw123sam1_, _krasnovia_ ),\n\n\n_inLgConf_ ( _krasnovia_, _baja_ ),\n\n\n_mseTT_ ( _krasnovia_, 2) _._\n\n\n**G** _AM_ : _evidOf_ ( _mojave_, _worm123_ ),\n\n\n_motiv_ ( _baja_, _krasnovia_ ),\n\n\n_expCw_ ( _baja_ ),\n\n\n_tgt_ ( _krasnovia_, _worm123_ ) _._                \n\n8 Cyber Attribution: An Argumentation-Based Approach 155\n\n\nFor a given set of ground atoms, a _world_ is a subset of the atoms that are considered\nto be true (ground atoms not in the world are false). Hence, there are 2 [|] **[G]** _[EM]_ [|] possible worlds in the EM and 2 [|] **[G]** _[AM]_ [|] worlds in the AM, denoted with _WEM_ and _WAM_,\nrespectively.\nClearly, even a moderate number of ground atoms can yield an enormous number\nof worlds to explore. One way to reduce the number of worlds is to include _integrity_\n_constraints_, which allow us to eliminate certain worlds from consideration—they\nsimplyarenotpossibleinthesettingbeingmodeled. Ourprincipleintegrityconstraint\nwill be of the form:\n\n\noneOf( _A_ [′] )\n\n\nwhere _A_ [′] is a subset of ground atoms. Intuitively, this says that any world where\nmore than one of the atoms from set _A_ [′] appear is invalid. Let **IC** _EM_ and **IC** _AM_ be the\nsets of integrity constraints for the EM and AM, respectively, and the sets of worlds\nthat conform to these constraints be _WEM_ ( **IC** _EM_ ), _WAM_ ( **IC** _AM_ ), respectively.\nAtoms can also be combined into formulas using standard logical connectives:\nconjunction ( _and_ ), disjunction ( _or_ ), and negation ( _not_ ). These are written using the\nsymbols ∧, ∨, ¬, respectively. We say a world ( _w_ ) _satisfies_ a formula ( _f_ ), written\n_w_ |= _f_, based on the following inductive definition:\n\n\n- if _f_ is a single atom, then _w_ |= _f_ iff _f_ ∈ _w_ ;\n\n- if _f_ = ¬ _f_ [′] then _w_ |= _f_ iff _w_ ̸|= _f_ [′] ;\n\n- if _f_ = _f_ [′] ∧ _f_ [′′] then _w_ |= _f_ iff _w_ |= _f_ [′] and _w_ |= _f_ [′′] ; and\n\n- if _f_ = _f_ [′] ∨ _f_ [′′] then _w_ |= _f_ iff _w_ |= _f_ [′] or _w_ |= _f_ [′′] .\n\n\nWe use the notation _f ormulaEM_, _f ormulaAM_ to denote the set of all possible\n(ground) formulas in the EM andAM, respectively.Also, note that we use the notation\n⊤, ⊥ to represent tautologies (formulas that are true in all worlds) and contradictions\n(formulas that are false in all worlds), respectively.\n\n\n_**8.2.1**_ _**Environmental Model**_\n\n\nIn this section we describe the first of the two models, namely the EM or environmental model. This model is largely based on the probabilistic logic of (Nilsson 1986),\nwhich we now briefly review.\nFirst, we define a _probabilistic formula_ that consists of a formula _f_ over\natoms from **G** _EM_, a real number _p_ in the interval [0, 1], and an error tolerance\n_ε_ ∈ [0, min( _p_, 1 − _p_ )]. A probabilistic formula is written as: _f_ : _p_ ± _ε_ . Intuitively,\nthis statement is interpreted as “formula _f_ is true with probability between _p_ - _ε_\nand _p_ + _ε_ ”—note that we make no statement about the probability distribution over\nthis interval. The uncertainty regarding the probability values stems from the fact\nthat certain assumptions (such as probabilistic independence) may not be suitable in\nthe environment being modeled.\n\n\n156 P. Shakarian et al.\n\n\n_Example 3_ To continue our running example, consider the following set _ΠEM_ :\n\n\n_f_ 1 = _govCybLab_ ( _baja_ ) : 0 _._ 8 ± 0 _._ 1\n\n_f_ 2 = _cybCapAge_ ( _baja_, 5) : 0 _._ 2 ± 0 _._ 1\n\n_f_ 3 = _mseTT_ ( _baja_, 2) : 0 _._ 8 ± 0 _._ 1\n\n_f_ 4 = _mwHint_ ( _mw123sam1_, _mojave_ ) ∧ _compilLang_ ( _worm123_, _english_ ) : 0 _._ 7 ± 0 _._ 2\n\n_f_ 5 = _malwInOp_ ( _mw123sam1_, _worm123_ )\n\n\n∧ _malwareRel_ ( _mw123sam1_, _mw123sam2_ )\n\n\n∧ _mwHint_ ( _mw123sam2_, _mojave_ ) : 0 _._ 6 ± 0 _._ 1\n\n\n_f_ 6 = _inLgConf_ ( _baja_, _krasnovia_ ) ∨¬ _cooper_ ( _baja_, _krasnovia_ ) : 0 _._ 9 ± 0 _._ 1\n\n_f_ 7 = _origIP_ ( _mw123sam1_, _baja_ ) : 1 ± 0\n\n\nThroughout other examples in the rest of the paper, we will make use of the subset\n_ΠEM_ [′] [= {] _[f]_ [1][,] _[ f]_ [2][,] _[ f]_ [3][}][.] We now consider a probability distribution _Pr_ over the set _WEM_ ( **IC** _EM_ ). We\nsay that _Pr satisfies_ probabilistic formula _f_ : _p_ ± _ε_ iff the following holds:\n_p_ - _ε_ ≤ [�] _w_ ∈ _WEM_ ( **IC** _EM_ ) _[Pr]_ [(] _[w]_ [)][ ≤] _[p]_ [ +] _[ ε.]_ [ A set] _[ Π][EM]_ [ of probabilistic formulas is]\ncalled a _knowledge base_ . We say that a probability distribution over _WEM_ ( **IC** _EM_ )\n_satisfies ΠEM_ if and only if it satisfies all probabilistic formulas in _ΠEM_ .\nIt is possible to create probabilistic knowledge bases for which there is no\nsatisfying probability distribution. The following is a simple example of this:\n\n\n_condOp_ ( _krasnovia_, _worm123_ ) ∨ _condOp_ ( _baja_, _worm123_ ) : 0 _._ 4 ± 0;\n\n\n_condOp_ ( _krasnovia_, _worm123_ ) ∧ _condOp_ ( _baja_, _worm123_ ) : 0 _._ 6 ± 0 _._ 1 _._\n\n\nFormulas and knowledge bases of this sort are _inconsistent_ . In this paper, we assume\nthat information is properly extracted from a set of historic data and hence consistent;\n(recall that inconsistent information can only be handled in the AM, not the EM). A\nconsistent knowledge base could also be obtained as a result of curation by experts,\nsuch that all inconsistencies were removed—see (Khuller et al. 2007; Shakarian et\nal. 2011) for algorithms for learning rules of this type.\nThe main kind of query that we require for the probabilistic model is the _maximum_\n_entailment_ problem: given a knowledge base _ΠEM_ and a (non-probabilistic) formula\n_q_, identify _p_, _ε_ such that all valid probability distributions _Pr_ that satisfy _ΠEM_ also\nsatisfy _q_ : _p_ ± _ε_, and there does not exist _p_ [′], _ε_ [′] s.t. [ _p_ - _ε_, _p_ + _ε_ ] ⊃ [ _p_ [′] - _ε_ [′], _p_ [′] + _ε_ [′] ],\nwhere all probability distributions _Pr_ that satisfy _ΠEM_ also satisfy _q_ : _p_ [′] ± _ε_ [′] . That\nis, given _q_, can we determine the probability (with maximum tolerance) of statement\n_q_ given the information in _ΠEM_ ? The approach adopted in (Nilsson et al. 1986) to\nsolve this problem works as follows. First, we must solve the linear program defined\nnext.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 157\n\n\n**Definition 1** ( **EM-LP-MIN)** Given a knowledge base _ΠEM_ and a formula _q_ :\n\n\n- create a variable _xi_ for each _wi_ ∈ _WEM_ ( **IC** _EM_ );\n\n- for each _fj_ : _pj_ ± _εj_ ∈ _ΠEM_, create constraint:\n\n      _pj_          - _εj_ ≤ _xi_ ≤ _pj_ + _εj_ ;\n\n_wi_ ∈ _WEM_ ( **IC** _EM_ ) _s.t.wi_ |= _fj_\n\n\n- finally, we also have a constraint:\n\n      \n_xi_ = 1 _._\n_wi_ ∈ _WEM_ ( **IC** _EM_ )\n\n\nThe objective is to minimize the function:\n\n      \n_xi._\n_wi_ ∈ _WEM_ ( **IC** _EM_ ) _s.t.wi_ |= _q_\n\n\nWe use the notation EP-LP-MIN( _ΠEM_, _q_ ) to refer to the value of the objective\nfunction in the solution to the EM-LP-MIN constraints.\nLet _ℓ_ be the result of the process described in Definition 1. The next step is to\nsolve the linear program a second time, but instead maximizing the objective function\n(we shall refer to this as EM-LP-MAX)—let _u_ be the result of this operation. In\n(Nilsson 1986), it is shown that _ε_ = _u_ −2 _ℓ_ and _p_ = _ℓ_ + _ε_ is the solution to the\n\nmaximum entailment problem. We note that although the above linear program has\nan exponential number of variables in the worst case (i.e., no integrity constraints),\nthe presence of constraints has the potential to greatly reduce this space. Further,\nthere are also good heuristics (cf. Khuller et al. 2007; Simari et al. 2012) that have\nbeen shown to provide highly accurate approximations with a reduced-size linear\nprogram.\n\n_Example 4_ Consider KB _ΠEM_ [′] [from Example][ 3][ and a set of ground atoms restricted]\nto those that appear in that program. Hence, we have:\n\n\n_w_ 1 = { _govCybLab_ ( _baja_ ), _cybCapAge_ ( _baja_, 5), _mseTT_ ( _baja_, 2)}\n\n_w_ 2 = { _govCybLab_ ( _baja_ ), _cybCapAge_ ( _baja_, 5)}\n\n_w_ 3 = { _govCybLab_ ( _baja_ ), _mseTT_ ( _baja_, 2)}\n\n_w_ 4 = { _cybCapAge_ ( _baja_, 5), _mseTT_ ( _baja_, 2)}\n\n_w_ 5 = { _cybCapAge_ ( _baja_, 5)}\n\n_w_ 6 = { _govCybLab_ ( _baja_ )}\n\n_w_ 7 = { _mseTT_ ( _baja_, 2)}\n\n_w_ 8 = ∅\n\n\nand suppose we wish to compute the probability for formula:\n\n\n_q_ = _govCybLab_ ( _baja_ ) ∨ _mseTT_ ( _baja_, 2) _._\n\n\n158 P. Shakarian et al.\n\n\nFor each formula in _ΠEM_ we have a constraint, and for each world above we have a\nvariable. An objective function is created based on the worlds that satisfy the query\nformula (here, worlds _w_ 1 − _w_ 4, _w_ 6, _w_ 7). Hence, EP-LP-MIN( _ΠEM_ [′] [,] _[ q]_ [) can be written]\nas follows:\n\n\nmax _x_ 1 + _x_ 2 + _x_ 3 + _x_ 4 + _x_ 6 + _x_ 7 _w.r.t._ :\n\n0 _._ 7 ≤ _x_ 1 + _x_ 2 + _x_ 3 + _x_ 6 ≤ 0 _._ 9\n\n0 _._ 1 ≤ _x_ 1 + _x_ 2 + _x_ 4 + _x_ 5 ≤ 0 _._ 3\n\n0 _._ 8 ≤ _x_ 1 + _x_ 3 + _x_ 4 + _x_ 7 ≤ 1\n\n_x_ 1 + _x_ 2 + _x_ 3 + _x_ 4 + _x_ 5 + _x_ 6 + _x_ 7 + _x_ 8 = 1\n\nWe can now solve EP-LP-MAX( _ΠEM_ [′] [,] _[ q]_ [) and][ EP-LP-MIN][(] _[Π]_ _EM_ [′] [,] _[ q]_ [) to get solution]\n0 _._ 9 ± 0 _._ 1. \n\n_**8.2.2**_ _**Analytical Model**_\n\n\nFor the analytical model (AM), we choose a structured argumentation framework\n(Rahwan et al. 2009) due to several characteristics that make such frameworks highly\napplicable to cyber-warfare domains. Unlike the EM, which describes probabilistic\ninformation about the state of the real world, the AM must allow for competing\nideas—it _must be able to represent contradictory information_ . The algorithmic\napproach allows for the creation of _arguments_ based on the AM that may “compete” with each other to describe who conducted a given cyber-operation. In this\ncompetition—known as a _dialectical process_ —one argument may defeat another\nbased on a _comparison criterion_ that determines the prevailing argument. Resulting\nfrom this process, the InCA framework will determine arguments that are _war-_\n_ranted_ (those that are not _defeated_ by other arguments) thereby providing a suitable\nexplanation for a given cyber-operation.\nThe transparency provided by the system can allow analysts to identify potentially\nincorrect input information and fine-tune the models or, alternatively, collect more\ninformation. In short, argumentation-based reasoning has been studied as a natural\nway to manage a set of inconsistent information—it is the way humans settle disputes. As we will see, another desirable characteristic of (structured) argumentation\nframeworks is that, once a conclusion is reached, we are left with an explanation of\nhow we arrived at it and information about why a given argument is warranted; this\nis very important information for analysts to have. In this section, we recall some\npreliminaries of the underlying argumentation framework used, and then introduce\nthe analytical model (AM).\n\n\n**Defeasible Logic Programming with Presumptions**\n\n\nDeLP with Presumptions (PreDeLP) (Martinez et al. 2012) is a formalism combining Logic Programming with Defeasible Argumentation. We now briefly recall\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 159\n\n\n**Fig. 8.3** A ground argumentation framework\n\n\nthe basics of PreDeLP; we refer the reader to (García and Simari 2004; Martinez\net al. 2012) for the complete presentation. The formalism contains several different\nconstructs: facts, presumptions, strict rules, and defeasible rules. Facts are statements\nabout the analysis that can always be considered to be true, while presumptions are\nstatements that may or may not be true. Strict rules specify logical consequences of\na set of facts or presumptions (similar to an implication, though not the same) that\nmust always occur, while defeasible rules specify logical consequences that may be\nassumed to be true when no contradicting information is present. These constructs\nare used in the construction of _arguments_, and are part of a PreDeLP program, which\nis a set of facts, strict rules, presumptions, and defeasible rules. Formally, we use\nthe notation _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) to denote a PreDeLP program, where _Ω_ is the set\nof strict rules, _Θ_ is the set of facts, _Δ_ is the set of defeasible rules, and _Φ_ is the set\nof presumptions. In Fig. 8.3, we provide an example _ΠAM_ . We now describe each of\nthese constructs in detail.\n\n\n**Facts** ( _Θ_ ) are ground literals representing atomic information or its negation, using\nstrong negation “¬”. Note that all of the literals in our framework must be formed\nwith a predicate from the set **P** _AM_ . Note that information in this form cannot be\ncontradicted.\n\n\n**Strict Rules** ( _Ω_ ) represent non-defeasible cause-and-effect information that resembles a material implication (though the semantics is different since the contrapositive\n\n\n160 P. Shakarian et al.\n\n\ndoes not hold) and are of the form _L_ 0 ←− _L_ 1, _. . ._, _Ln_, where _L_ 0 is a ground literal\nand { _Li_ } _i>_ 0 is a set of ground literals.\n\n\n**Presumptions** ( _Φ_ ) are ground literals of the same form as facts, except that they\nare not taken as being true but rather defeasible, which means that they can be\ncontradicted. Presumptions are denoted in the same manner as facts, except that the\nsymbol –≺ is added. While any literal can be used as a presumption in InCA, we\nspecifically require all literals created with the predicate _condOp_ to be defeasible.\n\n\n**Defeasible Rules** ( _Δ_ ) represent tentative knowledge that can be used if nothing\ncan be posed against it. Just as presumptions are the defeasible counterpart of facts,\ndefeasible rules are the defeasible counterpart of strict rules. They are of the form\n_L_ 0 –≺ _L_ 1, _. . ._, _Ln_, where _L_ 0 is a ground literal and { _Li_ } _i>_ 0 is a set of ground literals.\nNote that with both strict and defeasible rules, _strong negation_ is allowed in the head\nof rules, and hence may be used to represent contradictory knowledge.\nWe note that strict rules and facts are necessary constructs as they may not be true\nin all environmental conditions. We shall discuss this further in the next section with\nthe introduction of an annotation function.\nEven though the above constructs are ground, we allow for schematic versions\nwith variables that are used to represent sets of ground rules. We denote variables\nwith strings starting with an uppercase letter; Fig. 8.4 shows a non-ground example.\nWhen a cyber-operation occurs, InCA must derive arguments as to who could\nhave potentially conducted the action. Informally, an argument for a particular actor\n_x_ conducting cyber-operation _y_ is a consistent subset of the analytical model that\nentails the atom _condOp_ ( _x_, _y_ ). If the argument contains only strict rules and facts,\nthen it is _factual_ . If it contains presumptions or defeasible rules, then it _defeasibly_\n_derives_ that actor _x_ conducted operation _y_ .\nDerivation follows the same mechanism of Logic Programming (Lloyd 1987).\nSince rule heads can contain strong negation, it is possible to defeasibly derive contradictory literals from a program. For the treatment of contradictory knowledge,\nPreDeLP incorporates a defeasible argumentation formalism that allows the identification of the pieces of knowledge that are in conflict, and through the previously\nmentioned _dialectical process_ decides which information prevails as warranted.\nThis dialectical process involves the construction and evaluation of arguments\nthat either support or interfere with a given query, building a _dialectical tree_ in the\nprocess. Formally, we have:\n\n\n**Definition 2 (Argument)** An _argument A_, _L_ ⟩ for a literal _L_ is a pair of the literal\nand a (possibly empty) set of the EM ( _A_ ⊆ _ΠAM_ ) that provides a minimal proof for _L_\nmeeting the requirements: (1) _L_ is defeasibly derived from _A_, (2) _Ω_ ∪ _Θ_ ∪ _A_ is not\ncontradictory, and (3) _A_ is a minimal subset of _Δ_ ∪ _Φ_ satisfying 1 and 2, denoted\n⟨ _A_, _L_ ⟩.\nLiteral _L_ is called the _conclusion_ supported by the argument, and _A_ is the _support_\nof the argument. An argument ⟨ _B_, _L_ ⟩ is a _subargument_ of ⟨ _A_, _L_ [′] ⟩ iff _B_ ⊆ _A_ . An\nargument ⟨ _A_, _L_ ⟩ is _presumptive_ iff _A_ ∩ _Φ_ is not empty. We will also use _Ω_ ( _A_ ) =\n_A_ ∩ _Ω_, _Θ_ ( _A_ ) = _A_ ∩ _Θ_, _Δ_ ( _A_ ) = _A_ ∩ _Δ_, and _Φ_ ( _A_ ) = _A_ ∩ _Φ_ .\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 161\n\n\n**Fig. 8.4** A non-ground argumentation framework\n\n\n**Fig. 8.5** Example ground arguments from Fig. 8.3\n\n\nNote that our definition differs slightly from that of (Simari and Loui 1992) where\nDeLP is introduced, as we include strict rules and facts as part of the argument. The\nreason for this will become clear in Sect. 8.3. Arguments for our scenario are shown\nin the following example.\n\n\n_Example 5_ Figure 8.5 shows example arguments based on the knowledge base from\nFig. 8.3. Note that the following relationship exists:\n\n\n⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ is a sub-argument of\n\n\n⟨ _A_ 2, _condOp_ ( _baja_, _worm123_ )⟩ and\n\n\n⟨ _A_ 3, _condOp_ ( _baja_, _worm123_ )⟩ _._         \n\nGiven argument ⟨ _A_ 1, _L_ 1⟩, counter-arguments are arguments that contradict it.\nArgument ⟨ _A_ 2, _L_ 2⟩ _counterargues_ or _attacks_ ⟨ _A_ 1, _L_ 1⟩ literal _L_ [′] iff there exists a\nsubargument ⟨ _A_, _L_ [′′] ⟩ of ⟨ _A_ 1, _L_ 1⟩ s.t. set _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _Θ_ ( _A_ 1) ∪ _Θ_ ( _A_ 2) ∪\n{ _L_ 2, _L_ [′′] } is contradictory.\n\n\n_Example 6_ Consider the arguments from Example 5. The following are some of\nthe attack relationships between them: _A_ 1, _A_ 2, _A_ 3, and _A_ 4 all attack _A_ 6; _A_ 5 attacks\n_A_ 7; and _A_ 7 attacks _A_ 2. \n\n162 P. Shakarian et al.\n\n\nA _proper defeater_ of an argument ⟨ _A_, _L_ ⟩ is a counter-argument that—by some\ncriterion—is considered to be better than ⟨ _A_, _L_ ⟩; if the two are incomparable according to this criterion, the counterargument is said to be a _blocking_ defeater. An\nimportantcharacteristicofPreDeLPisthattheargumentcomparisoncriterionismodular, and thus the most appropriate criterion for the domain that is being represented\ncan be selected; the default criterion used in classical defeasible logic programming (from which PreDeLP is derived) is _generalized specificity_ (Stolzenburg et al.\n2003), though an extension of this criterion is required for arguments using presumptions (Martinez et al. 2012). We briefly recall this criterion next—the first\ndefinition is for generalized specificity, which is subsequently used in the definition\nof presumption-enabled specificity.\n\n\n**Definition 3** Let _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) be a PreDeLP program and let _F_ be the\nset of all literals that have a defeasible derivation from _ΠAM_ . An argument ⟨ _A_ 1, _L_ 1⟩\nis _preferred to_ ⟨ _A_ 2, _L_ 2⟩, denoted with _A_ 1 ≻ _PS A_ 2 iff the two following conditions\nhold:\n\n\n1. For all _H_ ⊆ _F_, _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ is non-contradictory: if there is a derivation\nfor _L_ 1 from _Ω_ ( _A_ 2) ∪ _Ω_ ( _A_ 1) ∪ _Δ_ ( _A_ 1) ∪ _H_, and there is no derivation for _L_ 1\nfrom _Ω_ ( _A_ 1)∪ _Ω_ ( _A_ 2)∪ _H_, then there is a derivation for _L_ 2 from _Ω_ ( _A_ 1)∪ _Ω_ ( _A_ 2)∪\n_Δ_ ( _A_ 2) ∪ _H_ .\n2. There is at least one set _H_ [′] ⊆ _F_, _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] is non-contradictory,\nsuch that there is a derivation for _L_ 2 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] ∪ _Δ_ ( _A_ 2), there\nis no derivation for _L_ 2 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′], and there is no derivation for\n_L_ 1 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] ∪ _Δ_ ( _A_ 1).\n\n\nIntuitively, the principle of specificity says that, in the presence of two conflicting\nlines of argument about a proposition, the one that uses more of the available information is more convincing.A classic example involves a bird, Tweety, and arguments\nstating that it both flies (because it is a bird) and doesn’t fly (because it is a penguin).\nThe latter argument uses more information about Tweety—it is more specific—and\nis thus the stronger of the two.\n\n\n**Definition 4** (Martinez et al. 2012) Let _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) be a PreDeLP program. An argument ⟨ _A_ 1, _L_ 1⟩ is _preferred to_ ⟨ _A_ 2, _L_ 2⟩, denoted with _A_ 1 ≻ _A_ 2 iff any\nof the following conditions hold:\n\n\n1. ⟨ _A_ 1, _L_ 1⟩ and ⟨ _A_ 2, _L_ 2⟩ are both factual arguments and ⟨ _A_ 1, _L_ 1⟩≻ _PS_ ⟨ _A_ 2, _L_ 2⟩.\n2. ⟨ _A_ 1, _L_ 1⟩ is a factual argument and ⟨ _A_ 2, _L_ 2⟩ is a presumptive argument.\n3. ⟨ _A_ 1, _L_ 1⟩ and ⟨ _A_ 2, _L_ 2⟩ are presumptive arguments, and\na) ¬( _Φ_ ( _A_ 1) ⊆ _Φ_ ( _A_ 2)), or\nb) _Φ_ ( _A_ 1) = _Φ_ ( _A_ 2) and ⟨ _A_ 1, _L_ 1⟩≻ _PS_ ⟨ _A_ 2, _L_ 2⟩.\n\n\nGenerally, if _A_, _B_ are arguments with rules _X_ and _Y_, resp., and _X_ ⊂ _Y_, then _A_ is\nstronger than _B_ . This also holds when _A_ and _B_ use presumptions _P_ 1 and _P_ 2, resp.,\nand _P_ 1 ⊂ _P_ 2.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 163\n\n\n_Example 7_ The following are relationships between arguments from Example 5,\nbased on Definitions 3 and 4:\n\n\n_A_ 1 and _A_ 6 are incomparable (blocking defeaters);\n\n_A_ 6 ≻ _A_ 2, and thus _A_ 6 defeats _A_ 2;\n\n_A_ 6 ≻ _A_ 3, and thus _A_ 6 defeats _A_ 3;\n\n_A_ 6 ≻ _A_ 4, and thus _A_ 6 defeats _A_ 4;\n\n_A_ 5 and _A_ 7 are incomparable (blocking defeaters) _._     \n\nA sequence of arguments called an _argumentation line_ thus arises from this attack relation, where each argument defeats its predecessor. To avoid undesirable\nsequences, that may represent circular or fallacious argumentation lines, in DeLP\nan _argumentation line_ is _acceptable_ if it satisfies certain constraints (see García and\nSimari 2004). A literal _L_ is _warranted_ if there exists a non-defeated argument _A_\nsupporting _L_ .\nClearly, there can be more than one defeater for a particular argument ⟨ _A_, _L_ ⟩.\nTherefore, many acceptable argumentation lines could arise from ⟨ _A_, _L_ ⟩, leading\nto a tree structure. The tree is built from the set of all argumentation lines rooted\nin the initial argument. In a dialectical tree, every node (except the root) represents\na defeater of its parent, and leaves correspond to undefeated arguments. Each path\nfrom the root to a leaf corresponds to a different acceptable argumentation line. A\ndialectical tree provides a structure for considering all the possible acceptable argumentation lines that can be generated for deciding whether an argument is defeated.\nWe call this tree _dialectical_ because it represents an exhaustive dialectical analysis\n(in the sense of providing reasons for and against a position) for the argument in its\nroot. For argument ⟨ _A_, _L_ ⟩, we denote its dialectical tree with _T_ (⟨ _A_, _L_ ⟩).\nGiven a literal _L_ and an argument ⟨ _A_, _L_ ⟩, in order to decide whether or not a literal\n_L_ is warranted, every node in the dialectical tree _T_ (⟨ _A_, _L_ ⟩) is recursively marked as\n“D” ( _defeated_ ) or “U” ( _undefeated_ ), obtaining a marked dialectical tree _T_ [∗] (⟨ _A_, _L_ ⟩)\nwhere:\n\n\n- All leaves in _T_ [∗] (⟨ _A_, _L_ ⟩) are marked as “U”s, and\n\n- Let ⟨ _B_, _q_ ⟩ be an inner node of _T_ [∗] (⟨ _A_, _L_ ⟩). Then, ⟨ _B_, _q_ ⟩ will be marked as “U” iff\nevery child of ⟨ _B_, _q_ ⟩ is marked as “D”. Node ⟨ _B_, _q_ ⟩ will be marked as “D” iff it\nhas at least a child marked as “U”.\n\n\nGiven argument ⟨ _A_, _L_ ⟩ over _ΠAM_, if the root of _T_ [∗] (⟨ _A_, _L_ ⟩) is marked “U”, then\n_T_ [∗] (⟨ _A_, _h_ ⟩) _warrants L_ and that _L_ is _warranted_ from _ΠAM_ . (Warranted arguments\ncorrespond to those in the grounded extension of a Dung argumentation system\n(Dung 1995)).\nWe can then extend the idea of a dialectical tree to a _dialectical forest_ . For a\ngiven literal _L_, a dialectical forest _F_ ( _L_ ) consists of the set of dialectical trees for all\narguments for _L_ . We shall denote a marked dialectical forest, the set of all marked\ndialectical trees for arguments for _L_, as _F_ [∗] ( _L_ ). Hence, for a literal _L_, we say it is\n_warranted_ if there is at least one argument for that literal in the dialectical forest\n\n\n164 P. Shakarian et al.\n\n\n**Fig. 8.6** Example annotation function\n\n\n_F_ [∗] ( _L_ ) that is labeled “U”, _not warranted_ if there is at least one argument for literal\n¬ _L_ in the forest _F_ [∗] (¬ _L_ ) that is labeled “U”, and _undecided_ otherwise.\n\n\n**8.3** **The InCA Framework**\n\n\nHaving defined our environmental and analytical models ( _ΠEM_, _ΠAM_ respectively),\nwe now define how the two relate, which allows us to complete the definition of our\nInCA framework.\nThe key intuition here is that given a _ΠAM_, every element of _Ω_ ∪ _ΘΔ_ ∪ _Φ_\nmight only hold in certain worlds in the set _WEM_ —that is, worlds specified by the\nenvironment model. As formulas over the environmental atoms in set **G** _EM_ specify\nsubsets of _WEM_ (i.e., the worlds that satisfy them), we can use these formulas to\nidentify the conditions under which a component of _Ω_ ∪ _ΘΔ_ ∪ _Φ can be_ true.\nRecall that we use the notation _formulaEM_ to denote the set of all possible formulas\nover **G** _EM_ . Therefore, it makes sense to associate elements of _Ω_ ∪ _Θ_ ∪ _Φ_ with a\nformula from _formulaEM_ . In doing so, we can in turn compute the probabilities\nof subsets of _Ω_ ∪ _ΘΔ_ ∪ _Φ_ using the information contained in _ΠEM_, which we\nshall describe shortly. We first introduce the notion of _annotation function_, which\nassociates elements of _Ω_ ∪ _Θ_ ∪ _Φ_ with elements of _formulaEM_ .\nWe also note that, by using the annotation function (see Fig. 8.6), we may have\ncertain statements that appear as both facts and presumptions (likewise for strict and\ndefeasible rules). However, these constructs would have different annotations, and\nthus be applicable in different worlds. Suppose we added the following presumptions\nto our running example:\n\n\n_φ_ 3 = _evidOf_ ( _X_, _O_ ) **–** ≺, and\n_φ_ 4 = _motiv_ ( _X_, _X_ [′] ) **–** ≺.\n\n\nNote that these presumptions are constructed using the same formulas as facts\n_θ_ 1, _θ_ 2.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 165\n\n\nSuppose we extend _af_ as follows:\n\n\n_af_ ( _φ_ 3) = _malwInOp_ ( _M_, _O_ ) ∧ _malwareRel_ ( _M_, _M_ [′] ) ∧ _mwHint_ ( _M_ [′], _X_ )\n\n_af_ ( _φ_ 4) = _inLgConf_ ( _Y_, _X_ [′] ) ∧ _cooper_ ( _X_, _Y_ )\n\n\nSo, for instance, unlike _θ_ 1, _φ_ 3 can potentially be true in any world of the form:\n\n\n{ _malwInOp_ ( _M_, _O_ ), _malwareRel_ ( _M_, _M_ [′] ), _mwHint_ ( _M_ [′], _X_ )}\n\n\nwhile _θ_ 1 cannot be considered in any those worlds.\nWith the annotation function, we now have all the components to formally define\nan InCA framework.\n\n\n**Definition 5 (InCA Framework)** Given environmental model _ΠEM_, analytical\nmodel _ΠAM_, and annotation function _af_, _I_ = ( _ΠEM_, _ΠAM_, _af_ ) is an **InCA framework** .\nGiventhesetupdescribedabove, weconsidera _world-based_ approach—thedefeat\nrelationship among arguments will depend on the current state of the world (based\non the EM). Hence, we now define the status of an argument with respect to a given\nworld.\n\n\n**Definition 6 (Validity)** Given InCA framework _I_ = ( _ΠEM_, _ΠAM_, _af_ ), argument\n⟨ _A_, _L_ ⟩ is valid w.r.t. world _w_ ∈ _WEM_ iff ∀ _c_ ∈ _A_, _w_ |= _af_ ( _c_ ).\nIn other words, an argument is valid with respect to _w_ if the rules, facts, and\npresumptions in that argument are present in _w_ —the argument can then be built\nfrom information that is available in that world. In this paper, we extend the notion\nof validity to argumentation lines, dialectical trees, and dialectical forests in the\nexpected way (an argumentation line is valid w.r.t. _w_ iff all arguments that comprise\nthat line are valid w.r.t. _w_ ).\n\n\n_Example 8_ Consider worlds _w_ 1, _. . ._, _w_ 8 from Example 4 along with the argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ from Example 5. This argument is valid in worlds\n_w_ 1— _w_ 4, _w_ 6, and _w_ 7. We now extend the idea of a dialectical tree w.r.t. worlds—so, for a given world\n_w_ ∈ _WEM_, the dialectical (resp., marked dialectical) tree induced by _w_ is denoted\nby _Tw_ ⟨ _A_, _L_ ⟩ (resp., _Tw_ [∗][⟨] _[A]_ [,] _[ L]_ [⟩][). We require that all arguments and defeaters in these]\ntrees to be valid with respect to _w_ . Likewise, we extend the notion of dialectical\nforests in the same manner (denoted with _Fw_ ( _L_ ) and _Fw_ [∗][(] _[L]_ [), respectively). Based on]\nthese concepts we introduce the notion of _warranting scenario_ .\n\n\n**Definition 7 (Warranting Scenario)** Let _I_ = ( _ΠEM_, _ΠAM_, _af_ ) be an InCA framework and _L_ be a ground literal over **G** _AM_ ; a world _w_ ∈ _WEM_ is said to be a _warranting_\n_scenario_ for _L_ (denoted _w_ ⊢war _L_ ) iff there is a dialectical forest _Fw_ [∗][(] _[L]_ [) in which] _[ L]_\nis warranted and _Fw_ [∗][(] _[L]_ [) is valid w.r.t] _[ w]_ [.]\n\n\n_Example 9_ Following from Example 8, argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ is\nwarranted in worlds _w_ 3, _w_ 6, and _w_ 7. \n\n166 P. Shakarian et al.\n\n\nHence, the set of worlds in the EM where a literal _L_ in the AM _must_ be true is\nexactly the set of warranting scenarios—these are the “necessary” worlds, denoted:\n\n\n_nec_ ( _L_ ) = { _w_ ∈ _WEM_ | ( _w_ ⊢war _L_ )} _._\n\n\nNow, the set of worlds in the EM where AM literal _L can_ be true is the following—\nthese are the “possible” worlds, denoted:\n\n\n_poss_ ( _L_ ) = { _w_ ∈ _WEM_ | _w_ ̸⊢war ¬ _L_ } _._\n\n\nThe following example illustrates these concepts.\n\n\n_Example 10_ Following from Example 8:\n\n\n_nec_ ( _isCap_ ( _baja_, _worm123_ )) = { _w_ 3, _w_ 6, _w_ 7} and\n\n\n_poss_ ( _isCap_ ( _baja_, _worm123_ )) = { _w_ 1, _w_ 2, _w_ 3, _w_ 4, _w_ 6, _w_ 7} _._     \n\nHence, for a given InCA framework _I_, if we are given a probability distribution\n_Pr_ over the worlds in the EM, then we can compute an upper and lower bound on\nthe probability of literal _L_ (denoted **P** _L_, _Pr_, _I_ ) as follows:\n\n\n      _ℓL_, _Pr_, _I_ = _Pr_ ( _w_ ),\n\n_w_ ∈ _nec_ ( _L_ )\n\n\n\nand\n\n\n\n\n  _uL_, _Pr_, _I_ = _Pr_ ( _w_ ),\n\n_w_ ∈ _poss_ ( _L_ )\n\n\n_ℓL_, _Pr_, _I_ ≤ **P** _L_, _Pr_, _I_ ≤ _uL_, _Pr_, _I._\n\n\n\nNow let us consider the computation of probability bounds on a literal when we\nare given a knowledge base _ΠEM_ in the environmental model, which is specified in\n_I_, instead of a probability distribution over all worlds. For a given world _w_ ∈ _WEM_,\nlet _f or_ ( _w_ ) = _(_ [�] _a_ ∈ _w_ _[a)]_ [ ∧] _[(]_ [ �] _a /_ ∈ _w_ [¬] _[a)]_ [—that is, a formula that is satisfied only by]\nworld _w_ . Now we can determine the upper and lower bounds on the probability of a\nliteral w.r.t. _ΠEM_ (denoted **P** _L_, _I_ ) as follows:\n\n\n\n_ℓL_, _I_ = EP-LP-MIN\n\n\n_uL_, _I_ = EP-LP-MAX\n\n\n\n⎛ ⎞\n\n  ⎝ _ΠEM_, _f or_ ( _w_ )⎠,\n\n_w_ ∈ _nec_ ( _L_ )\n\n\n⎛ ⎞\n\n\n\n\n  ⎝ _ΠEM_, _f or_ ( _w_ )⎠,\n\n_w_ ∈ _poss_ ( _L_ )\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 167\n\n\nand\n\n\n_ℓL_, _I_ ≤ **P** _L_, _I_ ≤ _uL_, _I._\n\n\nHence, we have:\n\n\n\n\n   **P** _L_, _I_ = _ℓL_, _I_ + _[u][L]_ [,] _[I]_ [−] _[ℓ][L]_ [,] _[I]_\n\n2\n\n\n\n\n\n[−] _[ℓ][L]_ [,] _[I]_\n± _[u][L]_ [,] _[I]_ _._\n\n2\n\n\n\n_Example 11_ Following from Example 8, argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩,\nwe can compute **P** _isCap_ ( _baja_, _worm123_ ), _I_ (where _I_ = ( _ΠEM_ [′] [,] _[ Π][AM]_ [,] _[ af]_ [)). Note that for]\nthe upper bound, the linear program we need to set up is as in Example 4. For the\nlower bound, the objective function changes to: min _x_ 3 + _x_ 6 + _x_ 7. From these linear\nconstraints, we obtain: **P** _isCap_ ( _baja_, _worm123_ ), _I_ = 0 _._ 75 ± 0 _._ 25 _._ \n\n**8.4** **Attribution Queries**\n\n\nWe now have the necessary elements required to formally define the kind of queries\nthat correspond to the attribution problems studied in this paper.\n\n\n**Definition 8** Let _I_ = ( _ΠEM_, _ΠAM_, _af_ ) be an InCA framework, _S_ ⊆ **C** _act_ (the set of\n“suspects”), _O_ ∈ **C** _ops_ (the “operation”), and _E_ ⊆ **G** _EM_ (the “evidence”). An actor\nA ∈ _S_ is said to be a _most probable suspect_ iff there does not exist A [′] ∈ _S_ such that\n**P** _condOp_ ( **A** [′], _O_ ), _I_ ′ _>_ **P** _condOp_ ( _A_, _O_ ) _I_ ′ where _I_ [′] = ( _ΠEM_ ∪ _ΠE_, _ΠAM_, _af_ [′] ) with _ΠE_ defined\nas [�] _c_ ∈ _E_ [{] _[c]_ [ : 1][ ±][ 0][}][.]\nGiven the above definition, we refer to _Q_ = ( _I_, _S_, _O_, _E_ ) as an _attribution query_,\nand A as an _answer_ to _Q_ . We note that in the above definition, the items of evidence\nare added to the environmental model with a probability of 1 ± 0. While in general\nthis may be the case, there are often instances in analysis of a cyber-operation where\nthe evidence may be true with some degree of uncertainty. Allowing for probabilistic\nevidence is a simple extension to Definition 8 that does not cause any changes to the\nresults of this paper.\nTo understand how uncertain evidence can be present in a cyber-security scenario,\nconsiderthefollowing. InSymantec’sinitialanalysisoftheStuxnetworm, theyfound\nthe routine designed to attack the S7-417 logic controller was incomplete, and hence\nwould not function (Falliere et al. 2011). However, industrial control system expert\nRalph Langner claimed that the incomplete code would run provided a missing\ndata block is generated, which he thought was possible (Langner et al. 2011). In\nthis case, though the code was incomplete, there was clearly uncertainty regarding\nits usability. This situation provides a real-world example of the need to compare\narguments—in this case, in the worlds where both arguments are valid, Langner’s\nargument would likely defeat Symantec’s by generalized specificity (the outcome,\nof course, will depend on the exact formalization of the two). Note that Langner was\n\n\n168 P. Shakarian et al.\n\n\nlater vindicated by the discovery of an older sample, Stuxnet 0.5, which generated\nthe data block. [1]\n\nInCA also allows for a variety of relevant scenarios to the attribution problem.\nFor instance, we can easily allow for the modeling of non-state actors by extending\nthe available constants—for example, traditional groups such as Hezbollah, which\nhas previously wielded its cyber-warfare capabilities in operations against Israel\n(Shakarian et al. 2013). Likewise, the InCA can also be used to model cooperation\namong different actors in performing an attack, including the relationship between\nnon-state actors and nation-states, such as the potential connection between Iran and\nmilitants stealing UAV feeds in Iraq, or the much-hypothesized relationship between\nhacktivist youth groups and the Russian government (Shakarian et al. 2013).Another\naspect that can be modeled is deception where, for instance, an actor may leave false\nclues in a piece of malware to lead an analyst to believe a third party conducted\nthe operation. Such a deception scenario can be easily created by adding additional\nrules in the AM that allow for the creation of such counter-arguments. Another type\nof deception that could occur include attacks being launched from a system not in\nthe responsible party’s area, but under their control (e.g., see Shadows in the Cloud\n2010). Again, modeling who controls a given system can be easily accomplished in\nour framework, and doing so would simply entail extending an argumentation line.\nFurther, campaigns of cyber-operations can also be modeled, as well as relationships\namong malware and/or attacks (as detailed in APT1 2013).\nAs with all of these abilities, InCA provides the analyst the means to model a\ncomplex situation in cyber-warfare but saves him from carrying out the reasoning\nassociated with such a situation. Additionally, InCA results are constructive, so an\nanalyst can “trace-back” results to better understand how the system arrived at a\ngiven conclusion.\n\n\n**8.5** **Open Questions**\n\n\nIn this section we review some major areas of research to address to move InCA\ntoward a deployed system.\n\n\n_**8.5.1**_ _**Rule Learning**_\n\n\nThe InCA framework depends on logical rules and statements as part of the input,\nthough there are existing bodies of work we can leverage (decision tree rule learning,\ninductive logic programming, etc.) there are some specific challenges with regard to\nInCA that we must account for, specifically:\n\n\n1 http://www.symantec.com/connect/blogs/stuxnet-05-disrupting-uranium-processing-natanz.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 169\n\n\n- Quickly learning probabilistic rules from data received as an input stream\n\n- Learning of the annotation function\n\n- Identification of the diagnosticity of new additions to the knowledgebase\n\n- Learning rules that combine multiple, disparate sources (i.e. malware analysis\nand PCAP files, for instance)\n\n\n_**8.5.2**_ _**Belief Revision**_\n\n\nEven though we allow for inconsistencies in the AM portion of the model, inconsistency can arise even with a consistent EM. In a companion paper, (Shakarian et al.\n2014) we introduce the following notion of consistency.\n\n\n**Definition 9** InCA program _I_ = ( _ΠEM_, _ΠAM_, _af_ ), with _ΠAM_ = ⟨ _Θ_, _Ω_, _Φ_, _Δ_ ⟩, is\n_Type II consistent_ iff: given any probability distribution _Pr_ that satisfies _ΠEM_, if\nthere exists a world _w_ ∈ _WEM_ such that [�] _x_ ∈ _Θ_ ∪ _Ω_ | _w_ |= _af_ ( _x_ ) [{] _[x]_ [}][ is inconsistent, then we]\nhave _Pr_ ( _w_ ) = 0.\nThus, any EM world in which the set of associated facts and strict rules are\ninconsistent (we refer to this as “classical consistency”) must always be assigned a\nzero probability. The intuition is as follows: any subset of facts and strict rules are\nthought to be true under certain circumstances—these circumstances are determined\nthrough the annotation function and can be expressed as sets of EM worlds. Suppose\nthere is a world where two contradictory facts can both be considered to be true (based\non the annotation function). If this occurs, then there must not exist a probability\ndistribution that satisfies the program _ΠEM_ that assigns such a world a non-zero\nprobability, as this world leads to an inconsistency.\nWhile we have studied this theoretically (Shakarian et al. 2014), several important\nchallenges remain: How do different belief revision methods affect the results of\nattribution queries? In particular, can we develop tractable algorithms for belief\nrevision in the InCA framework? Further, finding efficient methods for re-computing\nattribution queries following a belief revision operation is a related concern for future\nwork.\n\n\n_**8.5.3**_ _**Temporal Reasoning**_\n\n\nCyber-security data often has an inherent temporal component (in particular, PCAP\nfiles, system logs, and traditional intelligence). One way to represent this type of\ninformation in InCA is by replacing the EM with a probabilistic temporal logic (i.e.\nHansson and Jonsson 1994; Dekhtyar et al. 1999; Shakarian et al. 2011; Shakarian\nand Simari 2012). However, even though this would be a relatively straightforward\nadjustment to the framework, it leads to several interesting questions, specifically:\n\n\n170 P. Shakarian et al.\n\n\n- Can we identify hacking groups responsible for a series of incidents over a period\nof time (a cyber campaign)?\n\n- Can we identify the group responsible for a campaign if it is not known a priori?\n\n- Can we differentiate between multiple campaigns conducted by multiple culprits\nin time-series data?\n\n\n_**8.5.4**_ _**Abductive Inference Queries**_\n\n\nWe may often have a case where more than one culprit is attributed to the same\ncyber-attack with nearly the same probabilities. In this case, can we identify certain\nevidence that, if found, can lead us to better differentiate among the potential culprits?\nIn the intelligence community, this is often referred as identifying _intelligence gaps_ .\nWe can also frame this as an abductive inference problem (Reggia and Peng 1990).\nThis type of problems leads to several interesting challenges:\n\n\n- Can we identify all pieces of diagnostic evidence that would satisfy an important\nintelligence gap?\n\n- Can we identify diagnostic evidence under constraints (i.e., taking into account\nlimitations on the type of evidence that can be collected)?\n\n- In the case where a culprit is attributed with a high probability, can we identify\nevidence that can falsify the finding?\n\n\n**8.6** **Conclusions**\n\n\nIn this paper we introduced InCA, a new framework that allows the modeling of various cyber-warfare/cyber-security scenarios in order to help answer the attribution\nquestion by means of a combination of probabilistic modeling and argumentative\nreasoning. This is the first framework, to our knowledge, that addresses the attribution problem while allowing for multiple pieces of evidence from different sources,\nincluding traditional (non-cyber) forms of intelligence such as human intelligence.\nFurther, our framework is the first to extend Defeasible Logic Programming with\nprobabilistic information. Currently, we are implementing InCA along with the\nassociated algorithms and heuristics to answer these queries.\n\n\n**Acknowledgments** This work was supported by UK EPSRC grant EP/J008346/1—“PrOQAW”,\nERC grant 246858—“DIADEM”, by NSF grant #1117761, by the National Security Agency under\nthe Science of Security Lablet grant (SoSL), Army Research Office project 2GDATXR042, and\nDARPA project R.0004972.001.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 171\n\n\n**References**\n\n\nShadows in the Cloud: Investigating Cyber Espionage 2.0. Tech. rep., Information Warfare Monitor\nand Shadowserver Foundation (2010)\nAPT1: Exposing one of China’s cyber espionage units. Mandiant (tech. report) (2013)\nAltheide, C.: Digital Forensics with Open Source Tools. Syngress (2011)\nDekhtyar,A., Dekhtyar, M.I., Subrahmanian, V.S.: Temporal probabilistic logic programs. In: ICLP\n1999, pp. 109–123. The MIT Press, Cambridge, MA, USA (1999)\nDung, P.M.: On the acceptability of arguments and its fundamental role in nonmonotonic reasoning,\nlogic programming and _n_ -person games. Artif. Intell. **77**, pp. 321–357 (1995)\nFalliere, N., Murchu, L.O., Chien, E.: W32.Stuxnet Dossier Version 1.4. Symantec Corporation\n(2011)\nGarcía, A.J., Simari, G.R.: Defeasible logic programming: An argumentative approach. TPLP\n**4** (1–2), 95–138 (2004)\nHansson, H., Jonsson, B.: A logic for reasoning about time and probability. Formal Aspects of\nComputing **6**, 512–535 (1994)\nHeuer, R.J.: Psychology of Intelligence Analysis. Center for the Study of Intelligence (1999)\nKhuller, S., Martinez, M.V., Nau, D.S., Sliva, A., Simari, G.I., Subrahmanian, V.S.: Computing\nmost probable worlds of action probabilistic logic programs: scalable estimation for 10 [30,000]\n\nworlds. AMAI **51(2–4)**, 295–331 (2007)\nLangner, R.: Matching Langner Stuxnet analysis and Symantic dossier update. Langner Communications GmbH (2011)\nLloyd, J.W.: Foundations of Logic Programming, 2nd Edition. Springer (1987)\nMartinez, M.V., García, A.J., Simari, G.R.: On the use of presumptions in structured defeasible\nreasoning. In: Proc. of COMMA, pp. 185–196 (2012)\nNilsson, N.J.: Probabilistic logic. Artif. Intell. **28** (1), 71–87 (1986)\nRahwan, I., Simari, G.R.: Argumentation in Artificial Intelligence. Springer (2009)\nReggia, J.A., Peng,Y.:Abductive inference models for diagnostic problem-solving. Springer-Verlag\nNew York, Inc., New York, NY, USA (1990)\nShakarian, P., Parker, A., Simari, G.I., Subrahmanian, V.S.: Annotated probabilistic temporal logic.\nTOCL **12** (2), 14 (2011)\nShakarian, P., Simari, G.I., Subrahmanian, V.S.: Annotated probabilistic temporal logic: Approximate fixpoint implementation. ACM Trans. Comput. Log. **13** (2), 13 (2012)\nShakarian, P., Shakarian, J., Ruef, A.: Introduction to Cyber-Warfare: A Multidisciplinary\nApproach. Syngress (2013)\nShakarian, P., Simari, G.I., Falappa, M.A.: Belief revision in structured probabilistic argumentation.\nIn: Proceedings of FoIKS, pp. 324–343 (2014)\nSimari, G.R., Loui, R.P.: A mathematical treatment of defeasible reasoning and its implementation.\nArtif. Intell. **53** (2-3), 125–157 (1992)\nSimari, G.I., Martinez, M.V., Sliva, A., Subrahmanian, V.S.: Focused most probable world\ncomputations in probabilistic logic programs. AMAI **64** (2–3), 113–143 (2012)\nSpitzner, L.: Honeypots: Catching the Insider Threat. In: Proc. ofACSAC 2003, pp. 170–179. IEEE\nComputer Society (2003)\nStolzenburg, F., García, A., Chesñevar, C.I., Simari, G.R.: Computing Generalized Specificity.\nJournal of Non-Classical Logics **13** (1), 87–113 (2003)\nThonnard, O., Mees, W., Dacier, M.: On a multicriteria clustering approach for attack attribution.\nSIGKDD Explorations **12** (1), 11–20 (2010)",
  "html": "## **Chapter 8**\n# **Cyber Attribution: An Argumentation-Based** **Approach**\n\n**Paulo Shakarian, Gerardo I. Simari, Geoffrey Moores and Simon Parsons**\n\n\n**Abstract** Attributing a cyber-operation through the use of multiple pieces of\ntechnical evidence (i.e., malware reverse-engineering and source tracking) and conventional intelligence sources (i.e., human or signals intelligence) is a difficult\nproblem not only due to the effort required to obtain evidence, but the ease with\nwhich an adversary can plant false evidence. In this paper, we introduce a formal reasoning system called the InCA (Intelligent Cyber Attribution) framework\nthat is designed to aid an analyst in the attribution of a cyber-operation even when\nthe available information is conflicting and/or uncertain. Our approach combines\nargumentation-based reasoning, logic programming, and probabilistic models to not\nonly attribute an operation but also explain to the analyst why the system reaches its\nconclusions.\n\n\n**8.1** **Introduction**\n\n\nAn important issue in cyber-warfare is the puzzle of determining who was responsible\nfor a given cyber-operation—be it an incident of attack, reconnaissance, or information theft. This is known as the “attribution problem” (Shakarian et al. 2013).\nThe difficulty of this problem stems not only from the amount of effort required to\nfind forensic clues but also the ease with which an attacker can plant false clues to\n\n\nP. Shakarian (�)\nArizona Sate University, Tempe, AZ, USA\ne-mail: shak@asu.edu\n\n\nG. I. Simari\nDepartment of Computer Science and Engineering, Universidad Nacional del Sur,\nBahía Blanca, Argentina\ne-mail: gis@cs.uns.edu.ar\n\n\nG. Moores\nDepartment of Electrical Engineering and Computer Science, U.S. Military Academy,\nWest Point, NY, USA\ne-mail: geoffrey.moores@usma.edu\n\n\nS. Parsons\nDepartment of Computer Science, University of Liverpool, Liverpool, UK\ne-mail: s.d.parsons@liverpool.ac.uk\n\n© Springer International Publishing Switzerland 2015 151\nS. Jajodia et al. (eds.), _Cyber Warfare,_ Advances in Information Security 56,\nDOI 10.1007/978-3-319-14039-1_8\n\n\n152 P. Shakarian et al.\n\n\nmislead security personnel. Further, while techniques such as forensics and reverseengineering (Altheide 2011), source tracking (Thonnard et al. 2010), honeypots\n(Spitzner 2003), and sinkholing 2010 are commonly employed to find evidence that\ncan lead to attribution, it is unclear how this evidence is to be combined and reasoned\nabout. In a military setting, such evidence is augmented with normal intelligence collection, such as human intelligence (HUMINT), signals intelligence (SIGINT) and\nother means—this adds additional complications to the task of attributing a given\noperation. Essentially, cyber-attribution is a highly-technical intelligence analysis\nproblem where an analyst must consider a variety of sources, each with its associated level of confidence, to provide a decision maker (e.g., a military commander)\ninsight into who conducted a given operation.\nAs it is well known that people’s ability to conduct intelligence analysis is limited\n(Heuer 1999), and due to the highly technical nature of many cyber evidencegathering techniques, an automated reasoning system would be best suited for the\ntask. Such a system must be able to accomplish several goals, among which we\ndistinguish the following main capabilities:\n\n\n1. Reason about evidence in a formal, principled manner, i.e., relying on strong\nmathematical foundations.\n2. Consider evidence for cyber attribution associated with some level of probabilistic\nuncertainty.\n3. Consider logical rules that allow for the system to draw conclusions based on\ncertain pieces of evidence and iteratively apply such rules.\n4. Consider pieces of information that may not be compatible with each other, decide\nwhich information is most relevant, and express why.\n5. Attribute a given cyber-operation based on the above-described features and provide the analyst with the ability to understand how the system arrived at that\nconclusion.\n\n\nIn this paper we present the InCA (Intelligent Cyber Attribution) framework, which\nmeets all of the above qualities. Our approach relies on several techniques from\nthe artificial intelligence community, including argumentation, logic programming,\nand probabilistic reasoning. We first outline the underlying mathematical framework and provide a running example based on real-world cases of cyber-attribution\n(cf. Sect. 8.2); then, in Sects. 8.3 and 8.4, we formally present InCA and attribution\nqueries, respectively. Finally, we discuss conclusions and future work in Sect. 8.6.\n\n\n**8.2** **Two Kinds of Models**\n\n\nOur approach relies on _two separate models of the world_ . The first, called the _**environ-**_\n_**mentalmodel**_ (EM)isusedtodescribethebackgroundknowledgeandisprobabilistic\nin nature. The second one, called the _**analytical model**_ (AM) is used to analyze\ncompeting hypotheses that can account for a given phenomenon (in this case, a\ncyber-operation). The EM _must be consistent_ —this simply means that there must\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 153\n\n\nEM AM\n“Malware X was compiled on a system “Malware X was compiled on a system\nusing the English language.” English-speaking country Y.”\n“Malware W and malware X were created “Malware W and malware X are related.”\nin a similar coding sytle.”\n“Country Y and country Z are currently “Country Y has a motive to launch a\nat war.” cybre-attack against country Z.”\n“Country Y has a significant investment “Country Y has the capability to conduct\nin math-science-engineering (MSE) education.” a cyber-attack.”\n\n\n**Fig. 8.1** Example observations—EM vs. AM\n\n\nexist a probability distribution over the possible states of the world that satisfies all\nof the constraints in the model, as well as the axioms of probability theory. On the\ncontrary, the AM will allow for contradictory information as the system must have\nthe capability to reason about competing explanations for a given cyber-operation.\nIn general, the EM contains knowledge such as evidence, intelligence reporting, or\nknowledge about actors, software, and systems. TheAM, on the other hand, contains\nideas the analyst concludes based on the information in the EM. Figure 8.1 gives\nsome examples of the types of information in the two models. Note that an analyst\n(or automated system) could assign a probability to statements in the EM column\nwhereas statements in the AM column can be true or false depending on a certain\ncombination (or several possible combinations) of statements from the EM. We now\nformally describe these two models as well as a technique for _annotating_ knowledge\nin the AM with information from the EM—these annotations specify the conditions\nunder which the various statements in the AM can potentially be true.\nBefore describing the two models in detail, we first introduce the language used\nto describe them. Variable and constant symbols represent items such as computer\nsystems, types of cyber operations, actors (e.g., nation states, hacking groups), and\nother technical and/or intelligence information. The set of all variable symbols is\ndenoted with **V**, and the set of all constants is denoted with **C** . For our framework, we\nshallrequiretwosubsetsof **C**, **C** _act_ and **C** _ops_, thatspecifytheactorsthatcouldconduct\ncyber-operations and the operations themselves, respectively. In the examples in this\npaper, we will use capital letters to represent variables (e.g., _X_, _Y_, _Z_ ). The constants\nin **C** _act_ and **C** _ops_ that we use in the running example are specified in the following\nexample.\n\n\n_Example 1_ The following (fictitious) actors and cyber-operations will be used in\nour examples:\n\n\n**C** _act_ = { _baja_, _krasnovia_, _mojave_ } (8.1)\n\n**C** _ops_ = { _worm123_ } (8.2)\n\n\n                     The next component in the model is a set of predicate symbols. These constructs\ncan accept zero or more variables or constants as arguments, and map to either\n\n\n154 P. Shakarian et al.\n\n\n**P** _EM_ : _origIP_ ( _M,_ _X_ ) Malware _M_ originated from an IP address belonging to actor _X_ .\n_malwInOp_ ( _M,_ _O_ ) Malware _M_ was used in cyber-operation _O_ .\n_mwHint_ ( _M,_ _X_ ) Malware _M_ contained a hint that it was created by actor _X_ .\n_compilLang_ ( _M,C_ ) Malware _M_ was compiled in a system that used language _C_ .\n_nativLang_ ( _X,C_ ) Language _C_ is the native language of actor _X_ .\n_inLgConf_ ( _X,_ _X_ _[′]_ ) Actors _X_ and _X_ _[′]_ are in a larger conflict with each other.\n_mseTT_ ( _X,_ _N_ ) There are at least _N_ number of top-tier math-science-engineering\nuniversities in country _X_ .\n_infGovSys_ ( _X,_ _M_ ) Systems belonging to actor _X_ were infected with malware _M_ .\n_cybCapAge_ ( _X,_ _N_ ) Actor _X_ has had a cyber-warfare capability for _N_ years or less.\n_govCybLab_ ( _X_ ) Actor _X_ has a government cyber-security lab.\n\n\n**P** _AM_ : _condOp_ ( _X,_ _O_ ) Actor _X_ conducted cyber-operation _O_ .\n_evidOf_ ( _X,_ _O_ ) There is evidence that actor _X_ conducted cyber-operation _O_ .\n_motiv_ ( _X,_ _X_ _[′]_ ) Actor _X_ had a motive to launch a cyber-attack against actor _X_ _[′]_ .\n_isCap_ ( _X,_ _O_ ) Actor _X_ is capable of conducting cyber-operation _O_ .\n_tgt_ ( _X,_ _O_ ) Actor _X_ was the target of cyber-operation _O_ .\n_hasMseInvest_ ( _X_ ) Actor _X_ has a significant investment in math-science-engineering\neducation.\n_expCw_ ( _X_ ) Actor _X_ has experience in conducting cyber-operations.\n\n\n**Fig. 8.2** Predicate definitions for the environment and analytical models in the running example\n\n\n_true_ or false. _Note that the EM and AM use separate sets of predicate symbols_ however, they can share variables and constants. The sets of predicates for the\nEM and AM are denoted with **P** _EM_, **P** _AM_, respectively. In InCA, we require **P** _AM_\nto include the binary predicate _condOp_ ( _X_, _Y_ ), where _X_ is an actor and _Y_ is a cyberoperation. Intuitively, this means that actor _X_ conducted operation _Y_ . For instance,\n_condOp_ ( _baja_, _worm123_ )istrueif _baja_ wasresponsibleforcyber-operation _worm123_ .\nA sample set of predicate symbols for the analysis of a cyber attack between two\nstates over contention of a particular industry is shown in Fig. 8.2; these will be used\nin examples throughout the paper.\nA construct formed with a predicate and constants as arguments is known as a\n_ground atom_ (we shall often deal with ground atoms). The sets of all ground atoms\nfor EM and AM are denoted with **G** _EM_ and **G** _AM_, respectively.\n\n\n_Example 2_ The following are examples of ground atoms over the predicates given\nin Fig. 8.2.\n\n\n**G** _EM_ : _origIP_ ( _mw123sam1_, _krasnovia_ ),\n\n\n_mwHint_ ( _mw123sam1_, _krasnovia_ ),\n\n\n_inLgConf_ ( _krasnovia_, _baja_ ),\n\n\n_mseTT_ ( _krasnovia_, 2) _._\n\n\n**G** _AM_ : _evidOf_ ( _mojave_, _worm123_ ),\n\n\n_motiv_ ( _baja_, _krasnovia_ ),\n\n\n_expCw_ ( _baja_ ),\n\n\n_tgt_ ( _krasnovia_, _worm123_ ) _._                \n\n8 Cyber Attribution: An Argumentation-Based Approach 155\n\n\nFor a given set of ground atoms, a _world_ is a subset of the atoms that are considered\nto be true (ground atoms not in the world are false). Hence, there are 2 [|] **[G]** _[EM]_ [|] possible worlds in the EM and 2 [|] **[G]** _[AM]_ [|] worlds in the AM, denoted with _WEM_ and _WAM_,\nrespectively.\nClearly, even a moderate number of ground atoms can yield an enormous number\nof worlds to explore. One way to reduce the number of worlds is to include _integrity_\n_constraints_, which allow us to eliminate certain worlds from consideration—they\nsimplyarenotpossibleinthesettingbeingmodeled. Ourprincipleintegrityconstraint\nwill be of the form:\n\n\noneOf( _A_ [′] )\n\n\nwhere _A_ [′] is a subset of ground atoms. Intuitively, this says that any world where\nmore than one of the atoms from set _A_ [′] appear is invalid. Let **IC** _EM_ and **IC** _AM_ be the\nsets of integrity constraints for the EM and AM, respectively, and the sets of worlds\nthat conform to these constraints be _WEM_ ( **IC** _EM_ ), _WAM_ ( **IC** _AM_ ), respectively.\nAtoms can also be combined into formulas using standard logical connectives:\nconjunction ( _and_ ), disjunction ( _or_ ), and negation ( _not_ ). These are written using the\nsymbols ∧, ∨, ¬, respectively. We say a world ( _w_ ) _satisfies_ a formula ( _f_ ), written\n_w_ |= _f_, based on the following inductive definition:\n\n\n- if _f_ is a single atom, then _w_ |= _f_ iff _f_ ∈ _w_ ;\n\n- if _f_ = ¬ _f_ [′] then _w_ |= _f_ iff _w_ ̸|= _f_ [′] ;\n\n- if _f_ = _f_ [′] ∧ _f_ [′′] then _w_ |= _f_ iff _w_ |= _f_ [′] and _w_ |= _f_ [′′] ; and\n\n- if _f_ = _f_ [′] ∨ _f_ [′′] then _w_ |= _f_ iff _w_ |= _f_ [′] or _w_ |= _f_ [′′] .\n\n\nWe use the notation _f ormulaEM_, _f ormulaAM_ to denote the set of all possible\n(ground) formulas in the EM andAM, respectively.Also, note that we use the notation\n⊤, ⊥ to represent tautologies (formulas that are true in all worlds) and contradictions\n(formulas that are false in all worlds), respectively.\n\n\n_**8.2.1**_ _**Environmental Model**_\n\n\nIn this section we describe the first of the two models, namely the EM or environmental model. This model is largely based on the probabilistic logic of (Nilsson 1986),\nwhich we now briefly review.\nFirst, we define a _probabilistic formula_ that consists of a formula _f_ over\natoms from **G** _EM_, a real number _p_ in the interval [0, 1], and an error tolerance\n_ε_ ∈ [0, min( _p_, 1 − _p_ )]. A probabilistic formula is written as: _f_ : _p_ ± _ε_ . Intuitively,\nthis statement is interpreted as “formula _f_ is true with probability between _p_ - _ε_\nand _p_ + _ε_ ”—note that we make no statement about the probability distribution over\nthis interval. The uncertainty regarding the probability values stems from the fact\nthat certain assumptions (such as probabilistic independence) may not be suitable in\nthe environment being modeled.\n\n\n156 P. Shakarian et al.\n\n\n_Example 3_ To continue our running example, consider the following set _ΠEM_ :\n\n\n_f_ 1 = _govCybLab_ ( _baja_ ) : 0 _._ 8 ± 0 _._ 1\n\n_f_ 2 = _cybCapAge_ ( _baja_, 5) : 0 _._ 2 ± 0 _._ 1\n\n_f_ 3 = _mseTT_ ( _baja_, 2) : 0 _._ 8 ± 0 _._ 1\n\n_f_ 4 = _mwHint_ ( _mw123sam1_, _mojave_ ) ∧ _compilLang_ ( _worm123_, _english_ ) : 0 _._ 7 ± 0 _._ 2\n\n_f_ 5 = _malwInOp_ ( _mw123sam1_, _worm123_ )\n\n\n∧ _malwareRel_ ( _mw123sam1_, _mw123sam2_ )\n\n\n∧ _mwHint_ ( _mw123sam2_, _mojave_ ) : 0 _._ 6 ± 0 _._ 1\n\n\n_f_ 6 = _inLgConf_ ( _baja_, _krasnovia_ ) ∨¬ _cooper_ ( _baja_, _krasnovia_ ) : 0 _._ 9 ± 0 _._ 1\n\n_f_ 7 = _origIP_ ( _mw123sam1_, _baja_ ) : 1 ± 0\n\n\nThroughout other examples in the rest of the paper, we will make use of the subset\n_ΠEM_ [′] [= {] _[f]_ [1][,] _[ f]_ [2][,] _[ f]_ [3][}][.] We now consider a probability distribution _Pr_ over the set _WEM_ ( **IC** _EM_ ). We\nsay that _Pr satisfies_ probabilistic formula _f_ : _p_ ± _ε_ iff the following holds:\n_p_ - _ε_ ≤ [�] _w_ ∈ _WEM_ ( **IC** _EM_ ) _[Pr]_ [(] _[w]_ [)][ ≤] _[p]_ [ +] _[ ε.]_ [ A set] _[ Π][EM]_ [ of probabilistic formulas is]\ncalled a _knowledge base_ . We say that a probability distribution over _WEM_ ( **IC** _EM_ )\n_satisfies ΠEM_ if and only if it satisfies all probabilistic formulas in _ΠEM_ .\nIt is possible to create probabilistic knowledge bases for which there is no\nsatisfying probability distribution. The following is a simple example of this:\n\n\n_condOp_ ( _krasnovia_, _worm123_ ) ∨ _condOp_ ( _baja_, _worm123_ ) : 0 _._ 4 ± 0;\n\n\n_condOp_ ( _krasnovia_, _worm123_ ) ∧ _condOp_ ( _baja_, _worm123_ ) : 0 _._ 6 ± 0 _._ 1 _._\n\n\nFormulas and knowledge bases of this sort are _inconsistent_ . In this paper, we assume\nthat information is properly extracted from a set of historic data and hence consistent;\n(recall that inconsistent information can only be handled in the AM, not the EM). A\nconsistent knowledge base could also be obtained as a result of curation by experts,\nsuch that all inconsistencies were removed—see (Khuller et al. 2007; Shakarian et\nal. 2011) for algorithms for learning rules of this type.\nThe main kind of query that we require for the probabilistic model is the _maximum_\n_entailment_ problem: given a knowledge base _ΠEM_ and a (non-probabilistic) formula\n_q_, identify _p_, _ε_ such that all valid probability distributions _Pr_ that satisfy _ΠEM_ also\nsatisfy _q_ : _p_ ± _ε_, and there does not exist _p_ [′], _ε_ [′] s.t. [ _p_ - _ε_, _p_ + _ε_ ] ⊃ [ _p_ [′] - _ε_ [′], _p_ [′] + _ε_ [′] ],\nwhere all probability distributions _Pr_ that satisfy _ΠEM_ also satisfy _q_ : _p_ [′] ± _ε_ [′] . That\nis, given _q_, can we determine the probability (with maximum tolerance) of statement\n_q_ given the information in _ΠEM_ ? The approach adopted in (Nilsson et al. 1986) to\nsolve this problem works as follows. First, we must solve the linear program defined\nnext.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 157\n\n\n**Definition 1** ( **EM-LP-MIN)** Given a knowledge base _ΠEM_ and a formula _q_ :\n\n\n- create a variable _xi_ for each _wi_ ∈ _WEM_ ( **IC** _EM_ );\n\n- for each _fj_ : _pj_ ± _εj_ ∈ _ΠEM_, create constraint:\n\n      _pj_          - _εj_ ≤ _xi_ ≤ _pj_ + _εj_ ;\n\n_wi_ ∈ _WEM_ ( **IC** _EM_ ) _s.t.wi_ |= _fj_\n\n\n- finally, we also have a constraint:\n\n      \n_xi_ = 1 _._\n_wi_ ∈ _WEM_ ( **IC** _EM_ )\n\n\nThe objective is to minimize the function:\n\n      \n_xi._\n_wi_ ∈ _WEM_ ( **IC** _EM_ ) _s.t.wi_ |= _q_\n\n\nWe use the notation EP-LP-MIN( _ΠEM_, _q_ ) to refer to the value of the objective\nfunction in the solution to the EM-LP-MIN constraints.\nLet _ℓ_ be the result of the process described in Definition 1. The next step is to\nsolve the linear program a second time, but instead maximizing the objective function\n(we shall refer to this as EM-LP-MAX)—let _u_ be the result of this operation. In\n(Nilsson 1986), it is shown that _ε_ = _u_ −2 _ℓ_ and _p_ = _ℓ_ + _ε_ is the solution to the\n\nmaximum entailment problem. We note that although the above linear program has\nan exponential number of variables in the worst case (i.e., no integrity constraints),\nthe presence of constraints has the potential to greatly reduce this space. Further,\nthere are also good heuristics (cf. Khuller et al. 2007; Simari et al. 2012) that have\nbeen shown to provide highly accurate approximations with a reduced-size linear\nprogram.\n\n_Example 4_ Consider KB _ΠEM_ [′] [from Example][ 3][ and a set of ground atoms restricted]\nto those that appear in that program. Hence, we have:\n\n\n_w_ 1 = { _govCybLab_ ( _baja_ ), _cybCapAge_ ( _baja_, 5), _mseTT_ ( _baja_, 2)}\n\n_w_ 2 = { _govCybLab_ ( _baja_ ), _cybCapAge_ ( _baja_, 5)}\n\n_w_ 3 = { _govCybLab_ ( _baja_ ), _mseTT_ ( _baja_, 2)}\n\n_w_ 4 = { _cybCapAge_ ( _baja_, 5), _mseTT_ ( _baja_, 2)}\n\n_w_ 5 = { _cybCapAge_ ( _baja_, 5)}\n\n_w_ 6 = { _govCybLab_ ( _baja_ )}\n\n_w_ 7 = { _mseTT_ ( _baja_, 2)}\n\n_w_ 8 = ∅\n\n\nand suppose we wish to compute the probability for formula:\n\n\n_q_ = _govCybLab_ ( _baja_ ) ∨ _mseTT_ ( _baja_, 2) _._\n\n\n158 P. Shakarian et al.\n\n\nFor each formula in _ΠEM_ we have a constraint, and for each world above we have a\nvariable. An objective function is created based on the worlds that satisfy the query\nformula (here, worlds _w_ 1 − _w_ 4, _w_ 6, _w_ 7). Hence, EP-LP-MIN( _ΠEM_ [′] [,] _[ q]_ [) can be written]\nas follows:\n\n\nmax _x_ 1 + _x_ 2 + _x_ 3 + _x_ 4 + _x_ 6 + _x_ 7 _w.r.t._ :\n\n0 _._ 7 ≤ _x_ 1 + _x_ 2 + _x_ 3 + _x_ 6 ≤ 0 _._ 9\n\n0 _._ 1 ≤ _x_ 1 + _x_ 2 + _x_ 4 + _x_ 5 ≤ 0 _._ 3\n\n0 _._ 8 ≤ _x_ 1 + _x_ 3 + _x_ 4 + _x_ 7 ≤ 1\n\n_x_ 1 + _x_ 2 + _x_ 3 + _x_ 4 + _x_ 5 + _x_ 6 + _x_ 7 + _x_ 8 = 1\n\nWe can now solve EP-LP-MAX( _ΠEM_ [′] [,] _[ q]_ [) and][ EP-LP-MIN][(] _[Π]_ _EM_ [′] [,] _[ q]_ [) to get solution]\n0 _._ 9 ± 0 _._ 1. \n\n_**8.2.2**_ _**Analytical Model**_\n\n\nFor the analytical model (AM), we choose a structured argumentation framework\n(Rahwan et al. 2009) due to several characteristics that make such frameworks highly\napplicable to cyber-warfare domains. Unlike the EM, which describes probabilistic\ninformation about the state of the real world, the AM must allow for competing\nideas—it _must be able to represent contradictory information_ . The algorithmic\napproach allows for the creation of _arguments_ based on the AM that may “compete” with each other to describe who conducted a given cyber-operation. In this\ncompetition—known as a _dialectical process_ —one argument may defeat another\nbased on a _comparison criterion_ that determines the prevailing argument. Resulting\nfrom this process, the InCA framework will determine arguments that are _war-_\n_ranted_ (those that are not _defeated_ by other arguments) thereby providing a suitable\nexplanation for a given cyber-operation.\nThe transparency provided by the system can allow analysts to identify potentially\nincorrect input information and fine-tune the models or, alternatively, collect more\ninformation. In short, argumentation-based reasoning has been studied as a natural\nway to manage a set of inconsistent information—it is the way humans settle disputes. As we will see, another desirable characteristic of (structured) argumentation\nframeworks is that, once a conclusion is reached, we are left with an explanation of\nhow we arrived at it and information about why a given argument is warranted; this\nis very important information for analysts to have. In this section, we recall some\npreliminaries of the underlying argumentation framework used, and then introduce\nthe analytical model (AM).\n\n\n**Defeasible Logic Programming with Presumptions**\n\n\nDeLP with Presumptions (PreDeLP) (Martinez et al. 2012) is a formalism combining Logic Programming with Defeasible Argumentation. We now briefly recall\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 159\n\n\n**Fig. 8.3** A ground argumentation framework\n\n\nthe basics of PreDeLP; we refer the reader to (García and Simari 2004; Martinez\net al. 2012) for the complete presentation. The formalism contains several different\nconstructs: facts, presumptions, strict rules, and defeasible rules. Facts are statements\nabout the analysis that can always be considered to be true, while presumptions are\nstatements that may or may not be true. Strict rules specify logical consequences of\na set of facts or presumptions (similar to an implication, though not the same) that\nmust always occur, while defeasible rules specify logical consequences that may be\nassumed to be true when no contradicting information is present. These constructs\nare used in the construction of _arguments_, and are part of a PreDeLP program, which\nis a set of facts, strict rules, presumptions, and defeasible rules. Formally, we use\nthe notation _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) to denote a PreDeLP program, where _Ω_ is the set\nof strict rules, _Θ_ is the set of facts, _Δ_ is the set of defeasible rules, and _Φ_ is the set\nof presumptions. In Fig. 8.3, we provide an example _ΠAM_ . We now describe each of\nthese constructs in detail.\n\n\n**Facts** ( _Θ_ ) are ground literals representing atomic information or its negation, using\nstrong negation “¬”. Note that all of the literals in our framework must be formed\nwith a predicate from the set **P** _AM_ . Note that information in this form cannot be\ncontradicted.\n\n\n**Strict Rules** ( _Ω_ ) represent non-defeasible cause-and-effect information that resembles a material implication (though the semantics is different since the contrapositive\n\n\n160 P. Shakarian et al.\n\n\ndoes not hold) and are of the form _L_ 0 ←− _L_ 1, _. . ._, _Ln_, where _L_ 0 is a ground literal\nand { _Li_ } _i>_ 0 is a set of ground literals.\n\n\n**Presumptions** ( _Φ_ ) are ground literals of the same form as facts, except that they\nare not taken as being true but rather defeasible, which means that they can be\ncontradicted. Presumptions are denoted in the same manner as facts, except that the\nsymbol –≺ is added. While any literal can be used as a presumption in InCA, we\nspecifically require all literals created with the predicate _condOp_ to be defeasible.\n\n\n**Defeasible Rules** ( _Δ_ ) represent tentative knowledge that can be used if nothing\ncan be posed against it. Just as presumptions are the defeasible counterpart of facts,\ndefeasible rules are the defeasible counterpart of strict rules. They are of the form\n_L_ 0 –≺ _L_ 1, _. . ._, _Ln_, where _L_ 0 is a ground literal and { _Li_ } _i>_ 0 is a set of ground literals.\nNote that with both strict and defeasible rules, _strong negation_ is allowed in the head\nof rules, and hence may be used to represent contradictory knowledge.\nWe note that strict rules and facts are necessary constructs as they may not be true\nin all environmental conditions. We shall discuss this further in the next section with\nthe introduction of an annotation function.\nEven though the above constructs are ground, we allow for schematic versions\nwith variables that are used to represent sets of ground rules. We denote variables\nwith strings starting with an uppercase letter; Fig. 8.4 shows a non-ground example.\nWhen a cyber-operation occurs, InCA must derive arguments as to who could\nhave potentially conducted the action. Informally, an argument for a particular actor\n_x_ conducting cyber-operation _y_ is a consistent subset of the analytical model that\nentails the atom _condOp_ ( _x_, _y_ ). If the argument contains only strict rules and facts,\nthen it is _factual_ . If it contains presumptions or defeasible rules, then it _defeasibly_\n_derives_ that actor _x_ conducted operation _y_ .\nDerivation follows the same mechanism of Logic Programming (Lloyd 1987).\nSince rule heads can contain strong negation, it is possible to defeasibly derive contradictory literals from a program. For the treatment of contradictory knowledge,\nPreDeLP incorporates a defeasible argumentation formalism that allows the identification of the pieces of knowledge that are in conflict, and through the previously\nmentioned _dialectical process_ decides which information prevails as warranted.\nThis dialectical process involves the construction and evaluation of arguments\nthat either support or interfere with a given query, building a _dialectical tree_ in the\nprocess. Formally, we have:\n\n\n**Definition 2 (Argument)** An _argument A_, _L_ ⟩ for a literal _L_ is a pair of the literal\nand a (possibly empty) set of the EM ( _A_ ⊆ _ΠAM_ ) that provides a minimal proof for _L_\nmeeting the requirements: (1) _L_ is defeasibly derived from _A_, (2) _Ω_ ∪ _Θ_ ∪ _A_ is not\ncontradictory, and (3) _A_ is a minimal subset of _Δ_ ∪ _Φ_ satisfying 1 and 2, denoted\n⟨ _A_, _L_ ⟩.\nLiteral _L_ is called the _conclusion_ supported by the argument, and _A_ is the _support_\nof the argument. An argument ⟨ _B_, _L_ ⟩ is a _subargument_ of ⟨ _A_, _L_ [′] ⟩ iff _B_ ⊆ _A_ . An\nargument ⟨ _A_, _L_ ⟩ is _presumptive_ iff _A_ ∩ _Φ_ is not empty. We will also use _Ω_ ( _A_ ) =\n_A_ ∩ _Ω_, _Θ_ ( _A_ ) = _A_ ∩ _Θ_, _Δ_ ( _A_ ) = _A_ ∩ _Δ_, and _Φ_ ( _A_ ) = _A_ ∩ _Φ_ .\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 161\n\n\n**Fig. 8.4** A non-ground argumentation framework\n\n\n**Fig. 8.5** Example ground arguments from Fig. 8.3\n\n\nNote that our definition differs slightly from that of (Simari and Loui 1992) where\nDeLP is introduced, as we include strict rules and facts as part of the argument. The\nreason for this will become clear in Sect. 8.3. Arguments for our scenario are shown\nin the following example.\n\n\n_Example 5_ Figure 8.5 shows example arguments based on the knowledge base from\nFig. 8.3. Note that the following relationship exists:\n\n\n⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ is a sub-argument of\n\n\n⟨ _A_ 2, _condOp_ ( _baja_, _worm123_ )⟩ and\n\n\n⟨ _A_ 3, _condOp_ ( _baja_, _worm123_ )⟩ _._         \n\nGiven argument ⟨ _A_ 1, _L_ 1⟩, counter-arguments are arguments that contradict it.\nArgument ⟨ _A_ 2, _L_ 2⟩ _counterargues_ or _attacks_ ⟨ _A_ 1, _L_ 1⟩ literal _L_ [′] iff there exists a\nsubargument ⟨ _A_, _L_ [′′] ⟩ of ⟨ _A_ 1, _L_ 1⟩ s.t. set _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _Θ_ ( _A_ 1) ∪ _Θ_ ( _A_ 2) ∪\n{ _L_ 2, _L_ [′′] } is contradictory.\n\n\n_Example 6_ Consider the arguments from Example 5. The following are some of\nthe attack relationships between them: _A_ 1, _A_ 2, _A_ 3, and _A_ 4 all attack _A_ 6; _A_ 5 attacks\n_A_ 7; and _A_ 7 attacks _A_ 2. \n\n162 P. Shakarian et al.\n\n\nA _proper defeater_ of an argument ⟨ _A_, _L_ ⟩ is a counter-argument that—by some\ncriterion—is considered to be better than ⟨ _A_, _L_ ⟩; if the two are incomparable according to this criterion, the counterargument is said to be a _blocking_ defeater. An\nimportantcharacteristicofPreDeLPisthattheargumentcomparisoncriterionismodular, and thus the most appropriate criterion for the domain that is being represented\ncan be selected; the default criterion used in classical defeasible logic programming (from which PreDeLP is derived) is _generalized specificity_ (Stolzenburg et al.\n2003), though an extension of this criterion is required for arguments using presumptions (Martinez et al. 2012). We briefly recall this criterion next—the first\ndefinition is for generalized specificity, which is subsequently used in the definition\nof presumption-enabled specificity.\n\n\n**Definition 3** Let _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) be a PreDeLP program and let _F_ be the\nset of all literals that have a defeasible derivation from _ΠAM_ . An argument ⟨ _A_ 1, _L_ 1⟩\nis _preferred to_ ⟨ _A_ 2, _L_ 2⟩, denoted with _A_ 1 ≻ _PS A_ 2 iff the two following conditions\nhold:\n\n\n1. For all _H_ ⊆ _F_, _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ is non-contradictory: if there is a derivation\nfor _L_ 1 from _Ω_ ( _A_ 2) ∪ _Ω_ ( _A_ 1) ∪ _Δ_ ( _A_ 1) ∪ _H_, and there is no derivation for _L_ 1\nfrom _Ω_ ( _A_ 1)∪ _Ω_ ( _A_ 2)∪ _H_, then there is a derivation for _L_ 2 from _Ω_ ( _A_ 1)∪ _Ω_ ( _A_ 2)∪\n_Δ_ ( _A_ 2) ∪ _H_ .\n2. There is at least one set _H_ [′] ⊆ _F_, _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] is non-contradictory,\nsuch that there is a derivation for _L_ 2 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] ∪ _Δ_ ( _A_ 2), there\nis no derivation for _L_ 2 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′], and there is no derivation for\n_L_ 1 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] ∪ _Δ_ ( _A_ 1).\n\n\nIntuitively, the principle of specificity says that, in the presence of two conflicting\nlines of argument about a proposition, the one that uses more of the available information is more convincing.A classic example involves a bird, Tweety, and arguments\nstating that it both flies (because it is a bird) and doesn’t fly (because it is a penguin).\nThe latter argument uses more information about Tweety—it is more specific—and\nis thus the stronger of the two.\n\n\n**Definition 4** (Martinez et al. 2012) Let _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) be a PreDeLP program. An argument ⟨ _A_ 1, _L_ 1⟩ is _preferred to_ ⟨ _A_ 2, _L_ 2⟩, denoted with _A_ 1 ≻ _A_ 2 iff any\nof the following conditions hold:\n\n\n1. ⟨ _A_ 1, _L_ 1⟩ and ⟨ _A_ 2, _L_ 2⟩ are both factual arguments and ⟨ _A_ 1, _L_ 1⟩≻ _PS_ ⟨ _A_ 2, _L_ 2⟩.\n2. ⟨ _A_ 1, _L_ 1⟩ is a factual argument and ⟨ _A_ 2, _L_ 2⟩ is a presumptive argument.\n3. ⟨ _A_ 1, _L_ 1⟩ and ⟨ _A_ 2, _L_ 2⟩ are presumptive arguments, and\na) ¬( _Φ_ ( _A_ 1) ⊆ _Φ_ ( _A_ 2)), or\nb) _Φ_ ( _A_ 1) = _Φ_ ( _A_ 2) and ⟨ _A_ 1, _L_ 1⟩≻ _PS_ ⟨ _A_ 2, _L_ 2⟩.\n\n\nGenerally, if _A_, _B_ are arguments with rules _X_ and _Y_, resp., and _X_ ⊂ _Y_, then _A_ is\nstronger than _B_ . This also holds when _A_ and _B_ use presumptions _P_ 1 and _P_ 2, resp.,\nand _P_ 1 ⊂ _P_ 2.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 163\n\n\n_Example 7_ The following are relationships between arguments from Example 5,\nbased on Definitions 3 and 4:\n\n\n_A_ 1 and _A_ 6 are incomparable (blocking defeaters);\n\n_A_ 6 ≻ _A_ 2, and thus _A_ 6 defeats _A_ 2;\n\n_A_ 6 ≻ _A_ 3, and thus _A_ 6 defeats _A_ 3;\n\n_A_ 6 ≻ _A_ 4, and thus _A_ 6 defeats _A_ 4;\n\n_A_ 5 and _A_ 7 are incomparable (blocking defeaters) _._     \n\nA sequence of arguments called an _argumentation line_ thus arises from this attack relation, where each argument defeats its predecessor. To avoid undesirable\nsequences, that may represent circular or fallacious argumentation lines, in DeLP\nan _argumentation line_ is _acceptable_ if it satisfies certain constraints (see García and\nSimari 2004). A literal _L_ is _warranted_ if there exists a non-defeated argument _A_\nsupporting _L_ .\nClearly, there can be more than one defeater for a particular argument ⟨ _A_, _L_ ⟩.\nTherefore, many acceptable argumentation lines could arise from ⟨ _A_, _L_ ⟩, leading\nto a tree structure. The tree is built from the set of all argumentation lines rooted\nin the initial argument. In a dialectical tree, every node (except the root) represents\na defeater of its parent, and leaves correspond to undefeated arguments. Each path\nfrom the root to a leaf corresponds to a different acceptable argumentation line. A\ndialectical tree provides a structure for considering all the possible acceptable argumentation lines that can be generated for deciding whether an argument is defeated.\nWe call this tree _dialectical_ because it represents an exhaustive dialectical analysis\n(in the sense of providing reasons for and against a position) for the argument in its\nroot. For argument ⟨ _A_, _L_ ⟩, we denote its dialectical tree with _T_ (⟨ _A_, _L_ ⟩).\nGiven a literal _L_ and an argument ⟨ _A_, _L_ ⟩, in order to decide whether or not a literal\n_L_ is warranted, every node in the dialectical tree _T_ (⟨ _A_, _L_ ⟩) is recursively marked as\n“D” ( _defeated_ ) or “U” ( _undefeated_ ), obtaining a marked dialectical tree _T_ [∗] (⟨ _A_, _L_ ⟩)\nwhere:\n\n\n- All leaves in _T_ [∗] (⟨ _A_, _L_ ⟩) are marked as “U”s, and\n\n- Let ⟨ _B_, _q_ ⟩ be an inner node of _T_ [∗] (⟨ _A_, _L_ ⟩). Then, ⟨ _B_, _q_ ⟩ will be marked as “U” iff\nevery child of ⟨ _B_, _q_ ⟩ is marked as “D”. Node ⟨ _B_, _q_ ⟩ will be marked as “D” iff it\nhas at least a child marked as “U”.\n\n\nGiven argument ⟨ _A_, _L_ ⟩ over _ΠAM_, if the root of _T_ [∗] (⟨ _A_, _L_ ⟩) is marked “U”, then\n_T_ [∗] (⟨ _A_, _h_ ⟩) _warrants L_ and that _L_ is _warranted_ from _ΠAM_ . (Warranted arguments\ncorrespond to those in the grounded extension of a Dung argumentation system\n(Dung 1995)).\nWe can then extend the idea of a dialectical tree to a _dialectical forest_ . For a\ngiven literal _L_, a dialectical forest _F_ ( _L_ ) consists of the set of dialectical trees for all\narguments for _L_ . We shall denote a marked dialectical forest, the set of all marked\ndialectical trees for arguments for _L_, as _F_ [∗] ( _L_ ). Hence, for a literal _L_, we say it is\n_warranted_ if there is at least one argument for that literal in the dialectical forest\n\n\n164 P. Shakarian et al.\n\n\n**Fig. 8.6** Example annotation function\n\n\n_F_ [∗] ( _L_ ) that is labeled “U”, _not warranted_ if there is at least one argument for literal\n¬ _L_ in the forest _F_ [∗] (¬ _L_ ) that is labeled “U”, and _undecided_ otherwise.\n\n\n**8.3** **The InCA Framework**\n\n\nHaving defined our environmental and analytical models ( _ΠEM_, _ΠAM_ respectively),\nwe now define how the two relate, which allows us to complete the definition of our\nInCA framework.\nThe key intuition here is that given a _ΠAM_, every element of _Ω_ ∪ _ΘΔ_ ∪ _Φ_\nmight only hold in certain worlds in the set _WEM_ —that is, worlds specified by the\nenvironment model. As formulas over the environmental atoms in set **G** _EM_ specify\nsubsets of _WEM_ (i.e., the worlds that satisfy them), we can use these formulas to\nidentify the conditions under which a component of _Ω_ ∪ _ΘΔ_ ∪ _Φ can be_ true.\nRecall that we use the notation _formulaEM_ to denote the set of all possible formulas\nover **G** _EM_ . Therefore, it makes sense to associate elements of _Ω_ ∪ _Θ_ ∪ _Φ_ with a\nformula from _formulaEM_ . In doing so, we can in turn compute the probabilities\nof subsets of _Ω_ ∪ _ΘΔ_ ∪ _Φ_ using the information contained in _ΠEM_, which we\nshall describe shortly. We first introduce the notion of _annotation function_, which\nassociates elements of _Ω_ ∪ _Θ_ ∪ _Φ_ with elements of _formulaEM_ .\nWe also note that, by using the annotation function (see Fig. 8.6), we may have\ncertain statements that appear as both facts and presumptions (likewise for strict and\ndefeasible rules). However, these constructs would have different annotations, and\nthus be applicable in different worlds. Suppose we added the following presumptions\nto our running example:\n\n\n_φ_ 3 = _evidOf_ ( _X_, _O_ ) **–** ≺, and\n_φ_ 4 = _motiv_ ( _X_, _X_ [′] ) **–** ≺.\n\n\nNote that these presumptions are constructed using the same formulas as facts\n_θ_ 1, _θ_ 2.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 165\n\n\nSuppose we extend _af_ as follows:\n\n\n_af_ ( _φ_ 3) = _malwInOp_ ( _M_, _O_ ) ∧ _malwareRel_ ( _M_, _M_ [′] ) ∧ _mwHint_ ( _M_ [′], _X_ )\n\n_af_ ( _φ_ 4) = _inLgConf_ ( _Y_, _X_ [′] ) ∧ _cooper_ ( _X_, _Y_ )\n\n\nSo, for instance, unlike _θ_ 1, _φ_ 3 can potentially be true in any world of the form:\n\n\n{ _malwInOp_ ( _M_, _O_ ), _malwareRel_ ( _M_, _M_ [′] ), _mwHint_ ( _M_ [′], _X_ )}\n\n\nwhile _θ_ 1 cannot be considered in any those worlds.\nWith the annotation function, we now have all the components to formally define\nan InCA framework.\n\n\n**Definition 5 (InCA Framework)** Given environmental model _ΠEM_, analytical\nmodel _ΠAM_, and annotation function _af_, _I_ = ( _ΠEM_, _ΠAM_, _af_ ) is an **InCA framework** .\nGiventhesetupdescribedabove, weconsidera _world-based_ approach—thedefeat\nrelationship among arguments will depend on the current state of the world (based\non the EM). Hence, we now define the status of an argument with respect to a given\nworld.\n\n\n**Definition 6 (Validity)** Given InCA framework _I_ = ( _ΠEM_, _ΠAM_, _af_ ), argument\n⟨ _A_, _L_ ⟩ is valid w.r.t. world _w_ ∈ _WEM_ iff ∀ _c_ ∈ _A_, _w_ |= _af_ ( _c_ ).\nIn other words, an argument is valid with respect to _w_ if the rules, facts, and\npresumptions in that argument are present in _w_ —the argument can then be built\nfrom information that is available in that world. In this paper, we extend the notion\nof validity to argumentation lines, dialectical trees, and dialectical forests in the\nexpected way (an argumentation line is valid w.r.t. _w_ iff all arguments that comprise\nthat line are valid w.r.t. _w_ ).\n\n\n_Example 8_ Consider worlds _w_ 1, _. . ._, _w_ 8 from Example 4 along with the argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ from Example 5. This argument is valid in worlds\n_w_ 1— _w_ 4, _w_ 6, and _w_ 7. We now extend the idea of a dialectical tree w.r.t. worlds—so, for a given world\n_w_ ∈ _WEM_, the dialectical (resp., marked dialectical) tree induced by _w_ is denoted\nby _Tw_ ⟨ _A_, _L_ ⟩ (resp., _Tw_ [∗][⟨] _[A]_ [,] _[ L]_ [⟩][). We require that all arguments and defeaters in these]\ntrees to be valid with respect to _w_ . Likewise, we extend the notion of dialectical\nforests in the same manner (denoted with _Fw_ ( _L_ ) and _Fw_ [∗][(] _[L]_ [), respectively). Based on]\nthese concepts we introduce the notion of _warranting scenario_ .\n\n\n**Definition 7 (Warranting Scenario)** Let _I_ = ( _ΠEM_, _ΠAM_, _af_ ) be an InCA framework and _L_ be a ground literal over **G** _AM_ ; a world _w_ ∈ _WEM_ is said to be a _warranting_\n_scenario_ for _L_ (denoted _w_ ⊢war _L_ ) iff there is a dialectical forest _Fw_ [∗][(] _[L]_ [) in which] _[ L]_\nis warranted and _Fw_ [∗][(] _[L]_ [) is valid w.r.t] _[ w]_ [.]\n\n\n_Example 9_ Following from Example 8, argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ is\nwarranted in worlds _w_ 3, _w_ 6, and _w_ 7. \n\n166 P. Shakarian et al.\n\n\nHence, the set of worlds in the EM where a literal _L_ in the AM _must_ be true is\nexactly the set of warranting scenarios—these are the “necessary” worlds, denoted:\n\n\n_nec_ ( _L_ ) = { _w_ ∈ _WEM_ | ( _w_ ⊢war _L_ )} _._\n\n\nNow, the set of worlds in the EM where AM literal _L can_ be true is the following—\nthese are the “possible” worlds, denoted:\n\n\n_poss_ ( _L_ ) = { _w_ ∈ _WEM_ | _w_ ̸⊢war ¬ _L_ } _._\n\n\nThe following example illustrates these concepts.\n\n\n_Example 10_ Following from Example 8:\n\n\n_nec_ ( _isCap_ ( _baja_, _worm123_ )) = { _w_ 3, _w_ 6, _w_ 7} and\n\n\n_poss_ ( _isCap_ ( _baja_, _worm123_ )) = { _w_ 1, _w_ 2, _w_ 3, _w_ 4, _w_ 6, _w_ 7} _._     \n\nHence, for a given InCA framework _I_, if we are given a probability distribution\n_Pr_ over the worlds in the EM, then we can compute an upper and lower bound on\nthe probability of literal _L_ (denoted **P** _L_, _Pr_, _I_ ) as follows:\n\n\n      _ℓL_, _Pr_, _I_ = _Pr_ ( _w_ ),\n\n_w_ ∈ _nec_ ( _L_ )\n\n\n\nand\n\n\n\n\n  _uL_, _Pr_, _I_ = _Pr_ ( _w_ ),\n\n_w_ ∈ _poss_ ( _L_ )\n\n\n_ℓL_, _Pr_, _I_ ≤ **P** _L_, _Pr_, _I_ ≤ _uL_, _Pr_, _I._\n\n\n\nNow let us consider the computation of probability bounds on a literal when we\nare given a knowledge base _ΠEM_ in the environmental model, which is specified in\n_I_, instead of a probability distribution over all worlds. For a given world _w_ ∈ _WEM_,\nlet _f or_ ( _w_ ) = _(_ [�] _a_ ∈ _w_ _[a)]_ [ ∧] _[(]_ [ �] _a /_ ∈ _w_ [¬] _[a)]_ [—that is, a formula that is satisfied only by]\nworld _w_ . Now we can determine the upper and lower bounds on the probability of a\nliteral w.r.t. _ΠEM_ (denoted **P** _L_, _I_ ) as follows:\n\n\n\n_ℓL_, _I_ = EP-LP-MIN\n\n\n_uL_, _I_ = EP-LP-MAX\n\n\n\n⎛ ⎞\n\n  ⎝ _ΠEM_, _f or_ ( _w_ )⎠,\n\n_w_ ∈ _nec_ ( _L_ )\n\n\n⎛ ⎞\n\n\n\n\n  ⎝ _ΠEM_, _f or_ ( _w_ )⎠,\n\n_w_ ∈ _poss_ ( _L_ )\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 167\n\n\nand\n\n\n_ℓL_, _I_ ≤ **P** _L_, _I_ ≤ _uL_, _I._\n\n\nHence, we have:\n\n\n\n\n   **P** _L_, _I_ = _ℓL_, _I_ + _[u][L]_ [,] _[I]_ [−] _[ℓ][L]_ [,] _[I]_\n\n2\n\n\n\n\n\n[−] _[ℓ][L]_ [,] _[I]_\n± _[u][L]_ [,] _[I]_ _._\n\n2\n\n\n\n_Example 11_ Following from Example 8, argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩,\nwe can compute **P** _isCap_ ( _baja_, _worm123_ ), _I_ (where _I_ = ( _ΠEM_ [′] [,] _[ Π][AM]_ [,] _[ af]_ [)). Note that for]\nthe upper bound, the linear program we need to set up is as in Example 4. For the\nlower bound, the objective function changes to: min _x_ 3 + _x_ 6 + _x_ 7. From these linear\nconstraints, we obtain: **P** _isCap_ ( _baja_, _worm123_ ), _I_ = 0 _._ 75 ± 0 _._ 25 _._ \n\n**8.4** **Attribution Queries**\n\n\nWe now have the necessary elements required to formally define the kind of queries\nthat correspond to the attribution problems studied in this paper.\n\n\n**Definition 8** Let _I_ = ( _ΠEM_, _ΠAM_, _af_ ) be an InCA framework, _S_ ⊆ **C** _act_ (the set of\n“suspects”), _O_ ∈ **C** _ops_ (the “operation”), and _E_ ⊆ **G** _EM_ (the “evidence”). An actor\nA ∈ _S_ is said to be a _most probable suspect_ iff there does not exist A [′] ∈ _S_ such that\n**P** _condOp_ ( **A** [′], _O_ ), _I_ ′ _>_ **P** _condOp_ ( _A_, _O_ ) _I_ ′ where _I_ [′] = ( _ΠEM_ ∪ _ΠE_, _ΠAM_, _af_ [′] ) with _ΠE_ defined\nas [�] _c_ ∈ _E_ [{] _[c]_ [ : 1][ ±][ 0][}][.]\nGiven the above definition, we refer to _Q_ = ( _I_, _S_, _O_, _E_ ) as an _attribution query_,\nand A as an _answer_ to _Q_ . We note that in the above definition, the items of evidence\nare added to the environmental model with a probability of 1 ± 0. While in general\nthis may be the case, there are often instances in analysis of a cyber-operation where\nthe evidence may be true with some degree of uncertainty. Allowing for probabilistic\nevidence is a simple extension to Definition 8 that does not cause any changes to the\nresults of this paper.\nTo understand how uncertain evidence can be present in a cyber-security scenario,\nconsiderthefollowing. InSymantec’sinitialanalysisoftheStuxnetworm, theyfound\nthe routine designed to attack the S7-417 logic controller was incomplete, and hence\nwould not function (Falliere et al. 2011). However, industrial control system expert\nRalph Langner claimed that the incomplete code would run provided a missing\ndata block is generated, which he thought was possible (Langner et al. 2011). In\nthis case, though the code was incomplete, there was clearly uncertainty regarding\nits usability. This situation provides a real-world example of the need to compare\narguments—in this case, in the worlds where both arguments are valid, Langner’s\nargument would likely defeat Symantec’s by generalized specificity (the outcome,\nof course, will depend on the exact formalization of the two). Note that Langner was\n\n\n168 P. Shakarian et al.\n\n\nlater vindicated by the discovery of an older sample, Stuxnet 0.5, which generated\nthe data block. [1]\n\nInCA also allows for a variety of relevant scenarios to the attribution problem.\nFor instance, we can easily allow for the modeling of non-state actors by extending\nthe available constants—for example, traditional groups such as Hezbollah, which\nhas previously wielded its cyber-warfare capabilities in operations against Israel\n(Shakarian et al. 2013). Likewise, the InCA can also be used to model cooperation\namong different actors in performing an attack, including the relationship between\nnon-state actors and nation-states, such as the potential connection between Iran and\nmilitants stealing UAV feeds in Iraq, or the much-hypothesized relationship between\nhacktivist youth groups and the Russian government (Shakarian et al. 2013).Another\naspect that can be modeled is deception where, for instance, an actor may leave false\nclues in a piece of malware to lead an analyst to believe a third party conducted\nthe operation. Such a deception scenario can be easily created by adding additional\nrules in the AM that allow for the creation of such counter-arguments. Another type\nof deception that could occur include attacks being launched from a system not in\nthe responsible party’s area, but under their control (e.g., see Shadows in the Cloud\n2010). Again, modeling who controls a given system can be easily accomplished in\nour framework, and doing so would simply entail extending an argumentation line.\nFurther, campaigns of cyber-operations can also be modeled, as well as relationships\namong malware and/or attacks (as detailed in APT1 2013).\nAs with all of these abilities, InCA provides the analyst the means to model a\ncomplex situation in cyber-warfare but saves him from carrying out the reasoning\nassociated with such a situation. Additionally, InCA results are constructive, so an\nanalyst can “trace-back” results to better understand how the system arrived at a\ngiven conclusion.\n\n\n**8.5** **Open Questions**\n\n\nIn this section we review some major areas of research to address to move InCA\ntoward a deployed system.\n\n\n_**8.5.1**_ _**Rule Learning**_\n\n\nThe InCA framework depends on logical rules and statements as part of the input,\nthough there are existing bodies of work we can leverage (decision tree rule learning,\ninductive logic programming, etc.) there are some specific challenges with regard to\nInCA that we must account for, specifically:\n\n\n1 http://www.symantec.com/connect/blogs/stuxnet-05-disrupting-uranium-processing-natanz.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 169\n\n\n- Quickly learning probabilistic rules from data received as an input stream\n\n- Learning of the annotation function\n\n- Identification of the diagnosticity of new additions to the knowledgebase\n\n- Learning rules that combine multiple, disparate sources (i.e. malware analysis\nand PCAP files, for instance)\n\n\n_**8.5.2**_ _**Belief Revision**_\n\n\nEven though we allow for inconsistencies in the AM portion of the model, inconsistency can arise even with a consistent EM. In a companion paper, (Shakarian et al.\n2014) we introduce the following notion of consistency.\n\n\n**Definition 9** InCA program _I_ = ( _ΠEM_, _ΠAM_, _af_ ), with _ΠAM_ = ⟨ _Θ_, _Ω_, _Φ_, _Δ_ ⟩, is\n_Type II consistent_ iff: given any probability distribution _Pr_ that satisfies _ΠEM_, if\nthere exists a world _w_ ∈ _WEM_ such that [�] _x_ ∈ _Θ_ ∪ _Ω_ | _w_ |= _af_ ( _x_ ) [{] _[x]_ [}][ is inconsistent, then we]\nhave _Pr_ ( _w_ ) = 0.\nThus, any EM world in which the set of associated facts and strict rules are\ninconsistent (we refer to this as “classical consistency”) must always be assigned a\nzero probability. The intuition is as follows: any subset of facts and strict rules are\nthought to be true under certain circumstances—these circumstances are determined\nthrough the annotation function and can be expressed as sets of EM worlds. Suppose\nthere is a world where two contradictory facts can both be considered to be true (based\non the annotation function). If this occurs, then there must not exist a probability\ndistribution that satisfies the program _ΠEM_ that assigns such a world a non-zero\nprobability, as this world leads to an inconsistency.\nWhile we have studied this theoretically (Shakarian et al. 2014), several important\nchallenges remain: How do different belief revision methods affect the results of\nattribution queries? In particular, can we develop tractable algorithms for belief\nrevision in the InCA framework? Further, finding efficient methods for re-computing\nattribution queries following a belief revision operation is a related concern for future\nwork.\n\n\n_**8.5.3**_ _**Temporal Reasoning**_\n\n\nCyber-security data often has an inherent temporal component (in particular, PCAP\nfiles, system logs, and traditional intelligence). One way to represent this type of\ninformation in InCA is by replacing the EM with a probabilistic temporal logic (i.e.\nHansson and Jonsson 1994; Dekhtyar et al. 1999; Shakarian et al. 2011; Shakarian\nand Simari 2012). However, even though this would be a relatively straightforward\nadjustment to the framework, it leads to several interesting questions, specifically:\n\n\n170 P. Shakarian et al.\n\n\n- Can we identify hacking groups responsible for a series of incidents over a period\nof time (a cyber campaign)?\n\n- Can we identify the group responsible for a campaign if it is not known a priori?\n\n- Can we differentiate between multiple campaigns conducted by multiple culprits\nin time-series data?\n\n\n_**8.5.4**_ _**Abductive Inference Queries**_\n\n\nWe may often have a case where more than one culprit is attributed to the same\ncyber-attack with nearly the same probabilities. In this case, can we identify certain\nevidence that, if found, can lead us to better differentiate among the potential culprits?\nIn the intelligence community, this is often referred as identifying _intelligence gaps_ .\nWe can also frame this as an abductive inference problem (Reggia and Peng 1990).\nThis type of problems leads to several interesting challenges:\n\n\n- Can we identify all pieces of diagnostic evidence that would satisfy an important\nintelligence gap?\n\n- Can we identify diagnostic evidence under constraints (i.e., taking into account\nlimitations on the type of evidence that can be collected)?\n\n- In the case where a culprit is attributed with a high probability, can we identify\nevidence that can falsify the finding?\n\n\n**8.6** **Conclusions**\n\n\nIn this paper we introduced InCA, a new framework that allows the modeling of various cyber-warfare/cyber-security scenarios in order to help answer the attribution\nquestion by means of a combination of probabilistic modeling and argumentative\nreasoning. This is the first framework, to our knowledge, that addresses the attribution problem while allowing for multiple pieces of evidence from different sources,\nincluding traditional (non-cyber) forms of intelligence such as human intelligence.\nFurther, our framework is the first to extend Defeasible Logic Programming with\nprobabilistic information. Currently, we are implementing InCA along with the\nassociated algorithms and heuristics to answer these queries.\n\n\n**Acknowledgments** This work was supported by UK EPSRC grant EP/J008346/1—“PrOQAW”,\nERC grant 246858—“DIADEM”, by NSF grant #1117761, by the National Security Agency under\nthe Science of Security Lablet grant (SoSL), Army Research Office project 2GDATXR042, and\nDARPA project R.0004972.001.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 171\n\n\n**References**\n\n\nShadows in the Cloud: Investigating Cyber Espionage 2.0. Tech. rep., Information Warfare Monitor\nand Shadowserver Foundation (2010)\nAPT1: Exposing one of China’s cyber espionage units. Mandiant (tech. report) (2013)\nAltheide, C.: Digital Forensics with Open Source Tools. Syngress (2011)\nDekhtyar,A., Dekhtyar, M.I., Subrahmanian, V.S.: Temporal probabilistic logic programs. In: ICLP\n1999, pp. 109–123. The MIT Press, Cambridge, MA, USA (1999)\nDung, P.M.: On the acceptability of arguments and its fundamental role in nonmonotonic reasoning,\nlogic programming and _n_ -person games. Artif. Intell. **77**, pp. 321–357 (1995)\nFalliere, N., Murchu, L.O., Chien, E.: W32.Stuxnet Dossier Version 1.4. Symantec Corporation\n(2011)\nGarcía, A.J., Simari, G.R.: Defeasible logic programming: An argumentative approach. TPLP\n**4** (1–2), 95–138 (2004)\nHansson, H., Jonsson, B.: A logic for reasoning about time and probability. Formal Aspects of\nComputing **6**, 512–535 (1994)\nHeuer, R.J.: Psychology of Intelligence Analysis. Center for the Study of Intelligence (1999)\nKhuller, S., Martinez, M.V., Nau, D.S., Sliva, A., Simari, G.I., Subrahmanian, V.S.: Computing\nmost probable worlds of action probabilistic logic programs: scalable estimation for 10 [30,000]\n\nworlds. AMAI **51(2–4)**, 295–331 (2007)\nLangner, R.: Matching Langner Stuxnet analysis and Symantic dossier update. Langner Communications GmbH (2011)\nLloyd, J.W.: Foundations of Logic Programming, 2nd Edition. Springer (1987)\nMartinez, M.V., García, A.J., Simari, G.R.: On the use of presumptions in structured defeasible\nreasoning. In: Proc. of COMMA, pp. 185–196 (2012)\nNilsson, N.J.: Probabilistic logic. Artif. Intell. **28** (1), 71–87 (1986)\nRahwan, I., Simari, G.R.: Argumentation in Artificial Intelligence. Springer (2009)\nReggia, J.A., Peng,Y.:Abductive inference models for diagnostic problem-solving. Springer-Verlag\nNew York, Inc., New York, NY, USA (1990)\nShakarian, P., Parker, A., Simari, G.I., Subrahmanian, V.S.: Annotated probabilistic temporal logic.\nTOCL **12** (2), 14 (2011)\nShakarian, P., Simari, G.I., Subrahmanian, V.S.: Annotated probabilistic temporal logic: Approximate fixpoint implementation. ACM Trans. Comput. Log. **13** (2), 13 (2012)\nShakarian, P., Shakarian, J., Ruef, A.: Introduction to Cyber-Warfare: A Multidisciplinary\nApproach. Syngress (2013)\nShakarian, P., Simari, G.I., Falappa, M.A.: Belief revision in structured probabilistic argumentation.\nIn: Proceedings of FoIKS, pp. 324–343 (2014)\nSimari, G.R., Loui, R.P.: A mathematical treatment of defeasible reasoning and its implementation.\nArtif. Intell. **53** (2-3), 125–157 (1992)\nSimari, G.I., Martinez, M.V., Sliva, A., Subrahmanian, V.S.: Focused most probable world\ncomputations in probabilistic logic programs. AMAI **64** (2–3), 113–143 (2012)\nSpitzner, L.: Honeypots: Catching the Insider Threat. In: Proc. ofACSAC 2003, pp. 170–179. IEEE\nComputer Society (2003)\nStolzenburg, F., García, A., Chesñevar, C.I., Simari, G.R.: Computing Generalized Specificity.\nJournal of Non-Classical Logics **13** (1), 87–113 (2003)\nThonnard, O., Mees, W., Dacier, M.: On a multicriteria clustering approach for attack attribution.\nSIGKDD Explorations **12** (1), 11–20 (2010)",
  "toc": [],
  "sections": {},
  "process_log": {
    "mode": "offline_fallback",
    "reason": "missing_full_text",
    "method": "pymupdf4llm"
  },
  "word_count": 8890,
  "references": [],
  "citations": {
    "total": {},
    "results": [],
    "flat_text": "## **Chapter 8**\n# **Cyber Attribution: An Argumentation-Based** **Approach**\n\n**Paulo Shakarian, Gerardo I. Simari, Geoffrey Moores and Simon Parsons**\n\n\n**Abstract** Attributing a cyber-operation through the use of multiple pieces of\ntechnical evidence (i.e., malware reverse-engineering and source tracking) and conventional intelligence sources (i.e., human or signals intelligence) is a difficult\nproblem not only due to the effort required to obtain evidence, but the ease with\nwhich an adversary can plant false evidence. In this paper, we introduce a formal reasoning system called the InCA (Intelligent Cyber Attribution) framework\nthat is designed to aid an analyst in the attribution of a cyber-operation even when\nthe available information is conflicting and/or uncertain. Our approach combines\nargumentation-based reasoning, logic programming, and probabilistic models to not\nonly attribute an operation but also explain to the analyst why the system reaches its\nconclusions.\n\n\n**8.1** **Introduction**\n\n\nAn important issue in cyber-warfare is the puzzle of determining who was responsible\nfor a given cyber-operation—be it an incident of attack, reconnaissance, or information theft. This is known as the “attribution problem” (Shakarian et al. 2013).\nThe difficulty of this problem stems not only from the amount of effort required to\nfind forensic clues but also the ease with which an attacker can plant false clues to\n\n\nP. Shakarian (�)\nArizona Sate University, Tempe, AZ, USA\ne-mail: shak@asu.edu\n\n\nG. I. Simari\nDepartment of Computer Science and Engineering, Universidad Nacional del Sur,\nBahía Blanca, Argentina\ne-mail: gis@cs.uns.edu.ar\n\n\nG. Moores\nDepartment of Electrical Engineering and Computer Science, U.S. Military Academy,\nWest Point, NY, USA\ne-mail: geoffrey.moores@usma.edu\n\n\nS. Parsons\nDepartment of Computer Science, University of Liverpool, Liverpool, UK\ne-mail: s.d.parsons@liverpool.ac.uk\n\n© Springer International Publishing Switzerland 2015 151\nS. Jajodia et al. (eds.), _Cyber Warfare,_ Advances in Information Security 56,\nDOI 10.1007/978-3-319-14039-1_8\n\n\n152 P. Shakarian et al.\n\n\nmislead security personnel. Further, while techniques such as forensics and reverseengineering (Altheide 2011), source tracking (Thonnard et al. 2010), honeypots\n(Spitzner 2003), and sinkholing 2010 are commonly employed to find evidence that\ncan lead to attribution, it is unclear how this evidence is to be combined and reasoned\nabout. In a military setting, such evidence is augmented with normal intelligence collection, such as human intelligence (HUMINT), signals intelligence (SIGINT) and\nother means—this adds additional complications to the task of attributing a given\noperation. Essentially, cyber-attribution is a highly-technical intelligence analysis\nproblem where an analyst must consider a variety of sources, each with its associated level of confidence, to provide a decision maker (e.g., a military commander)\ninsight into who conducted a given operation.\nAs it is well known that people’s ability to conduct intelligence analysis is limited\n(Heuer 1999), and due to the highly technical nature of many cyber evidencegathering techniques, an automated reasoning system would be best suited for the\ntask. Such a system must be able to accomplish several goals, among which we\ndistinguish the following main capabilities:\n\n\n1. Reason about evidence in a formal, principled manner, i.e., relying on strong\nmathematical foundations.\n2. Consider evidence for cyber attribution associated with some level of probabilistic\nuncertainty.\n3. Consider logical rules that allow for the system to draw conclusions based on\ncertain pieces of evidence and iteratively apply such rules.\n4. Consider pieces of information that may not be compatible with each other, decide\nwhich information is most relevant, and express why.\n5. Attribute a given cyber-operation based on the above-described features and provide the analyst with the ability to understand how the system arrived at that\nconclusion.\n\n\nIn this paper we present the InCA (Intelligent Cyber Attribution) framework, which\nmeets all of the above qualities. Our approach relies on several techniques from\nthe artificial intelligence community, including argumentation, logic programming,\nand probabilistic reasoning. We first outline the underlying mathematical framework and provide a running example based on real-world cases of cyber-attribution\n(cf. Sect. 8.2); then, in Sects. 8.3 and 8.4, we formally present InCA and attribution\nqueries, respectively. Finally, we discuss conclusions and future work in Sect. 8.6.\n\n\n**8.2** **Two Kinds of Models**\n\n\nOur approach relies on _two separate models of the world_ . The first, called the _**environ-**_\n_**mentalmodel**_ (EM)isusedtodescribethebackgroundknowledgeandisprobabilistic\nin nature. The second one, called the _**analytical model**_ (AM) is used to analyze\ncompeting hypotheses that can account for a given phenomenon (in this case, a\ncyber-operation). The EM _must be consistent_ —this simply means that there must\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 153\n\n\nEM AM\n“Malware X was compiled on a system “Malware X was compiled on a system\nusing the English language.” English-speaking country Y.”\n“Malware W and malware X were created “Malware W and malware X are related.”\nin a similar coding sytle.”\n“Country Y and country Z are currently “Country Y has a motive to launch a\nat war.” cybre-attack against country Z.”\n“Country Y has a significant investment “Country Y has the capability to conduct\nin math-science-engineering (MSE) education.” a cyber-attack.”\n\n\n**Fig. 8.1** Example observations—EM vs. AM\n\n\nexist a probability distribution over the possible states of the world that satisfies all\nof the constraints in the model, as well as the axioms of probability theory. On the\ncontrary, the AM will allow for contradictory information as the system must have\nthe capability to reason about competing explanations for a given cyber-operation.\nIn general, the EM contains knowledge such as evidence, intelligence reporting, or\nknowledge about actors, software, and systems. TheAM, on the other hand, contains\nideas the analyst concludes based on the information in the EM. Figure 8.1 gives\nsome examples of the types of information in the two models. Note that an analyst\n(or automated system) could assign a probability to statements in the EM column\nwhereas statements in the AM column can be true or false depending on a certain\ncombination (or several possible combinations) of statements from the EM. We now\nformally describe these two models as well as a technique for _annotating_ knowledge\nin the AM with information from the EM—these annotations specify the conditions\nunder which the various statements in the AM can potentially be true.\nBefore describing the two models in detail, we first introduce the language used\nto describe them. Variable and constant symbols represent items such as computer\nsystems, types of cyber operations, actors (e.g., nation states, hacking groups), and\nother technical and/or intelligence information. The set of all variable symbols is\ndenoted with **V**, and the set of all constants is denoted with **C** . For our framework, we\nshallrequiretwosubsetsof **C**, **C** _act_ and **C** _ops_, thatspecifytheactorsthatcouldconduct\ncyber-operations and the operations themselves, respectively. In the examples in this\npaper, we will use capital letters to represent variables (e.g., _X_, _Y_, _Z_ ). The constants\nin **C** _act_ and **C** _ops_ that we use in the running example are specified in the following\nexample.\n\n\n_Example 1_ The following (fictitious) actors and cyber-operations will be used in\nour examples:\n\n\n**C** _act_ = { _baja_, _krasnovia_, _mojave_ } (8.1)\n\n**C** _ops_ = { _worm123_ } (8.2)\n\n\n                     The next component in the model is a set of predicate symbols. These constructs\ncan accept zero or more variables or constants as arguments, and map to either\n\n\n154 P. Shakarian et al.\n\n\n**P** _EM_ : _origIP_ ( _M,_ _X_ ) Malware _M_ originated from an IP address belonging to actor _X_ .\n_malwInOp_ ( _M,_ _O_ ) Malware _M_ was used in cyber-operation _O_ .\n_mwHint_ ( _M,_ _X_ ) Malware _M_ contained a hint that it was created by actor _X_ .\n_compilLang_ ( _M,C_ ) Malware _M_ was compiled in a system that used language _C_ .\n_nativLang_ ( _X,C_ ) Language _C_ is the native language of actor _X_ .\n_inLgConf_ ( _X,_ _X_ _[′]_ ) Actors _X_ and _X_ _[′]_ are in a larger conflict with each other.\n_mseTT_ ( _X,_ _N_ ) There are at least _N_ number of top-tier math-science-engineering\nuniversities in country _X_ .\n_infGovSys_ ( _X,_ _M_ ) Systems belonging to actor _X_ were infected with malware _M_ .\n_cybCapAge_ ( _X,_ _N_ ) Actor _X_ has had a cyber-warfare capability for _N_ years or less.\n_govCybLab_ ( _X_ ) Actor _X_ has a government cyber-security lab.\n\n\n**P** _AM_ : _condOp_ ( _X,_ _O_ ) Actor _X_ conducted cyber-operation _O_ .\n_evidOf_ ( _X,_ _O_ ) There is evidence that actor _X_ conducted cyber-operation _O_ .\n_motiv_ ( _X,_ _X_ _[′]_ ) Actor _X_ had a motive to launch a cyber-attack against actor _X_ _[′]_ .\n_isCap_ ( _X,_ _O_ ) Actor _X_ is capable of conducting cyber-operation _O_ .\n_tgt_ ( _X,_ _O_ ) Actor _X_ was the target of cyber-operation _O_ .\n_hasMseInvest_ ( _X_ ) Actor _X_ has a significant investment in math-science-engineering\neducation.\n_expCw_ ( _X_ ) Actor _X_ has experience in conducting cyber-operations.\n\n\n**Fig. 8.2** Predicate definitions for the environment and analytical models in the running example\n\n\n_true_ or false. _Note that the EM and AM use separate sets of predicate symbols_ however, they can share variables and constants. The sets of predicates for the\nEM and AM are denoted with **P** _EM_, **P** _AM_, respectively. In InCA, we require **P** _AM_\nto include the binary predicate _condOp_ ( _X_, _Y_ ), where _X_ is an actor and _Y_ is a cyberoperation. Intuitively, this means that actor _X_ conducted operation _Y_ . For instance,\n_condOp_ ( _baja_, _worm123_ )istrueif _baja_ wasresponsibleforcyber-operation _worm123_ .\nA sample set of predicate symbols for the analysis of a cyber attack between two\nstates over contention of a particular industry is shown in Fig. 8.2; these will be used\nin examples throughout the paper.\nA construct formed with a predicate and constants as arguments is known as a\n_ground atom_ (we shall often deal with ground atoms). The sets of all ground atoms\nfor EM and AM are denoted with **G** _EM_ and **G** _AM_, respectively.\n\n\n_Example 2_ The following are examples of ground atoms over the predicates given\nin Fig. 8.2.\n\n\n**G** _EM_ : _origIP_ ( _mw123sam1_, _krasnovia_ ),\n\n\n_mwHint_ ( _mw123sam1_, _krasnovia_ ),\n\n\n_inLgConf_ ( _krasnovia_, _baja_ ),\n\n\n_mseTT_ ( _krasnovia_, 2) _._\n\n\n**G** _AM_ : _evidOf_ ( _mojave_, _worm123_ ),\n\n\n_motiv_ ( _baja_, _krasnovia_ ),\n\n\n_expCw_ ( _baja_ ),\n\n\n_tgt_ ( _krasnovia_, _worm123_ ) _._                \n\n8 Cyber Attribution: An Argumentation-Based Approach 155\n\n\nFor a given set of ground atoms, a _world_ is a subset of the atoms that are considered\nto be true (ground atoms not in the world are false). Hence, there are 2 [|] **[G]** _[EM]_ [|] possible worlds in the EM and 2 [|] **[G]** _[AM]_ [|] worlds in the AM, denoted with _WEM_ and _WAM_,\nrespectively.\nClearly, even a moderate number of ground atoms can yield an enormous number\nof worlds to explore. One way to reduce the number of worlds is to include _integrity_\n_constraints_, which allow us to eliminate certain worlds from consideration—they\nsimplyarenotpossibleinthesettingbeingmodeled. Ourprincipleintegrityconstraint\nwill be of the form:\n\n\noneOf( _A_ [′] )\n\n\nwhere _A_ [′] is a subset of ground atoms. Intuitively, this says that any world where\nmore than one of the atoms from set _A_ [′] appear is invalid. Let **IC** _EM_ and **IC** _AM_ be the\nsets of integrity constraints for the EM and AM, respectively, and the sets of worlds\nthat conform to these constraints be _WEM_ ( **IC** _EM_ ), _WAM_ ( **IC** _AM_ ), respectively.\nAtoms can also be combined into formulas using standard logical connectives:\nconjunction ( _and_ ), disjunction ( _or_ ), and negation ( _not_ ). These are written using the\nsymbols ∧, ∨, ¬, respectively. We say a world ( _w_ ) _satisfies_ a formula ( _f_ ), written\n_w_ |= _f_, based on the following inductive definition:\n\n\n- if _f_ is a single atom, then _w_ |= _f_ iff _f_ ∈ _w_ ;\n\n- if _f_ = ¬ _f_ [′] then _w_ |= _f_ iff _w_ ̸|= _f_ [′] ;\n\n- if _f_ = _f_ [′] ∧ _f_ [′′] then _w_ |= _f_ iff _w_ |= _f_ [′] and _w_ |= _f_ [′′] ; and\n\n- if _f_ = _f_ [′] ∨ _f_ [′′] then _w_ |= _f_ iff _w_ |= _f_ [′] or _w_ |= _f_ [′′] .\n\n\nWe use the notation _f ormulaEM_, _f ormulaAM_ to denote the set of all possible\n(ground) formulas in the EM andAM, respectively.Also, note that we use the notation\n⊤, ⊥ to represent tautologies (formulas that are true in all worlds) and contradictions\n(formulas that are false in all worlds), respectively.\n\n\n_**8.2.1**_ _**Environmental Model**_\n\n\nIn this section we describe the first of the two models, namely the EM or environmental model. This model is largely based on the probabilistic logic of (Nilsson 1986),\nwhich we now briefly review.\nFirst, we define a _probabilistic formula_ that consists of a formula _f_ over\natoms from **G** _EM_, a real number _p_ in the interval [0, 1], and an error tolerance\n_ε_ ∈ [0, min( _p_, 1 − _p_ )]. A probabilistic formula is written as: _f_ : _p_ ± _ε_ . Intuitively,\nthis statement is interpreted as “formula _f_ is true with probability between _p_ - _ε_\nand _p_ + _ε_ ”—note that we make no statement about the probability distribution over\nthis interval. The uncertainty regarding the probability values stems from the fact\nthat certain assumptions (such as probabilistic independence) may not be suitable in\nthe environment being modeled.\n\n\n156 P. Shakarian et al.\n\n\n_Example 3_ To continue our running example, consider the following set _ΠEM_ :\n\n\n_f_ 1 = _govCybLab_ ( _baja_ ) : 0 _._ 8 ± 0 _._ 1\n\n_f_ 2 = _cybCapAge_ ( _baja_, 5) : 0 _._ 2 ± 0 _._ 1\n\n_f_ 3 = _mseTT_ ( _baja_, 2) : 0 _._ 8 ± 0 _._ 1\n\n_f_ 4 = _mwHint_ ( _mw123sam1_, _mojave_ ) ∧ _compilLang_ ( _worm123_, _english_ ) : 0 _._ 7 ± 0 _._ 2\n\n_f_ 5 = _malwInOp_ ( _mw123sam1_, _worm123_ )\n\n\n∧ _malwareRel_ ( _mw123sam1_, _mw123sam2_ )\n\n\n∧ _mwHint_ ( _mw123sam2_, _mojave_ ) : 0 _._ 6 ± 0 _._ 1\n\n\n_f_ 6 = _inLgConf_ ( _baja_, _krasnovia_ ) ∨¬ _cooper_ ( _baja_, _krasnovia_ ) : 0 _._ 9 ± 0 _._ 1\n\n_f_ 7 = _origIP_ ( _mw123sam1_, _baja_ ) : 1 ± 0\n\n\nThroughout other examples in the rest of the paper, we will make use of the subset\n_ΠEM_ [′] [= {] _[f]_ [1][,] _[ f]_ [2][,] _[ f]_ [3][}][.] We now consider a probability distribution _Pr_ over the set _WEM_ ( **IC** _EM_ ). We\nsay that _Pr satisfies_ probabilistic formula _f_ : _p_ ± _ε_ iff the following holds:\n_p_ - _ε_ ≤ [�] _w_ ∈ _WEM_ ( **IC** _EM_ ) _[Pr]_ [(] _[w]_ [)][ ≤] _[p]_ [ +] _[ ε.]_ [ A set] _[ Π][EM]_ [ of probabilistic formulas is]\ncalled a _knowledge base_ . We say that a probability distribution over _WEM_ ( **IC** _EM_ )\n_satisfies ΠEM_ if and only if it satisfies all probabilistic formulas in _ΠEM_ .\nIt is possible to create probabilistic knowledge bases for which there is no\nsatisfying probability distribution. The following is a simple example of this:\n\n\n_condOp_ ( _krasnovia_, _worm123_ ) ∨ _condOp_ ( _baja_, _worm123_ ) : 0 _._ 4 ± 0;\n\n\n_condOp_ ( _krasnovia_, _worm123_ ) ∧ _condOp_ ( _baja_, _worm123_ ) : 0 _._ 6 ± 0 _._ 1 _._\n\n\nFormulas and knowledge bases of this sort are _inconsistent_ . In this paper, we assume\nthat information is properly extracted from a set of historic data and hence consistent;\n(recall that inconsistent information can only be handled in the AM, not the EM). A\nconsistent knowledge base could also be obtained as a result of curation by experts,\nsuch that all inconsistencies were removed—see (Khuller et al. 2007; Shakarian et\nal. 2011) for algorithms for learning rules of this type.\nThe main kind of query that we require for the probabilistic model is the _maximum_\n_entailment_ problem: given a knowledge base _ΠEM_ and a (non-probabilistic) formula\n_q_, identify _p_, _ε_ such that all valid probability distributions _Pr_ that satisfy _ΠEM_ also\nsatisfy _q_ : _p_ ± _ε_, and there does not exist _p_ [′], _ε_ [′] s.t. [ _p_ - _ε_, _p_ + _ε_ ] ⊃ [ _p_ [′] - _ε_ [′], _p_ [′] + _ε_ [′] ],\nwhere all probability distributions _Pr_ that satisfy _ΠEM_ also satisfy _q_ : _p_ [′] ± _ε_ [′] . That\nis, given _q_, can we determine the probability (with maximum tolerance) of statement\n_q_ given the information in _ΠEM_ ? The approach adopted in (Nilsson et al. 1986) to\nsolve this problem works as follows. First, we must solve the linear program defined\nnext.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 157\n\n\n**Definition 1** ( **EM-LP-MIN)** Given a knowledge base _ΠEM_ and a formula _q_ :\n\n\n- create a variable _xi_ for each _wi_ ∈ _WEM_ ( **IC** _EM_ );\n\n- for each _fj_ : _pj_ ± _εj_ ∈ _ΠEM_, create constraint:\n\n      _pj_          - _εj_ ≤ _xi_ ≤ _pj_ + _εj_ ;\n\n_wi_ ∈ _WEM_ ( **IC** _EM_ ) _s.t.wi_ |= _fj_\n\n\n- finally, we also have a constraint:\n\n      \n_xi_ = 1 _._\n_wi_ ∈ _WEM_ ( **IC** _EM_ )\n\n\nThe objective is to minimize the function:\n\n      \n_xi._\n_wi_ ∈ _WEM_ ( **IC** _EM_ ) _s.t.wi_ |= _q_\n\n\nWe use the notation EP-LP-MIN( _ΠEM_, _q_ ) to refer to the value of the objective\nfunction in the solution to the EM-LP-MIN constraints.\nLet _ℓ_ be the result of the process described in Definition 1. The next step is to\nsolve the linear program a second time, but instead maximizing the objective function\n(we shall refer to this as EM-LP-MAX)—let _u_ be the result of this operation. In\n(Nilsson 1986), it is shown that _ε_ = _u_ −2 _ℓ_ and _p_ = _ℓ_ + _ε_ is the solution to the\n\nmaximum entailment problem. We note that although the above linear program has\nan exponential number of variables in the worst case (i.e., no integrity constraints),\nthe presence of constraints has the potential to greatly reduce this space. Further,\nthere are also good heuristics (cf. Khuller et al. 2007; Simari et al. 2012) that have\nbeen shown to provide highly accurate approximations with a reduced-size linear\nprogram.\n\n_Example 4_ Consider KB _ΠEM_ [′] [from Example][ 3][ and a set of ground atoms restricted]\nto those that appear in that program. Hence, we have:\n\n\n_w_ 1 = { _govCybLab_ ( _baja_ ), _cybCapAge_ ( _baja_, 5), _mseTT_ ( _baja_, 2)}\n\n_w_ 2 = { _govCybLab_ ( _baja_ ), _cybCapAge_ ( _baja_, 5)}\n\n_w_ 3 = { _govCybLab_ ( _baja_ ), _mseTT_ ( _baja_, 2)}\n\n_w_ 4 = { _cybCapAge_ ( _baja_, 5), _mseTT_ ( _baja_, 2)}\n\n_w_ 5 = { _cybCapAge_ ( _baja_, 5)}\n\n_w_ 6 = { _govCybLab_ ( _baja_ )}\n\n_w_ 7 = { _mseTT_ ( _baja_, 2)}\n\n_w_ 8 = ∅\n\n\nand suppose we wish to compute the probability for formula:\n\n\n_q_ = _govCybLab_ ( _baja_ ) ∨ _mseTT_ ( _baja_, 2) _._\n\n\n158 P. Shakarian et al.\n\n\nFor each formula in _ΠEM_ we have a constraint, and for each world above we have a\nvariable. An objective function is created based on the worlds that satisfy the query\nformula (here, worlds _w_ 1 − _w_ 4, _w_ 6, _w_ 7). Hence, EP-LP-MIN( _ΠEM_ [′] [,] _[ q]_ [) can be written]\nas follows:\n\n\nmax _x_ 1 + _x_ 2 + _x_ 3 + _x_ 4 + _x_ 6 + _x_ 7 _w.r.t._ :\n\n0 _._ 7 ≤ _x_ 1 + _x_ 2 + _x_ 3 + _x_ 6 ≤ 0 _._ 9\n\n0 _._ 1 ≤ _x_ 1 + _x_ 2 + _x_ 4 + _x_ 5 ≤ 0 _._ 3\n\n0 _._ 8 ≤ _x_ 1 + _x_ 3 + _x_ 4 + _x_ 7 ≤ 1\n\n_x_ 1 + _x_ 2 + _x_ 3 + _x_ 4 + _x_ 5 + _x_ 6 + _x_ 7 + _x_ 8 = 1\n\nWe can now solve EP-LP-MAX( _ΠEM_ [′] [,] _[ q]_ [) and][ EP-LP-MIN][(] _[Π]_ _EM_ [′] [,] _[ q]_ [) to get solution]\n0 _._ 9 ± 0 _._ 1. \n\n_**8.2.2**_ _**Analytical Model**_\n\n\nFor the analytical model (AM), we choose a structured argumentation framework\n(Rahwan et al. 2009) due to several characteristics that make such frameworks highly\napplicable to cyber-warfare domains. Unlike the EM, which describes probabilistic\ninformation about the state of the real world, the AM must allow for competing\nideas—it _must be able to represent contradictory information_ . The algorithmic\napproach allows for the creation of _arguments_ based on the AM that may “compete” with each other to describe who conducted a given cyber-operation. In this\ncompetition—known as a _dialectical process_ —one argument may defeat another\nbased on a _comparison criterion_ that determines the prevailing argument. Resulting\nfrom this process, the InCA framework will determine arguments that are _war-_\n_ranted_ (those that are not _defeated_ by other arguments) thereby providing a suitable\nexplanation for a given cyber-operation.\nThe transparency provided by the system can allow analysts to identify potentially\nincorrect input information and fine-tune the models or, alternatively, collect more\ninformation. In short, argumentation-based reasoning has been studied as a natural\nway to manage a set of inconsistent information—it is the way humans settle disputes. As we will see, another desirable characteristic of (structured) argumentation\nframeworks is that, once a conclusion is reached, we are left with an explanation of\nhow we arrived at it and information about why a given argument is warranted; this\nis very important information for analysts to have. In this section, we recall some\npreliminaries of the underlying argumentation framework used, and then introduce\nthe analytical model (AM).\n\n\n**Defeasible Logic Programming with Presumptions**\n\n\nDeLP with Presumptions (PreDeLP) (Martinez et al. 2012) is a formalism combining Logic Programming with Defeasible Argumentation. We now briefly recall\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 159\n\n\n**Fig. 8.3** A ground argumentation framework\n\n\nthe basics of PreDeLP; we refer the reader to (García and Simari 2004; Martinez\net al. 2012) for the complete presentation. The formalism contains several different\nconstructs: facts, presumptions, strict rules, and defeasible rules. Facts are statements\nabout the analysis that can always be considered to be true, while presumptions are\nstatements that may or may not be true. Strict rules specify logical consequences of\na set of facts or presumptions (similar to an implication, though not the same) that\nmust always occur, while defeasible rules specify logical consequences that may be\nassumed to be true when no contradicting information is present. These constructs\nare used in the construction of _arguments_, and are part of a PreDeLP program, which\nis a set of facts, strict rules, presumptions, and defeasible rules. Formally, we use\nthe notation _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) to denote a PreDeLP program, where _Ω_ is the set\nof strict rules, _Θ_ is the set of facts, _Δ_ is the set of defeasible rules, and _Φ_ is the set\nof presumptions. In Fig. 8.3, we provide an example _ΠAM_ . We now describe each of\nthese constructs in detail.\n\n\n**Facts** ( _Θ_ ) are ground literals representing atomic information or its negation, using\nstrong negation “¬”. Note that all of the literals in our framework must be formed\nwith a predicate from the set **P** _AM_ . Note that information in this form cannot be\ncontradicted.\n\n\n**Strict Rules** ( _Ω_ ) represent non-defeasible cause-and-effect information that resembles a material implication (though the semantics is different since the contrapositive\n\n\n160 P. Shakarian et al.\n\n\ndoes not hold) and are of the form _L_ 0 ←− _L_ 1, _. . ._, _Ln_, where _L_ 0 is a ground literal\nand { _Li_ } _i>_ 0 is a set of ground literals.\n\n\n**Presumptions** ( _Φ_ ) are ground literals of the same form as facts, except that they\nare not taken as being true but rather defeasible, which means that they can be\ncontradicted. Presumptions are denoted in the same manner as facts, except that the\nsymbol –≺ is added. While any literal can be used as a presumption in InCA, we\nspecifically require all literals created with the predicate _condOp_ to be defeasible.\n\n\n**Defeasible Rules** ( _Δ_ ) represent tentative knowledge that can be used if nothing\ncan be posed against it. Just as presumptions are the defeasible counterpart of facts,\ndefeasible rules are the defeasible counterpart of strict rules. They are of the form\n_L_ 0 –≺ _L_ 1, _. . ._, _Ln_, where _L_ 0 is a ground literal and { _Li_ } _i>_ 0 is a set of ground literals.\nNote that with both strict and defeasible rules, _strong negation_ is allowed in the head\nof rules, and hence may be used to represent contradictory knowledge.\nWe note that strict rules and facts are necessary constructs as they may not be true\nin all environmental conditions. We shall discuss this further in the next section with\nthe introduction of an annotation function.\nEven though the above constructs are ground, we allow for schematic versions\nwith variables that are used to represent sets of ground rules. We denote variables\nwith strings starting with an uppercase letter; Fig. 8.4 shows a non-ground example.\nWhen a cyber-operation occurs, InCA must derive arguments as to who could\nhave potentially conducted the action. Informally, an argument for a particular actor\n_x_ conducting cyber-operation _y_ is a consistent subset of the analytical model that\nentails the atom _condOp_ ( _x_, _y_ ). If the argument contains only strict rules and facts,\nthen it is _factual_ . If it contains presumptions or defeasible rules, then it _defeasibly_\n_derives_ that actor _x_ conducted operation _y_ .\nDerivation follows the same mechanism of Logic Programming (Lloyd 1987).\nSince rule heads can contain strong negation, it is possible to defeasibly derive contradictory literals from a program. For the treatment of contradictory knowledge,\nPreDeLP incorporates a defeasible argumentation formalism that allows the identification of the pieces of knowledge that are in conflict, and through the previously\nmentioned _dialectical process_ decides which information prevails as warranted.\nThis dialectical process involves the construction and evaluation of arguments\nthat either support or interfere with a given query, building a _dialectical tree_ in the\nprocess. Formally, we have:\n\n\n**Definition 2 (Argument)** An _argument A_, _L_ ⟩ for a literal _L_ is a pair of the literal\nand a (possibly empty) set of the EM ( _A_ ⊆ _ΠAM_ ) that provides a minimal proof for _L_\nmeeting the requirements: (1) _L_ is defeasibly derived from _A_, (2) _Ω_ ∪ _Θ_ ∪ _A_ is not\ncontradictory, and (3) _A_ is a minimal subset of _Δ_ ∪ _Φ_ satisfying 1 and 2, denoted\n⟨ _A_, _L_ ⟩.\nLiteral _L_ is called the _conclusion_ supported by the argument, and _A_ is the _support_\nof the argument. An argument ⟨ _B_, _L_ ⟩ is a _subargument_ of ⟨ _A_, _L_ [′] ⟩ iff _B_ ⊆ _A_ . An\nargument ⟨ _A_, _L_ ⟩ is _presumptive_ iff _A_ ∩ _Φ_ is not empty. We will also use _Ω_ ( _A_ ) =\n_A_ ∩ _Ω_, _Θ_ ( _A_ ) = _A_ ∩ _Θ_, _Δ_ ( _A_ ) = _A_ ∩ _Δ_, and _Φ_ ( _A_ ) = _A_ ∩ _Φ_ .\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 161\n\n\n**Fig. 8.4** A non-ground argumentation framework\n\n\n**Fig. 8.5** Example ground arguments from Fig. 8.3\n\n\nNote that our definition differs slightly from that of (Simari and Loui 1992) where\nDeLP is introduced, as we include strict rules and facts as part of the argument. The\nreason for this will become clear in Sect. 8.3. Arguments for our scenario are shown\nin the following example.\n\n\n_Example 5_ Figure 8.5 shows example arguments based on the knowledge base from\nFig. 8.3. Note that the following relationship exists:\n\n\n⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ is a sub-argument of\n\n\n⟨ _A_ 2, _condOp_ ( _baja_, _worm123_ )⟩ and\n\n\n⟨ _A_ 3, _condOp_ ( _baja_, _worm123_ )⟩ _._         \n\nGiven argument ⟨ _A_ 1, _L_ 1⟩, counter-arguments are arguments that contradict it.\nArgument ⟨ _A_ 2, _L_ 2⟩ _counterargues_ or _attacks_ ⟨ _A_ 1, _L_ 1⟩ literal _L_ [′] iff there exists a\nsubargument ⟨ _A_, _L_ [′′] ⟩ of ⟨ _A_ 1, _L_ 1⟩ s.t. set _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _Θ_ ( _A_ 1) ∪ _Θ_ ( _A_ 2) ∪\n{ _L_ 2, _L_ [′′] } is contradictory.\n\n\n_Example 6_ Consider the arguments from Example 5. The following are some of\nthe attack relationships between them: _A_ 1, _A_ 2, _A_ 3, and _A_ 4 all attack _A_ 6; _A_ 5 attacks\n_A_ 7; and _A_ 7 attacks _A_ 2. \n\n162 P. Shakarian et al.\n\n\nA _proper defeater_ of an argument ⟨ _A_, _L_ ⟩ is a counter-argument that—by some\ncriterion—is considered to be better than ⟨ _A_, _L_ ⟩; if the two are incomparable according to this criterion, the counterargument is said to be a _blocking_ defeater. An\nimportantcharacteristicofPreDeLPisthattheargumentcomparisoncriterionismodular, and thus the most appropriate criterion for the domain that is being represented\ncan be selected; the default criterion used in classical defeasible logic programming (from which PreDeLP is derived) is _generalized specificity_ (Stolzenburg et al.\n2003), though an extension of this criterion is required for arguments using presumptions (Martinez et al. 2012). We briefly recall this criterion next—the first\ndefinition is for generalized specificity, which is subsequently used in the definition\nof presumption-enabled specificity.\n\n\n**Definition 3** Let _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) be a PreDeLP program and let _F_ be the\nset of all literals that have a defeasible derivation from _ΠAM_ . An argument ⟨ _A_ 1, _L_ 1⟩\nis _preferred to_ ⟨ _A_ 2, _L_ 2⟩, denoted with _A_ 1 ≻ _PS A_ 2 iff the two following conditions\nhold:\n\n\n1. For all _H_ ⊆ _F_, _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ is non-contradictory: if there is a derivation\nfor _L_ 1 from _Ω_ ( _A_ 2) ∪ _Ω_ ( _A_ 1) ∪ _Δ_ ( _A_ 1) ∪ _H_, and there is no derivation for _L_ 1\nfrom _Ω_ ( _A_ 1)∪ _Ω_ ( _A_ 2)∪ _H_, then there is a derivation for _L_ 2 from _Ω_ ( _A_ 1)∪ _Ω_ ( _A_ 2)∪\n_Δ_ ( _A_ 2) ∪ _H_ .\n2. There is at least one set _H_ [′] ⊆ _F_, _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] is non-contradictory,\nsuch that there is a derivation for _L_ 2 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] ∪ _Δ_ ( _A_ 2), there\nis no derivation for _L_ 2 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′], and there is no derivation for\n_L_ 1 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] ∪ _Δ_ ( _A_ 1).\n\n\nIntuitively, the principle of specificity says that, in the presence of two conflicting\nlines of argument about a proposition, the one that uses more of the available information is more convincing.A classic example involves a bird, Tweety, and arguments\nstating that it both flies (because it is a bird) and doesn’t fly (because it is a penguin).\nThe latter argument uses more information about Tweety—it is more specific—and\nis thus the stronger of the two.\n\n\n**Definition 4** (Martinez et al. 2012) Let _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) be a PreDeLP program. An argument ⟨ _A_ 1, _L_ 1⟩ is _preferred to_ ⟨ _A_ 2, _L_ 2⟩, denoted with _A_ 1 ≻ _A_ 2 iff any\nof the following conditions hold:\n\n\n1. ⟨ _A_ 1, _L_ 1⟩ and ⟨ _A_ 2, _L_ 2⟩ are both factual arguments and ⟨ _A_ 1, _L_ 1⟩≻ _PS_ ⟨ _A_ 2, _L_ 2⟩.\n2. ⟨ _A_ 1, _L_ 1⟩ is a factual argument and ⟨ _A_ 2, _L_ 2⟩ is a presumptive argument.\n3. ⟨ _A_ 1, _L_ 1⟩ and ⟨ _A_ 2, _L_ 2⟩ are presumptive arguments, and\na) ¬( _Φ_ ( _A_ 1) ⊆ _Φ_ ( _A_ 2)), or\nb) _Φ_ ( _A_ 1) = _Φ_ ( _A_ 2) and ⟨ _A_ 1, _L_ 1⟩≻ _PS_ ⟨ _A_ 2, _L_ 2⟩.\n\n\nGenerally, if _A_, _B_ are arguments with rules _X_ and _Y_, resp., and _X_ ⊂ _Y_, then _A_ is\nstronger than _B_ . This also holds when _A_ and _B_ use presumptions _P_ 1 and _P_ 2, resp.,\nand _P_ 1 ⊂ _P_ 2.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 163\n\n\n_Example 7_ The following are relationships between arguments from Example 5,\nbased on Definitions 3 and 4:\n\n\n_A_ 1 and _A_ 6 are incomparable (blocking defeaters);\n\n_A_ 6 ≻ _A_ 2, and thus _A_ 6 defeats _A_ 2;\n\n_A_ 6 ≻ _A_ 3, and thus _A_ 6 defeats _A_ 3;\n\n_A_ 6 ≻ _A_ 4, and thus _A_ 6 defeats _A_ 4;\n\n_A_ 5 and _A_ 7 are incomparable (blocking defeaters) _._     \n\nA sequence of arguments called an _argumentation line_ thus arises from this attack relation, where each argument defeats its predecessor. To avoid undesirable\nsequences, that may represent circular or fallacious argumentation lines, in DeLP\nan _argumentation line_ is _acceptable_ if it satisfies certain constraints (see García and\nSimari 2004). A literal _L_ is _warranted_ if there exists a non-defeated argument _A_\nsupporting _L_ .\nClearly, there can be more than one defeater for a particular argument ⟨ _A_, _L_ ⟩.\nTherefore, many acceptable argumentation lines could arise from ⟨ _A_, _L_ ⟩, leading\nto a tree structure. The tree is built from the set of all argumentation lines rooted\nin the initial argument. In a dialectical tree, every node (except the root) represents\na defeater of its parent, and leaves correspond to undefeated arguments. Each path\nfrom the root to a leaf corresponds to a different acceptable argumentation line. A\ndialectical tree provides a structure for considering all the possible acceptable argumentation lines that can be generated for deciding whether an argument is defeated.\nWe call this tree _dialectical_ because it represents an exhaustive dialectical analysis\n(in the sense of providing reasons for and against a position) for the argument in its\nroot. For argument ⟨ _A_, _L_ ⟩, we denote its dialectical tree with _T_ (⟨ _A_, _L_ ⟩).\nGiven a literal _L_ and an argument ⟨ _A_, _L_ ⟩, in order to decide whether or not a literal\n_L_ is warranted, every node in the dialectical tree _T_ (⟨ _A_, _L_ ⟩) is recursively marked as\n“D” ( _defeated_ ) or “U” ( _undefeated_ ), obtaining a marked dialectical tree _T_ [∗] (⟨ _A_, _L_ ⟩)\nwhere:\n\n\n- All leaves in _T_ [∗] (⟨ _A_, _L_ ⟩) are marked as “U”s, and\n\n- Let ⟨ _B_, _q_ ⟩ be an inner node of _T_ [∗] (⟨ _A_, _L_ ⟩). Then, ⟨ _B_, _q_ ⟩ will be marked as “U” iff\nevery child of ⟨ _B_, _q_ ⟩ is marked as “D”. Node ⟨ _B_, _q_ ⟩ will be marked as “D” iff it\nhas at least a child marked as “U”.\n\n\nGiven argument ⟨ _A_, _L_ ⟩ over _ΠAM_, if the root of _T_ [∗] (⟨ _A_, _L_ ⟩) is marked “U”, then\n_T_ [∗] (⟨ _A_, _h_ ⟩) _warrants L_ and that _L_ is _warranted_ from _ΠAM_ . (Warranted arguments\ncorrespond to those in the grounded extension of a Dung argumentation system\n(Dung 1995)).\nWe can then extend the idea of a dialectical tree to a _dialectical forest_ . For a\ngiven literal _L_, a dialectical forest _F_ ( _L_ ) consists of the set of dialectical trees for all\narguments for _L_ . We shall denote a marked dialectical forest, the set of all marked\ndialectical trees for arguments for _L_, as _F_ [∗] ( _L_ ). Hence, for a literal _L_, we say it is\n_warranted_ if there is at least one argument for that literal in the dialectical forest\n\n\n164 P. Shakarian et al.\n\n\n**Fig. 8.6** Example annotation function\n\n\n_F_ [∗] ( _L_ ) that is labeled “U”, _not warranted_ if there is at least one argument for literal\n¬ _L_ in the forest _F_ [∗] (¬ _L_ ) that is labeled “U”, and _undecided_ otherwise.\n\n\n**8.3** **The InCA Framework**\n\n\nHaving defined our environmental and analytical models ( _ΠEM_, _ΠAM_ respectively),\nwe now define how the two relate, which allows us to complete the definition of our\nInCA framework.\nThe key intuition here is that given a _ΠAM_, every element of _Ω_ ∪ _ΘΔ_ ∪ _Φ_\nmight only hold in certain worlds in the set _WEM_ —that is, worlds specified by the\nenvironment model. As formulas over the environmental atoms in set **G** _EM_ specify\nsubsets of _WEM_ (i.e., the worlds that satisfy them), we can use these formulas to\nidentify the conditions under which a component of _Ω_ ∪ _ΘΔ_ ∪ _Φ can be_ true.\nRecall that we use the notation _formulaEM_ to denote the set of all possible formulas\nover **G** _EM_ . Therefore, it makes sense to associate elements of _Ω_ ∪ _Θ_ ∪ _Φ_ with a\nformula from _formulaEM_ . In doing so, we can in turn compute the probabilities\nof subsets of _Ω_ ∪ _ΘΔ_ ∪ _Φ_ using the information contained in _ΠEM_, which we\nshall describe shortly. We first introduce the notion of _annotation function_, which\nassociates elements of _Ω_ ∪ _Θ_ ∪ _Φ_ with elements of _formulaEM_ .\nWe also note that, by using the annotation function (see Fig. 8.6), we may have\ncertain statements that appear as both facts and presumptions (likewise for strict and\ndefeasible rules). However, these constructs would have different annotations, and\nthus be applicable in different worlds. Suppose we added the following presumptions\nto our running example:\n\n\n_φ_ 3 = _evidOf_ ( _X_, _O_ ) **–** ≺, and\n_φ_ 4 = _motiv_ ( _X_, _X_ [′] ) **–** ≺.\n\n\nNote that these presumptions are constructed using the same formulas as facts\n_θ_ 1, _θ_ 2.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 165\n\n\nSuppose we extend _af_ as follows:\n\n\n_af_ ( _φ_ 3) = _malwInOp_ ( _M_, _O_ ) ∧ _malwareRel_ ( _M_, _M_ [′] ) ∧ _mwHint_ ( _M_ [′], _X_ )\n\n_af_ ( _φ_ 4) = _inLgConf_ ( _Y_, _X_ [′] ) ∧ _cooper_ ( _X_, _Y_ )\n\n\nSo, for instance, unlike _θ_ 1, _φ_ 3 can potentially be true in any world of the form:\n\n\n{ _malwInOp_ ( _M_, _O_ ), _malwareRel_ ( _M_, _M_ [′] ), _mwHint_ ( _M_ [′], _X_ )}\n\n\nwhile _θ_ 1 cannot be considered in any those worlds.\nWith the annotation function, we now have all the components to formally define\nan InCA framework.\n\n\n**Definition 5 (InCA Framework)** Given environmental model _ΠEM_, analytical\nmodel _ΠAM_, and annotation function _af_, _I_ = ( _ΠEM_, _ΠAM_, _af_ ) is an **InCA framework** .\nGiventhesetupdescribedabove, weconsidera _world-based_ approach—thedefeat\nrelationship among arguments will depend on the current state of the world (based\non the EM). Hence, we now define the status of an argument with respect to a given\nworld.\n\n\n**Definition 6 (Validity)** Given InCA framework _I_ = ( _ΠEM_, _ΠAM_, _af_ ), argument\n⟨ _A_, _L_ ⟩ is valid w.r.t. world _w_ ∈ _WEM_ iff ∀ _c_ ∈ _A_, _w_ |= _af_ ( _c_ ).\nIn other words, an argument is valid with respect to _w_ if the rules, facts, and\npresumptions in that argument are present in _w_ —the argument can then be built\nfrom information that is available in that world. In this paper, we extend the notion\nof validity to argumentation lines, dialectical trees, and dialectical forests in the\nexpected way (an argumentation line is valid w.r.t. _w_ iff all arguments that comprise\nthat line are valid w.r.t. _w_ ).\n\n\n_Example 8_ Consider worlds _w_ 1, _. . ._, _w_ 8 from Example 4 along with the argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ from Example 5. This argument is valid in worlds\n_w_ 1— _w_ 4, _w_ 6, and _w_ 7. We now extend the idea of a dialectical tree w.r.t. worlds—so, for a given world\n_w_ ∈ _WEM_, the dialectical (resp., marked dialectical) tree induced by _w_ is denoted\nby _Tw_ ⟨ _A_, _L_ ⟩ (resp., _Tw_ [∗][⟨] _[A]_ [,] _[ L]_ [⟩][). We require that all arguments and defeaters in these]\ntrees to be valid with respect to _w_ . Likewise, we extend the notion of dialectical\nforests in the same manner (denoted with _Fw_ ( _L_ ) and _Fw_ [∗][(] _[L]_ [), respectively). Based on]\nthese concepts we introduce the notion of _warranting scenario_ .\n\n\n**Definition 7 (Warranting Scenario)** Let _I_ = ( _ΠEM_, _ΠAM_, _af_ ) be an InCA framework and _L_ be a ground literal over **G** _AM_ ; a world _w_ ∈ _WEM_ is said to be a _warranting_\n_scenario_ for _L_ (denoted _w_ ⊢war _L_ ) iff there is a dialectical forest _Fw_ [∗][(] _[L]_ [) in which] _[ L]_\nis warranted and _Fw_ [∗][(] _[L]_ [) is valid w.r.t] _[ w]_ [.]\n\n\n_Example 9_ Following from Example 8, argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ is\nwarranted in worlds _w_ 3, _w_ 6, and _w_ 7. \n\n166 P. Shakarian et al.\n\n\nHence, the set of worlds in the EM where a literal _L_ in the AM _must_ be true is\nexactly the set of warranting scenarios—these are the “necessary” worlds, denoted:\n\n\n_nec_ ( _L_ ) = { _w_ ∈ _WEM_ | ( _w_ ⊢war _L_ )} _._\n\n\nNow, the set of worlds in the EM where AM literal _L can_ be true is the following—\nthese are the “possible” worlds, denoted:\n\n\n_poss_ ( _L_ ) = { _w_ ∈ _WEM_ | _w_ ̸⊢war ¬ _L_ } _._\n\n\nThe following example illustrates these concepts.\n\n\n_Example 10_ Following from Example 8:\n\n\n_nec_ ( _isCap_ ( _baja_, _worm123_ )) = { _w_ 3, _w_ 6, _w_ 7} and\n\n\n_poss_ ( _isCap_ ( _baja_, _worm123_ )) = { _w_ 1, _w_ 2, _w_ 3, _w_ 4, _w_ 6, _w_ 7} _._     \n\nHence, for a given InCA framework _I_, if we are given a probability distribution\n_Pr_ over the worlds in the EM, then we can compute an upper and lower bound on\nthe probability of literal _L_ (denoted **P** _L_, _Pr_, _I_ ) as follows:\n\n\n      _ℓL_, _Pr_, _I_ = _Pr_ ( _w_ ),\n\n_w_ ∈ _nec_ ( _L_ )\n\n\n\nand\n\n\n\n\n  _uL_, _Pr_, _I_ = _Pr_ ( _w_ ),\n\n_w_ ∈ _poss_ ( _L_ )\n\n\n_ℓL_, _Pr_, _I_ ≤ **P** _L_, _Pr_, _I_ ≤ _uL_, _Pr_, _I._\n\n\n\nNow let us consider the computation of probability bounds on a literal when we\nare given a knowledge base _ΠEM_ in the environmental model, which is specified in\n_I_, instead of a probability distribution over all worlds. For a given world _w_ ∈ _WEM_,\nlet _f or_ ( _w_ ) = _(_ [�] _a_ ∈ _w_ _[a)]_ [ ∧] _[(]_ [ �] _a /_ ∈ _w_ [¬] _[a)]_ [—that is, a formula that is satisfied only by]\nworld _w_ . Now we can determine the upper and lower bounds on the probability of a\nliteral w.r.t. _ΠEM_ (denoted **P** _L_, _I_ ) as follows:\n\n\n\n_ℓL_, _I_ = EP-LP-MIN\n\n\n_uL_, _I_ = EP-LP-MAX\n\n\n\n⎛ ⎞\n\n  ⎝ _ΠEM_, _f or_ ( _w_ )⎠,\n\n_w_ ∈ _nec_ ( _L_ )\n\n\n⎛ ⎞\n\n\n\n\n  ⎝ _ΠEM_, _f or_ ( _w_ )⎠,\n\n_w_ ∈ _poss_ ( _L_ )\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 167\n\n\nand\n\n\n_ℓL_, _I_ ≤ **P** _L_, _I_ ≤ _uL_, _I._\n\n\nHence, we have:\n\n\n\n\n   **P** _L_, _I_ = _ℓL_, _I_ + _[u][L]_ [,] _[I]_ [−] _[ℓ][L]_ [,] _[I]_\n\n2\n\n\n\n\n\n[−] _[ℓ][L]_ [,] _[I]_\n± _[u][L]_ [,] _[I]_ _._\n\n2\n\n\n\n_Example 11_ Following from Example 8, argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩,\nwe can compute **P** _isCap_ ( _baja_, _worm123_ ), _I_ (where _I_ = ( _ΠEM_ [′] [,] _[ Π][AM]_ [,] _[ af]_ [)). Note that for]\nthe upper bound, the linear program we need to set up is as in Example 4. For the\nlower bound, the objective function changes to: min _x_ 3 + _x_ 6 + _x_ 7. From these linear\nconstraints, we obtain: **P** _isCap_ ( _baja_, _worm123_ ), _I_ = 0 _._ 75 ± 0 _._ 25 _._ \n\n**8.4** **Attribution Queries**\n\n\nWe now have the necessary elements required to formally define the kind of queries\nthat correspond to the attribution problems studied in this paper.\n\n\n**Definition 8** Let _I_ = ( _ΠEM_, _ΠAM_, _af_ ) be an InCA framework, _S_ ⊆ **C** _act_ (the set of\n“suspects”), _O_ ∈ **C** _ops_ (the “operation”), and _E_ ⊆ **G** _EM_ (the “evidence”). An actor\nA ∈ _S_ is said to be a _most probable suspect_ iff there does not exist A [′] ∈ _S_ such that\n**P** _condOp_ ( **A** [′], _O_ ), _I_ ′ _>_ **P** _condOp_ ( _A_, _O_ ) _I_ ′ where _I_ [′] = ( _ΠEM_ ∪ _ΠE_, _ΠAM_, _af_ [′] ) with _ΠE_ defined\nas [�] _c_ ∈ _E_ [{] _[c]_ [ : 1][ ±][ 0][}][.]\nGiven the above definition, we refer to _Q_ = ( _I_, _S_, _O_, _E_ ) as an _attribution query_,\nand A as an _answer_ to _Q_ . We note that in the above definition, the items of evidence\nare added to the environmental model with a probability of 1 ± 0. While in general\nthis may be the case, there are often instances in analysis of a cyber-operation where\nthe evidence may be true with some degree of uncertainty. Allowing for probabilistic\nevidence is a simple extension to Definition 8 that does not cause any changes to the\nresults of this paper.\nTo understand how uncertain evidence can be present in a cyber-security scenario,\nconsiderthefollowing. InSymantec’sinitialanalysisoftheStuxnetworm, theyfound\nthe routine designed to attack the S7-417 logic controller was incomplete, and hence\nwould not function (Falliere et al. 2011). However, industrial control system expert\nRalph Langner claimed that the incomplete code would run provided a missing\ndata block is generated, which he thought was possible (Langner et al. 2011). In\nthis case, though the code was incomplete, there was clearly uncertainty regarding\nits usability. This situation provides a real-world example of the need to compare\narguments—in this case, in the worlds where both arguments are valid, Langner’s\nargument would likely defeat Symantec’s by generalized specificity (the outcome,\nof course, will depend on the exact formalization of the two). Note that Langner was\n\n\n168 P. Shakarian et al.\n\n\nlater vindicated by the discovery of an older sample, Stuxnet 0.5, which generated\nthe data block. [1]\n\nInCA also allows for a variety of relevant scenarios to the attribution problem.\nFor instance, we can easily allow for the modeling of non-state actors by extending\nthe available constants—for example, traditional groups such as Hezbollah, which\nhas previously wielded its cyber-warfare capabilities in operations against Israel\n(Shakarian et al. 2013). Likewise, the InCA can also be used to model cooperation\namong different actors in performing an attack, including the relationship between\nnon-state actors and nation-states, such as the potential connection between Iran and\nmilitants stealing UAV feeds in Iraq, or the much-hypothesized relationship between\nhacktivist youth groups and the Russian government (Shakarian et al. 2013).Another\naspect that can be modeled is deception where, for instance, an actor may leave false\nclues in a piece of malware to lead an analyst to believe a third party conducted\nthe operation. Such a deception scenario can be easily created by adding additional\nrules in the AM that allow for the creation of such counter-arguments. Another type\nof deception that could occur include attacks being launched from a system not in\nthe responsible party’s area, but under their control (e.g., see Shadows in the Cloud\n2010). Again, modeling who controls a given system can be easily accomplished in\nour framework, and doing so would simply entail extending an argumentation line.\nFurther, campaigns of cyber-operations can also be modeled, as well as relationships\namong malware and/or attacks (as detailed in APT1 2013).\nAs with all of these abilities, InCA provides the analyst the means to model a\ncomplex situation in cyber-warfare but saves him from carrying out the reasoning\nassociated with such a situation. Additionally, InCA results are constructive, so an\nanalyst can “trace-back” results to better understand how the system arrived at a\ngiven conclusion.\n\n\n**8.5** **Open Questions**\n\n\nIn this section we review some major areas of research to address to move InCA\ntoward a deployed system.\n\n\n_**8.5.1**_ _**Rule Learning**_\n\n\nThe InCA framework depends on logical rules and statements as part of the input,\nthough there are existing bodies of work we can leverage (decision tree rule learning,\ninductive logic programming, etc.) there are some specific challenges with regard to\nInCA that we must account for, specifically:\n\n\n1 http://www.symantec.com/connect/blogs/stuxnet-05-disrupting-uranium-processing-natanz.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 169\n\n\n- Quickly learning probabilistic rules from data received as an input stream\n\n- Learning of the annotation function\n\n- Identification of the diagnosticity of new additions to the knowledgebase\n\n- Learning rules that combine multiple, disparate sources (i.e. malware analysis\nand PCAP files, for instance)\n\n\n_**8.5.2**_ _**Belief Revision**_\n\n\nEven though we allow for inconsistencies in the AM portion of the model, inconsistency can arise even with a consistent EM. In a companion paper, (Shakarian et al.\n2014) we introduce the following notion of consistency.\n\n\n**Definition 9** InCA program _I_ = ( _ΠEM_, _ΠAM_, _af_ ), with _ΠAM_ = ⟨ _Θ_, _Ω_, _Φ_, _Δ_ ⟩, is\n_Type II consistent_ iff: given any probability distribution _Pr_ that satisfies _ΠEM_, if\nthere exists a world _w_ ∈ _WEM_ such that [�] _x_ ∈ _Θ_ ∪ _Ω_ | _w_ |= _af_ ( _x_ ) [{] _[x]_ [}][ is inconsistent, then we]\nhave _Pr_ ( _w_ ) = 0.\nThus, any EM world in which the set of associated facts and strict rules are\ninconsistent (we refer to this as “classical consistency”) must always be assigned a\nzero probability. The intuition is as follows: any subset of facts and strict rules are\nthought to be true under certain circumstances—these circumstances are determined\nthrough the annotation function and can be expressed as sets of EM worlds. Suppose\nthere is a world where two contradictory facts can both be considered to be true (based\non the annotation function). If this occurs, then there must not exist a probability\ndistribution that satisfies the program _ΠEM_ that assigns such a world a non-zero\nprobability, as this world leads to an inconsistency.\nWhile we have studied this theoretically (Shakarian et al. 2014), several important\nchallenges remain: How do different belief revision methods affect the results of\nattribution queries? In particular, can we develop tractable algorithms for belief\nrevision in the InCA framework? Further, finding efficient methods for re-computing\nattribution queries following a belief revision operation is a related concern for future\nwork.\n\n\n_**8.5.3**_ _**Temporal Reasoning**_\n\n\nCyber-security data often has an inherent temporal component (in particular, PCAP\nfiles, system logs, and traditional intelligence). One way to represent this type of\ninformation in InCA is by replacing the EM with a probabilistic temporal logic (i.e.\nHansson and Jonsson 1994; Dekhtyar et al. 1999; Shakarian et al. 2011; Shakarian\nand Simari 2012). However, even though this would be a relatively straightforward\nadjustment to the framework, it leads to several interesting questions, specifically:\n\n\n170 P. Shakarian et al.\n\n\n- Can we identify hacking groups responsible for a series of incidents over a period\nof time (a cyber campaign)?\n\n- Can we identify the group responsible for a campaign if it is not known a priori?\n\n- Can we differentiate between multiple campaigns conducted by multiple culprits\nin time-series data?\n\n\n_**8.5.4**_ _**Abductive Inference Queries**_\n\n\nWe may often have a case where more than one culprit is attributed to the same\ncyber-attack with nearly the same probabilities. In this case, can we identify certain\nevidence that, if found, can lead us to better differentiate among the potential culprits?\nIn the intelligence community, this is often referred as identifying _intelligence gaps_ .\nWe can also frame this as an abductive inference problem (Reggia and Peng 1990).\nThis type of problems leads to several interesting challenges:\n\n\n- Can we identify all pieces of diagnostic evidence that would satisfy an important\nintelligence gap?\n\n- Can we identify diagnostic evidence under constraints (i.e., taking into account\nlimitations on the type of evidence that can be collected)?\n\n- In the case where a culprit is attributed with a high probability, can we identify\nevidence that can falsify the finding?\n\n\n**8.6** **Conclusions**\n\n\nIn this paper we introduced InCA, a new framework that allows the modeling of various cyber-warfare/cyber-security scenarios in order to help answer the attribution\nquestion by means of a combination of probabilistic modeling and argumentative\nreasoning. This is the first framework, to our knowledge, that addresses the attribution problem while allowing for multiple pieces of evidence from different sources,\nincluding traditional (non-cyber) forms of intelligence such as human intelligence.\nFurther, our framework is the first to extend Defeasible Logic Programming with\nprobabilistic information. Currently, we are implementing InCA along with the\nassociated algorithms and heuristics to answer these queries.\n\n\n**Acknowledgments** This work was supported by UK EPSRC grant EP/J008346/1—“PrOQAW”,\nERC grant 246858—“DIADEM”, by NSF grant #1117761, by the National Security Agency under\nthe Science of Security Lablet grant (SoSL), Army Research Office project 2GDATXR042, and\nDARPA project R.0004972.001.\n\n\n8 Cyber Attribution: An Argumentation-Based Approach 171\n\n\n**References**\n\n\nShadows in the Cloud: Investigating Cyber Espionage 2.0. Tech. rep., Information Warfare Monitor\nand Shadowserver Foundation (2010)\nAPT1: Exposing one of China’s cyber espionage units. Mandiant (tech. report) (2013)\nAltheide, C.: Digital Forensics with Open Source Tools. Syngress (2011)\nDekhtyar,A., Dekhtyar, M.I., Subrahmanian, V.S.: Temporal probabilistic logic programs. In: ICLP\n1999, pp. 109–123. The MIT Press, Cambridge, MA, USA (1999)\nDung, P.M.: On the acceptability of arguments and its fundamental role in nonmonotonic reasoning,\nlogic programming and _n_ -person games. Artif. Intell. **77**, pp. 321–357 (1995)\nFalliere, N., Murchu, L.O., Chien, E.: W32.Stuxnet Dossier Version 1.4. Symantec Corporation\n(2011)\nGarcía, A.J., Simari, G.R.: Defeasible logic programming: An argumentative approach. TPLP\n**4** (1–2), 95–138 (2004)\nHansson, H., Jonsson, B.: A logic for reasoning about time and probability. Formal Aspects of\nComputing **6**, 512–535 (1994)\nHeuer, R.J.: Psychology of Intelligence Analysis. Center for the Study of Intelligence (1999)\nKhuller, S., Martinez, M.V., Nau, D.S., Sliva, A., Simari, G.I., Subrahmanian, V.S.: Computing\nmost probable worlds of action probabilistic logic programs: scalable estimation for 10 [30,000]\n\nworlds. AMAI **51(2–4)**, 295–331 (2007)\nLangner, R.: Matching Langner Stuxnet analysis and Symantic dossier update. Langner Communications GmbH (2011)\nLloyd, J.W.: Foundations of Logic Programming, 2nd Edition. Springer (1987)\nMartinez, M.V., García, A.J., Simari, G.R.: On the use of presumptions in structured defeasible\nreasoning. In: Proc. of COMMA, pp. 185–196 (2012)\nNilsson, N.J.: Probabilistic logic. Artif. Intell. **28** (1), 71–87 (1986)\nRahwan, I., Simari, G.R.: Argumentation in Artificial Intelligence. Springer (2009)\nReggia, J.A., Peng,Y.:Abductive inference models for diagnostic problem-solving. Springer-Verlag\nNew York, Inc., New York, NY, USA (1990)\nShakarian, P., Parker, A., Simari, G.I., Subrahmanian, V.S.: Annotated probabilistic temporal logic.\nTOCL **12** (2), 14 (2011)\nShakarian, P., Simari, G.I., Subrahmanian, V.S.: Annotated probabilistic temporal logic: Approximate fixpoint implementation. ACM Trans. Comput. Log. **13** (2), 13 (2012)\nShakarian, P., Shakarian, J., Ruef, A.: Introduction to Cyber-Warfare: A Multidisciplinary\nApproach. Syngress (2013)\nShakarian, P., Simari, G.I., Falappa, M.A.: Belief revision in structured probabilistic argumentation.\nIn: Proceedings of FoIKS, pp. 324–343 (2014)\nSimari, G.R., Loui, R.P.: A mathematical treatment of defeasible reasoning and its implementation.\nArtif. Intell. **53** (2-3), 125–157 (1992)\nSimari, G.I., Martinez, M.V., Sliva, A., Subrahmanian, V.S.: Focused most probable world\ncomputations in probabilistic logic programs. AMAI **64** (2–3), 113–143 (2012)\nSpitzner, L.: Honeypots: Catching the Insider Threat. In: Proc. ofACSAC 2003, pp. 170–179. IEEE\nComputer Society (2003)\nStolzenburg, F., García, A., Chesñevar, C.I., Simari, G.R.: Computing Generalized Specificity.\nJournal of Non-Classical Logics **13** (1), 87–113 (2003)\nThonnard, O., Mees, W., Dacier, M.: On a multicriteria clustering approach for attack attribution.\nSIGKDD Explorations **12** (1), 11–20 (2010)",
    "references": []
  },
  "summary": {
    "full_text": {
      "words": 8890,
      "tokens": 9281
    },
    "flat_text": {
      "words": 8890,
      "tokens": 9281
    }
  },
  "payload": {},
  "summary_log": {
    "mode": "offline_fallback",
    "reason": "missing_full_text",
    "method": "pymupdf4llm"
  }
}