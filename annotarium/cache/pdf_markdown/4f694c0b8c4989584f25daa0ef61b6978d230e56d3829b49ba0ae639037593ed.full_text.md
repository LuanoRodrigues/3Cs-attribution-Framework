## **Chapter 8**
# **Cyber Attribution: An Argumentation-Based** **Approach**

**Paulo Shakarian, Gerardo I. Simari, Geoffrey Moores and Simon Parsons**


**Abstract** Attributing a cyber-operation through the use of multiple pieces of
technical evidence (i.e., malware reverse-engineering and source tracking) and conventional intelligence sources (i.e., human or signals intelligence) is a difficult
problem not only due to the effort required to obtain evidence, but the ease with
which an adversary can plant false evidence. In this paper, we introduce a formal reasoning system called the InCA (Intelligent Cyber Attribution) framework
that is designed to aid an analyst in the attribution of a cyber-operation even when
the available information is conflicting and/or uncertain. Our approach combines
argumentation-based reasoning, logic programming, and probabilistic models to not
only attribute an operation but also explain to the analyst why the system reaches its
conclusions.


**8.1** **Introduction**


An important issue in cyber-warfare is the puzzle of determining who was responsible
for a given cyber-operation—be it an incident of attack, reconnaissance, or information theft. This is known as the “attribution problem” (Shakarian et al. 2013).
The difficulty of this problem stems not only from the amount of effort required to
find forensic clues but also the ease with which an attacker can plant false clues to


P. Shakarian (�)
Arizona Sate University, Tempe, AZ, USA
e-mail: shak@asu.edu


G. I. Simari
Department of Computer Science and Engineering, Universidad Nacional del Sur,
Bahía Blanca, Argentina
e-mail: gis@cs.uns.edu.ar


G. Moores
Department of Electrical Engineering and Computer Science, U.S. Military Academy,
West Point, NY, USA
e-mail: geoffrey.moores@usma.edu


S. Parsons
Department of Computer Science, University of Liverpool, Liverpool, UK
e-mail: s.d.parsons@liverpool.ac.uk

© Springer International Publishing Switzerland 2015 151
S. Jajodia et al. (eds.), _Cyber Warfare,_ Advances in Information Security 56,
DOI 10.1007/978-3-319-14039-1_8


152 P. Shakarian et al.


mislead security personnel. Further, while techniques such as forensics and reverseengineering (Altheide 2011), source tracking (Thonnard et al. 2010), honeypots
(Spitzner 2003), and sinkholing 2010 are commonly employed to find evidence that
can lead to attribution, it is unclear how this evidence is to be combined and reasoned
about. In a military setting, such evidence is augmented with normal intelligence collection, such as human intelligence (HUMINT), signals intelligence (SIGINT) and
other means—this adds additional complications to the task of attributing a given
operation. Essentially, cyber-attribution is a highly-technical intelligence analysis
problem where an analyst must consider a variety of sources, each with its associated level of confidence, to provide a decision maker (e.g., a military commander)
insight into who conducted a given operation.
As it is well known that people’s ability to conduct intelligence analysis is limited
(Heuer 1999), and due to the highly technical nature of many cyber evidencegathering techniques, an automated reasoning system would be best suited for the
task. Such a system must be able to accomplish several goals, among which we
distinguish the following main capabilities:


1. Reason about evidence in a formal, principled manner, i.e., relying on strong
mathematical foundations.
2. Consider evidence for cyber attribution associated with some level of probabilistic
uncertainty.
3. Consider logical rules that allow for the system to draw conclusions based on
certain pieces of evidence and iteratively apply such rules.
4. Consider pieces of information that may not be compatible with each other, decide
which information is most relevant, and express why.
5. Attribute a given cyber-operation based on the above-described features and provide the analyst with the ability to understand how the system arrived at that
conclusion.


In this paper we present the InCA (Intelligent Cyber Attribution) framework, which
meets all of the above qualities. Our approach relies on several techniques from
the artificial intelligence community, including argumentation, logic programming,
and probabilistic reasoning. We first outline the underlying mathematical framework and provide a running example based on real-world cases of cyber-attribution
(cf. Sect. 8.2); then, in Sects. 8.3 and 8.4, we formally present InCA and attribution
queries, respectively. Finally, we discuss conclusions and future work in Sect. 8.6.


**8.2** **Two Kinds of Models**


Our approach relies on _two separate models of the world_ . The first, called the _**environ-**_
_**mentalmodel**_ (EM)isusedtodescribethebackgroundknowledgeandisprobabilistic
in nature. The second one, called the _**analytical model**_ (AM) is used to analyze
competing hypotheses that can account for a given phenomenon (in this case, a
cyber-operation). The EM _must be consistent_ —this simply means that there must


8 Cyber Attribution: An Argumentation-Based Approach 153


EM AM
“Malware X was compiled on a system “Malware X was compiled on a system
using the English language.” English-speaking country Y.”
“Malware W and malware X were created “Malware W and malware X are related.”
in a similar coding sytle.”
“Country Y and country Z are currently “Country Y has a motive to launch a
at war.” cybre-attack against country Z.”
“Country Y has a significant investment “Country Y has the capability to conduct
in math-science-engineering (MSE) education.” a cyber-attack.”


**Fig. 8.1** Example observations—EM vs. AM


exist a probability distribution over the possible states of the world that satisfies all
of the constraints in the model, as well as the axioms of probability theory. On the
contrary, the AM will allow for contradictory information as the system must have
the capability to reason about competing explanations for a given cyber-operation.
In general, the EM contains knowledge such as evidence, intelligence reporting, or
knowledge about actors, software, and systems. TheAM, on the other hand, contains
ideas the analyst concludes based on the information in the EM. Figure 8.1 gives
some examples of the types of information in the two models. Note that an analyst
(or automated system) could assign a probability to statements in the EM column
whereas statements in the AM column can be true or false depending on a certain
combination (or several possible combinations) of statements from the EM. We now
formally describe these two models as well as a technique for _annotating_ knowledge
in the AM with information from the EM—these annotations specify the conditions
under which the various statements in the AM can potentially be true.
Before describing the two models in detail, we first introduce the language used
to describe them. Variable and constant symbols represent items such as computer
systems, types of cyber operations, actors (e.g., nation states, hacking groups), and
other technical and/or intelligence information. The set of all variable symbols is
denoted with **V**, and the set of all constants is denoted with **C** . For our framework, we
shallrequiretwosubsetsof **C**, **C** _act_ and **C** _ops_, thatspecifytheactorsthatcouldconduct
cyber-operations and the operations themselves, respectively. In the examples in this
paper, we will use capital letters to represent variables (e.g., _X_, _Y_, _Z_ ). The constants
in **C** _act_ and **C** _ops_ that we use in the running example are specified in the following
example.


_Example 1_ The following (fictitious) actors and cyber-operations will be used in
our examples:


**C** _act_ = { _baja_, _krasnovia_, _mojave_ } (8.1)

**C** _ops_ = { _worm123_ } (8.2)


                     The next component in the model is a set of predicate symbols. These constructs
can accept zero or more variables or constants as arguments, and map to either


154 P. Shakarian et al.


**P** _EM_ : _origIP_ ( _M,_ _X_ ) Malware _M_ originated from an IP address belonging to actor _X_ .
_malwInOp_ ( _M,_ _O_ ) Malware _M_ was used in cyber-operation _O_ .
_mwHint_ ( _M,_ _X_ ) Malware _M_ contained a hint that it was created by actor _X_ .
_compilLang_ ( _M,C_ ) Malware _M_ was compiled in a system that used language _C_ .
_nativLang_ ( _X,C_ ) Language _C_ is the native language of actor _X_ .
_inLgConf_ ( _X,_ _X_ _[′]_ ) Actors _X_ and _X_ _[′]_ are in a larger conflict with each other.
_mseTT_ ( _X,_ _N_ ) There are at least _N_ number of top-tier math-science-engineering
universities in country _X_ .
_infGovSys_ ( _X,_ _M_ ) Systems belonging to actor _X_ were infected with malware _M_ .
_cybCapAge_ ( _X,_ _N_ ) Actor _X_ has had a cyber-warfare capability for _N_ years or less.
_govCybLab_ ( _X_ ) Actor _X_ has a government cyber-security lab.


**P** _AM_ : _condOp_ ( _X,_ _O_ ) Actor _X_ conducted cyber-operation _O_ .
_evidOf_ ( _X,_ _O_ ) There is evidence that actor _X_ conducted cyber-operation _O_ .
_motiv_ ( _X,_ _X_ _[′]_ ) Actor _X_ had a motive to launch a cyber-attack against actor _X_ _[′]_ .
_isCap_ ( _X,_ _O_ ) Actor _X_ is capable of conducting cyber-operation _O_ .
_tgt_ ( _X,_ _O_ ) Actor _X_ was the target of cyber-operation _O_ .
_hasMseInvest_ ( _X_ ) Actor _X_ has a significant investment in math-science-engineering
education.
_expCw_ ( _X_ ) Actor _X_ has experience in conducting cyber-operations.


**Fig. 8.2** Predicate definitions for the environment and analytical models in the running example


_true_ or false. _Note that the EM and AM use separate sets of predicate symbols_ however, they can share variables and constants. The sets of predicates for the
EM and AM are denoted with **P** _EM_, **P** _AM_, respectively. In InCA, we require **P** _AM_
to include the binary predicate _condOp_ ( _X_, _Y_ ), where _X_ is an actor and _Y_ is a cyberoperation. Intuitively, this means that actor _X_ conducted operation _Y_ . For instance,
_condOp_ ( _baja_, _worm123_ )istrueif _baja_ wasresponsibleforcyber-operation _worm123_ .
A sample set of predicate symbols for the analysis of a cyber attack between two
states over contention of a particular industry is shown in Fig. 8.2; these will be used
in examples throughout the paper.
A construct formed with a predicate and constants as arguments is known as a
_ground atom_ (we shall often deal with ground atoms). The sets of all ground atoms
for EM and AM are denoted with **G** _EM_ and **G** _AM_, respectively.


_Example 2_ The following are examples of ground atoms over the predicates given
in Fig. 8.2.


**G** _EM_ : _origIP_ ( _mw123sam1_, _krasnovia_ ),


_mwHint_ ( _mw123sam1_, _krasnovia_ ),


_inLgConf_ ( _krasnovia_, _baja_ ),


_mseTT_ ( _krasnovia_, 2) _._


**G** _AM_ : _evidOf_ ( _mojave_, _worm123_ ),


_motiv_ ( _baja_, _krasnovia_ ),


_expCw_ ( _baja_ ),


_tgt_ ( _krasnovia_, _worm123_ ) _._                

8 Cyber Attribution: An Argumentation-Based Approach 155


For a given set of ground atoms, a _world_ is a subset of the atoms that are considered
to be true (ground atoms not in the world are false). Hence, there are 2 [|] **[G]** _[EM]_ [|] possible worlds in the EM and 2 [|] **[G]** _[AM]_ [|] worlds in the AM, denoted with _WEM_ and _WAM_,
respectively.
Clearly, even a moderate number of ground atoms can yield an enormous number
of worlds to explore. One way to reduce the number of worlds is to include _integrity_
_constraints_, which allow us to eliminate certain worlds from consideration—they
simplyarenotpossibleinthesettingbeingmodeled. Ourprincipleintegrityconstraint
will be of the form:


oneOf( _A_ [′] )


where _A_ [′] is a subset of ground atoms. Intuitively, this says that any world where
more than one of the atoms from set _A_ [′] appear is invalid. Let **IC** _EM_ and **IC** _AM_ be the
sets of integrity constraints for the EM and AM, respectively, and the sets of worlds
that conform to these constraints be _WEM_ ( **IC** _EM_ ), _WAM_ ( **IC** _AM_ ), respectively.
Atoms can also be combined into formulas using standard logical connectives:
conjunction ( _and_ ), disjunction ( _or_ ), and negation ( _not_ ). These are written using the
symbols ∧, ∨, ¬, respectively. We say a world ( _w_ ) _satisfies_ a formula ( _f_ ), written
_w_ |= _f_, based on the following inductive definition:


- if _f_ is a single atom, then _w_ |= _f_ iff _f_ ∈ _w_ ;

- if _f_ = ¬ _f_ [′] then _w_ |= _f_ iff _w_ ̸|= _f_ [′] ;

- if _f_ = _f_ [′] ∧ _f_ [′′] then _w_ |= _f_ iff _w_ |= _f_ [′] and _w_ |= _f_ [′′] ; and

- if _f_ = _f_ [′] ∨ _f_ [′′] then _w_ |= _f_ iff _w_ |= _f_ [′] or _w_ |= _f_ [′′] .


We use the notation _f ormulaEM_, _f ormulaAM_ to denote the set of all possible
(ground) formulas in the EM andAM, respectively.Also, note that we use the notation
⊤, ⊥ to represent tautologies (formulas that are true in all worlds) and contradictions
(formulas that are false in all worlds), respectively.


_**8.2.1**_ _**Environmental Model**_


In this section we describe the first of the two models, namely the EM or environmental model. This model is largely based on the probabilistic logic of (Nilsson 1986),
which we now briefly review.
First, we define a _probabilistic formula_ that consists of a formula _f_ over
atoms from **G** _EM_, a real number _p_ in the interval [0, 1], and an error tolerance
_ε_ ∈ [0, min( _p_, 1 − _p_ )]. A probabilistic formula is written as: _f_ : _p_ ± _ε_ . Intuitively,
this statement is interpreted as “formula _f_ is true with probability between _p_ - _ε_
and _p_ + _ε_ ”—note that we make no statement about the probability distribution over
this interval. The uncertainty regarding the probability values stems from the fact
that certain assumptions (such as probabilistic independence) may not be suitable in
the environment being modeled.


156 P. Shakarian et al.


_Example 3_ To continue our running example, consider the following set _ΠEM_ :


_f_ 1 = _govCybLab_ ( _baja_ ) : 0 _._ 8 ± 0 _._ 1

_f_ 2 = _cybCapAge_ ( _baja_, 5) : 0 _._ 2 ± 0 _._ 1

_f_ 3 = _mseTT_ ( _baja_, 2) : 0 _._ 8 ± 0 _._ 1

_f_ 4 = _mwHint_ ( _mw123sam1_, _mojave_ ) ∧ _compilLang_ ( _worm123_, _english_ ) : 0 _._ 7 ± 0 _._ 2

_f_ 5 = _malwInOp_ ( _mw123sam1_, _worm123_ )


∧ _malwareRel_ ( _mw123sam1_, _mw123sam2_ )


∧ _mwHint_ ( _mw123sam2_, _mojave_ ) : 0 _._ 6 ± 0 _._ 1


_f_ 6 = _inLgConf_ ( _baja_, _krasnovia_ ) ∨¬ _cooper_ ( _baja_, _krasnovia_ ) : 0 _._ 9 ± 0 _._ 1

_f_ 7 = _origIP_ ( _mw123sam1_, _baja_ ) : 1 ± 0


Throughout other examples in the rest of the paper, we will make use of the subset
_ΠEM_ [′] [= {] _[f]_ [1][,] _[ f]_ [2][,] _[ f]_ [3][}][.] We now consider a probability distribution _Pr_ over the set _WEM_ ( **IC** _EM_ ). We
say that _Pr satisfies_ probabilistic formula _f_ : _p_ ± _ε_ iff the following holds:
_p_ - _ε_ ≤ [�] _w_ ∈ _WEM_ ( **IC** _EM_ ) _[Pr]_ [(] _[w]_ [)][ ≤] _[p]_ [ +] _[ ε.]_ [ A set] _[ Π][EM]_ [ of probabilistic formulas is]
called a _knowledge base_ . We say that a probability distribution over _WEM_ ( **IC** _EM_ )
_satisfies ΠEM_ if and only if it satisfies all probabilistic formulas in _ΠEM_ .
It is possible to create probabilistic knowledge bases for which there is no
satisfying probability distribution. The following is a simple example of this:


_condOp_ ( _krasnovia_, _worm123_ ) ∨ _condOp_ ( _baja_, _worm123_ ) : 0 _._ 4 ± 0;


_condOp_ ( _krasnovia_, _worm123_ ) ∧ _condOp_ ( _baja_, _worm123_ ) : 0 _._ 6 ± 0 _._ 1 _._


Formulas and knowledge bases of this sort are _inconsistent_ . In this paper, we assume
that information is properly extracted from a set of historic data and hence consistent;
(recall that inconsistent information can only be handled in the AM, not the EM). A
consistent knowledge base could also be obtained as a result of curation by experts,
such that all inconsistencies were removed—see (Khuller et al. 2007; Shakarian et
al. 2011) for algorithms for learning rules of this type.
The main kind of query that we require for the probabilistic model is the _maximum_
_entailment_ problem: given a knowledge base _ΠEM_ and a (non-probabilistic) formula
_q_, identify _p_, _ε_ such that all valid probability distributions _Pr_ that satisfy _ΠEM_ also
satisfy _q_ : _p_ ± _ε_, and there does not exist _p_ [′], _ε_ [′] s.t. [ _p_ - _ε_, _p_ + _ε_ ] ⊃ [ _p_ [′] - _ε_ [′], _p_ [′] + _ε_ [′] ],
where all probability distributions _Pr_ that satisfy _ΠEM_ also satisfy _q_ : _p_ [′] ± _ε_ [′] . That
is, given _q_, can we determine the probability (with maximum tolerance) of statement
_q_ given the information in _ΠEM_ ? The approach adopted in (Nilsson et al. 1986) to
solve this problem works as follows. First, we must solve the linear program defined
next.


8 Cyber Attribution: An Argumentation-Based Approach 157


**Definition 1** ( **EM-LP-MIN)** Given a knowledge base _ΠEM_ and a formula _q_ :


- create a variable _xi_ for each _wi_ ∈ _WEM_ ( **IC** _EM_ );

- for each _fj_ : _pj_ ± _εj_ ∈ _ΠEM_, create constraint:

      _pj_          - _εj_ ≤ _xi_ ≤ _pj_ + _εj_ ;

_wi_ ∈ _WEM_ ( **IC** _EM_ ) _s.t.wi_ |= _fj_


- finally, we also have a constraint:

      
_xi_ = 1 _._
_wi_ ∈ _WEM_ ( **IC** _EM_ )


The objective is to minimize the function:

      
_xi._
_wi_ ∈ _WEM_ ( **IC** _EM_ ) _s.t.wi_ |= _q_


We use the notation EP-LP-MIN( _ΠEM_, _q_ ) to refer to the value of the objective
function in the solution to the EM-LP-MIN constraints.
Let _ℓ_ be the result of the process described in Definition 1. The next step is to
solve the linear program a second time, but instead maximizing the objective function
(we shall refer to this as EM-LP-MAX)—let _u_ be the result of this operation. In
(Nilsson 1986), it is shown that _ε_ = _u_ −2 _ℓ_ and _p_ = _ℓ_ + _ε_ is the solution to the

maximum entailment problem. We note that although the above linear program has
an exponential number of variables in the worst case (i.e., no integrity constraints),
the presence of constraints has the potential to greatly reduce this space. Further,
there are also good heuristics (cf. Khuller et al. 2007; Simari et al. 2012) that have
been shown to provide highly accurate approximations with a reduced-size linear
program.

_Example 4_ Consider KB _ΠEM_ [′] [from Example][ 3][ and a set of ground atoms restricted]
to those that appear in that program. Hence, we have:


_w_ 1 = { _govCybLab_ ( _baja_ ), _cybCapAge_ ( _baja_, 5), _mseTT_ ( _baja_, 2)}

_w_ 2 = { _govCybLab_ ( _baja_ ), _cybCapAge_ ( _baja_, 5)}

_w_ 3 = { _govCybLab_ ( _baja_ ), _mseTT_ ( _baja_, 2)}

_w_ 4 = { _cybCapAge_ ( _baja_, 5), _mseTT_ ( _baja_, 2)}

_w_ 5 = { _cybCapAge_ ( _baja_, 5)}

_w_ 6 = { _govCybLab_ ( _baja_ )}

_w_ 7 = { _mseTT_ ( _baja_, 2)}

_w_ 8 = ∅


and suppose we wish to compute the probability for formula:


_q_ = _govCybLab_ ( _baja_ ) ∨ _mseTT_ ( _baja_, 2) _._


158 P. Shakarian et al.


For each formula in _ΠEM_ we have a constraint, and for each world above we have a
variable. An objective function is created based on the worlds that satisfy the query
formula (here, worlds _w_ 1 − _w_ 4, _w_ 6, _w_ 7). Hence, EP-LP-MIN( _ΠEM_ [′] [,] _[ q]_ [) can be written]
as follows:


max _x_ 1 + _x_ 2 + _x_ 3 + _x_ 4 + _x_ 6 + _x_ 7 _w.r.t._ :

0 _._ 7 ≤ _x_ 1 + _x_ 2 + _x_ 3 + _x_ 6 ≤ 0 _._ 9

0 _._ 1 ≤ _x_ 1 + _x_ 2 + _x_ 4 + _x_ 5 ≤ 0 _._ 3

0 _._ 8 ≤ _x_ 1 + _x_ 3 + _x_ 4 + _x_ 7 ≤ 1

_x_ 1 + _x_ 2 + _x_ 3 + _x_ 4 + _x_ 5 + _x_ 6 + _x_ 7 + _x_ 8 = 1

We can now solve EP-LP-MAX( _ΠEM_ [′] [,] _[ q]_ [) and][ EP-LP-MIN][(] _[Π]_ _EM_ [′] [,] _[ q]_ [) to get solution]
0 _._ 9 ± 0 _._ 1. 

_**8.2.2**_ _**Analytical Model**_


For the analytical model (AM), we choose a structured argumentation framework
(Rahwan et al. 2009) due to several characteristics that make such frameworks highly
applicable to cyber-warfare domains. Unlike the EM, which describes probabilistic
information about the state of the real world, the AM must allow for competing
ideas—it _must be able to represent contradictory information_ . The algorithmic
approach allows for the creation of _arguments_ based on the AM that may “compete” with each other to describe who conducted a given cyber-operation. In this
competition—known as a _dialectical process_ —one argument may defeat another
based on a _comparison criterion_ that determines the prevailing argument. Resulting
from this process, the InCA framework will determine arguments that are _war-_
_ranted_ (those that are not _defeated_ by other arguments) thereby providing a suitable
explanation for a given cyber-operation.
The transparency provided by the system can allow analysts to identify potentially
incorrect input information and fine-tune the models or, alternatively, collect more
information. In short, argumentation-based reasoning has been studied as a natural
way to manage a set of inconsistent information—it is the way humans settle disputes. As we will see, another desirable characteristic of (structured) argumentation
frameworks is that, once a conclusion is reached, we are left with an explanation of
how we arrived at it and information about why a given argument is warranted; this
is very important information for analysts to have. In this section, we recall some
preliminaries of the underlying argumentation framework used, and then introduce
the analytical model (AM).


**Defeasible Logic Programming with Presumptions**


DeLP with Presumptions (PreDeLP) (Martinez et al. 2012) is a formalism combining Logic Programming with Defeasible Argumentation. We now briefly recall


8 Cyber Attribution: An Argumentation-Based Approach 159


**Fig. 8.3** A ground argumentation framework


the basics of PreDeLP; we refer the reader to (García and Simari 2004; Martinez
et al. 2012) for the complete presentation. The formalism contains several different
constructs: facts, presumptions, strict rules, and defeasible rules. Facts are statements
about the analysis that can always be considered to be true, while presumptions are
statements that may or may not be true. Strict rules specify logical consequences of
a set of facts or presumptions (similar to an implication, though not the same) that
must always occur, while defeasible rules specify logical consequences that may be
assumed to be true when no contradicting information is present. These constructs
are used in the construction of _arguments_, and are part of a PreDeLP program, which
is a set of facts, strict rules, presumptions, and defeasible rules. Formally, we use
the notation _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) to denote a PreDeLP program, where _Ω_ is the set
of strict rules, _Θ_ is the set of facts, _Δ_ is the set of defeasible rules, and _Φ_ is the set
of presumptions. In Fig. 8.3, we provide an example _ΠAM_ . We now describe each of
these constructs in detail.


**Facts** ( _Θ_ ) are ground literals representing atomic information or its negation, using
strong negation “¬”. Note that all of the literals in our framework must be formed
with a predicate from the set **P** _AM_ . Note that information in this form cannot be
contradicted.


**Strict Rules** ( _Ω_ ) represent non-defeasible cause-and-effect information that resembles a material implication (though the semantics is different since the contrapositive


160 P. Shakarian et al.


does not hold) and are of the form _L_ 0 ←− _L_ 1, _. . ._, _Ln_, where _L_ 0 is a ground literal
and { _Li_ } _i>_ 0 is a set of ground literals.


**Presumptions** ( _Φ_ ) are ground literals of the same form as facts, except that they
are not taken as being true but rather defeasible, which means that they can be
contradicted. Presumptions are denoted in the same manner as facts, except that the
symbol –≺ is added. While any literal can be used as a presumption in InCA, we
specifically require all literals created with the predicate _condOp_ to be defeasible.


**Defeasible Rules** ( _Δ_ ) represent tentative knowledge that can be used if nothing
can be posed against it. Just as presumptions are the defeasible counterpart of facts,
defeasible rules are the defeasible counterpart of strict rules. They are of the form
_L_ 0 –≺ _L_ 1, _. . ._, _Ln_, where _L_ 0 is a ground literal and { _Li_ } _i>_ 0 is a set of ground literals.
Note that with both strict and defeasible rules, _strong negation_ is allowed in the head
of rules, and hence may be used to represent contradictory knowledge.
We note that strict rules and facts are necessary constructs as they may not be true
in all environmental conditions. We shall discuss this further in the next section with
the introduction of an annotation function.
Even though the above constructs are ground, we allow for schematic versions
with variables that are used to represent sets of ground rules. We denote variables
with strings starting with an uppercase letter; Fig. 8.4 shows a non-ground example.
When a cyber-operation occurs, InCA must derive arguments as to who could
have potentially conducted the action. Informally, an argument for a particular actor
_x_ conducting cyber-operation _y_ is a consistent subset of the analytical model that
entails the atom _condOp_ ( _x_, _y_ ). If the argument contains only strict rules and facts,
then it is _factual_ . If it contains presumptions or defeasible rules, then it _defeasibly_
_derives_ that actor _x_ conducted operation _y_ .
Derivation follows the same mechanism of Logic Programming (Lloyd 1987).
Since rule heads can contain strong negation, it is possible to defeasibly derive contradictory literals from a program. For the treatment of contradictory knowledge,
PreDeLP incorporates a defeasible argumentation formalism that allows the identification of the pieces of knowledge that are in conflict, and through the previously
mentioned _dialectical process_ decides which information prevails as warranted.
This dialectical process involves the construction and evaluation of arguments
that either support or interfere with a given query, building a _dialectical tree_ in the
process. Formally, we have:


**Definition 2 (Argument)** An _argument A_, _L_ ⟩ for a literal _L_ is a pair of the literal
and a (possibly empty) set of the EM ( _A_ ⊆ _ΠAM_ ) that provides a minimal proof for _L_
meeting the requirements: (1) _L_ is defeasibly derived from _A_, (2) _Ω_ ∪ _Θ_ ∪ _A_ is not
contradictory, and (3) _A_ is a minimal subset of _Δ_ ∪ _Φ_ satisfying 1 and 2, denoted
⟨ _A_, _L_ ⟩.
Literal _L_ is called the _conclusion_ supported by the argument, and _A_ is the _support_
of the argument. An argument ⟨ _B_, _L_ ⟩ is a _subargument_ of ⟨ _A_, _L_ [′] ⟩ iff _B_ ⊆ _A_ . An
argument ⟨ _A_, _L_ ⟩ is _presumptive_ iff _A_ ∩ _Φ_ is not empty. We will also use _Ω_ ( _A_ ) =
_A_ ∩ _Ω_, _Θ_ ( _A_ ) = _A_ ∩ _Θ_, _Δ_ ( _A_ ) = _A_ ∩ _Δ_, and _Φ_ ( _A_ ) = _A_ ∩ _Φ_ .


8 Cyber Attribution: An Argumentation-Based Approach 161


**Fig. 8.4** A non-ground argumentation framework


**Fig. 8.5** Example ground arguments from Fig. 8.3


Note that our definition differs slightly from that of (Simari and Loui 1992) where
DeLP is introduced, as we include strict rules and facts as part of the argument. The
reason for this will become clear in Sect. 8.3. Arguments for our scenario are shown
in the following example.


_Example 5_ Figure 8.5 shows example arguments based on the knowledge base from
Fig. 8.3. Note that the following relationship exists:


⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ is a sub-argument of


⟨ _A_ 2, _condOp_ ( _baja_, _worm123_ )⟩ and


⟨ _A_ 3, _condOp_ ( _baja_, _worm123_ )⟩ _._         

Given argument ⟨ _A_ 1, _L_ 1⟩, counter-arguments are arguments that contradict it.
Argument ⟨ _A_ 2, _L_ 2⟩ _counterargues_ or _attacks_ ⟨ _A_ 1, _L_ 1⟩ literal _L_ [′] iff there exists a
subargument ⟨ _A_, _L_ [′′] ⟩ of ⟨ _A_ 1, _L_ 1⟩ s.t. set _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _Θ_ ( _A_ 1) ∪ _Θ_ ( _A_ 2) ∪
{ _L_ 2, _L_ [′′] } is contradictory.


_Example 6_ Consider the arguments from Example 5. The following are some of
the attack relationships between them: _A_ 1, _A_ 2, _A_ 3, and _A_ 4 all attack _A_ 6; _A_ 5 attacks
_A_ 7; and _A_ 7 attacks _A_ 2. 

162 P. Shakarian et al.


A _proper defeater_ of an argument ⟨ _A_, _L_ ⟩ is a counter-argument that—by some
criterion—is considered to be better than ⟨ _A_, _L_ ⟩; if the two are incomparable according to this criterion, the counterargument is said to be a _blocking_ defeater. An
importantcharacteristicofPreDeLPisthattheargumentcomparisoncriterionismodular, and thus the most appropriate criterion for the domain that is being represented
can be selected; the default criterion used in classical defeasible logic programming (from which PreDeLP is derived) is _generalized specificity_ (Stolzenburg et al.
2003), though an extension of this criterion is required for arguments using presumptions (Martinez et al. 2012). We briefly recall this criterion next—the first
definition is for generalized specificity, which is subsequently used in the definition
of presumption-enabled specificity.


**Definition 3** Let _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) be a PreDeLP program and let _F_ be the
set of all literals that have a defeasible derivation from _ΠAM_ . An argument ⟨ _A_ 1, _L_ 1⟩
is _preferred to_ ⟨ _A_ 2, _L_ 2⟩, denoted with _A_ 1 ≻ _PS A_ 2 iff the two following conditions
hold:


1. For all _H_ ⊆ _F_, _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ is non-contradictory: if there is a derivation
for _L_ 1 from _Ω_ ( _A_ 2) ∪ _Ω_ ( _A_ 1) ∪ _Δ_ ( _A_ 1) ∪ _H_, and there is no derivation for _L_ 1
from _Ω_ ( _A_ 1)∪ _Ω_ ( _A_ 2)∪ _H_, then there is a derivation for _L_ 2 from _Ω_ ( _A_ 1)∪ _Ω_ ( _A_ 2)∪
_Δ_ ( _A_ 2) ∪ _H_ .
2. There is at least one set _H_ [′] ⊆ _F_, _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] is non-contradictory,
such that there is a derivation for _L_ 2 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] ∪ _Δ_ ( _A_ 2), there
is no derivation for _L_ 2 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′], and there is no derivation for
_L_ 1 from _Ω_ ( _A_ 1) ∪ _Ω_ ( _A_ 2) ∪ _H_ [′] ∪ _Δ_ ( _A_ 1).


Intuitively, the principle of specificity says that, in the presence of two conflicting
lines of argument about a proposition, the one that uses more of the available information is more convincing.A classic example involves a bird, Tweety, and arguments
stating that it both flies (because it is a bird) and doesn’t fly (because it is a penguin).
The latter argument uses more information about Tweety—it is more specific—and
is thus the stronger of the two.


**Definition 4** (Martinez et al. 2012) Let _ΠAM_ = ( _Θ_, _Ω_, _Φ_, _Δ_ ) be a PreDeLP program. An argument ⟨ _A_ 1, _L_ 1⟩ is _preferred to_ ⟨ _A_ 2, _L_ 2⟩, denoted with _A_ 1 ≻ _A_ 2 iff any
of the following conditions hold:


1. ⟨ _A_ 1, _L_ 1⟩ and ⟨ _A_ 2, _L_ 2⟩ are both factual arguments and ⟨ _A_ 1, _L_ 1⟩≻ _PS_ ⟨ _A_ 2, _L_ 2⟩.
2. ⟨ _A_ 1, _L_ 1⟩ is a factual argument and ⟨ _A_ 2, _L_ 2⟩ is a presumptive argument.
3. ⟨ _A_ 1, _L_ 1⟩ and ⟨ _A_ 2, _L_ 2⟩ are presumptive arguments, and
a) ¬( _Φ_ ( _A_ 1) ⊆ _Φ_ ( _A_ 2)), or
b) _Φ_ ( _A_ 1) = _Φ_ ( _A_ 2) and ⟨ _A_ 1, _L_ 1⟩≻ _PS_ ⟨ _A_ 2, _L_ 2⟩.


Generally, if _A_, _B_ are arguments with rules _X_ and _Y_, resp., and _X_ ⊂ _Y_, then _A_ is
stronger than _B_ . This also holds when _A_ and _B_ use presumptions _P_ 1 and _P_ 2, resp.,
and _P_ 1 ⊂ _P_ 2.


8 Cyber Attribution: An Argumentation-Based Approach 163


_Example 7_ The following are relationships between arguments from Example 5,
based on Definitions 3 and 4:


_A_ 1 and _A_ 6 are incomparable (blocking defeaters);

_A_ 6 ≻ _A_ 2, and thus _A_ 6 defeats _A_ 2;

_A_ 6 ≻ _A_ 3, and thus _A_ 6 defeats _A_ 3;

_A_ 6 ≻ _A_ 4, and thus _A_ 6 defeats _A_ 4;

_A_ 5 and _A_ 7 are incomparable (blocking defeaters) _._     

A sequence of arguments called an _argumentation line_ thus arises from this attack relation, where each argument defeats its predecessor. To avoid undesirable
sequences, that may represent circular or fallacious argumentation lines, in DeLP
an _argumentation line_ is _acceptable_ if it satisfies certain constraints (see García and
Simari 2004). A literal _L_ is _warranted_ if there exists a non-defeated argument _A_
supporting _L_ .
Clearly, there can be more than one defeater for a particular argument ⟨ _A_, _L_ ⟩.
Therefore, many acceptable argumentation lines could arise from ⟨ _A_, _L_ ⟩, leading
to a tree structure. The tree is built from the set of all argumentation lines rooted
in the initial argument. In a dialectical tree, every node (except the root) represents
a defeater of its parent, and leaves correspond to undefeated arguments. Each path
from the root to a leaf corresponds to a different acceptable argumentation line. A
dialectical tree provides a structure for considering all the possible acceptable argumentation lines that can be generated for deciding whether an argument is defeated.
We call this tree _dialectical_ because it represents an exhaustive dialectical analysis
(in the sense of providing reasons for and against a position) for the argument in its
root. For argument ⟨ _A_, _L_ ⟩, we denote its dialectical tree with _T_ (⟨ _A_, _L_ ⟩).
Given a literal _L_ and an argument ⟨ _A_, _L_ ⟩, in order to decide whether or not a literal
_L_ is warranted, every node in the dialectical tree _T_ (⟨ _A_, _L_ ⟩) is recursively marked as
“D” ( _defeated_ ) or “U” ( _undefeated_ ), obtaining a marked dialectical tree _T_ [∗] (⟨ _A_, _L_ ⟩)
where:


- All leaves in _T_ [∗] (⟨ _A_, _L_ ⟩) are marked as “U”s, and

- Let ⟨ _B_, _q_ ⟩ be an inner node of _T_ [∗] (⟨ _A_, _L_ ⟩). Then, ⟨ _B_, _q_ ⟩ will be marked as “U” iff
every child of ⟨ _B_, _q_ ⟩ is marked as “D”. Node ⟨ _B_, _q_ ⟩ will be marked as “D” iff it
has at least a child marked as “U”.


Given argument ⟨ _A_, _L_ ⟩ over _ΠAM_, if the root of _T_ [∗] (⟨ _A_, _L_ ⟩) is marked “U”, then
_T_ [∗] (⟨ _A_, _h_ ⟩) _warrants L_ and that _L_ is _warranted_ from _ΠAM_ . (Warranted arguments
correspond to those in the grounded extension of a Dung argumentation system
(Dung 1995)).
We can then extend the idea of a dialectical tree to a _dialectical forest_ . For a
given literal _L_, a dialectical forest _F_ ( _L_ ) consists of the set of dialectical trees for all
arguments for _L_ . We shall denote a marked dialectical forest, the set of all marked
dialectical trees for arguments for _L_, as _F_ [∗] ( _L_ ). Hence, for a literal _L_, we say it is
_warranted_ if there is at least one argument for that literal in the dialectical forest


164 P. Shakarian et al.


**Fig. 8.6** Example annotation function


_F_ [∗] ( _L_ ) that is labeled “U”, _not warranted_ if there is at least one argument for literal
¬ _L_ in the forest _F_ [∗] (¬ _L_ ) that is labeled “U”, and _undecided_ otherwise.


**8.3** **The InCA Framework**


Having defined our environmental and analytical models ( _ΠEM_, _ΠAM_ respectively),
we now define how the two relate, which allows us to complete the definition of our
InCA framework.
The key intuition here is that given a _ΠAM_, every element of _Ω_ ∪ _ΘΔ_ ∪ _Φ_
might only hold in certain worlds in the set _WEM_ —that is, worlds specified by the
environment model. As formulas over the environmental atoms in set **G** _EM_ specify
subsets of _WEM_ (i.e., the worlds that satisfy them), we can use these formulas to
identify the conditions under which a component of _Ω_ ∪ _ΘΔ_ ∪ _Φ can be_ true.
Recall that we use the notation _formulaEM_ to denote the set of all possible formulas
over **G** _EM_ . Therefore, it makes sense to associate elements of _Ω_ ∪ _Θ_ ∪ _Φ_ with a
formula from _formulaEM_ . In doing so, we can in turn compute the probabilities
of subsets of _Ω_ ∪ _ΘΔ_ ∪ _Φ_ using the information contained in _ΠEM_, which we
shall describe shortly. We first introduce the notion of _annotation function_, which
associates elements of _Ω_ ∪ _Θ_ ∪ _Φ_ with elements of _formulaEM_ .
We also note that, by using the annotation function (see Fig. 8.6), we may have
certain statements that appear as both facts and presumptions (likewise for strict and
defeasible rules). However, these constructs would have different annotations, and
thus be applicable in different worlds. Suppose we added the following presumptions
to our running example:


_φ_ 3 = _evidOf_ ( _X_, _O_ ) **–** ≺, and
_φ_ 4 = _motiv_ ( _X_, _X_ [′] ) **–** ≺.


Note that these presumptions are constructed using the same formulas as facts
_θ_ 1, _θ_ 2.


8 Cyber Attribution: An Argumentation-Based Approach 165


Suppose we extend _af_ as follows:


_af_ ( _φ_ 3) = _malwInOp_ ( _M_, _O_ ) ∧ _malwareRel_ ( _M_, _M_ [′] ) ∧ _mwHint_ ( _M_ [′], _X_ )

_af_ ( _φ_ 4) = _inLgConf_ ( _Y_, _X_ [′] ) ∧ _cooper_ ( _X_, _Y_ )


So, for instance, unlike _θ_ 1, _φ_ 3 can potentially be true in any world of the form:


{ _malwInOp_ ( _M_, _O_ ), _malwareRel_ ( _M_, _M_ [′] ), _mwHint_ ( _M_ [′], _X_ )}


while _θ_ 1 cannot be considered in any those worlds.
With the annotation function, we now have all the components to formally define
an InCA framework.


**Definition 5 (InCA Framework)** Given environmental model _ΠEM_, analytical
model _ΠAM_, and annotation function _af_, _I_ = ( _ΠEM_, _ΠAM_, _af_ ) is an **InCA framework** .
Giventhesetupdescribedabove, weconsidera _world-based_ approach—thedefeat
relationship among arguments will depend on the current state of the world (based
on the EM). Hence, we now define the status of an argument with respect to a given
world.


**Definition 6 (Validity)** Given InCA framework _I_ = ( _ΠEM_, _ΠAM_, _af_ ), argument
⟨ _A_, _L_ ⟩ is valid w.r.t. world _w_ ∈ _WEM_ iff ∀ _c_ ∈ _A_, _w_ |= _af_ ( _c_ ).
In other words, an argument is valid with respect to _w_ if the rules, facts, and
presumptions in that argument are present in _w_ —the argument can then be built
from information that is available in that world. In this paper, we extend the notion
of validity to argumentation lines, dialectical trees, and dialectical forests in the
expected way (an argumentation line is valid w.r.t. _w_ iff all arguments that comprise
that line are valid w.r.t. _w_ ).


_Example 8_ Consider worlds _w_ 1, _. . ._, _w_ 8 from Example 4 along with the argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ from Example 5. This argument is valid in worlds
_w_ 1— _w_ 4, _w_ 6, and _w_ 7. We now extend the idea of a dialectical tree w.r.t. worlds—so, for a given world
_w_ ∈ _WEM_, the dialectical (resp., marked dialectical) tree induced by _w_ is denoted
by _Tw_ ⟨ _A_, _L_ ⟩ (resp., _Tw_ [∗][⟨] _[A]_ [,] _[ L]_ [⟩][). We require that all arguments and defeaters in these]
trees to be valid with respect to _w_ . Likewise, we extend the notion of dialectical
forests in the same manner (denoted with _Fw_ ( _L_ ) and _Fw_ [∗][(] _[L]_ [), respectively). Based on]
these concepts we introduce the notion of _warranting scenario_ .


**Definition 7 (Warranting Scenario)** Let _I_ = ( _ΠEM_, _ΠAM_, _af_ ) be an InCA framework and _L_ be a ground literal over **G** _AM_ ; a world _w_ ∈ _WEM_ is said to be a _warranting_
_scenario_ for _L_ (denoted _w_ ⊢war _L_ ) iff there is a dialectical forest _Fw_ [∗][(] _[L]_ [) in which] _[ L]_
is warranted and _Fw_ [∗][(] _[L]_ [) is valid w.r.t] _[ w]_ [.]


_Example 9_ Following from Example 8, argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩ is
warranted in worlds _w_ 3, _w_ 6, and _w_ 7. 

166 P. Shakarian et al.


Hence, the set of worlds in the EM where a literal _L_ in the AM _must_ be true is
exactly the set of warranting scenarios—these are the “necessary” worlds, denoted:


_nec_ ( _L_ ) = { _w_ ∈ _WEM_ | ( _w_ ⊢war _L_ )} _._


Now, the set of worlds in the EM where AM literal _L can_ be true is the following—
these are the “possible” worlds, denoted:


_poss_ ( _L_ ) = { _w_ ∈ _WEM_ | _w_ ̸⊢war ¬ _L_ } _._


The following example illustrates these concepts.


_Example 10_ Following from Example 8:


_nec_ ( _isCap_ ( _baja_, _worm123_ )) = { _w_ 3, _w_ 6, _w_ 7} and


_poss_ ( _isCap_ ( _baja_, _worm123_ )) = { _w_ 1, _w_ 2, _w_ 3, _w_ 4, _w_ 6, _w_ 7} _._     

Hence, for a given InCA framework _I_, if we are given a probability distribution
_Pr_ over the worlds in the EM, then we can compute an upper and lower bound on
the probability of literal _L_ (denoted **P** _L_, _Pr_, _I_ ) as follows:


      _ℓL_, _Pr_, _I_ = _Pr_ ( _w_ ),

_w_ ∈ _nec_ ( _L_ )



and




  _uL_, _Pr_, _I_ = _Pr_ ( _w_ ),

_w_ ∈ _poss_ ( _L_ )


_ℓL_, _Pr_, _I_ ≤ **P** _L_, _Pr_, _I_ ≤ _uL_, _Pr_, _I._



Now let us consider the computation of probability bounds on a literal when we
are given a knowledge base _ΠEM_ in the environmental model, which is specified in
_I_, instead of a probability distribution over all worlds. For a given world _w_ ∈ _WEM_,
let _f or_ ( _w_ ) = _(_ [�] _a_ ∈ _w_ _[a)]_ [ ∧] _[(]_ [ �] _a /_ ∈ _w_ [¬] _[a)]_ [—that is, a formula that is satisfied only by]
world _w_ . Now we can determine the upper and lower bounds on the probability of a
literal w.r.t. _ΠEM_ (denoted **P** _L_, _I_ ) as follows:



_ℓL_, _I_ = EP-LP-MIN


_uL_, _I_ = EP-LP-MAX



⎛ ⎞

  ⎝ _ΠEM_, _f or_ ( _w_ )⎠,

_w_ ∈ _nec_ ( _L_ )


⎛ ⎞




  ⎝ _ΠEM_, _f or_ ( _w_ )⎠,

_w_ ∈ _poss_ ( _L_ )


8 Cyber Attribution: An Argumentation-Based Approach 167


and


_ℓL_, _I_ ≤ **P** _L_, _I_ ≤ _uL_, _I._


Hence, we have:




   **P** _L_, _I_ = _ℓL_, _I_ + _[u][L]_ [,] _[I]_ [−] _[ℓ][L]_ [,] _[I]_

2





[−] _[ℓ][L]_ [,] _[I]_
± _[u][L]_ [,] _[I]_ _._

2



_Example 11_ Following from Example 8, argument ⟨ _A_ 5, _isCap_ ( _baja_, _worm123_ )⟩,
we can compute **P** _isCap_ ( _baja_, _worm123_ ), _I_ (where _I_ = ( _ΠEM_ [′] [,] _[ Π][AM]_ [,] _[ af]_ [)). Note that for]
the upper bound, the linear program we need to set up is as in Example 4. For the
lower bound, the objective function changes to: min _x_ 3 + _x_ 6 + _x_ 7. From these linear
constraints, we obtain: **P** _isCap_ ( _baja_, _worm123_ ), _I_ = 0 _._ 75 ± 0 _._ 25 _._ 

**8.4** **Attribution Queries**


We now have the necessary elements required to formally define the kind of queries
that correspond to the attribution problems studied in this paper.


**Definition 8** Let _I_ = ( _ΠEM_, _ΠAM_, _af_ ) be an InCA framework, _S_ ⊆ **C** _act_ (the set of
“suspects”), _O_ ∈ **C** _ops_ (the “operation”), and _E_ ⊆ **G** _EM_ (the “evidence”). An actor
A ∈ _S_ is said to be a _most probable suspect_ iff there does not exist A [′] ∈ _S_ such that
**P** _condOp_ ( **A** [′], _O_ ), _I_ ′ _>_ **P** _condOp_ ( _A_, _O_ ) _I_ ′ where _I_ [′] = ( _ΠEM_ ∪ _ΠE_, _ΠAM_, _af_ [′] ) with _ΠE_ defined
as [�] _c_ ∈ _E_ [{] _[c]_ [ : 1][ ±][ 0][}][.]
Given the above definition, we refer to _Q_ = ( _I_, _S_, _O_, _E_ ) as an _attribution query_,
and A as an _answer_ to _Q_ . We note that in the above definition, the items of evidence
are added to the environmental model with a probability of 1 ± 0. While in general
this may be the case, there are often instances in analysis of a cyber-operation where
the evidence may be true with some degree of uncertainty. Allowing for probabilistic
evidence is a simple extension to Definition 8 that does not cause any changes to the
results of this paper.
To understand how uncertain evidence can be present in a cyber-security scenario,
considerthefollowing. InSymantec’sinitialanalysisoftheStuxnetworm, theyfound
the routine designed to attack the S7-417 logic controller was incomplete, and hence
would not function (Falliere et al. 2011). However, industrial control system expert
Ralph Langner claimed that the incomplete code would run provided a missing
data block is generated, which he thought was possible (Langner et al. 2011). In
this case, though the code was incomplete, there was clearly uncertainty regarding
its usability. This situation provides a real-world example of the need to compare
arguments—in this case, in the worlds where both arguments are valid, Langner’s
argument would likely defeat Symantec’s by generalized specificity (the outcome,
of course, will depend on the exact formalization of the two). Note that Langner was


168 P. Shakarian et al.


later vindicated by the discovery of an older sample, Stuxnet 0.5, which generated
the data block. [1]

InCA also allows for a variety of relevant scenarios to the attribution problem.
For instance, we can easily allow for the modeling of non-state actors by extending
the available constants—for example, traditional groups such as Hezbollah, which
has previously wielded its cyber-warfare capabilities in operations against Israel
(Shakarian et al. 2013). Likewise, the InCA can also be used to model cooperation
among different actors in performing an attack, including the relationship between
non-state actors and nation-states, such as the potential connection between Iran and
militants stealing UAV feeds in Iraq, or the much-hypothesized relationship between
hacktivist youth groups and the Russian government (Shakarian et al. 2013).Another
aspect that can be modeled is deception where, for instance, an actor may leave false
clues in a piece of malware to lead an analyst to believe a third party conducted
the operation. Such a deception scenario can be easily created by adding additional
rules in the AM that allow for the creation of such counter-arguments. Another type
of deception that could occur include attacks being launched from a system not in
the responsible party’s area, but under their control (e.g., see Shadows in the Cloud
2010). Again, modeling who controls a given system can be easily accomplished in
our framework, and doing so would simply entail extending an argumentation line.
Further, campaigns of cyber-operations can also be modeled, as well as relationships
among malware and/or attacks (as detailed in APT1 2013).
As with all of these abilities, InCA provides the analyst the means to model a
complex situation in cyber-warfare but saves him from carrying out the reasoning
associated with such a situation. Additionally, InCA results are constructive, so an
analyst can “trace-back” results to better understand how the system arrived at a
given conclusion.


**8.5** **Open Questions**


In this section we review some major areas of research to address to move InCA
toward a deployed system.


_**8.5.1**_ _**Rule Learning**_


The InCA framework depends on logical rules and statements as part of the input,
though there are existing bodies of work we can leverage (decision tree rule learning,
inductive logic programming, etc.) there are some specific challenges with regard to
InCA that we must account for, specifically:


1 http://www.symantec.com/connect/blogs/stuxnet-05-disrupting-uranium-processing-natanz.


8 Cyber Attribution: An Argumentation-Based Approach 169


- Quickly learning probabilistic rules from data received as an input stream

- Learning of the annotation function

- Identification of the diagnosticity of new additions to the knowledgebase

- Learning rules that combine multiple, disparate sources (i.e. malware analysis
and PCAP files, for instance)


_**8.5.2**_ _**Belief Revision**_


Even though we allow for inconsistencies in the AM portion of the model, inconsistency can arise even with a consistent EM. In a companion paper, (Shakarian et al.
2014) we introduce the following notion of consistency.


**Definition 9** InCA program _I_ = ( _ΠEM_, _ΠAM_, _af_ ), with _ΠAM_ = ⟨ _Θ_, _Ω_, _Φ_, _Δ_ ⟩, is
_Type II consistent_ iff: given any probability distribution _Pr_ that satisfies _ΠEM_, if
there exists a world _w_ ∈ _WEM_ such that [�] _x_ ∈ _Θ_ ∪ _Ω_ | _w_ |= _af_ ( _x_ ) [{] _[x]_ [}][ is inconsistent, then we]
have _Pr_ ( _w_ ) = 0.
Thus, any EM world in which the set of associated facts and strict rules are
inconsistent (we refer to this as “classical consistency”) must always be assigned a
zero probability. The intuition is as follows: any subset of facts and strict rules are
thought to be true under certain circumstances—these circumstances are determined
through the annotation function and can be expressed as sets of EM worlds. Suppose
there is a world where two contradictory facts can both be considered to be true (based
on the annotation function). If this occurs, then there must not exist a probability
distribution that satisfies the program _ΠEM_ that assigns such a world a non-zero
probability, as this world leads to an inconsistency.
While we have studied this theoretically (Shakarian et al. 2014), several important
challenges remain: How do different belief revision methods affect the results of
attribution queries? In particular, can we develop tractable algorithms for belief
revision in the InCA framework? Further, finding efficient methods for re-computing
attribution queries following a belief revision operation is a related concern for future
work.


_**8.5.3**_ _**Temporal Reasoning**_


Cyber-security data often has an inherent temporal component (in particular, PCAP
files, system logs, and traditional intelligence). One way to represent this type of
information in InCA is by replacing the EM with a probabilistic temporal logic (i.e.
Hansson and Jonsson 1994; Dekhtyar et al. 1999; Shakarian et al. 2011; Shakarian
and Simari 2012). However, even though this would be a relatively straightforward
adjustment to the framework, it leads to several interesting questions, specifically:


170 P. Shakarian et al.


- Can we identify hacking groups responsible for a series of incidents over a period
of time (a cyber campaign)?

- Can we identify the group responsible for a campaign if it is not known a priori?

- Can we differentiate between multiple campaigns conducted by multiple culprits
in time-series data?


_**8.5.4**_ _**Abductive Inference Queries**_


We may often have a case where more than one culprit is attributed to the same
cyber-attack with nearly the same probabilities. In this case, can we identify certain
evidence that, if found, can lead us to better differentiate among the potential culprits?
In the intelligence community, this is often referred as identifying _intelligence gaps_ .
We can also frame this as an abductive inference problem (Reggia and Peng 1990).
This type of problems leads to several interesting challenges:


- Can we identify all pieces of diagnostic evidence that would satisfy an important
intelligence gap?

- Can we identify diagnostic evidence under constraints (i.e., taking into account
limitations on the type of evidence that can be collected)?

- In the case where a culprit is attributed with a high probability, can we identify
evidence that can falsify the finding?


**8.6** **Conclusions**


In this paper we introduced InCA, a new framework that allows the modeling of various cyber-warfare/cyber-security scenarios in order to help answer the attribution
question by means of a combination of probabilistic modeling and argumentative
reasoning. This is the first framework, to our knowledge, that addresses the attribution problem while allowing for multiple pieces of evidence from different sources,
including traditional (non-cyber) forms of intelligence such as human intelligence.
Further, our framework is the first to extend Defeasible Logic Programming with
probabilistic information. Currently, we are implementing InCA along with the
associated algorithms and heuristics to answer these queries.


**Acknowledgments** This work was supported by UK EPSRC grant EP/J008346/1—“PrOQAW”,
ERC grant 246858—“DIADEM”, by NSF grant #1117761, by the National Security Agency under
the Science of Security Lablet grant (SoSL), Army Research Office project 2GDATXR042, and
DARPA project R.0004972.001.


8 Cyber Attribution: An Argumentation-Based Approach 171


**References**


Shadows in the Cloud: Investigating Cyber Espionage 2.0. Tech. rep., Information Warfare Monitor
and Shadowserver Foundation (2010)
APT1: Exposing one of China’s cyber espionage units. Mandiant (tech. report) (2013)
Altheide, C.: Digital Forensics with Open Source Tools. Syngress (2011)
Dekhtyar,A., Dekhtyar, M.I., Subrahmanian, V.S.: Temporal probabilistic logic programs. In: ICLP
1999, pp. 109–123. The MIT Press, Cambridge, MA, USA (1999)
Dung, P.M.: On the acceptability of arguments and its fundamental role in nonmonotonic reasoning,
logic programming and _n_ -person games. Artif. Intell. **77**, pp. 321–357 (1995)
Falliere, N., Murchu, L.O., Chien, E.: W32.Stuxnet Dossier Version 1.4. Symantec Corporation
(2011)
García, A.J., Simari, G.R.: Defeasible logic programming: An argumentative approach. TPLP
**4** (1–2), 95–138 (2004)
Hansson, H., Jonsson, B.: A logic for reasoning about time and probability. Formal Aspects of
Computing **6**, 512–535 (1994)
Heuer, R.J.: Psychology of Intelligence Analysis. Center for the Study of Intelligence (1999)
Khuller, S., Martinez, M.V., Nau, D.S., Sliva, A., Simari, G.I., Subrahmanian, V.S.: Computing
most probable worlds of action probabilistic logic programs: scalable estimation for 10 [30,000]

worlds. AMAI **51(2–4)**, 295–331 (2007)
Langner, R.: Matching Langner Stuxnet analysis and Symantic dossier update. Langner Communications GmbH (2011)
Lloyd, J.W.: Foundations of Logic Programming, 2nd Edition. Springer (1987)
Martinez, M.V., García, A.J., Simari, G.R.: On the use of presumptions in structured defeasible
reasoning. In: Proc. of COMMA, pp. 185–196 (2012)
Nilsson, N.J.: Probabilistic logic. Artif. Intell. **28** (1), 71–87 (1986)
Rahwan, I., Simari, G.R.: Argumentation in Artificial Intelligence. Springer (2009)
Reggia, J.A., Peng,Y.:Abductive inference models for diagnostic problem-solving. Springer-Verlag
New York, Inc., New York, NY, USA (1990)
Shakarian, P., Parker, A., Simari, G.I., Subrahmanian, V.S.: Annotated probabilistic temporal logic.
TOCL **12** (2), 14 (2011)
Shakarian, P., Simari, G.I., Subrahmanian, V.S.: Annotated probabilistic temporal logic: Approximate fixpoint implementation. ACM Trans. Comput. Log. **13** (2), 13 (2012)
Shakarian, P., Shakarian, J., Ruef, A.: Introduction to Cyber-Warfare: A Multidisciplinary
Approach. Syngress (2013)
Shakarian, P., Simari, G.I., Falappa, M.A.: Belief revision in structured probabilistic argumentation.
In: Proceedings of FoIKS, pp. 324–343 (2014)
Simari, G.R., Loui, R.P.: A mathematical treatment of defeasible reasoning and its implementation.
Artif. Intell. **53** (2-3), 125–157 (1992)
Simari, G.I., Martinez, M.V., Sliva, A., Subrahmanian, V.S.: Focused most probable world
computations in probabilistic logic programs. AMAI **64** (2–3), 113–143 (2012)
Spitzner, L.: Honeypots: Catching the Insider Threat. In: Proc. ofACSAC 2003, pp. 170–179. IEEE
Computer Society (2003)
Stolzenburg, F., García, A., Chesñevar, C.I., Simari, G.R.: Computing Generalized Specificity.
Journal of Non-Classical Logics **13** (1), 87–113 (2003)
Thonnard, O., Mees, W., Dacier, M.: On a multicriteria clustering approach for attack attribution.
SIGKDD Explorations **12** (1), 11–20 (2010)