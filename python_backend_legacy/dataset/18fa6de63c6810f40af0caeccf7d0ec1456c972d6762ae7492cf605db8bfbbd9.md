{
  "full_text": "# ABSTRACT\nHow could AI-infused cyber capabilities be used to subvert, or otherwise compromise, the reliability, control and use of states' nuclear forces? This article argues that a new generation of artificial intelligence (AI) enhanced cyber capabilities will amplify the risk of inadvertent escalation caused by the co-mingling of nuclear and strategic non-nuclear weapons and the increasing speed of warfare, thereby increasing the risk of nuclear confrontation. It examines the potential implications of cyber (offensive and defensive) capabilities augmented with AI applications for nuclear security. The article concludes that future iterations of AI-enhanced cyber counterforce capabilities will complicate the existing challenges of cyber defence, and in turn, compromise nuclear assets and increase the escalatory effects of offensive cyber capabilities.\n# KEYWORDS:\n- Artificial intelligence\n- cyber-security\n- US-China Relations\n- nuclear security\n- deterrence policy\n- emerging technology\n# Introduction\nGiven the hyperbole surrounding AI, it is easy to overstate the opportunities and challenges posed by the development and deployment of AI in the military sphere (Boulanin Citation 2019). Today, there remains a large amount of debate among AI researchers surrounding the significance of several significant technical and operational challenges in the deployment of AI-infused systems (e.g. drone swarming, command and control decision-making support systems and a broader range of autonomous weapon systems). Moreover, many of the risks posed by AI in the nuclear domain today are not necessarily new (Boulanin Citation 2019). Recent advances in AI (especially machine learning techniques) exacerbate existing risks to escalation and stability rather than generating entirely new ones. While AI could enable significant improvements in many military domains (including nuclear weapons), for the foreseeable future, developments in military AI will likely be far more prosaic than implied in popular culture. The main worry for nuclear stability, therefore, is that militaries will underestimate (or overstate) or ignore the potential shortcomings and risks associated with the use of the current generation of AI technology in the safety-critical military sphere – and especially the nuclear domain.\nBecause of the diverse approaches to AI-based research, there is no universally accepted definition of ‘artificial intelligence’, which is confusing when the term is applied generically to make grandiose claims about its revolutionary effects. Footnote  ${}^{1}$  AI can be understood as a universal term for improving the performance of automated (or autonomous) systems to solve a wide variety of complex tasks including: perception (sensors, computer vision, audio and image processing); reasoning and decision-making (problem solving, searching, planning and reasoning); learning and knowledge representation (machine learning, deep networks and modelling); Footnote  ${}^{2}$  communication (language processing); autonomy and robotics ; and human-AI collaboration (humans define the systems’ purpose, goals and context).\nIn a military context, as a potential enabler and force multiplier of advanced weapon systems, AI is more akin to electricity, radio, radar and intelligence, surveillance and reconnaissance (ISR) support systems than a 'weapon' per se. Much like these enabling support systems, AI is also strategically vital, vulnerable, inherently cross-domain and dependent on advanced technology. Thus, even if AI applications are unable to make better battlefield decisions than humans, Footnote  $^{3}$  militaries that use AI in human-machine teaming will doubtless gain significant advantages (e.g. remote-sensing, situational-awareness, battlefield-manoeuvres and a compressed decision-making loop), compared to those who depend on human judgment – and semi-autonomous technology – alone. This is particularly the case, in operating systems in operating environments that demand endurance and rapid decision-making across multiple combat zones (Ayoub and Payne Citation 2016).\nThis article argues that a new generation of AI-augmented offensive cyber capabilities will amplify the risk of inadvertent escalation posed by the co-mingling of nuclear and strategic (or counterforce) non-nuclear weapons and the increasing speed of warfare, and in turn, increase the risk of nuclear confrontation. The article's thesis is grounded in three core themes. First, AI does not exist a vacuum, that is, in isolation, AI will unlikely be a strategic game changer. Instead, it will likely mutually reinforce the destabilizing effects of existing advanced capabilities thereby increasing the speed of warfare and compressing the decision-making timeframe (Johnson Citation 2019a). AI-enabled and enhanced capabilities will have a more significant impact (positive or negative) on strategic stability than the sum of its parts. Furthermore, the intersection of AI with nuclear weapons, and a broader spectrum of strategic non-nuclear weapons, will likely accelerate the erosion of the survivability of nuclear arsenals associated with the 'computer revolution'. Put another way, military AI and the advanced capabilities it enables are a natural manifestation – rather than the cause or origin – of an established trend, which could lead states to adopt destabilizing launch postures due to the increasing speed of war and co-mingling.\nSecond, AI's impact on stability, deterrence and escalation will likely be determined as much (or more) by states' perception of its functionality as much as by what it is capable of doing. In the case of nuclear strategy and deterrence, the perception of an adversary's capabilities and\nintentions is as (or more) important than its actual capability. Thus, in addition to the importance of military force postures, capabilities and doctrine, the effects of AI will also have a strong cognitive element, increasing the risk of inadvertent escalation as a result of misperception and misunderstanding.\nFinally, and related to the above, the increasingly competitive and contested nuclear multipolar world order will compound the destabilizing effects of AI, and in turn, increase the escalation risks in future warfare between great military powers – especially China and the United States (Johnson Citation 2019b). Moreover, the potential operational and strategic advantages offered by AI-augmented capabilities could prove irresistible to nuclear-armed strategic rivals, causing them to eschew the limitations of AI, and compromise safety and verification standards to protect, or attempt to capture, technological superiority on the future digitised battlefield.\n# Military AI primer\nArtificial intelligence research began as early as the 1950s, as a broad concept concerned with the science and engineering of making intelligent machines. Since the 1950s, AI research has gone through several development phases – from early exploitations in the 1950s and 1960s and the 'AI Summer' during the 1970s, through to the early 1980s and the 'AI Winter' from the 1980s – each of which failed to live up to its initial, and often over-hyped, expectations. In the past decade, the explosion of interest in the field (or 'AI renaissance') occurred due to the convergence of four enabling developments: (1) the exponential growth in computing processing power; (2) expanded data sets (especially 'big data' sources) (Gray et al. Citation 2015); Footnote  $^{4}$  (3) advances in the implementation of machine learning techniques and algorithms (especially deep 'neural networks') (Schmidhuber Citation 2015); Footnote  $^{5}$  and (4) the rapid expansion of commercial interest and investment in AI (Hoadley and Nathan Citation 2018). Footnote  $^{6}$\nAI is concerned with machines that emulate capabilities that are usually associated with human intelligence, such as language, reasoning, learning, heuristics and observation. Today, all practical (i.e. technically feasible) AI applications fall into the 'narrow' category, and less so, artificial general intelligence (AGI) or 'superintelligence' (Reedy Citation 2017). Footnote  $^{7}$  'Narrow AI' has been widely used in a broad range of civilian and military tasks since the 1960s, (Nilsson Citation 2010; Russel and Norvig Citation 2014) and involves statistical algorithms (mostly based on machine learning techniques) that learn procedures through analysis of large training data sets designed to approximate and replicate human cognitive tasks (Russel and Norvig Citation 2014). Footnote  $^{8}$  'Narrow AI' is the category of artificial intelligence to which this article refers when it assesses the impact of this technology in a military context.\nGiven the diverse approaches to research in AI (Boden Citation 2016 ; Vernon Citation 2014), Footnote  $^{9}$  there is no universally accepted definition of AI. A recent US congressional report defines AI as follows: 'Any artificial system that performs tasks under varying and unpredictable circumstances, without significant human oversight, or that can learn from their experience and improve their performance ... they may solve tasks requiring human-like perception, cognition, planning, learning, communication, or physical action' (Hoadley and Nathan Citation 2018).\nToday, a large gap exists between the reality of what AI technology is capable of doing in a military context, and the expectations and fears of public opinion, policymakers and global defence communities. How will AI technologies influence public opinion about the use of military force and warfare? (Sechser, Narang, and Talmadge Citation 2019). The misrepresentations and misconceptions that exist today in the narratives surrounding AI (especially in societal, economic and national security debates) and are in large part caused by the hyperbole and exaggerated depictions of AI in science fiction (Zarkadakis Citation 2015). Footnote  $^{10}$  Misrepresentations of the potential opportunities and risks associated with this technology in the military sphere can obscure constructive and crucial debate on these topics. Specifically, this includes the challenge of balancing the potential operational, tactical and strategic benefits of leveraging AI while managing the risks posed to nuclear security in the (especially premature) pursuit of these advantages.\nThe historical record on technological change in a military context counsels scepticism against alarmist predictions and extrapolations from emerging trends: technologies rarely evolve in the way futurists predict and many applications have had countervailing or conditional effects that have ameliorated the dystopian predictions of naysayers (Metz Citation 2000 ; Metz and Kievit Citation 1995). In short, AI's impact on nuclear stability will, like previous generations of emerging technology (i.e. chemical and biological weapons, cyber and nuclear weapons), have both stabilising and destabilizing (and possibly contradictory) effects on strategic stability. For example, many expected that chemical weapons would instantly and dramatically change the nature of warfare and deterrence after the British used poison gas during World War I (Sechser, Narang, and Talmadge Citation 2019). However, chemical weapons proved far less practical, impactful, disruptive and relatively easier to defend against than conventional explosives. As a corollary, the degree to which military AI poses risks to future strategic stability will depend in large part on the pace and scope with which this technology facilitates new ways to improve the delivery of, and defence against, nuclear weapons and strategic non-nuclear weapons.\nConceptually, AI-augmented applications can be categorized into those that have predominately operational, tactical and strategic implications in future warfare. At the operational and tactical level, applications include: autonomy and robotics; multi-actor interaction red teaming wargaming; Footnote  $^{11}$  big data-driven modelling; Footnote  $^{12}$  intelligence collection and analysis (e.g. to locate and monitor mobile missiles, troops movement) (Conklin Citation 2018); Footnote\n$^{13}$ cybersecurity talent management; predictive maintenance, logistics, planning and forecasting; and vendor contract and budget management. In preparation for a new AI strategy, US Strategic Operations Command (SOCOM) recently crafted a roadmap that invests heavily in AI and machine learning technology based on several pilot projects with three overarching goals to create an algorithmic warfare multi-operational team: 'AI-ready workforce, AI-applications, and AI-outreach' (Lee Citation 2019).\nTherefore, AI's impact on strategic stability and escalation will also be shaped by broader and more nuanced factors which influence the trajectory of emerging technologies, including: military strategy and doctrine – that seeks to manipulate escalation risks – military culture and organisation, alliance structures, domestic politics and public opinion, to name but a few. The Cold War record demonstrates that emerging technologies act primarily to enable independent variables which, in combination with other endogenous factors, can heighten escalation risks between adversaries. Therefore, in isolation, technology is not a major exogenous cause of (inadvertent or intentional) military escalation.\nAt a strategic level of warfare, AI uses include: qualitative improvements to the nuclear command, control, communications, and intelligence (C3I) architecture; enhancing target acquisition, tracking, guidance systems and discrimination of missile and air defence systems; being force multipliers of both offensive and defensive machine-learning infused cyber capabilities; and qualitatively bolstering nuclear and non-nuclear missile delivery systems – including hypersonic variants. Footnote 14\nIn combination, AI machine learning algorithms fused with advances in sensor technology could be engineered to allow nuclear delivery systems to operate more autonomously and precisely, with less reliance on human operators calibrating navigation and guidance parameters and equipped with more robust countermeasures against jamming or spoofing attacks. At the strategic level, AI-augmented command and control systems could potentially mitigate many of the shortcomings inherent to human strategic decision-making during the 'fog of war', such as the susceptibility to invest in sunk costs, skewed risk judgment, cognitive heuristics and groupthink.\nIn sum, the fusion of AI machine learning and human judgment to gauge an adversary's intentions (and predict escalation) for the purposes of planning and directing future wars for the pursuit of political objectives, is, therefore, a far less unlikely prospect in the near future than the use of AI to achieve tactical and operational ends (e.g. drone swarming and cyber defence).\n# Blurring the AI-cyber offence-defence line: (Glaser and Kaufmann Citation 1998 ; Lynn-Jones Citation 1995 ) Footnote 15\nSeveral US national security officials believe that AI, used as force multipliers for both defensive and offensive cyber weapons, will have a transformative impact on cybersecurity (Gartzke and Lindsay Citation 2015 ; Rid Citation 2012 ; Slayton Citation 2017 ; Hoffman Citation 2019). Footnote 16 Director of US National Intelligence, Daniel Coats, recently warned that AI could increase US vulnerability to cyberattacks, weaken its ability to attribute such attacks, improve the effectiveness and capabilities of adversaries weapon and intelligence systems, and cause accidents and related liability issues. To be sure, the line between AI cyber-offence and cyber-defence will likely remain an obscure one. Footnote 17 Bernard Brodie’s words of caution about the advent of nuclear weapons almost six decades ago resonate: ‘The [military] bias towards the offensive creates special problems in any technologically new situation where there is little or no relevant war experience to help one to reach a balanced judgment’ (Brodie Citation 1959).\nOn the one hand, AI might reduce a military’s vulnerability to cyberattacks. AI cyber-defence tools (or ‘counter-AI’), designed to recognise changes to patterns of behaviour in a network and detect anomalies, automatically detect software code vulnerabilities, and apply machine learning techniques (such as ‘deep learning’) to detect deviations and anomalies from normal network activity, Footnote 18 could form a more robust defence against cyber subversions (Johnson Citation 2019a). Footnote 19 According to the Pentagon’s new AI strategic report, ‘AI can enhance our ability to predict, identify, and respond to cyber and physical threats from a range of sources’ (US Department of Defense Citation 2019). In addition, the DoD’s Defense Innovation Unit (DIU) is prototyping an application (related to Project VOLTRON) that leverages AI to decipher high-level strategic questions, map probabilistic chains of events and develop alternative strategies in order to make DoD systems more resilient to AI-augmented cyberattacks and configure and fix errors more quickly than humans.\nOn the other hand, autonomy itself might increase a military’s vulnerability to cyberattacks, which rely on stealth, deception and stratagem. For example, an adversary could use malware to take control, manipulate or fool the behaviour and pattern recognition systems of autonomous systems, such as DoD’s Project Maven. Offensive attacks such as this would be relatively easy to execute, but very difficult to detect, attribute or effectively counter (Shachtman Citation 2011). Footnote 20 This problem set is compounded by the lack of an agreed framework or understanding of what constitutes escalatory behaviour (or ‘firebreaks’) in cyberspace. Thus, a cyber operation intended as a signal (i.e. for coercive diplomacy) could go undetected by the target, or worse, misinterpreted as an offensive attack. Even if information relating to an\noperation of this kind is accurately identified in a timely manner, the motives behind them could remain ambiguous, or misperceived. According to Robert Jervis, ‘it is likely that the country that is the object of the attack would assume that any effect was the intended one’ (Jervis Citation 2016).\nUS Cyber Fleet Command Commander, Michael Gilday, recently told the Senate Armed Services Committee that the US Navy must ‘improve an ability to proactively detect new and unknown malware … so we [the US] can act quickly using advanced analytics enabled by AI and machine learning’, which may give the US a ‘tactical advantage’ to identify malicious activity early on (Osborn Citation 2018). Even if analysts can obtain high-quality and reliable intelligence, however, they may not want to reveal it, because doing so could compromise a source, capability or tactic (Johnson Citation 2019a). Moreover, most observers now acknowledge that no strategy (i.e. combining offence and defence cyber operations) could realistically be expected to deter all malign cyber-attacks (Hoffman Citation 2019). Footnote 21 While AI-enhanced cyber capabilities can enhance deterrence, they can simultaneously incentivize others to attack thereby exacerbating the paradox of enhanced capabilities and increased vulnerabilities in the cyber domain (Gartzke and Lindsay Citation 2017; Libicki Citation 2016; Slayton Citation 2017). Footnote 22 As the historical record attests, this capability-vulnerability paradox is heightened when states are both dependent on a particular capability (such as AI and cyber tools) and their access or ability to use the capabilities is vulnerable to an adversary’s exploitation or subversion – creating first-mover incentives (Rid Citation 2013). Footnote 23\n# Cybersecurity and nuclear weapon systems: nuclear risk redux?\nAt a strategic level of conflict, AI applications designed to enhance cybersecurity for nuclear forces could simultaneously make cyber-dependent nuclear weapon systems (i.e. communications, data processing or early-warning sensors) more vulnerable to cyberattacks. It is now thought possible that a cyberattack (i.e. spoofing, hacking, manipulation and digital jamming) could infiltrate a nuclear weapons system, threaten the integrity of its communications, and ultimately (and possibly unbeknown to its target) gain control of its (nuclear and non-nuclear) command and control systems. Footnote 24 AI technology has not yet evolved to a point where it would allow nuclear-armed states to credibly threaten the survivability of each other’s nuclear second-strike capability. Thus, the development trajectory of AI (and its enabling technologies) means its impact on nuclear security will likely be theoretical and speculative for the foreseeable future.\nBecause of the intense time pressures that would likely loom large with the decision to use nuclear weapons – especially where a state maintains a launch-on-warning posture – AI-enhanced\ncyberattacks against nuclear systems would be almost impossible to detect and the warning signals difficult to authenticate, let alone attribute, within the short timeframe for initiating a nuclear strike. According to open sources, operators at the North American Aerospace Defence Command (NORAD) have less than three minutes to assess and confirm initial indications from early-warning systems of an incoming attack. This compressed decision-making time frame could put political leaders under intense pressure to make a decision to escalate during a crisis, with incomplete (and possibly false) information of a situation. Ironically, new technologies designed to enhance information (i.e. modernised nuclear C3I systems augmented by 5G networks, machine learning, big-data analytics and quantum computing) can also undermine clear and reliable information flow and communication, critical for effective deterrence (Gartzke and Lindsay Citation 2019). Footnote 25\nAdvances in AI could also exacerbate this cybersecurity challenge by enabling improvements to the cyber offence. Machine learning and AI by automating advanced persistent threat (APT) operations might dramatically reduce the extensive manpower resources and high levels of technical skill required to execute APT operations (or 'hunting for weaknesses'), especially against hardened nuclear targets. Footnote 26 The machine speed of AI-augmented cyber tools could enable a low-skilled and capital-restricted attacker to exploit a narrow window of opportunity to penetrate an adversary's cyber-defences or use APT tools to find new vulnerabilities. For example, when docked for maintenance air-gapped nuclear-powered submarines, considered secure when submerged, could become increasingly vulnerable to a new generation of low-cost – possibly black-market – and highly automated APT cyberattacks.\nAn attacker could also apply AI machine learning techniques to target autonomous dual-use early-warning and other operating systems (e.g. C3I, ISR, early-warning and robotic control networks) with 'weaponized software' such as hacking, subverting, spoofing or tricking, causing unpredictable and potentially undetectable errors, malfunctions and behavioural manipulation to weapons systems – or 'data-poisoning'. Footnote 27 Furthermore, as the linkages between digital and physical systems (or the 'Internet of Things') expand, the potential for to an adversary to use cyberattacks in both kinetic and non-kinetic attacks will increase. A significant risk variable in the operation of autonomous systems is the time that passes between a system failure (i.e. performing in a manner other than how the human operator intended) and the time it takes for a human operator to take corrective action. If the system failure is the result of a deliberate act, this time frame will be compressed (Johnson Citation 2019a).\nEven if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. This skewed assessment by states in the context of nuclear weapons, which are ready to launch nuclear weapons at a moment's notice, would likely precipitate worst-case scenario thinking that may spark an inadvertent escalation (Talmadge\nCitation 2017). Footnote 28 During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 29 For example, an AI-enabled third party generated 'deepfake' (audio and video manipulation), coupled with data-poisoning cyberattacks, could spark an escalatory crisis between two (or more) nuclear states (Fitzpatrick Citation 2019). Footnote 30\nHow could AI-augmented cyber capabilities create new pathways for accidental or inadvertent escalation? To illustrate these dynamics: State A launches a malicious AI-enhanced cyberattack to spoof State B's AI-enabled autonomous sensor platforms and automated target recognition systems, in such a way that the weapon system (e.g. a human-supervised ATR system) is fooled into interpreting a civilian object as a military target. B in response, based on manipulated or erroneous information, and the inability of human supervisors to detect the spoofed imagery that fooled the weapons' automated target recognition algorithm in time to take corrective action, accidentally (and unintentionally) escalates a situation (Libicki Citation 2018). In this example, the spoofing attack on the weapon systems' algorithm is executed in such a way that the imagery appears to the recognition system as indistinguishable from a valid military target (Rohrbach et al. Citation 2018) Footnote 31 escalating a situation based on a false premise that would unlikely to fool the human eye. Footnote 32\nAlso, the explainability (or 'black box') problem associated with AI applications could compound these potential escalation dynamics. Footnote 33 Insufficient understanding of how and why AI algorithms reach a particular judgment or decision might complicate the task of determining whether data sets had been deliberately compromised to manufacture false outcomes (e.g. attacking incorrect targets or misdirecting allies during combat). Footnote 34 Moreover, as humans and AI team up to accomplish particular missions, the opacity associated with how AI systems reach a decision may cause an operator to have either too much or too little confidence in a system's performance. As a result, unless the system's machine learning algorithm is terminated, at the end of the training phase, once deployed, it could potentially learn something it was not intended to, or even perform a task or mission that its human designers do not expect it to do. Footnote 35\nIn sum, technologies that improve the reliability and speed with which information is processed and disseminated by early-warning systems may paradoxically also increase the vulnerabilities of these networks, creating new first-mover advantages and escalation pathways during a crisis that spark unintentional (or intentional) warfare (Schneider Citation 2019). Footnote 36 Put another way, AI might be developed in order to enable a state to pursue a pre-determined escalatory path. Thus, increased escalation risk as a result of technology is not always inadvertent or accidental (Long and Rittenhouse Green Citation 2015). Footnote 37\n# 'Cyber guns' supercharged with AI machine learning\nWhile manipulations and system subversions are possible with existing cyber offence tools, rapid advances in AI and increasing degrees of military autonomy could amplify the potential speed, power and scale of future attacks in cyberspace. Footnote 38 There are three significant intersections between AI systems and cybersecurity that are most salient to the military domain: (1) advances in autonomy and machine learning techniques mean that a much broader range of physical systems are now vulnerable to cyberattacks (i.e. hacking and data-poisoning) (Greenberg Citation 2016); Footnote 39 (2) cyberattacks on AI systems can offer attackers access to machine learning algorithms, trained models being used by the application and a potential vast amounts of data from facial recognition and intelligence collection and analysis systems (e.g. satellite navigation and imagery systems used to cue precision munitions and supported intelligence, surveillance, reconnaissance missions); and (3) the use of AI systems in conjunction with existing cyber offence tools will enable sophisticated cyberattacks to be executed at larger scale (both geographically and across networks), at faster speeds, and simultaneously across multiple military domains with improved anonymity. In short, despite the relatively benign augmentation mechanisms used to develop cyber offensive capabilities with AI systems, the resultant impact on the speed and scope of AI cyber tools will likely have destabilizing effects, nonetheless. Footnote 40\nRecent advances in machine learning have significantly contributed to resolving several technical bottlenecks in several fields of AI, which could allow for significant qualitative improvements to a large variety of autonomous weapon systems and applications. In combination, machine learning and autonomy could intersect with nuclear security in a multitude of ways, with both positive and negative implications for strategic stability. For example, machine learning could enhance the detection capabilities of (nuclear and non-nuclear) early-warning systems and improve the collection and cross-analysis of ISR information, thus reducing the risk of inadvertent and accidental escalation. Machine learning may also bolster the cyber-security of command and control systems, create new and possibilities for arms control, testing, verification and monitoring nuclear stockpiles, and enhance conventional counterforce capabilities. Unmanned autonomous systems could be deployed in complex missions in hitherto inaccessible and cluttered environments (e.g. under-sea anti-submarine warfare) and aerial and underwater drones in swarms might eventually replace intercontinental ballistic missiles (ICBMs) and nuclear-armed submarines (SSBNs) for the delivery of nuclear weapons.\nIn response to these anticipated vulnerabilities, and to achieve the (perceived) first-mover advantages the increased speed in AI-augmented cyber tools could confer, China, Russia and the United States have continued to harden their AI cyber defences. Despite these parallel efforts, divergent views exist on how strategic communities perceive the nature of the risks posed by AI-enhanced cyberattacks against dual-use C3I systems. Thus, AI-augmented cyber intelligence-\ngathering tools targeting an adversary's command and control assets (especially dual-use facilities) during a crisis may be misinterpreted as a prelude to an imminent pre-emptive attack on its nuclear force and therefore as undermining strategic stability.\nMoreover, AI machine learning could enable new offensive concepts such as a so-called 'left-of-launch operation' (Ellison Citation 2015), Footnote  $^{41}$  thereby compounding the ambiguities and fear about pre-emptive attacks in cyberspace, and increasing first-mover incentives – or use-them-or-lose-them situations (Buchanan Citation 2017; Buchanan and Miller Citation 2017). Footnote  $^{42}$  In the cyber domain, it is generally unclear in the early stages of a cyber operation whether an adversary intends to collect intelligence or prepare for an offensive attack, which is more likely to be used early on in a conflict scenario. Open sources suggest, for example, that Chinese analysts view the vulnerability of China's nuclear command, control, and communications (C3I) systems to cyber infiltrations – even if an attacker's objective was limited to cyberespionage – as a highly escalatory national security threat. By contrast, Russian analysts tend to view Russia's nuclear C3I network as relatively isolated, and thus insulated from cyberattacks.\nIrrespective of these differences, the uncertainty caused by the possibility, actual or otherwise, that AI-enhanced offensive cyber tools might be used (or threatened) to undermine the credibility and reliability of states' nuclear forces would be destabilizing, both as result of the existence of enhanced cyber offensive capabilities and the potential efficacy of these tools against nuclear forces. As the historical record attests, rational actors have incentives to misrepresent what they know (i.e. lie and bluff) in order to improve the terms of any settlement that may emerge from the wartime bargaining process – or the risk-reward payoff. Even a modicum of uncertainty about the effectiveness of AI-augmented cyber capabilities during a crisis or conflict would, therefore, reduce both sides' risk tolerance and increase the incentive to strike pre-emptively as a hedging strategy.\nA virtuous cycle that might flow from enhanced reassurances premised on comprehensive intelligence would require information symmetry (i.e. equal access to intelligence and analysis systems) between great and rising powers, and shared confidence in the accuracy and credibility of these systems. Perhaps most challenging of all in a world of 'revisionist' rising powers, the intentions of all rival states would need to be genuinely benign for this optimistic view to occur. Under crisis conditions, for example, an offensive AI cyber tool that succeeds in compromising an adversary's nuclear weapon systems, resulting in an 'asymmetric information' situation, could cause either or both sides to overstate (or understate) its retaliatory capabilities, and in turn, be more inclined to act in a risky and escalatory fashion. In short, in a competitive strategic environment, where states are inclined to assume the worst of others' intentions, one state's efforts to enhance the survivability of its strategic forces may be viewed by others as a threat to their nuclear retaliatory capability – or second-strike capacity.\n# Manipulation of the information landscape\nWhile machine learning big-data analytics, sensing technology, networks supported by 5G networks, could alert commanders of incoming threats with increased speed and precision, and make accidents caused by human error potentially less likely (especially accidents and malfunctions caused by false warning), these advances could simultaneously amplify escalation risks in two ways. Footnote 43\nFirst, AI machine learning used as force multipliers for cyber offence (e.g. data poisoning spoofing, 'deepfakes', manipulation, hacking and digital jamming) would be considerably more difficult to detect – especially if an attacker used AI advanced persistent threat (APT) tools – or 'hunting for weaknesses' in spectrum-contested environments. Second, in the unlikely event an attack was successfully detected, threat identification (or attribution) at machine speed would be virtually impossible. In addition to the co-mingling and the speed of warfare issues, AI machine learning systems might also exacerbate the existing – well-established and non-kinetic – risk of inadvertent (and intentional) escalation from the manipulation of the information landscape in which decisions about nuclear weapons are located. For example, the use of deepfake videos built-in real-time and disseminated online for malicious purposes.\nHuman cognition, and thus effective deterrence and signalling intentions, is predicated on reliable and clear information; if an adversary is concerned that the information available to them is limited (or worse, inaccurate), they will likely assume the worst and act accordingly. Asymmetric information situations between rivals about the balance of military power could undermine crisis stability, and in turn, create rational incentives to escalate to nuclear confrontation. Consequently, states will be more inclined to assume the worst of others' intentions, especially in situations where the legitimacy of the status quo is contested (i.e. maritime Asia).\nIn the competitive strategic environment, it is easy to imagine unprovoked escalation caused by a malicious third-party (or state-proxy) clandestine action. Even if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 44 For example, in an effort to incite conflict between two rival states, State A uses proxy hackers to use a AI machine learning technique (i.e. generative adversarial networks GAN) to launch 'deepfake' video or audio material, depicting senior military commanders of State B conspiring to launch a pre-emptive strike on State C. Footnote 45 Then, this 'deepfake' footage is deliberately leaked into C's AI-augmented intelligence collection and analysis systems, provoking C to escalate the situation with strategic consequences. B responds in kind. Footnote 46\nThe current underdeveloped state of ‘counter-AI’ capabilities and other fail-safe mechanisms (e.g. circuit breakers) to de-escalate escalation in cyberspace, will make the unprovoked and unintentional escalation dynamics depicted in this scenario very challenging to mitigate. Moreover, in the emerging ‘deepfakes’ arms race (much like cybersecurity more generally), detection software will likely lag behind advances in offensive enabling solutions – or offence-dominant ones. According to computer science expert Hany Farid, there are probably 100–1000 times ‘more people developing the technology to manipulate content than there is to detect [it]’ (Fontaine and Frederick Citation 2019).\nThe overreliance on automation (or ‘automation bias’) in the deployment of increasing complex AI-augmented capabilities – such as cyber, C3I systems, AWS and precision missile munitions – designed to provide a distinct tactical advantage of machine-speed, will increase the vulnerability of these capabilities to exploitation. Moreover, the increasing substitution of human cognition for logic derived from machines will likely increase the opportunities for adversaries to exploit the limitations of narrow AI technology, i.e. lack of human intuition, brittleness in complex real-world situations and an inability to effectively detect or counter manipulation attacks (Libicki Citation 2018). Footnote 47 Therefore, until researchers unravel some of the unexplainable features of AI, human error and machine error will likely compound one another, with unpredictable results. Footnote 48 Simply put, we are at a critical crossroads in the parallel (and symbiotic) evolution of AI and cyberspace that national security communities, globally, will need to prepare for proactively (Johnson Citation 2019a).\n## Policy interventions\nA prominent theme that runs through this article – and central to understanding the potential impact AI for strategic stability and nuclear security more broadly – is the concern that AI systems operating at machine-speed will push the pace of combat to a point where the actions of machines surpass the (cognitive and physical) ability of human decision makers to control (or even comprehend) events.\nPossible multi-track policy responses to push back against the threat posed to stability from AI in a multipolar can be broadly categorized into: (1) those that focus on enhancing debate and discussion between researchers, global defence communities, decision makers, academics and other political and societal stakeholders; and (2) a range of specific policy recommendations and tools for great military powers to negotiate and implement.\n## Enhancing debate and dialogue\nFirst, to mitigate (or at least manage) the destabilizing and escalatory risks posed by the AI-cybersecurity nexus, great military powers must closely coordinate their confidence-building\nmeasures in an effort to pre-empt some of the risks to stability outlined above. Track-1 and track-2 discussions should include diplomats and military leaders (especially from China, Russian and the United States), industry experts, AI researchers and multi-disciplinary academics. Specifically, great military powers should establish an international framework for governance, norms, behaviour and the regulation of policy on machine learning-augmented cyber capabilities. Resistance to these efforts will likely come from states who worry that in revealing their cyber (especially offensive) capabilities, they could upend the deterrence utility of these tools.\nTo be sure, the challenges posed in the coordination and implementation of these policies will require bold and visionary leadership to circumvent the inevitable regional agendas, interdisciplinary resistance and burgeoning security dilemmas between rival states. Because of the rapid technological change in AI formal treaties, associated with arms-control agreements that require lengthy and complex negotiation and ratification processes, legal frameworks risk becoming obsolete before they come into effect. The historical record has demonstrated on several occasions that these kinds of challenges facing humanity can be overcome (Krimsky Citation 1962; Naur and Randell Citation 1968). Footnote 49\nSecond, the think tank community, academics and AI research experts should pool their resources to investigate the implications of the AI-cyber nexus for a range of potential security issues such as: the impact of AI bias on future military-use applications; how to prepare for (and react to) artificial general intelligence; and measures to mitigate, prevent and manage offensive uses of AI. Footnote 50 If necessary, steps could be taken to amend existing legal definitions of offensive cyber operations, such as hacking, data-poisoning attacks and spoofing, to account for the increased proliferation and potential damage caused by AI.\nThird, and related, due to the intrinsic dual-use nature of the AI-cyber security problem set, this dialogue should also be expanded to include other stakeholders such as private-sector AI and cybersecurity experts, the commercial sector, ethicists, philosophers, civil society and public opinion. Footnote 51 States should also collaborate on dual-use AI research to leverage AI's low-cost and scaling advantages (i.e. in autonomy and robotics). Further, a focus on the safety, testing and robustness of AI systems is a critical step in mitigating potential vulnerabilities and risks caused by errors, bias, and explainability in uncontrolled and complex environments. Footnote 52 In the case of offensive AI-related cybersecurity research, where public distribution might cause vulnerabilities and worsen security, publication could be confined to trusted organisations and entities.\nBest practices that exist in more mature methods for addressing dual-use concerns, like computer security, where applicable, might be applied to AI. For example, the wide use of red teaming exercises to enhance network security, organisation and practices. Specifically, AI-cyber red teaming – like DARPA's Cyber Grand Challenge – will enable engineers and operators to better\nunderstand the skills needed to execute particular offensive and defensive operations (especially using machine learning techniques), and to better manage system vulnerabilities, adversarial exploitation, stress-testing and social engineering challenges (Anderson, Woodbridge, and Filar Citation 2016).\nA recent study explored case studies of previous examples (biological weapons, cryptography and nuclear technology) of dual-use technologies – with civilian and military uses – to examine potential insights for AI dual-use risk management policies such as export controls and prepublication reviews. These insights also highlight the potential challenges of establishing regulatory, legal and normative frameworks for dual-use technologies – for example, the cautionary tale of ineffective efforts, in the late 1990s, to regulate cryptographic algorithms and cyber-network security tools through export controls.\nThe extent to which the integration of AI into dual-use weapon systems (such as cybersecurity) might influence actors' attitude to risk, the offence-defence balance and, in turn, perceptions of others' intentions and capabilities, could have profound implications for strategic deterrence and nuclear security. Footnote 53 In sum, cognizant that some states have deployed (or imminently plan to deploy) AI systems, experts generally agree that AI requires further experimentation, testing and development before being integrated into lethal weapon systems and their decision-making support systems.\n# Taking the lead from the cybersecurity community\nSome examples of cybersecurity-centered measures that might be further researched and implemented to pre-empt and mitigate some of risks posed by AI-augmented cyberattacks (especially involving safety-critical nuclear systems) highlighted in this paper include the following.\nFirst, coordinating AI-simulated war games, red teaming creative thinking exercises and creating redundancies (i.e. back-ups or fail-safes) in networks to detect errors, fix vulnerabilities and increase the reliability and robustness of military (especially nuclear-centric) systems. Specific procedures could be implemented to enable confidential reporting and fixing the vulnerabilities, subversions and other kinds of manipulations detected in AI systems. These findings could be used to track the proliferation of AI-related incursions and then countermeasures could be developed and standardised to manage these threats. Footnote 54\nSecond, states should formalise verification methods and protocols and consider issues such as: to what extent, under what circumstances, and for what types of AI systems can formal verification be implemented? Might other approaches be developed to achieve similar goals (e.g. machine learning and big data analysis augmented verification methods)? For example, DARPA's Assured Autonomy Program combines data-driven machine learning algorithms to assure the\nsafety of autonomous cyber physical systems (air, ground, sea and undersea unmanned vehicles), which continue to learn throughout their lifespans.\nThis learning characteristic makes assurance or verification using traditional methods especially challenging. Whether AI applications in the military domain can be formally verified, for now, is an unanswered question. Footnote 55 Much like that of cyber systems, the complexity of AI systems and the difficulty of defining their properties for formal verification makes them less amenable to verification compared to other types of technology.\nThird, the global defence communities should actively invest in the development of AI cyber-defence tools (e.g. analysing classification errors, automatic detection of remote vulnerability scanning and model extraction improvements), AI-centric secure hardware, and other fail-safe mechanisms, to allow for de-escalation and to prevent unintentional or accidental escalation.\nSeveral issues that need further exploration related to these measures include: the extent to which existing tools might be effective against vulnerabilities in AI systems? How can these tools be tailored for AI systems across multiple military domains? Is there an equivalent to 'patching' in military AI systems? What kinds of policies might incentivize, and ensure compliance with, meaningful reforms to existing hardware in the military sphere? While these questions are challenging, and necessarily speculative for now, answers will become more evident as the technology matures.\n# Disclosure statement\nNo potential conflict of interest was reported by the author.",
  "flat_text": "# ABSTRACT\nHow could AI-infused cyber capabilities be used to subvert, or otherwise compromise, the reliability, control and use of states' nuclear forces? This article argues that a new generation of artificial intelligence (AI) enhanced cyber capabilities will amplify the risk of inadvertent escalation caused by the co-mingling of nuclear and strategic non-nuclear weapons and the increasing speed of warfare, thereby increasing the risk of nuclear confrontation. It examines the potential implications of cyber (offensive and defensive) capabilities augmented with AI applications for nuclear security. The article concludes that future iterations of AI-enhanced cyber counterforce capabilities will complicate the existing challenges of cyber defence, and in turn, compromise nuclear assets and increase the escalatory effects of offensive cyber capabilities.\n# KEYWORDS:\n- Artificial intelligence\n- cyber-security\n- US-China Relations\n- nuclear security\n- deterrence policy\n- emerging technology\n# Introduction\nGiven the hyperbole surrounding AI, it is easy to overstate the opportunities and challenges posed by the development and deployment of AI in the military sphere (Boulanin Citation 2019). Today, there remains a large amount of debate among AI researchers surrounding the significance of several significant technical and operational challenges in the deployment of AI-infused systems (e.g. drone swarming, command and control decision-making support systems and a broader range of autonomous weapon systems). Moreover, many of the risks posed by AI in the nuclear domain today are not necessarily new (Boulanin Citation 2019). Recent advances in AI (especially machine learning techniques) exacerbate existing risks to escalation and stability rather than generating entirely new ones. While AI could enable significant improvements in many military domains (including nuclear weapons), for the foreseeable future, developments in military AI will likely be far more prosaic than implied in popular culture. The main worry for nuclear stability, therefore, is that militaries will underestimate (or overstate) or ignore the potential shortcomings and risks associated with the use of the current generation of AI technology in the safety-critical military sphere – and especially the nuclear domain.\nBecause of the diverse approaches to AI-based research, there is no universally accepted definition of ‘artificial intelligence’, which is confusing when the term is applied generically to make grandiose claims about its revolutionary effects. Footnote    AI can be understood as a universal term for improving the performance of automated (or autonomous) systems to solve a wide variety of complex tasks including: perception (sensors, computer vision, audio and image processing); reasoning and decision-making (problem solving, searching, planning and reasoning); learning and knowledge representation (machine learning, deep networks and modelling); Footnote    communication (language processing); autonomy and robotics ; and human-AI collaboration (humans define the systems’ purpose, goals and context).\nIn a military context, as a potential enabler and force multiplier of advanced weapon systems, AI is more akin to electricity, radio, radar and intelligence, surveillance and reconnaissance (ISR) support systems than a 'weapon' per se. Much like these enabling support systems, AI is also strategically vital, vulnerable, inherently cross-domain and dependent on advanced technology. Thus, even if AI applications are unable to make better battlefield decisions than humans, Footnote  $^{3}$  militaries that use AI in human-machine teaming will doubtless gain significant advantages (e.g. remote-sensing, situational-awareness, battlefield-manoeuvres and a compressed decision-making loop), compared to those who depend on human judgment – and semi-autonomous technology – alone. This is particularly the case, in operating systems in operating environments that demand endurance and rapid decision-making across multiple combat zones (Ayoub and Payne Citation 2016).\nThis article argues that a new generation of AI-augmented offensive cyber capabilities will amplify the risk of inadvertent escalation posed by the co-mingling of nuclear and strategic (or counterforce) non-nuclear weapons and the increasing speed of warfare, and in turn, increase the risk of nuclear confrontation. The article's thesis is grounded in three core themes. First, AI does not exist a vacuum, that is, in isolation, AI will unlikely be a strategic game changer. Instead, it will likely mutually reinforce the destabilizing effects of existing advanced capabilities thereby increasing the speed of warfare and compressing the decision-making timeframe (Johnson Citation 2019a). AI-enabled and enhanced capabilities will have a more significant impact (positive or negative) on strategic stability than the sum of its parts. Furthermore, the intersection of AI with nuclear weapons, and a broader spectrum of strategic non-nuclear weapons, will likely accelerate the erosion of the survivability of nuclear arsenals associated with the 'computer revolution'. Put another way, military AI and the advanced capabilities it enables are a natural manifestation – rather than the cause or origin – of an established trend, which could lead states to adopt destabilizing launch postures due to the increasing speed of war and co-mingling.\nSecond, AI's impact on stability, deterrence and escalation will likely be determined as much (or more) by states' perception of its functionality as much as by what it is capable of doing. In the case of nuclear strategy and deterrence, the perception of an adversary's capabilities and\nintentions is as (or more) important than its actual capability. Thus, in addition to the importance of military force postures, capabilities and doctrine, the effects of AI will also have a strong cognitive element, increasing the risk of inadvertent escalation as a result of misperception and misunderstanding.\nFinally, and related to the above, the increasingly competitive and contested nuclear multipolar world order will compound the destabilizing effects of AI, and in turn, increase the escalation risks in future warfare between great military powers – especially China and the United States (Johnson Citation 2019b). Moreover, the potential operational and strategic advantages offered by AI-augmented capabilities could prove irresistible to nuclear-armed strategic rivals, causing them to eschew the limitations of AI, and compromise safety and verification standards to protect, or attempt to capture, technological superiority on the future digitised battlefield.\n# Military AI primer\nArtificial intelligence research began as early as the 1950s, as a broad concept concerned with the science and engineering of making intelligent machines. Since the 1950s, AI research has gone through several development phases – from early exploitations in the 1950s and 1960s and the 'AI Summer' during the 1970s, through to the early 1980s and the 'AI Winter' from the 1980s – each of which failed to live up to its initial, and often over-hyped, expectations. In the past decade, the explosion of interest in the field (or 'AI renaissance') occurred due to the convergence of four enabling developments: (1) the exponential growth in computing processing power; (2) expanded data sets (especially 'big data' sources) (Gray et al. Citation 2015); Footnote  $^{4}$  (3) advances in the implementation of machine learning techniques and algorithms (especially deep 'neural networks') (Schmidhuber Citation 2015); Footnote  $^{5}$  and (4) the rapid expansion of commercial interest and investment in AI (Hoadley and Nathan Citation 2018). Footnote  $^{6}$\nAI is concerned with machines that emulate capabilities that are usually associated with human intelligence, such as language, reasoning, learning, heuristics and observation. Today, all practical (i.e. technically feasible) AI applications fall into the 'narrow' category, and less so, artificial general intelligence (AGI) or 'superintelligence' (Reedy Citation 2017). Footnote  $^{7}$  'Narrow AI' has been widely used in a broad range of civilian and military tasks since the 1960s, (Nilsson Citation 2010; Russel and Norvig Citation 2014) and involves statistical algorithms (mostly based on machine learning techniques) that learn procedures through analysis of large training data sets designed to approximate and replicate human cognitive tasks (Russel and Norvig Citation 2014). Footnote  $^{8}$  'Narrow AI' is the category of artificial intelligence to which this article refers when it assesses the impact of this technology in a military context.\nGiven the diverse approaches to research in AI (Boden Citation 2016 ; Vernon Citation 2014), Footnote  $^{9}$  there is no universally accepted definition of AI. A recent US congressional report defines AI as follows: 'Any artificial system that performs tasks under varying and unpredictable circumstances, without significant human oversight, or that can learn from their experience and improve their performance ... they may solve tasks requiring human-like perception, cognition, planning, learning, communication, or physical action' (Hoadley and Nathan Citation 2018).\nToday, a large gap exists between the reality of what AI technology is capable of doing in a military context, and the expectations and fears of public opinion, policymakers and global defence communities. How will AI technologies influence public opinion about the use of military force and warfare? (Sechser, Narang, and Talmadge Citation 2019). The misrepresentations and misconceptions that exist today in the narratives surrounding AI (especially in societal, economic and national security debates) and are in large part caused by the hyperbole and exaggerated depictions of AI in science fiction (Zarkadakis Citation 2015). Footnote  $^{10}$  Misrepresentations of the potential opportunities and risks associated with this technology in the military sphere can obscure constructive and crucial debate on these topics. Specifically, this includes the challenge of balancing the potential operational, tactical and strategic benefits of leveraging AI while managing the risks posed to nuclear security in the (especially premature) pursuit of these advantages.\nThe historical record on technological change in a military context counsels scepticism against alarmist predictions and extrapolations from emerging trends: technologies rarely evolve in the way futurists predict and many applications have had countervailing or conditional effects that have ameliorated the dystopian predictions of naysayers (Metz Citation 2000 ; Metz and Kievit Citation 1995). In short, AI's impact on nuclear stability will, like previous generations of emerging technology (i.e. chemical and biological weapons, cyber and nuclear weapons), have both stabilising and destabilizing (and possibly contradictory) effects on strategic stability. For example, many expected that chemical weapons would instantly and dramatically change the nature of warfare and deterrence after the British used poison gas during World War I (Sechser, Narang, and Talmadge Citation 2019). However, chemical weapons proved far less practical, impactful, disruptive and relatively easier to defend against than conventional explosives. As a corollary, the degree to which military AI poses risks to future strategic stability will depend in large part on the pace and scope with which this technology facilitates new ways to improve the delivery of, and defence against, nuclear weapons and strategic non-nuclear weapons.\nConceptually, AI-augmented applications can be categorized into those that have predominately operational, tactical and strategic implications in future warfare. At the operational and tactical level, applications include: autonomy and robotics; multi-actor interaction red teaming wargaming; Footnote  $^{11}$  big data-driven modelling; Footnote  $^{12}$  intelligence collection and analysis (e.g. to locate and monitor mobile missiles, troops movement) (Conklin Citation 2018); Footnote\n$^{13}$ cybersecurity talent management; predictive maintenance, logistics, planning and forecasting; and vendor contract and budget management. In preparation for a new AI strategy, US Strategic Operations Command (SOCOM) recently crafted a roadmap that invests heavily in AI and machine learning technology based on several pilot projects with three overarching goals to create an algorithmic warfare multi-operational team: 'AI-ready workforce, AI-applications, and AI-outreach' (Lee Citation 2019).\nTherefore, AI's impact on strategic stability and escalation will also be shaped by broader and more nuanced factors which influence the trajectory of emerging technologies, including: military strategy and doctrine – that seeks to manipulate escalation risks – military culture and organisation, alliance structures, domestic politics and public opinion, to name but a few. The Cold War record demonstrates that emerging technologies act primarily to enable independent variables which, in combination with other endogenous factors, can heighten escalation risks between adversaries. Therefore, in isolation, technology is not a major exogenous cause of (inadvertent or intentional) military escalation.\nAt a strategic level of warfare, AI uses include: qualitative improvements to the nuclear command, control, communications, and intelligence (C3I) architecture; enhancing target acquisition, tracking, guidance systems and discrimination of missile and air defence systems; being force multipliers of both offensive and defensive machine-learning infused cyber capabilities; and qualitatively bolstering nuclear and non-nuclear missile delivery systems – including hypersonic variants. Footnote 14\nIn combination, AI machine learning algorithms fused with advances in sensor technology could be engineered to allow nuclear delivery systems to operate more autonomously and precisely, with less reliance on human operators calibrating navigation and guidance parameters and equipped with more robust countermeasures against jamming or spoofing attacks. At the strategic level, AI-augmented command and control systems could potentially mitigate many of the shortcomings inherent to human strategic decision-making during the 'fog of war', such as the susceptibility to invest in sunk costs, skewed risk judgment, cognitive heuristics and groupthink.\nIn sum, the fusion of AI machine learning and human judgment to gauge an adversary's intentions (and predict escalation) for the purposes of planning and directing future wars for the pursuit of political objectives, is, therefore, a far less unlikely prospect in the near future than the use of AI to achieve tactical and operational ends (e.g. drone swarming and cyber defence).\n# Blurring the AI-cyber offence-defence line: (Glaser and Kaufmann Citation 1998 ; Lynn-Jones Citation 1995 ) Footnote 15\nSeveral US national security officials believe that AI, used as force multipliers for both defensive and offensive cyber weapons, will have a transformative impact on cybersecurity (Gartzke and Lindsay Citation 2015 ; Rid Citation 2012 ; Slayton Citation 2017 ; Hoffman Citation 2019). Footnote 16 Director of US National Intelligence, Daniel Coats, recently warned that AI could increase US vulnerability to cyberattacks, weaken its ability to attribute such attacks, improve the effectiveness and capabilities of adversaries weapon and intelligence systems, and cause accidents and related liability issues. To be sure, the line between AI cyber-offence and cyber-defence will likely remain an obscure one. Footnote 17 Bernard Brodie’s words of caution about the advent of nuclear weapons almost six decades ago resonate: ‘The [military] bias towards the offensive creates special problems in any technologically new situation where there is little or no relevant war experience to help one to reach a balanced judgment’ (Brodie Citation 1959).\nOn the one hand, AI might reduce a military’s vulnerability to cyberattacks. AI cyber-defence tools (or ‘counter-AI’), designed to recognise changes to patterns of behaviour in a network and detect anomalies, automatically detect software code vulnerabilities, and apply machine learning techniques (such as ‘deep learning’) to detect deviations and anomalies from normal network activity, Footnote 18 could form a more robust defence against cyber subversions (Johnson Citation 2019a). Footnote 19 According to the Pentagon’s new AI strategic report, ‘AI can enhance our ability to predict, identify, and respond to cyber and physical threats from a range of sources’ (US Department of Defense Citation 2019). In addition, the DoD’s Defense Innovation Unit (DIU) is prototyping an application (related to Project VOLTRON) that leverages AI to decipher high-level strategic questions, map probabilistic chains of events and develop alternative strategies in order to make DoD systems more resilient to AI-augmented cyberattacks and configure and fix errors more quickly than humans.\nOn the other hand, autonomy itself might increase a military’s vulnerability to cyberattacks, which rely on stealth, deception and stratagem. For example, an adversary could use malware to take control, manipulate or fool the behaviour and pattern recognition systems of autonomous systems, such as DoD’s Project Maven. Offensive attacks such as this would be relatively easy to execute, but very difficult to detect, attribute or effectively counter (Shachtman Citation 2011). Footnote 20 This problem set is compounded by the lack of an agreed framework or understanding of what constitutes escalatory behaviour (or ‘firebreaks’) in cyberspace. Thus, a cyber operation intended as a signal (i.e. for coercive diplomacy) could go undetected by the target, or worse, misinterpreted as an offensive attack. Even if information relating to an\noperation of this kind is accurately identified in a timely manner, the motives behind them could remain ambiguous, or misperceived. According to Robert Jervis, ‘it is likely that the country that is the object of the attack would assume that any effect was the intended one’ (Jervis Citation 2016).\nUS Cyber Fleet Command Commander, Michael Gilday, recently told the Senate Armed Services Committee that the US Navy must ‘improve an ability to proactively detect new and unknown malware … so we [the US] can act quickly using advanced analytics enabled by AI and machine learning’, which may give the US a ‘tactical advantage’ to identify malicious activity early on (Osborn Citation 2018). Even if analysts can obtain high-quality and reliable intelligence, however, they may not want to reveal it, because doing so could compromise a source, capability or tactic (Johnson Citation 2019a). Moreover, most observers now acknowledge that no strategy (i.e. combining offence and defence cyber operations) could realistically be expected to deter all malign cyber-attacks (Hoffman Citation 2019). Footnote 21 While AI-enhanced cyber capabilities can enhance deterrence, they can simultaneously incentivize others to attack thereby exacerbating the paradox of enhanced capabilities and increased vulnerabilities in the cyber domain (Gartzke and Lindsay Citation 2017; Libicki Citation 2016; Slayton Citation 2017). Footnote 22 As the historical record attests, this capability-vulnerability paradox is heightened when states are both dependent on a particular capability (such as AI and cyber tools) and their access or ability to use the capabilities is vulnerable to an adversary’s exploitation or subversion – creating first-mover incentives (Rid Citation 2013). Footnote 23\n# Cybersecurity and nuclear weapon systems: nuclear risk redux?\nAt a strategic level of conflict, AI applications designed to enhance cybersecurity for nuclear forces could simultaneously make cyber-dependent nuclear weapon systems (i.e. communications, data processing or early-warning sensors) more vulnerable to cyberattacks. It is now thought possible that a cyberattack (i.e. spoofing, hacking, manipulation and digital jamming) could infiltrate a nuclear weapons system, threaten the integrity of its communications, and ultimately (and possibly unbeknown to its target) gain control of its (nuclear and non-nuclear) command and control systems. Footnote 24 AI technology has not yet evolved to a point where it would allow nuclear-armed states to credibly threaten the survivability of each other’s nuclear second-strike capability. Thus, the development trajectory of AI (and its enabling technologies) means its impact on nuclear security will likely be theoretical and speculative for the foreseeable future.\nBecause of the intense time pressures that would likely loom large with the decision to use nuclear weapons – especially where a state maintains a launch-on-warning posture – AI-enhanced\ncyberattacks against nuclear systems would be almost impossible to detect and the warning signals difficult to authenticate, let alone attribute, within the short timeframe for initiating a nuclear strike. According to open sources, operators at the North American Aerospace Defence Command (NORAD) have less than three minutes to assess and confirm initial indications from early-warning systems of an incoming attack. This compressed decision-making time frame could put political leaders under intense pressure to make a decision to escalate during a crisis, with incomplete (and possibly false) information of a situation. Ironically, new technologies designed to enhance information (i.e. modernised nuclear C3I systems augmented by 5G networks, machine learning, big-data analytics and quantum computing) can also undermine clear and reliable information flow and communication, critical for effective deterrence (Gartzke and Lindsay Citation 2019). Footnote 25\nAdvances in AI could also exacerbate this cybersecurity challenge by enabling improvements to the cyber offence. Machine learning and AI by automating advanced persistent threat (APT) operations might dramatically reduce the extensive manpower resources and high levels of technical skill required to execute APT operations (or 'hunting for weaknesses'), especially against hardened nuclear targets. Footnote 26 The machine speed of AI-augmented cyber tools could enable a low-skilled and capital-restricted attacker to exploit a narrow window of opportunity to penetrate an adversary's cyber-defences or use APT tools to find new vulnerabilities. For example, when docked for maintenance air-gapped nuclear-powered submarines, considered secure when submerged, could become increasingly vulnerable to a new generation of low-cost – possibly black-market – and highly automated APT cyberattacks.\nAn attacker could also apply AI machine learning techniques to target autonomous dual-use early-warning and other operating systems (e.g. C3I, ISR, early-warning and robotic control networks) with 'weaponized software' such as hacking, subverting, spoofing or tricking, causing unpredictable and potentially undetectable errors, malfunctions and behavioural manipulation to weapons systems – or 'data-poisoning'. Footnote 27 Furthermore, as the linkages between digital and physical systems (or the 'Internet of Things') expand, the potential for to an adversary to use cyberattacks in both kinetic and non-kinetic attacks will increase. A significant risk variable in the operation of autonomous systems is the time that passes between a system failure (i.e. performing in a manner other than how the human operator intended) and the time it takes for a human operator to take corrective action. If the system failure is the result of a deliberate act, this time frame will be compressed (Johnson Citation 2019a).\nEven if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. This skewed assessment by states in the context of nuclear weapons, which are ready to launch nuclear weapons at a moment's notice, would likely precipitate worst-case scenario thinking that may spark an inadvertent escalation (Talmadge\nCitation 2017). Footnote 28 During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 29 For example, an AI-enabled third party generated 'deepfake' (audio and video manipulation), coupled with data-poisoning cyberattacks, could spark an escalatory crisis between two (or more) nuclear states (Fitzpatrick Citation 2019). Footnote 30\nHow could AI-augmented cyber capabilities create new pathways for accidental or inadvertent escalation? To illustrate these dynamics: State A launches a malicious AI-enhanced cyberattack to spoof State B's AI-enabled autonomous sensor platforms and automated target recognition systems, in such a way that the weapon system (e.g. a human-supervised ATR system) is fooled into interpreting a civilian object as a military target. B in response, based on manipulated or erroneous information, and the inability of human supervisors to detect the spoofed imagery that fooled the weapons' automated target recognition algorithm in time to take corrective action, accidentally (and unintentionally) escalates a situation (Libicki Citation 2018). In this example, the spoofing attack on the weapon systems' algorithm is executed in such a way that the imagery appears to the recognition system as indistinguishable from a valid military target (Rohrbach et al. Citation 2018) Footnote 31 escalating a situation based on a false premise that would unlikely to fool the human eye. Footnote 32\nAlso, the explainability (or 'black box') problem associated with AI applications could compound these potential escalation dynamics. Footnote 33 Insufficient understanding of how and why AI algorithms reach a particular judgment or decision might complicate the task of determining whether data sets had been deliberately compromised to manufacture false outcomes (e.g. attacking incorrect targets or misdirecting allies during combat). Footnote 34 Moreover, as humans and AI team up to accomplish particular missions, the opacity associated with how AI systems reach a decision may cause an operator to have either too much or too little confidence in a system's performance. As a result, unless the system's machine learning algorithm is terminated, at the end of the training phase, once deployed, it could potentially learn something it was not intended to, or even perform a task or mission that its human designers do not expect it to do. Footnote 35\nIn sum, technologies that improve the reliability and speed with which information is processed and disseminated by early-warning systems may paradoxically also increase the vulnerabilities of these networks, creating new first-mover advantages and escalation pathways during a crisis that spark unintentional (or intentional) warfare (Schneider Citation 2019). Footnote 36 Put another way, AI might be developed in order to enable a state to pursue a pre-determined escalatory path. Thus, increased escalation risk as a result of technology is not always inadvertent or accidental (Long and Rittenhouse Green Citation 2015). Footnote 37\n# 'Cyber guns' supercharged with AI machine learning\nWhile manipulations and system subversions are possible with existing cyber offence tools, rapid advances in AI and increasing degrees of military autonomy could amplify the potential speed, power and scale of future attacks in cyberspace. Footnote 38 There are three significant intersections between AI systems and cybersecurity that are most salient to the military domain: (1) advances in autonomy and machine learning techniques mean that a much broader range of physical systems are now vulnerable to cyberattacks (i.e. hacking and data-poisoning) (Greenberg Citation 2016); Footnote 39 (2) cyberattacks on AI systems can offer attackers access to machine learning algorithms, trained models being used by the application and a potential vast amounts of data from facial recognition and intelligence collection and analysis systems (e.g. satellite navigation and imagery systems used to cue precision munitions and supported intelligence, surveillance, reconnaissance missions); and (3) the use of AI systems in conjunction with existing cyber offence tools will enable sophisticated cyberattacks to be executed at larger scale (both geographically and across networks), at faster speeds, and simultaneously across multiple military domains with improved anonymity. In short, despite the relatively benign augmentation mechanisms used to develop cyber offensive capabilities with AI systems, the resultant impact on the speed and scope of AI cyber tools will likely have destabilizing effects, nonetheless. Footnote 40\nRecent advances in machine learning have significantly contributed to resolving several technical bottlenecks in several fields of AI, which could allow for significant qualitative improvements to a large variety of autonomous weapon systems and applications. In combination, machine learning and autonomy could intersect with nuclear security in a multitude of ways, with both positive and negative implications for strategic stability. For example, machine learning could enhance the detection capabilities of (nuclear and non-nuclear) early-warning systems and improve the collection and cross-analysis of ISR information, thus reducing the risk of inadvertent and accidental escalation. Machine learning may also bolster the cyber-security of command and control systems, create new and possibilities for arms control, testing, verification and monitoring nuclear stockpiles, and enhance conventional counterforce capabilities. Unmanned autonomous systems could be deployed in complex missions in hitherto inaccessible and cluttered environments (e.g. under-sea anti-submarine warfare) and aerial and underwater drones in swarms might eventually replace intercontinental ballistic missiles (ICBMs) and nuclear-armed submarines (SSBNs) for the delivery of nuclear weapons.\nIn response to these anticipated vulnerabilities, and to achieve the (perceived) first-mover advantages the increased speed in AI-augmented cyber tools could confer, China, Russia and the United States have continued to harden their AI cyber defences. Despite these parallel efforts, divergent views exist on how strategic communities perceive the nature of the risks posed by AI-enhanced cyberattacks against dual-use C3I systems. Thus, AI-augmented cyber intelligence-\ngathering tools targeting an adversary's command and control assets (especially dual-use facilities) during a crisis may be misinterpreted as a prelude to an imminent pre-emptive attack on its nuclear force and therefore as undermining strategic stability.\nMoreover, AI machine learning could enable new offensive concepts such as a so-called 'left-of-launch operation' (Ellison Citation 2015), Footnote  $^{41}$  thereby compounding the ambiguities and fear about pre-emptive attacks in cyberspace, and increasing first-mover incentives – or use-them-or-lose-them situations (Buchanan Citation 2017; Buchanan and Miller Citation 2017). Footnote  $^{42}$  In the cyber domain, it is generally unclear in the early stages of a cyber operation whether an adversary intends to collect intelligence or prepare for an offensive attack, which is more likely to be used early on in a conflict scenario. Open sources suggest, for example, that Chinese analysts view the vulnerability of China's nuclear command, control, and communications (C3I) systems to cyber infiltrations – even if an attacker's objective was limited to cyberespionage – as a highly escalatory national security threat. By contrast, Russian analysts tend to view Russia's nuclear C3I network as relatively isolated, and thus insulated from cyberattacks.\nIrrespective of these differences, the uncertainty caused by the possibility, actual or otherwise, that AI-enhanced offensive cyber tools might be used (or threatened) to undermine the credibility and reliability of states' nuclear forces would be destabilizing, both as result of the existence of enhanced cyber offensive capabilities and the potential efficacy of these tools against nuclear forces. As the historical record attests, rational actors have incentives to misrepresent what they know (i.e. lie and bluff) in order to improve the terms of any settlement that may emerge from the wartime bargaining process – or the risk-reward payoff. Even a modicum of uncertainty about the effectiveness of AI-augmented cyber capabilities during a crisis or conflict would, therefore, reduce both sides' risk tolerance and increase the incentive to strike pre-emptively as a hedging strategy.\nA virtuous cycle that might flow from enhanced reassurances premised on comprehensive intelligence would require information symmetry (i.e. equal access to intelligence and analysis systems) between great and rising powers, and shared confidence in the accuracy and credibility of these systems. Perhaps most challenging of all in a world of 'revisionist' rising powers, the intentions of all rival states would need to be genuinely benign for this optimistic view to occur. Under crisis conditions, for example, an offensive AI cyber tool that succeeds in compromising an adversary's nuclear weapon systems, resulting in an 'asymmetric information' situation, could cause either or both sides to overstate (or understate) its retaliatory capabilities, and in turn, be more inclined to act in a risky and escalatory fashion. In short, in a competitive strategic environment, where states are inclined to assume the worst of others' intentions, one state's efforts to enhance the survivability of its strategic forces may be viewed by others as a threat to their nuclear retaliatory capability – or second-strike capacity.\n# Manipulation of the information landscape\nWhile machine learning big-data analytics, sensing technology, networks supported by 5G networks, could alert commanders of incoming threats with increased speed and precision, and make accidents caused by human error potentially less likely (especially accidents and malfunctions caused by false warning), these advances could simultaneously amplify escalation risks in two ways. Footnote 43\nFirst, AI machine learning used as force multipliers for cyber offence (e.g. data poisoning spoofing, 'deepfakes', manipulation, hacking and digital jamming) would be considerably more difficult to detect – especially if an attacker used AI advanced persistent threat (APT) tools – or 'hunting for weaknesses' in spectrum-contested environments. Second, in the unlikely event an attack was successfully detected, threat identification (or attribution) at machine speed would be virtually impossible. In addition to the co-mingling and the speed of warfare issues, AI machine learning systems might also exacerbate the existing – well-established and non-kinetic – risk of inadvertent (and intentional) escalation from the manipulation of the information landscape in which decisions about nuclear weapons are located. For example, the use of deepfake videos built-in real-time and disseminated online for malicious purposes.\nHuman cognition, and thus effective deterrence and signalling intentions, is predicated on reliable and clear information; if an adversary is concerned that the information available to them is limited (or worse, inaccurate), they will likely assume the worst and act accordingly. Asymmetric information situations between rivals about the balance of military power could undermine crisis stability, and in turn, create rational incentives to escalate to nuclear confrontation. Consequently, states will be more inclined to assume the worst of others' intentions, especially in situations where the legitimacy of the status quo is contested (i.e. maritime Asia).\nIn the competitive strategic environment, it is easy to imagine unprovoked escalation caused by a malicious third-party (or state-proxy) clandestine action. Even if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 44 For example, in an effort to incite conflict between two rival states, State A uses proxy hackers to use a AI machine learning technique (i.e. generative adversarial networks GAN) to launch 'deepfake' video or audio material, depicting senior military commanders of State B conspiring to launch a pre-emptive strike on State C. Footnote 45 Then, this 'deepfake' footage is deliberately leaked into C's AI-augmented intelligence collection and analysis systems, provoking C to escalate the situation with strategic consequences. B responds in kind. Footnote 46\nThe current underdeveloped state of ‘counter-AI’ capabilities and other fail-safe mechanisms (e.g. circuit breakers) to de-escalate escalation in cyberspace, will make the unprovoked and unintentional escalation dynamics depicted in this scenario very challenging to mitigate. Moreover, in the emerging ‘deepfakes’ arms race (much like cybersecurity more generally), detection software will likely lag behind advances in offensive enabling solutions – or offence-dominant ones. According to computer science expert Hany Farid, there are probably 100–1000 times ‘more people developing the technology to manipulate content than there is to detect [it]’ (Fontaine and Frederick Citation 2019).\nThe overreliance on automation (or ‘automation bias’) in the deployment of increasing complex AI-augmented capabilities – such as cyber, C3I systems, AWS and precision missile munitions – designed to provide a distinct tactical advantage of machine-speed, will increase the vulnerability of these capabilities to exploitation. Moreover, the increasing substitution of human cognition for logic derived from machines will likely increase the opportunities for adversaries to exploit the limitations of narrow AI technology, i.e. lack of human intuition, brittleness in complex real-world situations and an inability to effectively detect or counter manipulation attacks (Libicki Citation 2018). Footnote 47 Therefore, until researchers unravel some of the unexplainable features of AI, human error and machine error will likely compound one another, with unpredictable results. Footnote 48 Simply put, we are at a critical crossroads in the parallel (and symbiotic) evolution of AI and cyberspace that national security communities, globally, will need to prepare for proactively (Johnson Citation 2019a).\n## Policy interventions\nA prominent theme that runs through this article – and central to understanding the potential impact AI for strategic stability and nuclear security more broadly – is the concern that AI systems operating at machine-speed will push the pace of combat to a point where the actions of machines surpass the (cognitive and physical) ability of human decision makers to control (or even comprehend) events.\nPossible multi-track policy responses to push back against the threat posed to stability from AI in a multipolar can be broadly categorized into: (1) those that focus on enhancing debate and discussion between researchers, global defence communities, decision makers, academics and other political and societal stakeholders; and (2) a range of specific policy recommendations and tools for great military powers to negotiate and implement.\n## Enhancing debate and dialogue\nFirst, to mitigate (or at least manage) the destabilizing and escalatory risks posed by the AI-cybersecurity nexus, great military powers must closely coordinate their confidence-building\nmeasures in an effort to pre-empt some of the risks to stability outlined above. Track-1 and track-2 discussions should include diplomats and military leaders (especially from China, Russian and the United States), industry experts, AI researchers and multi-disciplinary academics. Specifically, great military powers should establish an international framework for governance, norms, behaviour and the regulation of policy on machine learning-augmented cyber capabilities. Resistance to these efforts will likely come from states who worry that in revealing their cyber (especially offensive) capabilities, they could upend the deterrence utility of these tools.\nTo be sure, the challenges posed in the coordination and implementation of these policies will require bold and visionary leadership to circumvent the inevitable regional agendas, interdisciplinary resistance and burgeoning security dilemmas between rival states. Because of the rapid technological change in AI formal treaties, associated with arms-control agreements that require lengthy and complex negotiation and ratification processes, legal frameworks risk becoming obsolete before they come into effect. The historical record has demonstrated on several occasions that these kinds of challenges facing humanity can be overcome (Krimsky Citation 1962; Naur and Randell Citation 1968). Footnote 49\nSecond, the think tank community, academics and AI research experts should pool their resources to investigate the implications of the AI-cyber nexus for a range of potential security issues such as: the impact of AI bias on future military-use applications; how to prepare for (and react to) artificial general intelligence; and measures to mitigate, prevent and manage offensive uses of AI. Footnote 50 If necessary, steps could be taken to amend existing legal definitions of offensive cyber operations, such as hacking, data-poisoning attacks and spoofing, to account for the increased proliferation and potential damage caused by AI.\nThird, and related, due to the intrinsic dual-use nature of the AI-cyber security problem set, this dialogue should also be expanded to include other stakeholders such as private-sector AI and cybersecurity experts, the commercial sector, ethicists, philosophers, civil society and public opinion. Footnote 51 States should also collaborate on dual-use AI research to leverage AI's low-cost and scaling advantages (i.e. in autonomy and robotics). Further, a focus on the safety, testing and robustness of AI systems is a critical step in mitigating potential vulnerabilities and risks caused by errors, bias, and explainability in uncontrolled and complex environments. Footnote 52 In the case of offensive AI-related cybersecurity research, where public distribution might cause vulnerabilities and worsen security, publication could be confined to trusted organisations and entities.\nBest practices that exist in more mature methods for addressing dual-use concerns, like computer security, where applicable, might be applied to AI. For example, the wide use of red teaming exercises to enhance network security, organisation and practices. Specifically, AI-cyber red teaming – like DARPA's Cyber Grand Challenge – will enable engineers and operators to better\nunderstand the skills needed to execute particular offensive and defensive operations (especially using machine learning techniques), and to better manage system vulnerabilities, adversarial exploitation, stress-testing and social engineering challenges (Anderson, Woodbridge, and Filar Citation 2016).\nA recent study explored case studies of previous examples (biological weapons, cryptography and nuclear technology) of dual-use technologies – with civilian and military uses – to examine potential insights for AI dual-use risk management policies such as export controls and prepublication reviews. These insights also highlight the potential challenges of establishing regulatory, legal and normative frameworks for dual-use technologies – for example, the cautionary tale of ineffective efforts, in the late 1990s, to regulate cryptographic algorithms and cyber-network security tools through export controls.\nThe extent to which the integration of AI into dual-use weapon systems (such as cybersecurity) might influence actors' attitude to risk, the offence-defence balance and, in turn, perceptions of others' intentions and capabilities, could have profound implications for strategic deterrence and nuclear security. Footnote 53 In sum, cognizant that some states have deployed (or imminently plan to deploy) AI systems, experts generally agree that AI requires further experimentation, testing and development before being integrated into lethal weapon systems and their decision-making support systems.\n# Taking the lead from the cybersecurity community\nSome examples of cybersecurity-centered measures that might be further researched and implemented to pre-empt and mitigate some of risks posed by AI-augmented cyberattacks (especially involving safety-critical nuclear systems) highlighted in this paper include the following.\nFirst, coordinating AI-simulated war games, red teaming creative thinking exercises and creating redundancies (i.e. back-ups or fail-safes) in networks to detect errors, fix vulnerabilities and increase the reliability and robustness of military (especially nuclear-centric) systems. Specific procedures could be implemented to enable confidential reporting and fixing the vulnerabilities, subversions and other kinds of manipulations detected in AI systems. These findings could be used to track the proliferation of AI-related incursions and then countermeasures could be developed and standardised to manage these threats. Footnote 54\nSecond, states should formalise verification methods and protocols and consider issues such as: to what extent, under what circumstances, and for what types of AI systems can formal verification be implemented? Might other approaches be developed to achieve similar goals (e.g. machine learning and big data analysis augmented verification methods)? For example, DARPA's Assured Autonomy Program combines data-driven machine learning algorithms to assure the\nsafety of autonomous cyber physical systems (air, ground, sea and undersea unmanned vehicles), which continue to learn throughout their lifespans.\nThis learning characteristic makes assurance or verification using traditional methods especially challenging. Whether AI applications in the military domain can be formally verified, for now, is an unanswered question. Footnote 55 Much like that of cyber systems, the complexity of AI systems and the difficulty of defining their properties for formal verification makes them less amenable to verification compared to other types of technology.\nThird, the global defence communities should actively invest in the development of AI cyber-defence tools (e.g. analysing classification errors, automatic detection of remote vulnerability scanning and model extraction improvements), AI-centric secure hardware, and other fail-safe mechanisms, to allow for de-escalation and to prevent unintentional or accidental escalation.\nSeveral issues that need further exploration related to these measures include: the extent to which existing tools might be effective against vulnerabilities in AI systems? How can these tools be tailored for AI systems across multiple military domains? Is there an equivalent to 'patching' in military AI systems? What kinds of policies might incentivize, and ensure compliance with, meaningful reforms to existing hardware in the military sphere? While these questions are challenging, and necessarily speculative for now, answers will become more evident as the technology matures.\n# Disclosure statement\nNo potential conflict of interest was reported by the author.",
  "toc": [
    [
      1,
      "__preamble__"
    ],
    [
      1,
      "# Introduction"
    ],
    [
      1,
      "# Military AI primer"
    ],
    [
      1,
      "# Blurring the AI-cyber offence-defence line: (Glaser and Kaufmann Citation 1998 ; Lynn-Jones Citation 1995 ) Footnote 15"
    ],
    [
      1,
      "# Cybersecurity and nuclear weapon systems: nuclear risk redux?"
    ],
    [
      1,
      "# 'Cyber guns' supercharged with AI machine learning"
    ],
    [
      1,
      "# Manipulation of the information landscape"
    ],
    [
      1,
      "# Taking the lead from the cybersecurity community"
    ],
    [
      1,
      "# Disclosure statement"
    ]
  ],
  "sections": {
    "__preamble__": "# ABSTRACT\nHow could AI-infused cyber capabilities be used to subvert, or otherwise compromise, the reliability, control and use of states' nuclear forces? This article argues that a new generation of artificial intelligence (AI) enhanced cyber capabilities will amplify the risk of inadvertent escalation caused by the co-mingling of nuclear and strategic non-nuclear weapons and the increasing speed of warfare, thereby increasing the risk of nuclear confrontation. It examines the potential implications of cyber (offensive and defensive) capabilities augmented with AI applications for nuclear security. The article concludes that future iterations of AI-enhanced cyber counterforce capabilities will complicate the existing challenges of cyber defence, and in turn, compromise nuclear assets and increase the escalatory effects of offensive cyber capabilities.\n# KEYWORDS:\n- Artificial intelligence\n- cyber-security\n- US-China Relations\n- nuclear security\n- deterrence policy\n- emerging technology",
    "# Introduction": "Given the hyperbole surrounding AI, it is easy to overstate the opportunities and challenges posed by the development and deployment of AI in the military sphere (Boulanin Citation 2019). Today, there remains a large amount of debate among AI researchers surrounding the significance of several significant technical and operational challenges in the deployment of AI-infused systems (e.g. drone swarming, command and control decision-making support systems and a broader range of autonomous weapon systems). Moreover, many of the risks posed by AI in the nuclear domain today are not necessarily new (Boulanin Citation 2019). Recent advances in AI (especially machine learning techniques) exacerbate existing risks to escalation and stability rather than generating entirely new ones. While AI could enable significant improvements in many military domains (including nuclear weapons), for the foreseeable future, developments in military AI will likely be far more prosaic than implied in popular culture. The main worry for nuclear stability, therefore, is that militaries will underestimate (or overstate) or ignore the potential shortcomings and risks associated with the use of the current generation of AI technology in the safety-critical military sphere – and especially the nuclear domain.\nBecause of the diverse approaches to AI-based research, there is no universally accepted definition of ‘artificial intelligence’, which is confusing when the term is applied generically to make grandiose claims about its revolutionary effects. Footnote    AI can be understood as a universal term for improving the performance of automated (or autonomous) systems to solve a wide variety of complex tasks including: perception (sensors, computer vision, audio and image processing); reasoning and decision-making (problem solving, searching, planning and reasoning); learning and knowledge representation (machine learning, deep networks and modelling); Footnote    communication (language processing); autonomy and robotics ; and human-AI collaboration (humans define the systems’ purpose, goals and context).\nIn a military context, as a potential enabler and force multiplier of advanced weapon systems, AI is more akin to electricity, radio, radar and intelligence, surveillance and reconnaissance (ISR) support systems than a 'weapon' per se. Much like these enabling support systems, AI is also strategically vital, vulnerable, inherently cross-domain and dependent on advanced technology. Thus, even if AI applications are unable to make better battlefield decisions than humans, Footnote  $^{3}$  militaries that use AI in human-machine teaming will doubtless gain significant advantages (e.g. remote-sensing, situational-awareness, battlefield-manoeuvres and a compressed decision-making loop), compared to those who depend on human judgment – and semi-autonomous technology – alone. This is particularly the case, in operating systems in operating environments that demand endurance and rapid decision-making across multiple combat zones (Ayoub and Payne Citation 2016).\nThis article argues that a new generation of AI-augmented offensive cyber capabilities will amplify the risk of inadvertent escalation posed by the co-mingling of nuclear and strategic (or counterforce) non-nuclear weapons and the increasing speed of warfare, and in turn, increase the risk of nuclear confrontation. The article's thesis is grounded in three core themes. First, AI does not exist a vacuum, that is, in isolation, AI will unlikely be a strategic game changer. Instead, it will likely mutually reinforce the destabilizing effects of existing advanced capabilities thereby increasing the speed of warfare and compressing the decision-making timeframe (Johnson Citation 2019a). AI-enabled and enhanced capabilities will have a more significant impact (positive or negative) on strategic stability than the sum of its parts. Furthermore, the intersection of AI with nuclear weapons, and a broader spectrum of strategic non-nuclear weapons, will likely accelerate the erosion of the survivability of nuclear arsenals associated with the 'computer revolution'. Put another way, military AI and the advanced capabilities it enables are a natural manifestation – rather than the cause or origin – of an established trend, which could lead states to adopt destabilizing launch postures due to the increasing speed of war and co-mingling.\nSecond, AI's impact on stability, deterrence and escalation will likely be determined as much (or more) by states' perception of its functionality as much as by what it is capable of doing. In the case of nuclear strategy and deterrence, the perception of an adversary's capabilities and\nintentions is as (or more) important than its actual capability. Thus, in addition to the importance of military force postures, capabilities and doctrine, the effects of AI will also have a strong cognitive element, increasing the risk of inadvertent escalation as a result of misperception and misunderstanding.\nFinally, and related to the above, the increasingly competitive and contested nuclear multipolar world order will compound the destabilizing effects of AI, and in turn, increase the escalation risks in future warfare between great military powers – especially China and the United States (Johnson Citation 2019b). Moreover, the potential operational and strategic advantages offered by AI-augmented capabilities could prove irresistible to nuclear-armed strategic rivals, causing them to eschew the limitations of AI, and compromise safety and verification standards to protect, or attempt to capture, technological superiority on the future digitised battlefield.",
    "# Military AI primer": "Artificial intelligence research began as early as the 1950s, as a broad concept concerned with the science and engineering of making intelligent machines. Since the 1950s, AI research has gone through several development phases – from early exploitations in the 1950s and 1960s and the 'AI Summer' during the 1970s, through to the early 1980s and the 'AI Winter' from the 1980s – each of which failed to live up to its initial, and often over-hyped, expectations. In the past decade, the explosion of interest in the field (or 'AI renaissance') occurred due to the convergence of four enabling developments: (1) the exponential growth in computing processing power; (2) expanded data sets (especially 'big data' sources) (Gray et al. Citation 2015); Footnote  $^{4}$  (3) advances in the implementation of machine learning techniques and algorithms (especially deep 'neural networks') (Schmidhuber Citation 2015); Footnote  $^{5}$  and (4) the rapid expansion of commercial interest and investment in AI (Hoadley and Nathan Citation 2018). Footnote  $^{6}$\nAI is concerned with machines that emulate capabilities that are usually associated with human intelligence, such as language, reasoning, learning, heuristics and observation. Today, all practical (i.e. technically feasible) AI applications fall into the 'narrow' category, and less so, artificial general intelligence (AGI) or 'superintelligence' (Reedy Citation 2017). Footnote  $^{7}$  'Narrow AI' has been widely used in a broad range of civilian and military tasks since the 1960s, (Nilsson Citation 2010; Russel and Norvig Citation 2014) and involves statistical algorithms (mostly based on machine learning techniques) that learn procedures through analysis of large training data sets designed to approximate and replicate human cognitive tasks (Russel and Norvig Citation 2014). Footnote  $^{8}$  'Narrow AI' is the category of artificial intelligence to which this article refers when it assesses the impact of this technology in a military context.\nGiven the diverse approaches to research in AI (Boden Citation 2016 ; Vernon Citation 2014), Footnote  $^{9}$  there is no universally accepted definition of AI. A recent US congressional report defines AI as follows: 'Any artificial system that performs tasks under varying and unpredictable circumstances, without significant human oversight, or that can learn from their experience and improve their performance ... they may solve tasks requiring human-like perception, cognition, planning, learning, communication, or physical action' (Hoadley and Nathan Citation 2018).\nToday, a large gap exists between the reality of what AI technology is capable of doing in a military context, and the expectations and fears of public opinion, policymakers and global defence communities. How will AI technologies influence public opinion about the use of military force and warfare? (Sechser, Narang, and Talmadge Citation 2019). The misrepresentations and misconceptions that exist today in the narratives surrounding AI (especially in societal, economic and national security debates) and are in large part caused by the hyperbole and exaggerated depictions of AI in science fiction (Zarkadakis Citation 2015). Footnote  $^{10}$  Misrepresentations of the potential opportunities and risks associated with this technology in the military sphere can obscure constructive and crucial debate on these topics. Specifically, this includes the challenge of balancing the potential operational, tactical and strategic benefits of leveraging AI while managing the risks posed to nuclear security in the (especially premature) pursuit of these advantages.\nThe historical record on technological change in a military context counsels scepticism against alarmist predictions and extrapolations from emerging trends: technologies rarely evolve in the way futurists predict and many applications have had countervailing or conditional effects that have ameliorated the dystopian predictions of naysayers (Metz Citation 2000 ; Metz and Kievit Citation 1995). In short, AI's impact on nuclear stability will, like previous generations of emerging technology (i.e. chemical and biological weapons, cyber and nuclear weapons), have both stabilising and destabilizing (and possibly contradictory) effects on strategic stability. For example, many expected that chemical weapons would instantly and dramatically change the nature of warfare and deterrence after the British used poison gas during World War I (Sechser, Narang, and Talmadge Citation 2019). However, chemical weapons proved far less practical, impactful, disruptive and relatively easier to defend against than conventional explosives. As a corollary, the degree to which military AI poses risks to future strategic stability will depend in large part on the pace and scope with which this technology facilitates new ways to improve the delivery of, and defence against, nuclear weapons and strategic non-nuclear weapons.\nConceptually, AI-augmented applications can be categorized into those that have predominately operational, tactical and strategic implications in future warfare. At the operational and tactical level, applications include: autonomy and robotics; multi-actor interaction red teaming wargaming; Footnote  $^{11}$  big data-driven modelling; Footnote  $^{12}$  intelligence collection and analysis (e.g. to locate and monitor mobile missiles, troops movement) (Conklin Citation 2018); Footnote\n$^{13}$ cybersecurity talent management; predictive maintenance, logistics, planning and forecasting; and vendor contract and budget management. In preparation for a new AI strategy, US Strategic Operations Command (SOCOM) recently crafted a roadmap that invests heavily in AI and machine learning technology based on several pilot projects with three overarching goals to create an algorithmic warfare multi-operational team: 'AI-ready workforce, AI-applications, and AI-outreach' (Lee Citation 2019).\nTherefore, AI's impact on strategic stability and escalation will also be shaped by broader and more nuanced factors which influence the trajectory of emerging technologies, including: military strategy and doctrine – that seeks to manipulate escalation risks – military culture and organisation, alliance structures, domestic politics and public opinion, to name but a few. The Cold War record demonstrates that emerging technologies act primarily to enable independent variables which, in combination with other endogenous factors, can heighten escalation risks between adversaries. Therefore, in isolation, technology is not a major exogenous cause of (inadvertent or intentional) military escalation.\nAt a strategic level of warfare, AI uses include: qualitative improvements to the nuclear command, control, communications, and intelligence (C3I) architecture; enhancing target acquisition, tracking, guidance systems and discrimination of missile and air defence systems; being force multipliers of both offensive and defensive machine-learning infused cyber capabilities; and qualitatively bolstering nuclear and non-nuclear missile delivery systems – including hypersonic variants. Footnote 14\nIn combination, AI machine learning algorithms fused with advances in sensor technology could be engineered to allow nuclear delivery systems to operate more autonomously and precisely, with less reliance on human operators calibrating navigation and guidance parameters and equipped with more robust countermeasures against jamming or spoofing attacks. At the strategic level, AI-augmented command and control systems could potentially mitigate many of the shortcomings inherent to human strategic decision-making during the 'fog of war', such as the susceptibility to invest in sunk costs, skewed risk judgment, cognitive heuristics and groupthink.\nIn sum, the fusion of AI machine learning and human judgment to gauge an adversary's intentions (and predict escalation) for the purposes of planning and directing future wars for the pursuit of political objectives, is, therefore, a far less unlikely prospect in the near future than the use of AI to achieve tactical and operational ends (e.g. drone swarming and cyber defence).",
    "# Blurring the AI-cyber offence-defence line: (Glaser and Kaufmann Citation 1998 ; Lynn-Jones Citation 1995 ) Footnote 15": "Several US national security officials believe that AI, used as force multipliers for both defensive and offensive cyber weapons, will have a transformative impact on cybersecurity (Gartzke and Lindsay Citation 2015 ; Rid Citation 2012 ; Slayton Citation 2017 ; Hoffman Citation 2019). Footnote 16 Director of US National Intelligence, Daniel Coats, recently warned that AI could increase US vulnerability to cyberattacks, weaken its ability to attribute such attacks, improve the effectiveness and capabilities of adversaries weapon and intelligence systems, and cause accidents and related liability issues. To be sure, the line between AI cyber-offence and cyber-defence will likely remain an obscure one. Footnote 17 Bernard Brodie’s words of caution about the advent of nuclear weapons almost six decades ago resonate: ‘The [military] bias towards the offensive creates special problems in any technologically new situation where there is little or no relevant war experience to help one to reach a balanced judgment’ (Brodie Citation 1959).\nOn the one hand, AI might reduce a military’s vulnerability to cyberattacks. AI cyber-defence tools (or ‘counter-AI’), designed to recognise changes to patterns of behaviour in a network and detect anomalies, automatically detect software code vulnerabilities, and apply machine learning techniques (such as ‘deep learning’) to detect deviations and anomalies from normal network activity, Footnote 18 could form a more robust defence against cyber subversions (Johnson Citation 2019a). Footnote 19 According to the Pentagon’s new AI strategic report, ‘AI can enhance our ability to predict, identify, and respond to cyber and physical threats from a range of sources’ (US Department of Defense Citation 2019). In addition, the DoD’s Defense Innovation Unit (DIU) is prototyping an application (related to Project VOLTRON) that leverages AI to decipher high-level strategic questions, map probabilistic chains of events and develop alternative strategies in order to make DoD systems more resilient to AI-augmented cyberattacks and configure and fix errors more quickly than humans.\nOn the other hand, autonomy itself might increase a military’s vulnerability to cyberattacks, which rely on stealth, deception and stratagem. For example, an adversary could use malware to take control, manipulate or fool the behaviour and pattern recognition systems of autonomous systems, such as DoD’s Project Maven. Offensive attacks such as this would be relatively easy to execute, but very difficult to detect, attribute or effectively counter (Shachtman Citation 2011). Footnote 20 This problem set is compounded by the lack of an agreed framework or understanding of what constitutes escalatory behaviour (or ‘firebreaks’) in cyberspace. Thus, a cyber operation intended as a signal (i.e. for coercive diplomacy) could go undetected by the target, or worse, misinterpreted as an offensive attack. Even if information relating to an\noperation of this kind is accurately identified in a timely manner, the motives behind them could remain ambiguous, or misperceived. According to Robert Jervis, ‘it is likely that the country that is the object of the attack would assume that any effect was the intended one’ (Jervis Citation 2016).\nUS Cyber Fleet Command Commander, Michael Gilday, recently told the Senate Armed Services Committee that the US Navy must ‘improve an ability to proactively detect new and unknown malware … so we [the US] can act quickly using advanced analytics enabled by AI and machine learning’, which may give the US a ‘tactical advantage’ to identify malicious activity early on (Osborn Citation 2018). Even if analysts can obtain high-quality and reliable intelligence, however, they may not want to reveal it, because doing so could compromise a source, capability or tactic (Johnson Citation 2019a). Moreover, most observers now acknowledge that no strategy (i.e. combining offence and defence cyber operations) could realistically be expected to deter all malign cyber-attacks (Hoffman Citation 2019). Footnote 21 While AI-enhanced cyber capabilities can enhance deterrence, they can simultaneously incentivize others to attack thereby exacerbating the paradox of enhanced capabilities and increased vulnerabilities in the cyber domain (Gartzke and Lindsay Citation 2017; Libicki Citation 2016; Slayton Citation 2017). Footnote 22 As the historical record attests, this capability-vulnerability paradox is heightened when states are both dependent on a particular capability (such as AI and cyber tools) and their access or ability to use the capabilities is vulnerable to an adversary’s exploitation or subversion – creating first-mover incentives (Rid Citation 2013). Footnote 23",
    "# Cybersecurity and nuclear weapon systems: nuclear risk redux?": "At a strategic level of conflict, AI applications designed to enhance cybersecurity for nuclear forces could simultaneously make cyber-dependent nuclear weapon systems (i.e. communications, data processing or early-warning sensors) more vulnerable to cyberattacks. It is now thought possible that a cyberattack (i.e. spoofing, hacking, manipulation and digital jamming) could infiltrate a nuclear weapons system, threaten the integrity of its communications, and ultimately (and possibly unbeknown to its target) gain control of its (nuclear and non-nuclear) command and control systems. Footnote 24 AI technology has not yet evolved to a point where it would allow nuclear-armed states to credibly threaten the survivability of each other’s nuclear second-strike capability. Thus, the development trajectory of AI (and its enabling technologies) means its impact on nuclear security will likely be theoretical and speculative for the foreseeable future.\nBecause of the intense time pressures that would likely loom large with the decision to use nuclear weapons – especially where a state maintains a launch-on-warning posture – AI-enhanced\ncyberattacks against nuclear systems would be almost impossible to detect and the warning signals difficult to authenticate, let alone attribute, within the short timeframe for initiating a nuclear strike. According to open sources, operators at the North American Aerospace Defence Command (NORAD) have less than three minutes to assess and confirm initial indications from early-warning systems of an incoming attack. This compressed decision-making time frame could put political leaders under intense pressure to make a decision to escalate during a crisis, with incomplete (and possibly false) information of a situation. Ironically, new technologies designed to enhance information (i.e. modernised nuclear C3I systems augmented by 5G networks, machine learning, big-data analytics and quantum computing) can also undermine clear and reliable information flow and communication, critical for effective deterrence (Gartzke and Lindsay Citation 2019). Footnote 25\nAdvances in AI could also exacerbate this cybersecurity challenge by enabling improvements to the cyber offence. Machine learning and AI by automating advanced persistent threat (APT) operations might dramatically reduce the extensive manpower resources and high levels of technical skill required to execute APT operations (or 'hunting for weaknesses'), especially against hardened nuclear targets. Footnote 26 The machine speed of AI-augmented cyber tools could enable a low-skilled and capital-restricted attacker to exploit a narrow window of opportunity to penetrate an adversary's cyber-defences or use APT tools to find new vulnerabilities. For example, when docked for maintenance air-gapped nuclear-powered submarines, considered secure when submerged, could become increasingly vulnerable to a new generation of low-cost – possibly black-market – and highly automated APT cyberattacks.\nAn attacker could also apply AI machine learning techniques to target autonomous dual-use early-warning and other operating systems (e.g. C3I, ISR, early-warning and robotic control networks) with 'weaponized software' such as hacking, subverting, spoofing or tricking, causing unpredictable and potentially undetectable errors, malfunctions and behavioural manipulation to weapons systems – or 'data-poisoning'. Footnote 27 Furthermore, as the linkages between digital and physical systems (or the 'Internet of Things') expand, the potential for to an adversary to use cyberattacks in both kinetic and non-kinetic attacks will increase. A significant risk variable in the operation of autonomous systems is the time that passes between a system failure (i.e. performing in a manner other than how the human operator intended) and the time it takes for a human operator to take corrective action. If the system failure is the result of a deliberate act, this time frame will be compressed (Johnson Citation 2019a).\nEven if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. This skewed assessment by states in the context of nuclear weapons, which are ready to launch nuclear weapons at a moment's notice, would likely precipitate worst-case scenario thinking that may spark an inadvertent escalation (Talmadge\nCitation 2017). Footnote 28 During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 29 For example, an AI-enabled third party generated 'deepfake' (audio and video manipulation), coupled with data-poisoning cyberattacks, could spark an escalatory crisis between two (or more) nuclear states (Fitzpatrick Citation 2019). Footnote 30\nHow could AI-augmented cyber capabilities create new pathways for accidental or inadvertent escalation? To illustrate these dynamics: State A launches a malicious AI-enhanced cyberattack to spoof State B's AI-enabled autonomous sensor platforms and automated target recognition systems, in such a way that the weapon system (e.g. a human-supervised ATR system) is fooled into interpreting a civilian object as a military target. B in response, based on manipulated or erroneous information, and the inability of human supervisors to detect the spoofed imagery that fooled the weapons' automated target recognition algorithm in time to take corrective action, accidentally (and unintentionally) escalates a situation (Libicki Citation 2018). In this example, the spoofing attack on the weapon systems' algorithm is executed in such a way that the imagery appears to the recognition system as indistinguishable from a valid military target (Rohrbach et al. Citation 2018) Footnote 31 escalating a situation based on a false premise that would unlikely to fool the human eye. Footnote 32\nAlso, the explainability (or 'black box') problem associated with AI applications could compound these potential escalation dynamics. Footnote 33 Insufficient understanding of how and why AI algorithms reach a particular judgment or decision might complicate the task of determining whether data sets had been deliberately compromised to manufacture false outcomes (e.g. attacking incorrect targets or misdirecting allies during combat). Footnote 34 Moreover, as humans and AI team up to accomplish particular missions, the opacity associated with how AI systems reach a decision may cause an operator to have either too much or too little confidence in a system's performance. As a result, unless the system's machine learning algorithm is terminated, at the end of the training phase, once deployed, it could potentially learn something it was not intended to, or even perform a task or mission that its human designers do not expect it to do. Footnote 35\nIn sum, technologies that improve the reliability and speed with which information is processed and disseminated by early-warning systems may paradoxically also increase the vulnerabilities of these networks, creating new first-mover advantages and escalation pathways during a crisis that spark unintentional (or intentional) warfare (Schneider Citation 2019). Footnote 36 Put another way, AI might be developed in order to enable a state to pursue a pre-determined escalatory path. Thus, increased escalation risk as a result of technology is not always inadvertent or accidental (Long and Rittenhouse Green Citation 2015). Footnote 37",
    "# 'Cyber guns' supercharged with AI machine learning": "While manipulations and system subversions are possible with existing cyber offence tools, rapid advances in AI and increasing degrees of military autonomy could amplify the potential speed, power and scale of future attacks in cyberspace. Footnote 38 There are three significant intersections between AI systems and cybersecurity that are most salient to the military domain: (1) advances in autonomy and machine learning techniques mean that a much broader range of physical systems are now vulnerable to cyberattacks (i.e. hacking and data-poisoning) (Greenberg Citation 2016); Footnote 39 (2) cyberattacks on AI systems can offer attackers access to machine learning algorithms, trained models being used by the application and a potential vast amounts of data from facial recognition and intelligence collection and analysis systems (e.g. satellite navigation and imagery systems used to cue precision munitions and supported intelligence, surveillance, reconnaissance missions); and (3) the use of AI systems in conjunction with existing cyber offence tools will enable sophisticated cyberattacks to be executed at larger scale (both geographically and across networks), at faster speeds, and simultaneously across multiple military domains with improved anonymity. In short, despite the relatively benign augmentation mechanisms used to develop cyber offensive capabilities with AI systems, the resultant impact on the speed and scope of AI cyber tools will likely have destabilizing effects, nonetheless. Footnote 40\nRecent advances in machine learning have significantly contributed to resolving several technical bottlenecks in several fields of AI, which could allow for significant qualitative improvements to a large variety of autonomous weapon systems and applications. In combination, machine learning and autonomy could intersect with nuclear security in a multitude of ways, with both positive and negative implications for strategic stability. For example, machine learning could enhance the detection capabilities of (nuclear and non-nuclear) early-warning systems and improve the collection and cross-analysis of ISR information, thus reducing the risk of inadvertent and accidental escalation. Machine learning may also bolster the cyber-security of command and control systems, create new and possibilities for arms control, testing, verification and monitoring nuclear stockpiles, and enhance conventional counterforce capabilities. Unmanned autonomous systems could be deployed in complex missions in hitherto inaccessible and cluttered environments (e.g. under-sea anti-submarine warfare) and aerial and underwater drones in swarms might eventually replace intercontinental ballistic missiles (ICBMs) and nuclear-armed submarines (SSBNs) for the delivery of nuclear weapons.\nIn response to these anticipated vulnerabilities, and to achieve the (perceived) first-mover advantages the increased speed in AI-augmented cyber tools could confer, China, Russia and the United States have continued to harden their AI cyber defences. Despite these parallel efforts, divergent views exist on how strategic communities perceive the nature of the risks posed by AI-enhanced cyberattacks against dual-use C3I systems. Thus, AI-augmented cyber intelligence-\ngathering tools targeting an adversary's command and control assets (especially dual-use facilities) during a crisis may be misinterpreted as a prelude to an imminent pre-emptive attack on its nuclear force and therefore as undermining strategic stability.\nMoreover, AI machine learning could enable new offensive concepts such as a so-called 'left-of-launch operation' (Ellison Citation 2015), Footnote  $^{41}$  thereby compounding the ambiguities and fear about pre-emptive attacks in cyberspace, and increasing first-mover incentives – or use-them-or-lose-them situations (Buchanan Citation 2017; Buchanan and Miller Citation 2017). Footnote  $^{42}$  In the cyber domain, it is generally unclear in the early stages of a cyber operation whether an adversary intends to collect intelligence or prepare for an offensive attack, which is more likely to be used early on in a conflict scenario. Open sources suggest, for example, that Chinese analysts view the vulnerability of China's nuclear command, control, and communications (C3I) systems to cyber infiltrations – even if an attacker's objective was limited to cyberespionage – as a highly escalatory national security threat. By contrast, Russian analysts tend to view Russia's nuclear C3I network as relatively isolated, and thus insulated from cyberattacks.\nIrrespective of these differences, the uncertainty caused by the possibility, actual or otherwise, that AI-enhanced offensive cyber tools might be used (or threatened) to undermine the credibility and reliability of states' nuclear forces would be destabilizing, both as result of the existence of enhanced cyber offensive capabilities and the potential efficacy of these tools against nuclear forces. As the historical record attests, rational actors have incentives to misrepresent what they know (i.e. lie and bluff) in order to improve the terms of any settlement that may emerge from the wartime bargaining process – or the risk-reward payoff. Even a modicum of uncertainty about the effectiveness of AI-augmented cyber capabilities during a crisis or conflict would, therefore, reduce both sides' risk tolerance and increase the incentive to strike pre-emptively as a hedging strategy.\nA virtuous cycle that might flow from enhanced reassurances premised on comprehensive intelligence would require information symmetry (i.e. equal access to intelligence and analysis systems) between great and rising powers, and shared confidence in the accuracy and credibility of these systems. Perhaps most challenging of all in a world of 'revisionist' rising powers, the intentions of all rival states would need to be genuinely benign for this optimistic view to occur. Under crisis conditions, for example, an offensive AI cyber tool that succeeds in compromising an adversary's nuclear weapon systems, resulting in an 'asymmetric information' situation, could cause either or both sides to overstate (or understate) its retaliatory capabilities, and in turn, be more inclined to act in a risky and escalatory fashion. In short, in a competitive strategic environment, where states are inclined to assume the worst of others' intentions, one state's efforts to enhance the survivability of its strategic forces may be viewed by others as a threat to their nuclear retaliatory capability – or second-strike capacity.",
    "# Manipulation of the information landscape": "While machine learning big-data analytics, sensing technology, networks supported by 5G networks, could alert commanders of incoming threats with increased speed and precision, and make accidents caused by human error potentially less likely (especially accidents and malfunctions caused by false warning), these advances could simultaneously amplify escalation risks in two ways. Footnote 43\nFirst, AI machine learning used as force multipliers for cyber offence (e.g. data poisoning spoofing, 'deepfakes', manipulation, hacking and digital jamming) would be considerably more difficult to detect – especially if an attacker used AI advanced persistent threat (APT) tools – or 'hunting for weaknesses' in spectrum-contested environments. Second, in the unlikely event an attack was successfully detected, threat identification (or attribution) at machine speed would be virtually impossible. In addition to the co-mingling and the speed of warfare issues, AI machine learning systems might also exacerbate the existing – well-established and non-kinetic – risk of inadvertent (and intentional) escalation from the manipulation of the information landscape in which decisions about nuclear weapons are located. For example, the use of deepfake videos built-in real-time and disseminated online for malicious purposes.\nHuman cognition, and thus effective deterrence and signalling intentions, is predicated on reliable and clear information; if an adversary is concerned that the information available to them is limited (or worse, inaccurate), they will likely assume the worst and act accordingly. Asymmetric information situations between rivals about the balance of military power could undermine crisis stability, and in turn, create rational incentives to escalate to nuclear confrontation. Consequently, states will be more inclined to assume the worst of others' intentions, especially in situations where the legitimacy of the status quo is contested (i.e. maritime Asia).\nIn the competitive strategic environment, it is easy to imagine unprovoked escalation caused by a malicious third-party (or state-proxy) clandestine action. Even if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 44 For example, in an effort to incite conflict between two rival states, State A uses proxy hackers to use a AI machine learning technique (i.e. generative adversarial networks GAN) to launch 'deepfake' video or audio material, depicting senior military commanders of State B conspiring to launch a pre-emptive strike on State C. Footnote 45 Then, this 'deepfake' footage is deliberately leaked into C's AI-augmented intelligence collection and analysis systems, provoking C to escalate the situation with strategic consequences. B responds in kind. Footnote 46\nThe current underdeveloped state of ‘counter-AI’ capabilities and other fail-safe mechanisms (e.g. circuit breakers) to de-escalate escalation in cyberspace, will make the unprovoked and unintentional escalation dynamics depicted in this scenario very challenging to mitigate. Moreover, in the emerging ‘deepfakes’ arms race (much like cybersecurity more generally), detection software will likely lag behind advances in offensive enabling solutions – or offence-dominant ones. According to computer science expert Hany Farid, there are probably 100–1000 times ‘more people developing the technology to manipulate content than there is to detect [it]’ (Fontaine and Frederick Citation 2019).\nThe overreliance on automation (or ‘automation bias’) in the deployment of increasing complex AI-augmented capabilities – such as cyber, C3I systems, AWS and precision missile munitions – designed to provide a distinct tactical advantage of machine-speed, will increase the vulnerability of these capabilities to exploitation. Moreover, the increasing substitution of human cognition for logic derived from machines will likely increase the opportunities for adversaries to exploit the limitations of narrow AI technology, i.e. lack of human intuition, brittleness in complex real-world situations and an inability to effectively detect or counter manipulation attacks (Libicki Citation 2018). Footnote 47 Therefore, until researchers unravel some of the unexplainable features of AI, human error and machine error will likely compound one another, with unpredictable results. Footnote 48 Simply put, we are at a critical crossroads in the parallel (and symbiotic) evolution of AI and cyberspace that national security communities, globally, will need to prepare for proactively (Johnson Citation 2019a).\n## Policy interventions\nA prominent theme that runs through this article – and central to understanding the potential impact AI for strategic stability and nuclear security more broadly – is the concern that AI systems operating at machine-speed will push the pace of combat to a point where the actions of machines surpass the (cognitive and physical) ability of human decision makers to control (or even comprehend) events.\nPossible multi-track policy responses to push back against the threat posed to stability from AI in a multipolar can be broadly categorized into: (1) those that focus on enhancing debate and discussion between researchers, global defence communities, decision makers, academics and other political and societal stakeholders; and (2) a range of specific policy recommendations and tools for great military powers to negotiate and implement.\n## Enhancing debate and dialogue\nFirst, to mitigate (or at least manage) the destabilizing and escalatory risks posed by the AI-cybersecurity nexus, great military powers must closely coordinate their confidence-building\nmeasures in an effort to pre-empt some of the risks to stability outlined above. Track-1 and track-2 discussions should include diplomats and military leaders (especially from China, Russian and the United States), industry experts, AI researchers and multi-disciplinary academics. Specifically, great military powers should establish an international framework for governance, norms, behaviour and the regulation of policy on machine learning-augmented cyber capabilities. Resistance to these efforts will likely come from states who worry that in revealing their cyber (especially offensive) capabilities, they could upend the deterrence utility of these tools.\nTo be sure, the challenges posed in the coordination and implementation of these policies will require bold and visionary leadership to circumvent the inevitable regional agendas, interdisciplinary resistance and burgeoning security dilemmas between rival states. Because of the rapid technological change in AI formal treaties, associated with arms-control agreements that require lengthy and complex negotiation and ratification processes, legal frameworks risk becoming obsolete before they come into effect. The historical record has demonstrated on several occasions that these kinds of challenges facing humanity can be overcome (Krimsky Citation 1962; Naur and Randell Citation 1968). Footnote 49\nSecond, the think tank community, academics and AI research experts should pool their resources to investigate the implications of the AI-cyber nexus for a range of potential security issues such as: the impact of AI bias on future military-use applications; how to prepare for (and react to) artificial general intelligence; and measures to mitigate, prevent and manage offensive uses of AI. Footnote 50 If necessary, steps could be taken to amend existing legal definitions of offensive cyber operations, such as hacking, data-poisoning attacks and spoofing, to account for the increased proliferation and potential damage caused by AI.\nThird, and related, due to the intrinsic dual-use nature of the AI-cyber security problem set, this dialogue should also be expanded to include other stakeholders such as private-sector AI and cybersecurity experts, the commercial sector, ethicists, philosophers, civil society and public opinion. Footnote 51 States should also collaborate on dual-use AI research to leverage AI's low-cost and scaling advantages (i.e. in autonomy and robotics). Further, a focus on the safety, testing and robustness of AI systems is a critical step in mitigating potential vulnerabilities and risks caused by errors, bias, and explainability in uncontrolled and complex environments. Footnote 52 In the case of offensive AI-related cybersecurity research, where public distribution might cause vulnerabilities and worsen security, publication could be confined to trusted organisations and entities.\nBest practices that exist in more mature methods for addressing dual-use concerns, like computer security, where applicable, might be applied to AI. For example, the wide use of red teaming exercises to enhance network security, organisation and practices. Specifically, AI-cyber red teaming – like DARPA's Cyber Grand Challenge – will enable engineers and operators to better\nunderstand the skills needed to execute particular offensive and defensive operations (especially using machine learning techniques), and to better manage system vulnerabilities, adversarial exploitation, stress-testing and social engineering challenges (Anderson, Woodbridge, and Filar Citation 2016).\nA recent study explored case studies of previous examples (biological weapons, cryptography and nuclear technology) of dual-use technologies – with civilian and military uses – to examine potential insights for AI dual-use risk management policies such as export controls and prepublication reviews. These insights also highlight the potential challenges of establishing regulatory, legal and normative frameworks for dual-use technologies – for example, the cautionary tale of ineffective efforts, in the late 1990s, to regulate cryptographic algorithms and cyber-network security tools through export controls.\nThe extent to which the integration of AI into dual-use weapon systems (such as cybersecurity) might influence actors' attitude to risk, the offence-defence balance and, in turn, perceptions of others' intentions and capabilities, could have profound implications for strategic deterrence and nuclear security. Footnote 53 In sum, cognizant that some states have deployed (or imminently plan to deploy) AI systems, experts generally agree that AI requires further experimentation, testing and development before being integrated into lethal weapon systems and their decision-making support systems.",
    "# Taking the lead from the cybersecurity community": "Some examples of cybersecurity-centered measures that might be further researched and implemented to pre-empt and mitigate some of risks posed by AI-augmented cyberattacks (especially involving safety-critical nuclear systems) highlighted in this paper include the following.\nFirst, coordinating AI-simulated war games, red teaming creative thinking exercises and creating redundancies (i.e. back-ups or fail-safes) in networks to detect errors, fix vulnerabilities and increase the reliability and robustness of military (especially nuclear-centric) systems. Specific procedures could be implemented to enable confidential reporting and fixing the vulnerabilities, subversions and other kinds of manipulations detected in AI systems. These findings could be used to track the proliferation of AI-related incursions and then countermeasures could be developed and standardised to manage these threats. Footnote 54\nSecond, states should formalise verification methods and protocols and consider issues such as: to what extent, under what circumstances, and for what types of AI systems can formal verification be implemented? Might other approaches be developed to achieve similar goals (e.g. machine learning and big data analysis augmented verification methods)? For example, DARPA's Assured Autonomy Program combines data-driven machine learning algorithms to assure the\nsafety of autonomous cyber physical systems (air, ground, sea and undersea unmanned vehicles), which continue to learn throughout their lifespans.\nThis learning characteristic makes assurance or verification using traditional methods especially challenging. Whether AI applications in the military domain can be formally verified, for now, is an unanswered question. Footnote 55 Much like that of cyber systems, the complexity of AI systems and the difficulty of defining their properties for formal verification makes them less amenable to verification compared to other types of technology.\nThird, the global defence communities should actively invest in the development of AI cyber-defence tools (e.g. analysing classification errors, automatic detection of remote vulnerability scanning and model extraction improvements), AI-centric secure hardware, and other fail-safe mechanisms, to allow for de-escalation and to prevent unintentional or accidental escalation.\nSeveral issues that need further exploration related to these measures include: the extent to which existing tools might be effective against vulnerabilities in AI systems? How can these tools be tailored for AI systems across multiple military domains? Is there an equivalent to 'patching' in military AI systems? What kinds of policies might incentivize, and ensure compliance with, meaningful reforms to existing hardware in the military sphere? While these questions are challenging, and necessarily speculative for now, answers will become more evident as the technology matures.",
    "# Disclosure statement": "No potential conflict of interest was reported by the author."
  },
  "process_log": {
    "scheme": "markdown",
    "numeric_check": {
      "first_num": null,
      "raw_count": 0,
      "raw_examples": [],
      "filtered_count": 0,
      "filtered_examples": [],
      "seq_score": 0.0
    },
    "roman_check": {
      "raw_count": 0,
      "count": 0,
      "min": null,
      "examples": [],
      "best_run": 0,
      "seq_score": 0.0,
      "requires_from_I": true
    },
    "markdown_numeric_hint": {
      "raw_count": 0,
      "count": 0,
      "min": null,
      "examples": [],
      "best_run": 0
    },
    "toc_count": 8,
    "section_count": 9
  },
  "word_count": 7022,
  "references": [],
  "citations": {
    "style": "tex_superscript",
    "flat_text": "# ABSTRACT\nHow could AI-infused cyber capabilities be used to subvert, or otherwise compromise, the reliability, control and use of states' nuclear forces? This article argues that a new generation of artificial intelligence (AI) enhanced cyber capabilities will amplify the risk of inadvertent escalation caused by the co-mingling of nuclear and strategic non-nuclear weapons and the increasing speed of warfare, thereby increasing the risk of nuclear confrontation. It examines the potential implications of cyber (offensive and defensive) capabilities augmented with AI applications for nuclear security. The article concludes that future iterations of AI-enhanced cyber counterforce capabilities will complicate the existing challenges of cyber defence, and in turn, compromise nuclear assets and increase the escalatory effects of offensive cyber capabilities.\n# KEYWORDS:\n- Artificial intelligence\n- cyber-security\n- US-China Relations\n- nuclear security\n- deterrence policy\n- emerging technology\n# Introduction\nGiven the hyperbole surrounding AI, it is easy to overstate the opportunities and challenges posed by the development and deployment of AI in the military sphere (Boulanin Citation 2019). Today, there remains a large amount of debate among AI researchers surrounding the significance of several significant technical and operational challenges in the deployment of AI-infused systems (e.g. drone swarming, command and control decision-making support systems and a broader range of autonomous weapon systems). Moreover, many of the risks posed by AI in the nuclear domain today are not necessarily new (Boulanin Citation 2019). Recent advances in AI (especially machine learning techniques) exacerbate existing risks to escalation and stability rather than generating entirely new ones. While AI could enable significant improvements in many military domains (including nuclear weapons), for the foreseeable future, developments in military AI will likely be far more prosaic than implied in popular culture. The main worry for nuclear stability, therefore, is that militaries will underestimate (or overstate) or ignore the potential shortcomings and risks associated with the use of the current generation of AI technology in the safety-critical military sphere – and especially the nuclear domain.\nBecause of the diverse approaches to AI-based research, there is no universally accepted definition of ‘artificial intelligence’, which is confusing when the term is applied generically to make grandiose claims about its revolutionary effects. Footnote    AI can be understood as a universal term for improving the performance of automated (or autonomous) systems to solve a wide variety of complex tasks including: perception (sensors, computer vision, audio and image processing); reasoning and decision-making (problem solving, searching, planning and reasoning); learning and knowledge representation (machine learning, deep networks and modelling); Footnote    communication (language processing); autonomy and robotics ; and human-AI collaboration (humans define the systems’ purpose, goals and context).\nIn a military context, as a potential enabler and force multiplier of advanced weapon systems, AI is more akin to electricity, radio, radar and intelligence, surveillance and reconnaissance (ISR) support systems than a 'weapon' per se. Much like these enabling support systems, AI is also strategically vital, vulnerable, inherently cross-domain and dependent on advanced technology. Thus, even if AI applications are unable to make better battlefield decisions than humans, Footnote  $^{3}$  militaries that use AI in human-machine teaming will doubtless gain significant advantages (e.g. remote-sensing, situational-awareness, battlefield-manoeuvres and a compressed decision-making loop), compared to those who depend on human judgment – and semi-autonomous technology – alone. This is particularly the case, in operating systems in operating environments that demand endurance and rapid decision-making across multiple combat zones (Ayoub and Payne Citation 2016).\nThis article argues that a new generation of AI-augmented offensive cyber capabilities will amplify the risk of inadvertent escalation posed by the co-mingling of nuclear and strategic (or counterforce) non-nuclear weapons and the increasing speed of warfare, and in turn, increase the risk of nuclear confrontation. The article's thesis is grounded in three core themes. First, AI does not exist a vacuum, that is, in isolation, AI will unlikely be a strategic game changer. Instead, it will likely mutually reinforce the destabilizing effects of existing advanced capabilities thereby increasing the speed of warfare and compressing the decision-making timeframe (Johnson Citation 2019a). AI-enabled and enhanced capabilities will have a more significant impact (positive or negative) on strategic stability than the sum of its parts. Furthermore, the intersection of AI with nuclear weapons, and a broader spectrum of strategic non-nuclear weapons, will likely accelerate the erosion of the survivability of nuclear arsenals associated with the 'computer revolution'. Put another way, military AI and the advanced capabilities it enables are a natural manifestation – rather than the cause or origin – of an established trend, which could lead states to adopt destabilizing launch postures due to the increasing speed of war and co-mingling.\nSecond, AI's impact on stability, deterrence and escalation will likely be determined as much (or more) by states' perception of its functionality as much as by what it is capable of doing. In the case of nuclear strategy and deterrence, the perception of an adversary's capabilities and\nintentions is as (or more) important than its actual capability. Thus, in addition to the importance of military force postures, capabilities and doctrine, the effects of AI will also have a strong cognitive element, increasing the risk of inadvertent escalation as a result of misperception and misunderstanding.\nFinally, and related to the above, the increasingly competitive and contested nuclear multipolar world order will compound the destabilizing effects of AI, and in turn, increase the escalation risks in future warfare between great military powers – especially China and the United States (Johnson Citation 2019b). Moreover, the potential operational and strategic advantages offered by AI-augmented capabilities could prove irresistible to nuclear-armed strategic rivals, causing them to eschew the limitations of AI, and compromise safety and verification standards to protect, or attempt to capture, technological superiority on the future digitised battlefield.\n# Military AI primer\nArtificial intelligence research began as early as the 1950s, as a broad concept concerned with the science and engineering of making intelligent machines. Since the 1950s, AI research has gone through several development phases – from early exploitations in the 1950s and 1960s and the 'AI Summer' during the 1970s, through to the early 1980s and the 'AI Winter' from the 1980s – each of which failed to live up to its initial, and often over-hyped, expectations. In the past decade, the explosion of interest in the field (or 'AI renaissance') occurred due to the convergence of four enabling developments: (1) the exponential growth in computing processing power; (2) expanded data sets (especially 'big data' sources) (Gray et al. Citation 2015); Footnote  $^{4}$  (3) advances in the implementation of machine learning techniques and algorithms (especially deep 'neural networks') (Schmidhuber Citation 2015); Footnote  $^{5}$  and (4) the rapid expansion of commercial interest and investment in AI (Hoadley and Nathan Citation 2018). Footnote  $^{6}$\nAI is concerned with machines that emulate capabilities that are usually associated with human intelligence, such as language, reasoning, learning, heuristics and observation. Today, all practical (i.e. technically feasible) AI applications fall into the 'narrow' category, and less so, artificial general intelligence (AGI) or 'superintelligence' (Reedy Citation 2017). Footnote  $^{7}$  'Narrow AI' has been widely used in a broad range of civilian and military tasks since the 1960s, (Nilsson Citation 2010; Russel and Norvig Citation 2014) and involves statistical algorithms (mostly based on machine learning techniques) that learn procedures through analysis of large training data sets designed to approximate and replicate human cognitive tasks (Russel and Norvig Citation 2014). Footnote  $^{8}$  'Narrow AI' is the category of artificial intelligence to which this article refers when it assesses the impact of this technology in a military context.\nGiven the diverse approaches to research in AI (Boden Citation 2016 ; Vernon Citation 2014), Footnote  $^{9}$  there is no universally accepted definition of AI. A recent US congressional report defines AI as follows: 'Any artificial system that performs tasks under varying and unpredictable circumstances, without significant human oversight, or that can learn from their experience and improve their performance ... they may solve tasks requiring human-like perception, cognition, planning, learning, communication, or physical action' (Hoadley and Nathan Citation 2018).\nToday, a large gap exists between the reality of what AI technology is capable of doing in a military context, and the expectations and fears of public opinion, policymakers and global defence communities. How will AI technologies influence public opinion about the use of military force and warfare? (Sechser, Narang, and Talmadge Citation 2019). The misrepresentations and misconceptions that exist today in the narratives surrounding AI (especially in societal, economic and national security debates) and are in large part caused by the hyperbole and exaggerated depictions of AI in science fiction (Zarkadakis Citation 2015). Footnote  $^{10}$  Misrepresentations of the potential opportunities and risks associated with this technology in the military sphere can obscure constructive and crucial debate on these topics. Specifically, this includes the challenge of balancing the potential operational, tactical and strategic benefits of leveraging AI while managing the risks posed to nuclear security in the (especially premature) pursuit of these advantages.\nThe historical record on technological change in a military context counsels scepticism against alarmist predictions and extrapolations from emerging trends: technologies rarely evolve in the way futurists predict and many applications have had countervailing or conditional effects that have ameliorated the dystopian predictions of naysayers (Metz Citation 2000 ; Metz and Kievit Citation 1995). In short, AI's impact on nuclear stability will, like previous generations of emerging technology (i.e. chemical and biological weapons, cyber and nuclear weapons), have both stabilising and destabilizing (and possibly contradictory) effects on strategic stability. For example, many expected that chemical weapons would instantly and dramatically change the nature of warfare and deterrence after the British used poison gas during World War I (Sechser, Narang, and Talmadge Citation 2019). However, chemical weapons proved far less practical, impactful, disruptive and relatively easier to defend against than conventional explosives. As a corollary, the degree to which military AI poses risks to future strategic stability will depend in large part on the pace and scope with which this technology facilitates new ways to improve the delivery of, and defence against, nuclear weapons and strategic non-nuclear weapons.\nConceptually, AI-augmented applications can be categorized into those that have predominately operational, tactical and strategic implications in future warfare. At the operational and tactical level, applications include: autonomy and robotics; multi-actor interaction red teaming wargaming; Footnote  $^{11}$  big data-driven modelling; Footnote  $^{12}$  intelligence collection and analysis (e.g. to locate and monitor mobile missiles, troops movement) (Conklin Citation 2018); Footnote\n$^{13}$ cybersecurity talent management; predictive maintenance, logistics, planning and forecasting; and vendor contract and budget management. In preparation for a new AI strategy, US Strategic Operations Command (SOCOM) recently crafted a roadmap that invests heavily in AI and machine learning technology based on several pilot projects with three overarching goals to create an algorithmic warfare multi-operational team: 'AI-ready workforce, AI-applications, and AI-outreach' (Lee Citation 2019).\nTherefore, AI's impact on strategic stability and escalation will also be shaped by broader and more nuanced factors which influence the trajectory of emerging technologies, including: military strategy and doctrine – that seeks to manipulate escalation risks – military culture and organisation, alliance structures, domestic politics and public opinion, to name but a few. The Cold War record demonstrates that emerging technologies act primarily to enable independent variables which, in combination with other endogenous factors, can heighten escalation risks between adversaries. Therefore, in isolation, technology is not a major exogenous cause of (inadvertent or intentional) military escalation.\nAt a strategic level of warfare, AI uses include: qualitative improvements to the nuclear command, control, communications, and intelligence (C3I) architecture; enhancing target acquisition, tracking, guidance systems and discrimination of missile and air defence systems; being force multipliers of both offensive and defensive machine-learning infused cyber capabilities; and qualitatively bolstering nuclear and non-nuclear missile delivery systems – including hypersonic variants. Footnote 14\nIn combination, AI machine learning algorithms fused with advances in sensor technology could be engineered to allow nuclear delivery systems to operate more autonomously and precisely, with less reliance on human operators calibrating navigation and guidance parameters and equipped with more robust countermeasures against jamming or spoofing attacks. At the strategic level, AI-augmented command and control systems could potentially mitigate many of the shortcomings inherent to human strategic decision-making during the 'fog of war', such as the susceptibility to invest in sunk costs, skewed risk judgment, cognitive heuristics and groupthink.\nIn sum, the fusion of AI machine learning and human judgment to gauge an adversary's intentions (and predict escalation) for the purposes of planning and directing future wars for the pursuit of political objectives, is, therefore, a far less unlikely prospect in the near future than the use of AI to achieve tactical and operational ends (e.g. drone swarming and cyber defence).\n# Blurring the AI-cyber offence-defence line: (Glaser and Kaufmann Citation 1998 ; Lynn-Jones Citation 1995 ) Footnote 15\nSeveral US national security officials believe that AI, used as force multipliers for both defensive and offensive cyber weapons, will have a transformative impact on cybersecurity (Gartzke and Lindsay Citation 2015 ; Rid Citation 2012 ; Slayton Citation 2017 ; Hoffman Citation 2019). Footnote 16 Director of US National Intelligence, Daniel Coats, recently warned that AI could increase US vulnerability to cyberattacks, weaken its ability to attribute such attacks, improve the effectiveness and capabilities of adversaries weapon and intelligence systems, and cause accidents and related liability issues. To be sure, the line between AI cyber-offence and cyber-defence will likely remain an obscure one. Footnote 17 Bernard Brodie’s words of caution about the advent of nuclear weapons almost six decades ago resonate: ‘The [military] bias towards the offensive creates special problems in any technologically new situation where there is little or no relevant war experience to help one to reach a balanced judgment’ (Brodie Citation 1959).\nOn the one hand, AI might reduce a military’s vulnerability to cyberattacks. AI cyber-defence tools (or ‘counter-AI’), designed to recognise changes to patterns of behaviour in a network and detect anomalies, automatically detect software code vulnerabilities, and apply machine learning techniques (such as ‘deep learning’) to detect deviations and anomalies from normal network activity, Footnote 18 could form a more robust defence against cyber subversions (Johnson Citation 2019a). Footnote 19 According to the Pentagon’s new AI strategic report, ‘AI can enhance our ability to predict, identify, and respond to cyber and physical threats from a range of sources’ (US Department of Defense Citation 2019). In addition, the DoD’s Defense Innovation Unit (DIU) is prototyping an application (related to Project VOLTRON) that leverages AI to decipher high-level strategic questions, map probabilistic chains of events and develop alternative strategies in order to make DoD systems more resilient to AI-augmented cyberattacks and configure and fix errors more quickly than humans.\nOn the other hand, autonomy itself might increase a military’s vulnerability to cyberattacks, which rely on stealth, deception and stratagem. For example, an adversary could use malware to take control, manipulate or fool the behaviour and pattern recognition systems of autonomous systems, such as DoD’s Project Maven. Offensive attacks such as this would be relatively easy to execute, but very difficult to detect, attribute or effectively counter (Shachtman Citation 2011). Footnote 20 This problem set is compounded by the lack of an agreed framework or understanding of what constitutes escalatory behaviour (or ‘firebreaks’) in cyberspace. Thus, a cyber operation intended as a signal (i.e. for coercive diplomacy) could go undetected by the target, or worse, misinterpreted as an offensive attack. Even if information relating to an\noperation of this kind is accurately identified in a timely manner, the motives behind them could remain ambiguous, or misperceived. According to Robert Jervis, ‘it is likely that the country that is the object of the attack would assume that any effect was the intended one’ (Jervis Citation 2016).\nUS Cyber Fleet Command Commander, Michael Gilday, recently told the Senate Armed Services Committee that the US Navy must ‘improve an ability to proactively detect new and unknown malware … so we [the US] can act quickly using advanced analytics enabled by AI and machine learning’, which may give the US a ‘tactical advantage’ to identify malicious activity early on (Osborn Citation 2018). Even if analysts can obtain high-quality and reliable intelligence, however, they may not want to reveal it, because doing so could compromise a source, capability or tactic (Johnson Citation 2019a). Moreover, most observers now acknowledge that no strategy (i.e. combining offence and defence cyber operations) could realistically be expected to deter all malign cyber-attacks (Hoffman Citation 2019). Footnote 21 While AI-enhanced cyber capabilities can enhance deterrence, they can simultaneously incentivize others to attack thereby exacerbating the paradox of enhanced capabilities and increased vulnerabilities in the cyber domain (Gartzke and Lindsay Citation 2017; Libicki Citation 2016; Slayton Citation 2017). Footnote 22 As the historical record attests, this capability-vulnerability paradox is heightened when states are both dependent on a particular capability (such as AI and cyber tools) and their access or ability to use the capabilities is vulnerable to an adversary’s exploitation or subversion – creating first-mover incentives (Rid Citation 2013). Footnote 23\n# Cybersecurity and nuclear weapon systems: nuclear risk redux?\nAt a strategic level of conflict, AI applications designed to enhance cybersecurity for nuclear forces could simultaneously make cyber-dependent nuclear weapon systems (i.e. communications, data processing or early-warning sensors) more vulnerable to cyberattacks. It is now thought possible that a cyberattack (i.e. spoofing, hacking, manipulation and digital jamming) could infiltrate a nuclear weapons system, threaten the integrity of its communications, and ultimately (and possibly unbeknown to its target) gain control of its (nuclear and non-nuclear) command and control systems. Footnote 24 AI technology has not yet evolved to a point where it would allow nuclear-armed states to credibly threaten the survivability of each other’s nuclear second-strike capability. Thus, the development trajectory of AI (and its enabling technologies) means its impact on nuclear security will likely be theoretical and speculative for the foreseeable future.\nBecause of the intense time pressures that would likely loom large with the decision to use nuclear weapons – especially where a state maintains a launch-on-warning posture – AI-enhanced\ncyberattacks against nuclear systems would be almost impossible to detect and the warning signals difficult to authenticate, let alone attribute, within the short timeframe for initiating a nuclear strike. According to open sources, operators at the North American Aerospace Defence Command (NORAD) have less than three minutes to assess and confirm initial indications from early-warning systems of an incoming attack. This compressed decision-making time frame could put political leaders under intense pressure to make a decision to escalate during a crisis, with incomplete (and possibly false) information of a situation. Ironically, new technologies designed to enhance information (i.e. modernised nuclear C3I systems augmented by 5G networks, machine learning, big-data analytics and quantum computing) can also undermine clear and reliable information flow and communication, critical for effective deterrence (Gartzke and Lindsay Citation 2019). Footnote 25\nAdvances in AI could also exacerbate this cybersecurity challenge by enabling improvements to the cyber offence. Machine learning and AI by automating advanced persistent threat (APT) operations might dramatically reduce the extensive manpower resources and high levels of technical skill required to execute APT operations (or 'hunting for weaknesses'), especially against hardened nuclear targets. Footnote 26 The machine speed of AI-augmented cyber tools could enable a low-skilled and capital-restricted attacker to exploit a narrow window of opportunity to penetrate an adversary's cyber-defences or use APT tools to find new vulnerabilities. For example, when docked for maintenance air-gapped nuclear-powered submarines, considered secure when submerged, could become increasingly vulnerable to a new generation of low-cost – possibly black-market – and highly automated APT cyberattacks.\nAn attacker could also apply AI machine learning techniques to target autonomous dual-use early-warning and other operating systems (e.g. C3I, ISR, early-warning and robotic control networks) with 'weaponized software' such as hacking, subverting, spoofing or tricking, causing unpredictable and potentially undetectable errors, malfunctions and behavioural manipulation to weapons systems – or 'data-poisoning'. Footnote 27 Furthermore, as the linkages between digital and physical systems (or the 'Internet of Things') expand, the potential for to an adversary to use cyberattacks in both kinetic and non-kinetic attacks will increase. A significant risk variable in the operation of autonomous systems is the time that passes between a system failure (i.e. performing in a manner other than how the human operator intended) and the time it takes for a human operator to take corrective action. If the system failure is the result of a deliberate act, this time frame will be compressed (Johnson Citation 2019a).\nEven if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. This skewed assessment by states in the context of nuclear weapons, which are ready to launch nuclear weapons at a moment's notice, would likely precipitate worst-case scenario thinking that may spark an inadvertent escalation (Talmadge\nCitation 2017). Footnote 28 During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 29 For example, an AI-enabled third party generated 'deepfake' (audio and video manipulation), coupled with data-poisoning cyberattacks, could spark an escalatory crisis between two (or more) nuclear states (Fitzpatrick Citation 2019). Footnote 30\nHow could AI-augmented cyber capabilities create new pathways for accidental or inadvertent escalation? To illustrate these dynamics: State A launches a malicious AI-enhanced cyberattack to spoof State B's AI-enabled autonomous sensor platforms and automated target recognition systems, in such a way that the weapon system (e.g. a human-supervised ATR system) is fooled into interpreting a civilian object as a military target. B in response, based on manipulated or erroneous information, and the inability of human supervisors to detect the spoofed imagery that fooled the weapons' automated target recognition algorithm in time to take corrective action, accidentally (and unintentionally) escalates a situation (Libicki Citation 2018). In this example, the spoofing attack on the weapon systems' algorithm is executed in such a way that the imagery appears to the recognition system as indistinguishable from a valid military target (Rohrbach et al. Citation 2018) Footnote 31 escalating a situation based on a false premise that would unlikely to fool the human eye. Footnote 32\nAlso, the explainability (or 'black box') problem associated with AI applications could compound these potential escalation dynamics. Footnote 33 Insufficient understanding of how and why AI algorithms reach a particular judgment or decision might complicate the task of determining whether data sets had been deliberately compromised to manufacture false outcomes (e.g. attacking incorrect targets or misdirecting allies during combat). Footnote 34 Moreover, as humans and AI team up to accomplish particular missions, the opacity associated with how AI systems reach a decision may cause an operator to have either too much or too little confidence in a system's performance. As a result, unless the system's machine learning algorithm is terminated, at the end of the training phase, once deployed, it could potentially learn something it was not intended to, or even perform a task or mission that its human designers do not expect it to do. Footnote 35\nIn sum, technologies that improve the reliability and speed with which information is processed and disseminated by early-warning systems may paradoxically also increase the vulnerabilities of these networks, creating new first-mover advantages and escalation pathways during a crisis that spark unintentional (or intentional) warfare (Schneider Citation 2019). Footnote 36 Put another way, AI might be developed in order to enable a state to pursue a pre-determined escalatory path. Thus, increased escalation risk as a result of technology is not always inadvertent or accidental (Long and Rittenhouse Green Citation 2015). Footnote 37\n# 'Cyber guns' supercharged with AI machine learning\nWhile manipulations and system subversions are possible with existing cyber offence tools, rapid advances in AI and increasing degrees of military autonomy could amplify the potential speed, power and scale of future attacks in cyberspace. Footnote 38 There are three significant intersections between AI systems and cybersecurity that are most salient to the military domain: (1) advances in autonomy and machine learning techniques mean that a much broader range of physical systems are now vulnerable to cyberattacks (i.e. hacking and data-poisoning) (Greenberg Citation 2016); Footnote 39 (2) cyberattacks on AI systems can offer attackers access to machine learning algorithms, trained models being used by the application and a potential vast amounts of data from facial recognition and intelligence collection and analysis systems (e.g. satellite navigation and imagery systems used to cue precision munitions and supported intelligence, surveillance, reconnaissance missions); and (3) the use of AI systems in conjunction with existing cyber offence tools will enable sophisticated cyberattacks to be executed at larger scale (both geographically and across networks), at faster speeds, and simultaneously across multiple military domains with improved anonymity. In short, despite the relatively benign augmentation mechanisms used to develop cyber offensive capabilities with AI systems, the resultant impact on the speed and scope of AI cyber tools will likely have destabilizing effects, nonetheless. Footnote 40\nRecent advances in machine learning have significantly contributed to resolving several technical bottlenecks in several fields of AI, which could allow for significant qualitative improvements to a large variety of autonomous weapon systems and applications. In combination, machine learning and autonomy could intersect with nuclear security in a multitude of ways, with both positive and negative implications for strategic stability. For example, machine learning could enhance the detection capabilities of (nuclear and non-nuclear) early-warning systems and improve the collection and cross-analysis of ISR information, thus reducing the risk of inadvertent and accidental escalation. Machine learning may also bolster the cyber-security of command and control systems, create new and possibilities for arms control, testing, verification and monitoring nuclear stockpiles, and enhance conventional counterforce capabilities. Unmanned autonomous systems could be deployed in complex missions in hitherto inaccessible and cluttered environments (e.g. under-sea anti-submarine warfare) and aerial and underwater drones in swarms might eventually replace intercontinental ballistic missiles (ICBMs) and nuclear-armed submarines (SSBNs) for the delivery of nuclear weapons.\nIn response to these anticipated vulnerabilities, and to achieve the (perceived) first-mover advantages the increased speed in AI-augmented cyber tools could confer, China, Russia and the United States have continued to harden their AI cyber defences. Despite these parallel efforts, divergent views exist on how strategic communities perceive the nature of the risks posed by AI-enhanced cyberattacks against dual-use C3I systems. Thus, AI-augmented cyber intelligence-\ngathering tools targeting an adversary's command and control assets (especially dual-use facilities) during a crisis may be misinterpreted as a prelude to an imminent pre-emptive attack on its nuclear force and therefore as undermining strategic stability.\nMoreover, AI machine learning could enable new offensive concepts such as a so-called 'left-of-launch operation' (Ellison Citation 2015), Footnote  $^{41}$  thereby compounding the ambiguities and fear about pre-emptive attacks in cyberspace, and increasing first-mover incentives – or use-them-or-lose-them situations (Buchanan Citation 2017; Buchanan and Miller Citation 2017). Footnote  $^{42}$  In the cyber domain, it is generally unclear in the early stages of a cyber operation whether an adversary intends to collect intelligence or prepare for an offensive attack, which is more likely to be used early on in a conflict scenario. Open sources suggest, for example, that Chinese analysts view the vulnerability of China's nuclear command, control, and communications (C3I) systems to cyber infiltrations – even if an attacker's objective was limited to cyberespionage – as a highly escalatory national security threat. By contrast, Russian analysts tend to view Russia's nuclear C3I network as relatively isolated, and thus insulated from cyberattacks.\nIrrespective of these differences, the uncertainty caused by the possibility, actual or otherwise, that AI-enhanced offensive cyber tools might be used (or threatened) to undermine the credibility and reliability of states' nuclear forces would be destabilizing, both as result of the existence of enhanced cyber offensive capabilities and the potential efficacy of these tools against nuclear forces. As the historical record attests, rational actors have incentives to misrepresent what they know (i.e. lie and bluff) in order to improve the terms of any settlement that may emerge from the wartime bargaining process – or the risk-reward payoff. Even a modicum of uncertainty about the effectiveness of AI-augmented cyber capabilities during a crisis or conflict would, therefore, reduce both sides' risk tolerance and increase the incentive to strike pre-emptively as a hedging strategy.\nA virtuous cycle that might flow from enhanced reassurances premised on comprehensive intelligence would require information symmetry (i.e. equal access to intelligence and analysis systems) between great and rising powers, and shared confidence in the accuracy and credibility of these systems. Perhaps most challenging of all in a world of 'revisionist' rising powers, the intentions of all rival states would need to be genuinely benign for this optimistic view to occur. Under crisis conditions, for example, an offensive AI cyber tool that succeeds in compromising an adversary's nuclear weapon systems, resulting in an 'asymmetric information' situation, could cause either or both sides to overstate (or understate) its retaliatory capabilities, and in turn, be more inclined to act in a risky and escalatory fashion. In short, in a competitive strategic environment, where states are inclined to assume the worst of others' intentions, one state's efforts to enhance the survivability of its strategic forces may be viewed by others as a threat to their nuclear retaliatory capability – or second-strike capacity.\n# Manipulation of the information landscape\nWhile machine learning big-data analytics, sensing technology, networks supported by 5G networks, could alert commanders of incoming threats with increased speed and precision, and make accidents caused by human error potentially less likely (especially accidents and malfunctions caused by false warning), these advances could simultaneously amplify escalation risks in two ways. Footnote 43\nFirst, AI machine learning used as force multipliers for cyber offence (e.g. data poisoning spoofing, 'deepfakes', manipulation, hacking and digital jamming) would be considerably more difficult to detect – especially if an attacker used AI advanced persistent threat (APT) tools – or 'hunting for weaknesses' in spectrum-contested environments. Second, in the unlikely event an attack was successfully detected, threat identification (or attribution) at machine speed would be virtually impossible. In addition to the co-mingling and the speed of warfare issues, AI machine learning systems might also exacerbate the existing – well-established and non-kinetic – risk of inadvertent (and intentional) escalation from the manipulation of the information landscape in which decisions about nuclear weapons are located. For example, the use of deepfake videos built-in real-time and disseminated online for malicious purposes.\nHuman cognition, and thus effective deterrence and signalling intentions, is predicated on reliable and clear information; if an adversary is concerned that the information available to them is limited (or worse, inaccurate), they will likely assume the worst and act accordingly. Asymmetric information situations between rivals about the balance of military power could undermine crisis stability, and in turn, create rational incentives to escalate to nuclear confrontation. Consequently, states will be more inclined to assume the worst of others' intentions, especially in situations where the legitimacy of the status quo is contested (i.e. maritime Asia).\nIn the competitive strategic environment, it is easy to imagine unprovoked escalation caused by a malicious third-party (or state-proxy) clandestine action. Even if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 44 For example, in an effort to incite conflict between two rival states, State A uses proxy hackers to use a AI machine learning technique (i.e. generative adversarial networks GAN) to launch 'deepfake' video or audio material, depicting senior military commanders of State B conspiring to launch a pre-emptive strike on State C. Footnote 45 Then, this 'deepfake' footage is deliberately leaked into C's AI-augmented intelligence collection and analysis systems, provoking C to escalate the situation with strategic consequences. B responds in kind. Footnote 46\nThe current underdeveloped state of ‘counter-AI’ capabilities and other fail-safe mechanisms (e.g. circuit breakers) to de-escalate escalation in cyberspace, will make the unprovoked and unintentional escalation dynamics depicted in this scenario very challenging to mitigate. Moreover, in the emerging ‘deepfakes’ arms race (much like cybersecurity more generally), detection software will likely lag behind advances in offensive enabling solutions – or offence-dominant ones. According to computer science expert Hany Farid, there are probably 100–1000 times ‘more people developing the technology to manipulate content than there is to detect [it]’ (Fontaine and Frederick Citation 2019).\nThe overreliance on automation (or ‘automation bias’) in the deployment of increasing complex AI-augmented capabilities – such as cyber, C3I systems, AWS and precision missile munitions – designed to provide a distinct tactical advantage of machine-speed, will increase the vulnerability of these capabilities to exploitation. Moreover, the increasing substitution of human cognition for logic derived from machines will likely increase the opportunities for adversaries to exploit the limitations of narrow AI technology, i.e. lack of human intuition, brittleness in complex real-world situations and an inability to effectively detect or counter manipulation attacks (Libicki Citation 2018). Footnote 47 Therefore, until researchers unravel some of the unexplainable features of AI, human error and machine error will likely compound one another, with unpredictable results. Footnote 48 Simply put, we are at a critical crossroads in the parallel (and symbiotic) evolution of AI and cyberspace that national security communities, globally, will need to prepare for proactively (Johnson Citation 2019a).\n## Policy interventions\nA prominent theme that runs through this article – and central to understanding the potential impact AI for strategic stability and nuclear security more broadly – is the concern that AI systems operating at machine-speed will push the pace of combat to a point where the actions of machines surpass the (cognitive and physical) ability of human decision makers to control (or even comprehend) events.\nPossible multi-track policy responses to push back against the threat posed to stability from AI in a multipolar can be broadly categorized into: (1) those that focus on enhancing debate and discussion between researchers, global defence communities, decision makers, academics and other political and societal stakeholders; and (2) a range of specific policy recommendations and tools for great military powers to negotiate and implement.\n## Enhancing debate and dialogue\nFirst, to mitigate (or at least manage) the destabilizing and escalatory risks posed by the AI-cybersecurity nexus, great military powers must closely coordinate their confidence-building\nmeasures in an effort to pre-empt some of the risks to stability outlined above. Track-1 and track-2 discussions should include diplomats and military leaders (especially from China, Russian and the United States), industry experts, AI researchers and multi-disciplinary academics. Specifically, great military powers should establish an international framework for governance, norms, behaviour and the regulation of policy on machine learning-augmented cyber capabilities. Resistance to these efforts will likely come from states who worry that in revealing their cyber (especially offensive) capabilities, they could upend the deterrence utility of these tools.\nTo be sure, the challenges posed in the coordination and implementation of these policies will require bold and visionary leadership to circumvent the inevitable regional agendas, interdisciplinary resistance and burgeoning security dilemmas between rival states. Because of the rapid technological change in AI formal treaties, associated with arms-control agreements that require lengthy and complex negotiation and ratification processes, legal frameworks risk becoming obsolete before they come into effect. The historical record has demonstrated on several occasions that these kinds of challenges facing humanity can be overcome (Krimsky Citation 1962; Naur and Randell Citation 1968). Footnote 49\nSecond, the think tank community, academics and AI research experts should pool their resources to investigate the implications of the AI-cyber nexus for a range of potential security issues such as: the impact of AI bias on future military-use applications; how to prepare for (and react to) artificial general intelligence; and measures to mitigate, prevent and manage offensive uses of AI. Footnote 50 If necessary, steps could be taken to amend existing legal definitions of offensive cyber operations, such as hacking, data-poisoning attacks and spoofing, to account for the increased proliferation and potential damage caused by AI.\nThird, and related, due to the intrinsic dual-use nature of the AI-cyber security problem set, this dialogue should also be expanded to include other stakeholders such as private-sector AI and cybersecurity experts, the commercial sector, ethicists, philosophers, civil society and public opinion. Footnote 51 States should also collaborate on dual-use AI research to leverage AI's low-cost and scaling advantages (i.e. in autonomy and robotics). Further, a focus on the safety, testing and robustness of AI systems is a critical step in mitigating potential vulnerabilities and risks caused by errors, bias, and explainability in uncontrolled and complex environments. Footnote 52 In the case of offensive AI-related cybersecurity research, where public distribution might cause vulnerabilities and worsen security, publication could be confined to trusted organisations and entities.\nBest practices that exist in more mature methods for addressing dual-use concerns, like computer security, where applicable, might be applied to AI. For example, the wide use of red teaming exercises to enhance network security, organisation and practices. Specifically, AI-cyber red teaming – like DARPA's Cyber Grand Challenge – will enable engineers and operators to better\nunderstand the skills needed to execute particular offensive and defensive operations (especially using machine learning techniques), and to better manage system vulnerabilities, adversarial exploitation, stress-testing and social engineering challenges (Anderson, Woodbridge, and Filar Citation 2016).\nA recent study explored case studies of previous examples (biological weapons, cryptography and nuclear technology) of dual-use technologies – with civilian and military uses – to examine potential insights for AI dual-use risk management policies such as export controls and prepublication reviews. These insights also highlight the potential challenges of establishing regulatory, legal and normative frameworks for dual-use technologies – for example, the cautionary tale of ineffective efforts, in the late 1990s, to regulate cryptographic algorithms and cyber-network security tools through export controls.\nThe extent to which the integration of AI into dual-use weapon systems (such as cybersecurity) might influence actors' attitude to risk, the offence-defence balance and, in turn, perceptions of others' intentions and capabilities, could have profound implications for strategic deterrence and nuclear security. Footnote 53 In sum, cognizant that some states have deployed (or imminently plan to deploy) AI systems, experts generally agree that AI requires further experimentation, testing and development before being integrated into lethal weapon systems and their decision-making support systems.\n# Taking the lead from the cybersecurity community\nSome examples of cybersecurity-centered measures that might be further researched and implemented to pre-empt and mitigate some of risks posed by AI-augmented cyberattacks (especially involving safety-critical nuclear systems) highlighted in this paper include the following.\nFirst, coordinating AI-simulated war games, red teaming creative thinking exercises and creating redundancies (i.e. back-ups or fail-safes) in networks to detect errors, fix vulnerabilities and increase the reliability and robustness of military (especially nuclear-centric) systems. Specific procedures could be implemented to enable confidential reporting and fixing the vulnerabilities, subversions and other kinds of manipulations detected in AI systems. These findings could be used to track the proliferation of AI-related incursions and then countermeasures could be developed and standardised to manage these threats. Footnote 54\nSecond, states should formalise verification methods and protocols and consider issues such as: to what extent, under what circumstances, and for what types of AI systems can formal verification be implemented? Might other approaches be developed to achieve similar goals (e.g. machine learning and big data analysis augmented verification methods)? For example, DARPA's Assured Autonomy Program combines data-driven machine learning algorithms to assure the\nsafety of autonomous cyber physical systems (air, ground, sea and undersea unmanned vehicles), which continue to learn throughout their lifespans.\nThis learning characteristic makes assurance or verification using traditional methods especially challenging. Whether AI applications in the military domain can be formally verified, for now, is an unanswered question. Footnote 55 Much like that of cyber systems, the complexity of AI systems and the difficulty of defining their properties for formal verification makes them less amenable to verification compared to other types of technology.\nThird, the global defence communities should actively invest in the development of AI cyber-defence tools (e.g. analysing classification errors, automatic detection of remote vulnerability scanning and model extraction improvements), AI-centric secure hardware, and other fail-safe mechanisms, to allow for de-escalation and to prevent unintentional or accidental escalation.\nSeveral issues that need further exploration related to these measures include: the extent to which existing tools might be effective against vulnerabilities in AI systems? How can these tools be tailored for AI systems across multiple military domains? Is there an equivalent to 'patching' in military AI systems? What kinds of policies might incentivize, and ensure compliance with, meaningful reforms to existing hardware in the military sphere? While these questions are challenging, and necessarily speculative for now, answers will become more evident as the technology matures.\n# Disclosure statement\nNo potential conflict of interest was reported by the author.",
    "footnotes": {
      "items": {},
      "intext": [
        {
          "index": "1",
          "intext_citation": "${}^{1}$",
          "preceding_text": "Footnote",
          "footnote": null
        },
        {
          "index": "2",
          "intext_citation": "${}^{2}$",
          "preceding_text": "Footnote  ${}^{1}$  AI can be understood as a universal term for improving the performance of automated (or autonomous) systems to solve a wide variety of complex tasks including: perception (sensors, computer vision, audio and image processing); reasoning and decision-making (problem solving, searching, planning and reasoning); learning and knowledge representation (machine learning, deep networks and modelling); Footnote",
          "footnote": null
        },
        {
          "index": "3",
          "intext_citation": "$^{3}$",
          "preceding_text": "Thus, even if AI applications are unable to make better battlefield decisions than humans, Footnote",
          "footnote": null
        },
        {
          "index": "4",
          "intext_citation": "$^{4}$",
          "preceding_text": "Citation 2015); Footnote",
          "footnote": null
        },
        {
          "index": "5",
          "intext_citation": "$^{5}$",
          "preceding_text": "Citation 2015); Footnote  $^{4}$  (3) advances in the implementation of machine learning techniques and algorithms (especially deep 'neural networks') (Schmidhuber Citation 2015); Footnote",
          "footnote": null
        },
        {
          "index": "6",
          "intext_citation": "$^{6}$",
          "preceding_text": "Footnote",
          "footnote": null
        },
        {
          "index": "7",
          "intext_citation": "$^{7}$",
          "preceding_text": "Footnote",
          "footnote": null
        },
        {
          "index": "8",
          "intext_citation": "$^{8}$",
          "preceding_text": "Footnote",
          "footnote": null
        },
        {
          "index": "9",
          "intext_citation": "$^{9}$",
          "preceding_text": "Given the diverse approaches to research in AI (Boden Citation 2016 ; Vernon Citation 2014), Footnote",
          "footnote": null
        },
        {
          "index": "10",
          "intext_citation": "$^{10}$",
          "preceding_text": "Footnote",
          "footnote": null
        },
        {
          "index": "11",
          "intext_citation": "$^{11}$",
          "preceding_text": "At the operational and tactical level, applications include: autonomy and robotics; multi-actor interaction red teaming wargaming; Footnote",
          "footnote": null
        },
        {
          "index": "12",
          "intext_citation": "$^{12}$",
          "preceding_text": "At the operational and tactical level, applications include: autonomy and robotics; multi-actor interaction red teaming wargaming; Footnote  $^{11}$  big data-driven modelling; Footnote",
          "footnote": null
        },
        {
          "index": "13",
          "intext_citation": "$^{13}$",
          "preceding_text": "to locate and monitor mobile missiles, troops movement) (Conklin Citation 2018); Footnote",
          "footnote": null
        },
        {
          "index": "41",
          "intext_citation": "$^{41}$",
          "preceding_text": "Moreover, AI machine learning could enable new offensive concepts such as a so-called 'left-of-launch operation' (Ellison Citation 2015), Footnote",
          "footnote": null
        },
        {
          "index": "42",
          "intext_citation": "$^{42}$",
          "preceding_text": "Footnote",
          "footnote": null
        }
      ],
      "stats": {
        "intext_total": 15,
        "success_occurrences": 0,
        "success_unique": 0,
        "bib_unique_total": 0,
        "occurrence_match_rate": 0.0,
        "bib_coverage_rate": 0.0,
        "success_percentage": 0.0,
        "missing_intext_expected_total": 27,
        "missing_intext_indices": [
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40
        ],
        "highest_intext_index": 42,
        "missing_footnotes_for_seen_total": 15,
        "missing_footnotes_for_seen_intext": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          41,
          42
        ],
        "uncited_footnote_total": 0,
        "uncited_footnote_indices": [],
        "style": "footnotes"
      }
    },
    "tex": {
      "total": {
        "intext_total": 1,
        "success_occurrences": 1,
        "success_unique": 1,
        "bib_unique_total": 1,
        "occurrence_match_rate": 1.0,
        "bib_coverage_rate": 1.0,
        "success_percentage": 100.0,
        "missing_intext_expected_total": 12,
        "missing_intext_indices": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12
        ],
        "highest_intext_index": 13,
        "missing_footnotes_for_seen_total": 0,
        "missing_footnotes_for_seen_intext": [],
        "uncited_footnote_total": 0,
        "uncited_footnote_indices": [],
        "style": "tex_superscript"
      },
      "results": [
        {
          "index": "13",
          "intext_citation": "${ }^{13}$",
          "preceding_text": "cations include: autonomy and robotics; multi-actor interaction red teaming wargaming; Footnote $^{11}$ big data-driven modelling; Footnote $^{12}$ intelligence collection and analysis (e.g. to locate and monitor mobile missiles, troops movement) (Conklin Citation 2018); Footnote",
          "footnote": "cybersecurity talent management; predictive maintenance, logistics, planning and forecasting; and vendor contract and budget management. In preparation for a new AI strategy, US Strategic Operations Command (SOCOM) recently crafted a roadmap that invests heavily in AI and machine learning technology based on several pilot projects with three overarching goals to create an algorithmic warfare multi-operational team: 'AI-ready workforce, AI-applications, and AI-outreach' (Lee Citation 2019)."
        }
      ],
      "flat_text": "# ABSTRACT\nHow could AI-infused cyber capabilities be used to subvert, or otherwise compromise, the reliability, control and use of states' nuclear forces? This article argues that a new generation of artificial intelligence (AI) enhanced cyber capabilities will amplify the risk of inadvertent escalation caused by the co-mingling of nuclear and strategic non-nuclear weapons and the increasing speed of warfare, thereby increasing the risk of nuclear confrontation. It examines the potential implications of cyber (offensive and defensive) capabilities augmented with AI applications for nuclear security. The article concludes that future iterations of AI-enhanced cyber counterforce capabilities will complicate the existing challenges of cyber defence, and in turn, compromise nuclear assets and increase the escalatory effects of offensive cyber capabilities.\n# KEYWORDS:\n- Artificial intelligence\n- cyber-security\n- US-China Relations\n- nuclear security\n- deterrence policy\n- emerging technology\n# Introduction\nGiven the hyperbole surrounding AI, it is easy to overstate the opportunities and challenges posed by the development and deployment of AI in the military sphere (Boulanin Citation 2019). Today, there remains a large amount of debate among AI researchers surrounding the significance of several significant technical and operational challenges in the deployment of AI-infused systems (e.g. drone swarming, command and control decision-making support systems and a broader range of autonomous weapon systems). Moreover, many of the risks posed by AI in the nuclear domain today are not necessarily new (Boulanin Citation 2019). Recent advances in AI (especially machine learning techniques) exacerbate existing risks to escalation and stability rather than generating entirely new ones. While AI could enable significant improvements in many military domains (including nuclear weapons), for the foreseeable future, developments in military AI will likely be far more prosaic than implied in popular culture. The main worry for nuclear stability, therefore, is that militaries will underestimate (or overstate) or ignore the potential shortcomings and risks associated with the use of the current generation of AI technology in the safety-critical military sphere – and especially the nuclear domain.\nBecause of the diverse approaches to AI-based research, there is no universally accepted definition of ‘artificial intelligence’, which is confusing when the term is applied generically to make grandiose claims about its revolutionary effects. Footnote    AI can be understood as a universal term for improving the performance of automated (or autonomous) systems to solve a wide variety of complex tasks including: perception (sensors, computer vision, audio and image processing); reasoning and decision-making (problem solving, searching, planning and reasoning); learning and knowledge representation (machine learning, deep networks and modelling); Footnote    communication (language processing); autonomy and robotics ; and human-AI collaboration (humans define the systems’ purpose, goals and context).\nIn a military context, as a potential enabler and force multiplier of advanced weapon systems, AI is more akin to electricity, radio, radar and intelligence, surveillance and reconnaissance (ISR) support systems than a 'weapon' per se. Much like these enabling support systems, AI is also strategically vital, vulnerable, inherently cross-domain and dependent on advanced technology. Thus, even if AI applications are unable to make better battlefield decisions than humans, Footnote  $^{3}$  militaries that use AI in human-machine teaming will doubtless gain significant advantages (e.g. remote-sensing, situational-awareness, battlefield-manoeuvres and a compressed decision-making loop), compared to those who depend on human judgment – and semi-autonomous technology – alone. This is particularly the case, in operating systems in operating environments that demand endurance and rapid decision-making across multiple combat zones (Ayoub and Payne Citation 2016).\nThis article argues that a new generation of AI-augmented offensive cyber capabilities will amplify the risk of inadvertent escalation posed by the co-mingling of nuclear and strategic (or counterforce) non-nuclear weapons and the increasing speed of warfare, and in turn, increase the risk of nuclear confrontation. The article's thesis is grounded in three core themes. First, AI does not exist a vacuum, that is, in isolation, AI will unlikely be a strategic game changer. Instead, it will likely mutually reinforce the destabilizing effects of existing advanced capabilities thereby increasing the speed of warfare and compressing the decision-making timeframe (Johnson Citation 2019a). AI-enabled and enhanced capabilities will have a more significant impact (positive or negative) on strategic stability than the sum of its parts. Furthermore, the intersection of AI with nuclear weapons, and a broader spectrum of strategic non-nuclear weapons, will likely accelerate the erosion of the survivability of nuclear arsenals associated with the 'computer revolution'. Put another way, military AI and the advanced capabilities it enables are a natural manifestation – rather than the cause or origin – of an established trend, which could lead states to adopt destabilizing launch postures due to the increasing speed of war and co-mingling.\nSecond, AI's impact on stability, deterrence and escalation will likely be determined as much (or more) by states' perception of its functionality as much as by what it is capable of doing. In the case of nuclear strategy and deterrence, the perception of an adversary's capabilities and\nintentions is as (or more) important than its actual capability. Thus, in addition to the importance of military force postures, capabilities and doctrine, the effects of AI will also have a strong cognitive element, increasing the risk of inadvertent escalation as a result of misperception and misunderstanding.\nFinally, and related to the above, the increasingly competitive and contested nuclear multipolar world order will compound the destabilizing effects of AI, and in turn, increase the escalation risks in future warfare between great military powers – especially China and the United States (Johnson Citation 2019b). Moreover, the potential operational and strategic advantages offered by AI-augmented capabilities could prove irresistible to nuclear-armed strategic rivals, causing them to eschew the limitations of AI, and compromise safety and verification standards to protect, or attempt to capture, technological superiority on the future digitised battlefield.\n# Military AI primer\nArtificial intelligence research began as early as the 1950s, as a broad concept concerned with the science and engineering of making intelligent machines. Since the 1950s, AI research has gone through several development phases – from early exploitations in the 1950s and 1960s and the 'AI Summer' during the 1970s, through to the early 1980s and the 'AI Winter' from the 1980s – each of which failed to live up to its initial, and often over-hyped, expectations. In the past decade, the explosion of interest in the field (or 'AI renaissance') occurred due to the convergence of four enabling developments: (1) the exponential growth in computing processing power; (2) expanded data sets (especially 'big data' sources) (Gray et al. Citation 2015); Footnote  $^{4}$  (3) advances in the implementation of machine learning techniques and algorithms (especially deep 'neural networks') (Schmidhuber Citation 2015); Footnote  $^{5}$  and (4) the rapid expansion of commercial interest and investment in AI (Hoadley and Nathan Citation 2018). Footnote  $^{6}$\nAI is concerned with machines that emulate capabilities that are usually associated with human intelligence, such as language, reasoning, learning, heuristics and observation. Today, all practical (i.e. technically feasible) AI applications fall into the 'narrow' category, and less so, artificial general intelligence (AGI) or 'superintelligence' (Reedy Citation 2017). Footnote  $^{7}$  'Narrow AI' has been widely used in a broad range of civilian and military tasks since the 1960s, (Nilsson Citation 2010; Russel and Norvig Citation 2014) and involves statistical algorithms (mostly based on machine learning techniques) that learn procedures through analysis of large training data sets designed to approximate and replicate human cognitive tasks (Russel and Norvig Citation 2014). Footnote  $^{8}$  'Narrow AI' is the category of artificial intelligence to which this article refers when it assesses the impact of this technology in a military context.\nGiven the diverse approaches to research in AI (Boden Citation 2016 ; Vernon Citation 2014), Footnote  $^{9}$  there is no universally accepted definition of AI. A recent US congressional report defines AI as follows: 'Any artificial system that performs tasks under varying and unpredictable circumstances, without significant human oversight, or that can learn from their experience and improve their performance ... they may solve tasks requiring human-like perception, cognition, planning, learning, communication, or physical action' (Hoadley and Nathan Citation 2018).\nToday, a large gap exists between the reality of what AI technology is capable of doing in a military context, and the expectations and fears of public opinion, policymakers and global defence communities. How will AI technologies influence public opinion about the use of military force and warfare? (Sechser, Narang, and Talmadge Citation 2019). The misrepresentations and misconceptions that exist today in the narratives surrounding AI (especially in societal, economic and national security debates) and are in large part caused by the hyperbole and exaggerated depictions of AI in science fiction (Zarkadakis Citation 2015). Footnote  $^{10}$  Misrepresentations of the potential opportunities and risks associated with this technology in the military sphere can obscure constructive and crucial debate on these topics. Specifically, this includes the challenge of balancing the potential operational, tactical and strategic benefits of leveraging AI while managing the risks posed to nuclear security in the (especially premature) pursuit of these advantages.\nThe historical record on technological change in a military context counsels scepticism against alarmist predictions and extrapolations from emerging trends: technologies rarely evolve in the way futurists predict and many applications have had countervailing or conditional effects that have ameliorated the dystopian predictions of naysayers (Metz Citation 2000 ; Metz and Kievit Citation 1995). In short, AI's impact on nuclear stability will, like previous generations of emerging technology (i.e. chemical and biological weapons, cyber and nuclear weapons), have both stabilising and destabilizing (and possibly contradictory) effects on strategic stability. For example, many expected that chemical weapons would instantly and dramatically change the nature of warfare and deterrence after the British used poison gas during World War I (Sechser, Narang, and Talmadge Citation 2019). However, chemical weapons proved far less practical, impactful, disruptive and relatively easier to defend against than conventional explosives. As a corollary, the degree to which military AI poses risks to future strategic stability will depend in large part on the pace and scope with which this technology facilitates new ways to improve the delivery of, and defence against, nuclear weapons and strategic non-nuclear weapons.\nConceptually, AI-augmented applications can be categorized into those that have predominately operational, tactical and strategic implications in future warfare. At the operational and tactical level, applications include: autonomy and robotics; multi-actor interaction red teaming wargaming; Footnote  $^{11}$  big data-driven modelling; Footnote  $^{12}$  intelligence collection and analysis (e.g. to locate and monitor mobile missiles, troops movement) (Conklin Citation 2018); Footnote\n$^{13}$ cybersecurity talent management; predictive maintenance, logistics, planning and forecasting; and vendor contract and budget management. In preparation for a new AI strategy, US Strategic Operations Command (SOCOM) recently crafted a roadmap that invests heavily in AI and machine learning technology based on several pilot projects with three overarching goals to create an algorithmic warfare multi-operational team: 'AI-ready workforce, AI-applications, and AI-outreach' (Lee Citation 2019).\nTherefore, AI's impact on strategic stability and escalation will also be shaped by broader and more nuanced factors which influence the trajectory of emerging technologies, including: military strategy and doctrine – that seeks to manipulate escalation risks – military culture and organisation, alliance structures, domestic politics and public opinion, to name but a few. The Cold War record demonstrates that emerging technologies act primarily to enable independent variables which, in combination with other endogenous factors, can heighten escalation risks between adversaries. Therefore, in isolation, technology is not a major exogenous cause of (inadvertent or intentional) military escalation.\nAt a strategic level of warfare, AI uses include: qualitative improvements to the nuclear command, control, communications, and intelligence (C3I) architecture; enhancing target acquisition, tracking, guidance systems and discrimination of missile and air defence systems; being force multipliers of both offensive and defensive machine-learning infused cyber capabilities; and qualitatively bolstering nuclear and non-nuclear missile delivery systems – including hypersonic variants. Footnote 14\nIn combination, AI machine learning algorithms fused with advances in sensor technology could be engineered to allow nuclear delivery systems to operate more autonomously and precisely, with less reliance on human operators calibrating navigation and guidance parameters and equipped with more robust countermeasures against jamming or spoofing attacks. At the strategic level, AI-augmented command and control systems could potentially mitigate many of the shortcomings inherent to human strategic decision-making during the 'fog of war', such as the susceptibility to invest in sunk costs, skewed risk judgment, cognitive heuristics and groupthink.\nIn sum, the fusion of AI machine learning and human judgment to gauge an adversary's intentions (and predict escalation) for the purposes of planning and directing future wars for the pursuit of political objectives, is, therefore, a far less unlikely prospect in the near future than the use of AI to achieve tactical and operational ends (e.g. drone swarming and cyber defence).\n# Blurring the AI-cyber offence-defence line: (Glaser and Kaufmann Citation 1998 ; Lynn-Jones Citation 1995 ) Footnote 15\nSeveral US national security officials believe that AI, used as force multipliers for both defensive and offensive cyber weapons, will have a transformative impact on cybersecurity (Gartzke and Lindsay Citation 2015 ; Rid Citation 2012 ; Slayton Citation 2017 ; Hoffman Citation 2019). Footnote 16 Director of US National Intelligence, Daniel Coats, recently warned that AI could increase US vulnerability to cyberattacks, weaken its ability to attribute such attacks, improve the effectiveness and capabilities of adversaries weapon and intelligence systems, and cause accidents and related liability issues. To be sure, the line between AI cyber-offence and cyber-defence will likely remain an obscure one. Footnote 17 Bernard Brodie’s words of caution about the advent of nuclear weapons almost six decades ago resonate: ‘The [military] bias towards the offensive creates special problems in any technologically new situation where there is little or no relevant war experience to help one to reach a balanced judgment’ (Brodie Citation 1959).\nOn the one hand, AI might reduce a military’s vulnerability to cyberattacks. AI cyber-defence tools (or ‘counter-AI’), designed to recognise changes to patterns of behaviour in a network and detect anomalies, automatically detect software code vulnerabilities, and apply machine learning techniques (such as ‘deep learning’) to detect deviations and anomalies from normal network activity, Footnote 18 could form a more robust defence against cyber subversions (Johnson Citation 2019a). Footnote 19 According to the Pentagon’s new AI strategic report, ‘AI can enhance our ability to predict, identify, and respond to cyber and physical threats from a range of sources’ (US Department of Defense Citation 2019). In addition, the DoD’s Defense Innovation Unit (DIU) is prototyping an application (related to Project VOLTRON) that leverages AI to decipher high-level strategic questions, map probabilistic chains of events and develop alternative strategies in order to make DoD systems more resilient to AI-augmented cyberattacks and configure and fix errors more quickly than humans.\nOn the other hand, autonomy itself might increase a military’s vulnerability to cyberattacks, which rely on stealth, deception and stratagem. For example, an adversary could use malware to take control, manipulate or fool the behaviour and pattern recognition systems of autonomous systems, such as DoD’s Project Maven. Offensive attacks such as this would be relatively easy to execute, but very difficult to detect, attribute or effectively counter (Shachtman Citation 2011). Footnote 20 This problem set is compounded by the lack of an agreed framework or understanding of what constitutes escalatory behaviour (or ‘firebreaks’) in cyberspace. Thus, a cyber operation intended as a signal (i.e. for coercive diplomacy) could go undetected by the target, or worse, misinterpreted as an offensive attack. Even if information relating to an\noperation of this kind is accurately identified in a timely manner, the motives behind them could remain ambiguous, or misperceived. According to Robert Jervis, ‘it is likely that the country that is the object of the attack would assume that any effect was the intended one’ (Jervis Citation 2016).\nUS Cyber Fleet Command Commander, Michael Gilday, recently told the Senate Armed Services Committee that the US Navy must ‘improve an ability to proactively detect new and unknown malware … so we [the US] can act quickly using advanced analytics enabled by AI and machine learning’, which may give the US a ‘tactical advantage’ to identify malicious activity early on (Osborn Citation 2018). Even if analysts can obtain high-quality and reliable intelligence, however, they may not want to reveal it, because doing so could compromise a source, capability or tactic (Johnson Citation 2019a). Moreover, most observers now acknowledge that no strategy (i.e. combining offence and defence cyber operations) could realistically be expected to deter all malign cyber-attacks (Hoffman Citation 2019). Footnote 21 While AI-enhanced cyber capabilities can enhance deterrence, they can simultaneously incentivize others to attack thereby exacerbating the paradox of enhanced capabilities and increased vulnerabilities in the cyber domain (Gartzke and Lindsay Citation 2017; Libicki Citation 2016; Slayton Citation 2017). Footnote 22 As the historical record attests, this capability-vulnerability paradox is heightened when states are both dependent on a particular capability (such as AI and cyber tools) and their access or ability to use the capabilities is vulnerable to an adversary’s exploitation or subversion – creating first-mover incentives (Rid Citation 2013). Footnote 23\n# Cybersecurity and nuclear weapon systems: nuclear risk redux?\nAt a strategic level of conflict, AI applications designed to enhance cybersecurity for nuclear forces could simultaneously make cyber-dependent nuclear weapon systems (i.e. communications, data processing or early-warning sensors) more vulnerable to cyberattacks. It is now thought possible that a cyberattack (i.e. spoofing, hacking, manipulation and digital jamming) could infiltrate a nuclear weapons system, threaten the integrity of its communications, and ultimately (and possibly unbeknown to its target) gain control of its (nuclear and non-nuclear) command and control systems. Footnote 24 AI technology has not yet evolved to a point where it would allow nuclear-armed states to credibly threaten the survivability of each other’s nuclear second-strike capability. Thus, the development trajectory of AI (and its enabling technologies) means its impact on nuclear security will likely be theoretical and speculative for the foreseeable future.\nBecause of the intense time pressures that would likely loom large with the decision to use nuclear weapons – especially where a state maintains a launch-on-warning posture – AI-enhanced\ncyberattacks against nuclear systems would be almost impossible to detect and the warning signals difficult to authenticate, let alone attribute, within the short timeframe for initiating a nuclear strike. According to open sources, operators at the North American Aerospace Defence Command (NORAD) have less than three minutes to assess and confirm initial indications from early-warning systems of an incoming attack. This compressed decision-making time frame could put political leaders under intense pressure to make a decision to escalate during a crisis, with incomplete (and possibly false) information of a situation. Ironically, new technologies designed to enhance information (i.e. modernised nuclear C3I systems augmented by 5G networks, machine learning, big-data analytics and quantum computing) can also undermine clear and reliable information flow and communication, critical for effective deterrence (Gartzke and Lindsay Citation 2019). Footnote 25\nAdvances in AI could also exacerbate this cybersecurity challenge by enabling improvements to the cyber offence. Machine learning and AI by automating advanced persistent threat (APT) operations might dramatically reduce the extensive manpower resources and high levels of technical skill required to execute APT operations (or 'hunting for weaknesses'), especially against hardened nuclear targets. Footnote 26 The machine speed of AI-augmented cyber tools could enable a low-skilled and capital-restricted attacker to exploit a narrow window of opportunity to penetrate an adversary's cyber-defences or use APT tools to find new vulnerabilities. For example, when docked for maintenance air-gapped nuclear-powered submarines, considered secure when submerged, could become increasingly vulnerable to a new generation of low-cost – possibly black-market – and highly automated APT cyberattacks.\nAn attacker could also apply AI machine learning techniques to target autonomous dual-use early-warning and other operating systems (e.g. C3I, ISR, early-warning and robotic control networks) with 'weaponized software' such as hacking, subverting, spoofing or tricking, causing unpredictable and potentially undetectable errors, malfunctions and behavioural manipulation to weapons systems – or 'data-poisoning'. Footnote 27 Furthermore, as the linkages between digital and physical systems (or the 'Internet of Things') expand, the potential for to an adversary to use cyberattacks in both kinetic and non-kinetic attacks will increase. A significant risk variable in the operation of autonomous systems is the time that passes between a system failure (i.e. performing in a manner other than how the human operator intended) and the time it takes for a human operator to take corrective action. If the system failure is the result of a deliberate act, this time frame will be compressed (Johnson Citation 2019a).\nEven if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. This skewed assessment by states in the context of nuclear weapons, which are ready to launch nuclear weapons at a moment's notice, would likely precipitate worst-case scenario thinking that may spark an inadvertent escalation (Talmadge\nCitation 2017). Footnote 28 During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 29 For example, an AI-enabled third party generated 'deepfake' (audio and video manipulation), coupled with data-poisoning cyberattacks, could spark an escalatory crisis between two (or more) nuclear states (Fitzpatrick Citation 2019). Footnote 30\nHow could AI-augmented cyber capabilities create new pathways for accidental or inadvertent escalation? To illustrate these dynamics: State A launches a malicious AI-enhanced cyberattack to spoof State B's AI-enabled autonomous sensor platforms and automated target recognition systems, in such a way that the weapon system (e.g. a human-supervised ATR system) is fooled into interpreting a civilian object as a military target. B in response, based on manipulated or erroneous information, and the inability of human supervisors to detect the spoofed imagery that fooled the weapons' automated target recognition algorithm in time to take corrective action, accidentally (and unintentionally) escalates a situation (Libicki Citation 2018). In this example, the spoofing attack on the weapon systems' algorithm is executed in such a way that the imagery appears to the recognition system as indistinguishable from a valid military target (Rohrbach et al. Citation 2018) Footnote 31 escalating a situation based on a false premise that would unlikely to fool the human eye. Footnote 32\nAlso, the explainability (or 'black box') problem associated with AI applications could compound these potential escalation dynamics. Footnote 33 Insufficient understanding of how and why AI algorithms reach a particular judgment or decision might complicate the task of determining whether data sets had been deliberately compromised to manufacture false outcomes (e.g. attacking incorrect targets or misdirecting allies during combat). Footnote 34 Moreover, as humans and AI team up to accomplish particular missions, the opacity associated with how AI systems reach a decision may cause an operator to have either too much or too little confidence in a system's performance. As a result, unless the system's machine learning algorithm is terminated, at the end of the training phase, once deployed, it could potentially learn something it was not intended to, or even perform a task or mission that its human designers do not expect it to do. Footnote 35\nIn sum, technologies that improve the reliability and speed with which information is processed and disseminated by early-warning systems may paradoxically also increase the vulnerabilities of these networks, creating new first-mover advantages and escalation pathways during a crisis that spark unintentional (or intentional) warfare (Schneider Citation 2019). Footnote 36 Put another way, AI might be developed in order to enable a state to pursue a pre-determined escalatory path. Thus, increased escalation risk as a result of technology is not always inadvertent or accidental (Long and Rittenhouse Green Citation 2015). Footnote 37\n# 'Cyber guns' supercharged with AI machine learning\nWhile manipulations and system subversions are possible with existing cyber offence tools, rapid advances in AI and increasing degrees of military autonomy could amplify the potential speed, power and scale of future attacks in cyberspace. Footnote 38 There are three significant intersections between AI systems and cybersecurity that are most salient to the military domain: (1) advances in autonomy and machine learning techniques mean that a much broader range of physical systems are now vulnerable to cyberattacks (i.e. hacking and data-poisoning) (Greenberg Citation 2016); Footnote 39 (2) cyberattacks on AI systems can offer attackers access to machine learning algorithms, trained models being used by the application and a potential vast amounts of data from facial recognition and intelligence collection and analysis systems (e.g. satellite navigation and imagery systems used to cue precision munitions and supported intelligence, surveillance, reconnaissance missions); and (3) the use of AI systems in conjunction with existing cyber offence tools will enable sophisticated cyberattacks to be executed at larger scale (both geographically and across networks), at faster speeds, and simultaneously across multiple military domains with improved anonymity. In short, despite the relatively benign augmentation mechanisms used to develop cyber offensive capabilities with AI systems, the resultant impact on the speed and scope of AI cyber tools will likely have destabilizing effects, nonetheless. Footnote 40\nRecent advances in machine learning have significantly contributed to resolving several technical bottlenecks in several fields of AI, which could allow for significant qualitative improvements to a large variety of autonomous weapon systems and applications. In combination, machine learning and autonomy could intersect with nuclear security in a multitude of ways, with both positive and negative implications for strategic stability. For example, machine learning could enhance the detection capabilities of (nuclear and non-nuclear) early-warning systems and improve the collection and cross-analysis of ISR information, thus reducing the risk of inadvertent and accidental escalation. Machine learning may also bolster the cyber-security of command and control systems, create new and possibilities for arms control, testing, verification and monitoring nuclear stockpiles, and enhance conventional counterforce capabilities. Unmanned autonomous systems could be deployed in complex missions in hitherto inaccessible and cluttered environments (e.g. under-sea anti-submarine warfare) and aerial and underwater drones in swarms might eventually replace intercontinental ballistic missiles (ICBMs) and nuclear-armed submarines (SSBNs) for the delivery of nuclear weapons.\nIn response to these anticipated vulnerabilities, and to achieve the (perceived) first-mover advantages the increased speed in AI-augmented cyber tools could confer, China, Russia and the United States have continued to harden their AI cyber defences. Despite these parallel efforts, divergent views exist on how strategic communities perceive the nature of the risks posed by AI-enhanced cyberattacks against dual-use C3I systems. Thus, AI-augmented cyber intelligence-\ngathering tools targeting an adversary's command and control assets (especially dual-use facilities) during a crisis may be misinterpreted as a prelude to an imminent pre-emptive attack on its nuclear force and therefore as undermining strategic stability.\nMoreover, AI machine learning could enable new offensive concepts such as a so-called 'left-of-launch operation' (Ellison Citation 2015), Footnote  $^{41}$  thereby compounding the ambiguities and fear about pre-emptive attacks in cyberspace, and increasing first-mover incentives – or use-them-or-lose-them situations (Buchanan Citation 2017; Buchanan and Miller Citation 2017). Footnote  $^{42}$  In the cyber domain, it is generally unclear in the early stages of a cyber operation whether an adversary intends to collect intelligence or prepare for an offensive attack, which is more likely to be used early on in a conflict scenario. Open sources suggest, for example, that Chinese analysts view the vulnerability of China's nuclear command, control, and communications (C3I) systems to cyber infiltrations – even if an attacker's objective was limited to cyberespionage – as a highly escalatory national security threat. By contrast, Russian analysts tend to view Russia's nuclear C3I network as relatively isolated, and thus insulated from cyberattacks.\nIrrespective of these differences, the uncertainty caused by the possibility, actual or otherwise, that AI-enhanced offensive cyber tools might be used (or threatened) to undermine the credibility and reliability of states' nuclear forces would be destabilizing, both as result of the existence of enhanced cyber offensive capabilities and the potential efficacy of these tools against nuclear forces. As the historical record attests, rational actors have incentives to misrepresent what they know (i.e. lie and bluff) in order to improve the terms of any settlement that may emerge from the wartime bargaining process – or the risk-reward payoff. Even a modicum of uncertainty about the effectiveness of AI-augmented cyber capabilities during a crisis or conflict would, therefore, reduce both sides' risk tolerance and increase the incentive to strike pre-emptively as a hedging strategy.\nA virtuous cycle that might flow from enhanced reassurances premised on comprehensive intelligence would require information symmetry (i.e. equal access to intelligence and analysis systems) between great and rising powers, and shared confidence in the accuracy and credibility of these systems. Perhaps most challenging of all in a world of 'revisionist' rising powers, the intentions of all rival states would need to be genuinely benign for this optimistic view to occur. Under crisis conditions, for example, an offensive AI cyber tool that succeeds in compromising an adversary's nuclear weapon systems, resulting in an 'asymmetric information' situation, could cause either or both sides to overstate (or understate) its retaliatory capabilities, and in turn, be more inclined to act in a risky and escalatory fashion. In short, in a competitive strategic environment, where states are inclined to assume the worst of others' intentions, one state's efforts to enhance the survivability of its strategic forces may be viewed by others as a threat to their nuclear retaliatory capability – or second-strike capacity.\n# Manipulation of the information landscape\nWhile machine learning big-data analytics, sensing technology, networks supported by 5G networks, could alert commanders of incoming threats with increased speed and precision, and make accidents caused by human error potentially less likely (especially accidents and malfunctions caused by false warning), these advances could simultaneously amplify escalation risks in two ways. Footnote 43\nFirst, AI machine learning used as force multipliers for cyber offence (e.g. data poisoning spoofing, 'deepfakes', manipulation, hacking and digital jamming) would be considerably more difficult to detect – especially if an attacker used AI advanced persistent threat (APT) tools – or 'hunting for weaknesses' in spectrum-contested environments. Second, in the unlikely event an attack was successfully detected, threat identification (or attribution) at machine speed would be virtually impossible. In addition to the co-mingling and the speed of warfare issues, AI machine learning systems might also exacerbate the existing – well-established and non-kinetic – risk of inadvertent (and intentional) escalation from the manipulation of the information landscape in which decisions about nuclear weapons are located. For example, the use of deepfake videos built-in real-time and disseminated online for malicious purposes.\nHuman cognition, and thus effective deterrence and signalling intentions, is predicated on reliable and clear information; if an adversary is concerned that the information available to them is limited (or worse, inaccurate), they will likely assume the worst and act accordingly. Asymmetric information situations between rivals about the balance of military power could undermine crisis stability, and in turn, create rational incentives to escalate to nuclear confrontation. Consequently, states will be more inclined to assume the worst of others' intentions, especially in situations where the legitimacy of the status quo is contested (i.e. maritime Asia).\nIn the competitive strategic environment, it is easy to imagine unprovoked escalation caused by a malicious third-party (or state-proxy) clandestine action. Even if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 44 For example, in an effort to incite conflict between two rival states, State A uses proxy hackers to use a AI machine learning technique (i.e. generative adversarial networks GAN) to launch 'deepfake' video or audio material, depicting senior military commanders of State B conspiring to launch a pre-emptive strike on State C. Footnote 45 Then, this 'deepfake' footage is deliberately leaked into C's AI-augmented intelligence collection and analysis systems, provoking C to escalate the situation with strategic consequences. B responds in kind. Footnote 46\nThe current underdeveloped state of ‘counter-AI’ capabilities and other fail-safe mechanisms (e.g. circuit breakers) to de-escalate escalation in cyberspace, will make the unprovoked and unintentional escalation dynamics depicted in this scenario very challenging to mitigate. Moreover, in the emerging ‘deepfakes’ arms race (much like cybersecurity more generally), detection software will likely lag behind advances in offensive enabling solutions – or offence-dominant ones. According to computer science expert Hany Farid, there are probably 100–1000 times ‘more people developing the technology to manipulate content than there is to detect [it]’ (Fontaine and Frederick Citation 2019).\nThe overreliance on automation (or ‘automation bias’) in the deployment of increasing complex AI-augmented capabilities – such as cyber, C3I systems, AWS and precision missile munitions – designed to provide a distinct tactical advantage of machine-speed, will increase the vulnerability of these capabilities to exploitation. Moreover, the increasing substitution of human cognition for logic derived from machines will likely increase the opportunities for adversaries to exploit the limitations of narrow AI technology, i.e. lack of human intuition, brittleness in complex real-world situations and an inability to effectively detect or counter manipulation attacks (Libicki Citation 2018). Footnote 47 Therefore, until researchers unravel some of the unexplainable features of AI, human error and machine error will likely compound one another, with unpredictable results. Footnote 48 Simply put, we are at a critical crossroads in the parallel (and symbiotic) evolution of AI and cyberspace that national security communities, globally, will need to prepare for proactively (Johnson Citation 2019a).\n## Policy interventions\nA prominent theme that runs through this article – and central to understanding the potential impact AI for strategic stability and nuclear security more broadly – is the concern that AI systems operating at machine-speed will push the pace of combat to a point where the actions of machines surpass the (cognitive and physical) ability of human decision makers to control (or even comprehend) events.\nPossible multi-track policy responses to push back against the threat posed to stability from AI in a multipolar can be broadly categorized into: (1) those that focus on enhancing debate and discussion between researchers, global defence communities, decision makers, academics and other political and societal stakeholders; and (2) a range of specific policy recommendations and tools for great military powers to negotiate and implement.\n## Enhancing debate and dialogue\nFirst, to mitigate (or at least manage) the destabilizing and escalatory risks posed by the AI-cybersecurity nexus, great military powers must closely coordinate their confidence-building\nmeasures in an effort to pre-empt some of the risks to stability outlined above. Track-1 and track-2 discussions should include diplomats and military leaders (especially from China, Russian and the United States), industry experts, AI researchers and multi-disciplinary academics. Specifically, great military powers should establish an international framework for governance, norms, behaviour and the regulation of policy on machine learning-augmented cyber capabilities. Resistance to these efforts will likely come from states who worry that in revealing their cyber (especially offensive) capabilities, they could upend the deterrence utility of these tools.\nTo be sure, the challenges posed in the coordination and implementation of these policies will require bold and visionary leadership to circumvent the inevitable regional agendas, interdisciplinary resistance and burgeoning security dilemmas between rival states. Because of the rapid technological change in AI formal treaties, associated with arms-control agreements that require lengthy and complex negotiation and ratification processes, legal frameworks risk becoming obsolete before they come into effect. The historical record has demonstrated on several occasions that these kinds of challenges facing humanity can be overcome (Krimsky Citation 1962; Naur and Randell Citation 1968). Footnote 49\nSecond, the think tank community, academics and AI research experts should pool their resources to investigate the implications of the AI-cyber nexus for a range of potential security issues such as: the impact of AI bias on future military-use applications; how to prepare for (and react to) artificial general intelligence; and measures to mitigate, prevent and manage offensive uses of AI. Footnote 50 If necessary, steps could be taken to amend existing legal definitions of offensive cyber operations, such as hacking, data-poisoning attacks and spoofing, to account for the increased proliferation and potential damage caused by AI.\nThird, and related, due to the intrinsic dual-use nature of the AI-cyber security problem set, this dialogue should also be expanded to include other stakeholders such as private-sector AI and cybersecurity experts, the commercial sector, ethicists, philosophers, civil society and public opinion. Footnote 51 States should also collaborate on dual-use AI research to leverage AI's low-cost and scaling advantages (i.e. in autonomy and robotics). Further, a focus on the safety, testing and robustness of AI systems is a critical step in mitigating potential vulnerabilities and risks caused by errors, bias, and explainability in uncontrolled and complex environments. Footnote 52 In the case of offensive AI-related cybersecurity research, where public distribution might cause vulnerabilities and worsen security, publication could be confined to trusted organisations and entities.\nBest practices that exist in more mature methods for addressing dual-use concerns, like computer security, where applicable, might be applied to AI. For example, the wide use of red teaming exercises to enhance network security, organisation and practices. Specifically, AI-cyber red teaming – like DARPA's Cyber Grand Challenge – will enable engineers and operators to better\nunderstand the skills needed to execute particular offensive and defensive operations (especially using machine learning techniques), and to better manage system vulnerabilities, adversarial exploitation, stress-testing and social engineering challenges (Anderson, Woodbridge, and Filar Citation 2016).\nA recent study explored case studies of previous examples (biological weapons, cryptography and nuclear technology) of dual-use technologies – with civilian and military uses – to examine potential insights for AI dual-use risk management policies such as export controls and prepublication reviews. These insights also highlight the potential challenges of establishing regulatory, legal and normative frameworks for dual-use technologies – for example, the cautionary tale of ineffective efforts, in the late 1990s, to regulate cryptographic algorithms and cyber-network security tools through export controls.\nThe extent to which the integration of AI into dual-use weapon systems (such as cybersecurity) might influence actors' attitude to risk, the offence-defence balance and, in turn, perceptions of others' intentions and capabilities, could have profound implications for strategic deterrence and nuclear security. Footnote 53 In sum, cognizant that some states have deployed (or imminently plan to deploy) AI systems, experts generally agree that AI requires further experimentation, testing and development before being integrated into lethal weapon systems and their decision-making support systems.\n# Taking the lead from the cybersecurity community\nSome examples of cybersecurity-centered measures that might be further researched and implemented to pre-empt and mitigate some of risks posed by AI-augmented cyberattacks (especially involving safety-critical nuclear systems) highlighted in this paper include the following.\nFirst, coordinating AI-simulated war games, red teaming creative thinking exercises and creating redundancies (i.e. back-ups or fail-safes) in networks to detect errors, fix vulnerabilities and increase the reliability and robustness of military (especially nuclear-centric) systems. Specific procedures could be implemented to enable confidential reporting and fixing the vulnerabilities, subversions and other kinds of manipulations detected in AI systems. These findings could be used to track the proliferation of AI-related incursions and then countermeasures could be developed and standardised to manage these threats. Footnote 54\nSecond, states should formalise verification methods and protocols and consider issues such as: to what extent, under what circumstances, and for what types of AI systems can formal verification be implemented? Might other approaches be developed to achieve similar goals (e.g. machine learning and big data analysis augmented verification methods)? For example, DARPA's Assured Autonomy Program combines data-driven machine learning algorithms to assure the\nsafety of autonomous cyber physical systems (air, ground, sea and undersea unmanned vehicles), which continue to learn throughout their lifespans.\nThis learning characteristic makes assurance or verification using traditional methods especially challenging. Whether AI applications in the military domain can be formally verified, for now, is an unanswered question. Footnote 55 Much like that of cyber systems, the complexity of AI systems and the difficulty of defining their properties for formal verification makes them less amenable to verification compared to other types of technology.\nThird, the global defence communities should actively invest in the development of AI cyber-defence tools (e.g. analysing classification errors, automatic detection of remote vulnerability scanning and model extraction improvements), AI-centric secure hardware, and other fail-safe mechanisms, to allow for de-escalation and to prevent unintentional or accidental escalation.\nSeveral issues that need further exploration related to these measures include: the extent to which existing tools might be effective against vulnerabilities in AI systems? How can these tools be tailored for AI systems across multiple military domains? Is there an equivalent to 'patching' in military AI systems? What kinds of policies might incentivize, and ensure compliance with, meaningful reforms to existing hardware in the military sphere? While these questions are challenging, and necessarily speculative for now, answers will become more evident as the technology matures.\n# Disclosure statement\nNo potential conflict of interest was reported by the author."
    },
    "numeric": {
      "total": {
        "intext_total": 0,
        "success_occurrences": 0,
        "success_unique": 0,
        "bib_unique_total": 0,
        "occurrence_match_rate": 0.0,
        "bib_coverage_rate": 0.0,
        "success_percentage": 0.0,
        "style": "numeric"
      },
      "results": [],
      "flat_text": "# ABSTRACT\nHow could AI-infused cyber capabilities be used to subvert, or otherwise compromise, the reliability, control and use of states' nuclear forces? This article argues that a new generation of artificial intelligence (AI) enhanced cyber capabilities will amplify the risk of inadvertent escalation caused by the co-mingling of nuclear and strategic non-nuclear weapons and the increasing speed of warfare, thereby increasing the risk of nuclear confrontation. It examines the potential implications of cyber (offensive and defensive) capabilities augmented with AI applications for nuclear security. The article concludes that future iterations of AI-enhanced cyber counterforce capabilities will complicate the existing challenges of cyber defence, and in turn, compromise nuclear assets and increase the escalatory effects of offensive cyber capabilities.\n# KEYWORDS:\n- Artificial intelligence\n- cyber-security\n- US-China Relations\n- nuclear security\n- deterrence policy\n- emerging technology\n# Introduction\nGiven the hyperbole surrounding AI, it is easy to overstate the opportunities and challenges posed by the development and deployment of AI in the military sphere (Boulanin Citation 2019). Today, there remains a large amount of debate among AI researchers surrounding the significance of several significant technical and operational challenges in the deployment of AI-infused systems (e.g. drone swarming, command and control decision-making support systems and a broader range of autonomous weapon systems). Moreover, many of the risks posed by AI in the nuclear domain today are not necessarily new (Boulanin Citation 2019). Recent advances in AI (especially machine learning techniques) exacerbate existing risks to escalation and stability rather than generating entirely new ones. While AI could enable significant improvements in many military domains (including nuclear weapons), for the foreseeable future, developments in military AI will likely be far more prosaic than implied in popular culture. The main worry for nuclear stability, therefore, is that militaries will underestimate (or overstate) or ignore the potential shortcomings and risks associated with the use of the current generation of AI technology in the safety-critical military sphere – and especially the nuclear domain.\nBecause of the diverse approaches to AI-based research, there is no universally accepted definition of ‘artificial intelligence’, which is confusing when the term is applied generically to make grandiose claims about its revolutionary effects. Footnote  ${}^{1}$  AI can be understood as a universal term for improving the performance of automated (or autonomous) systems to solve a wide variety of complex tasks including: perception (sensors, computer vision, audio and image processing); reasoning and decision-making (problem solving, searching, planning and reasoning); learning and knowledge representation (machine learning, deep networks and modelling); Footnote  ${}^{2}$  communication (language processing); autonomy and robotics ; and human-AI collaboration (humans define the systems’ purpose, goals and context).\nIn a military context, as a potential enabler and force multiplier of advanced weapon systems, AI is more akin to electricity, radio, radar and intelligence, surveillance and reconnaissance (ISR) support systems than a 'weapon' per se. Much like these enabling support systems, AI is also strategically vital, vulnerable, inherently cross-domain and dependent on advanced technology. Thus, even if AI applications are unable to make better battlefield decisions than humans, Footnote  $^{3}$  militaries that use AI in human-machine teaming will doubtless gain significant advantages (e.g. remote-sensing, situational-awareness, battlefield-manoeuvres and a compressed decision-making loop), compared to those who depend on human judgment – and semi-autonomous technology – alone. This is particularly the case, in operating systems in operating environments that demand endurance and rapid decision-making across multiple combat zones (Ayoub and Payne Citation 2016).\nThis article argues that a new generation of AI-augmented offensive cyber capabilities will amplify the risk of inadvertent escalation posed by the co-mingling of nuclear and strategic (or counterforce) non-nuclear weapons and the increasing speed of warfare, and in turn, increase the risk of nuclear confrontation. The article's thesis is grounded in three core themes. First, AI does not exist a vacuum, that is, in isolation, AI will unlikely be a strategic game changer. Instead, it will likely mutually reinforce the destabilizing effects of existing advanced capabilities thereby increasing the speed of warfare and compressing the decision-making timeframe (Johnson Citation 2019a). AI-enabled and enhanced capabilities will have a more significant impact (positive or negative) on strategic stability than the sum of its parts. Furthermore, the intersection of AI with nuclear weapons, and a broader spectrum of strategic non-nuclear weapons, will likely accelerate the erosion of the survivability of nuclear arsenals associated with the 'computer revolution'. Put another way, military AI and the advanced capabilities it enables are a natural manifestation – rather than the cause or origin – of an established trend, which could lead states to adopt destabilizing launch postures due to the increasing speed of war and co-mingling.\nSecond, AI's impact on stability, deterrence and escalation will likely be determined as much (or more) by states' perception of its functionality as much as by what it is capable of doing. In the case of nuclear strategy and deterrence, the perception of an adversary's capabilities and\nintentions is as (or more) important than its actual capability. Thus, in addition to the importance of military force postures, capabilities and doctrine, the effects of AI will also have a strong cognitive element, increasing the risk of inadvertent escalation as a result of misperception and misunderstanding.\nFinally, and related to the above, the increasingly competitive and contested nuclear multipolar world order will compound the destabilizing effects of AI, and in turn, increase the escalation risks in future warfare between great military powers – especially China and the United States (Johnson Citation 2019b). Moreover, the potential operational and strategic advantages offered by AI-augmented capabilities could prove irresistible to nuclear-armed strategic rivals, causing them to eschew the limitations of AI, and compromise safety and verification standards to protect, or attempt to capture, technological superiority on the future digitised battlefield.\n# Military AI primer\nArtificial intelligence research began as early as the 1950s, as a broad concept concerned with the science and engineering of making intelligent machines. Since the 1950s, AI research has gone through several development phases – from early exploitations in the 1950s and 1960s and the 'AI Summer' during the 1970s, through to the early 1980s and the 'AI Winter' from the 1980s – each of which failed to live up to its initial, and often over-hyped, expectations. In the past decade, the explosion of interest in the field (or 'AI renaissance') occurred due to the convergence of four enabling developments: (1) the exponential growth in computing processing power; (2) expanded data sets (especially 'big data' sources) (Gray et al. Citation 2015); Footnote  $^{4}$  (3) advances in the implementation of machine learning techniques and algorithms (especially deep 'neural networks') (Schmidhuber Citation 2015); Footnote  $^{5}$  and (4) the rapid expansion of commercial interest and investment in AI (Hoadley and Nathan Citation 2018). Footnote  $^{6}$\nAI is concerned with machines that emulate capabilities that are usually associated with human intelligence, such as language, reasoning, learning, heuristics and observation. Today, all practical (i.e. technically feasible) AI applications fall into the 'narrow' category, and less so, artificial general intelligence (AGI) or 'superintelligence' (Reedy Citation 2017). Footnote  $^{7}$  'Narrow AI' has been widely used in a broad range of civilian and military tasks since the 1960s, (Nilsson Citation 2010; Russel and Norvig Citation 2014) and involves statistical algorithms (mostly based on machine learning techniques) that learn procedures through analysis of large training data sets designed to approximate and replicate human cognitive tasks (Russel and Norvig Citation 2014). Footnote  $^{8}$  'Narrow AI' is the category of artificial intelligence to which this article refers when it assesses the impact of this technology in a military context.\nGiven the diverse approaches to research in AI (Boden Citation 2016 ; Vernon Citation 2014), Footnote  $^{9}$  there is no universally accepted definition of AI. A recent US congressional report defines AI as follows: 'Any artificial system that performs tasks under varying and unpredictable circumstances, without significant human oversight, or that can learn from their experience and improve their performance ... they may solve tasks requiring human-like perception, cognition, planning, learning, communication, or physical action' (Hoadley and Nathan Citation 2018).\nToday, a large gap exists between the reality of what AI technology is capable of doing in a military context, and the expectations and fears of public opinion, policymakers and global defence communities. How will AI technologies influence public opinion about the use of military force and warfare? (Sechser, Narang, and Talmadge Citation 2019). The misrepresentations and misconceptions that exist today in the narratives surrounding AI (especially in societal, economic and national security debates) and are in large part caused by the hyperbole and exaggerated depictions of AI in science fiction (Zarkadakis Citation 2015). Footnote  $^{10}$  Misrepresentations of the potential opportunities and risks associated with this technology in the military sphere can obscure constructive and crucial debate on these topics. Specifically, this includes the challenge of balancing the potential operational, tactical and strategic benefits of leveraging AI while managing the risks posed to nuclear security in the (especially premature) pursuit of these advantages.\nThe historical record on technological change in a military context counsels scepticism against alarmist predictions and extrapolations from emerging trends: technologies rarely evolve in the way futurists predict and many applications have had countervailing or conditional effects that have ameliorated the dystopian predictions of naysayers (Metz Citation 2000 ; Metz and Kievit Citation 1995). In short, AI's impact on nuclear stability will, like previous generations of emerging technology (i.e. chemical and biological weapons, cyber and nuclear weapons), have both stabilising and destabilizing (and possibly contradictory) effects on strategic stability. For example, many expected that chemical weapons would instantly and dramatically change the nature of warfare and deterrence after the British used poison gas during World War I (Sechser, Narang, and Talmadge Citation 2019). However, chemical weapons proved far less practical, impactful, disruptive and relatively easier to defend against than conventional explosives. As a corollary, the degree to which military AI poses risks to future strategic stability will depend in large part on the pace and scope with which this technology facilitates new ways to improve the delivery of, and defence against, nuclear weapons and strategic non-nuclear weapons.\nConceptually, AI-augmented applications can be categorized into those that have predominately operational, tactical and strategic implications in future warfare. At the operational and tactical level, applications include: autonomy and robotics; multi-actor interaction red teaming wargaming; Footnote  $^{11}$  big data-driven modelling; Footnote  $^{12}$  intelligence collection and analysis (e.g. to locate and monitor mobile missiles, troops movement) (Conklin Citation 2018); Footnote\n$^{13}$ cybersecurity talent management; predictive maintenance, logistics, planning and forecasting; and vendor contract and budget management. In preparation for a new AI strategy, US Strategic Operations Command (SOCOM) recently crafted a roadmap that invests heavily in AI and machine learning technology based on several pilot projects with three overarching goals to create an algorithmic warfare multi-operational team: 'AI-ready workforce, AI-applications, and AI-outreach' (Lee Citation 2019).\nTherefore, AI's impact on strategic stability and escalation will also be shaped by broader and more nuanced factors which influence the trajectory of emerging technologies, including: military strategy and doctrine – that seeks to manipulate escalation risks – military culture and organisation, alliance structures, domestic politics and public opinion, to name but a few. The Cold War record demonstrates that emerging technologies act primarily to enable independent variables which, in combination with other endogenous factors, can heighten escalation risks between adversaries. Therefore, in isolation, technology is not a major exogenous cause of (inadvertent or intentional) military escalation.\nAt a strategic level of warfare, AI uses include: qualitative improvements to the nuclear command, control, communications, and intelligence (C3I) architecture; enhancing target acquisition, tracking, guidance systems and discrimination of missile and air defence systems; being force multipliers of both offensive and defensive machine-learning infused cyber capabilities; and qualitatively bolstering nuclear and non-nuclear missile delivery systems – including hypersonic variants. Footnote 14\nIn combination, AI machine learning algorithms fused with advances in sensor technology could be engineered to allow nuclear delivery systems to operate more autonomously and precisely, with less reliance on human operators calibrating navigation and guidance parameters and equipped with more robust countermeasures against jamming or spoofing attacks. At the strategic level, AI-augmented command and control systems could potentially mitigate many of the shortcomings inherent to human strategic decision-making during the 'fog of war', such as the susceptibility to invest in sunk costs, skewed risk judgment, cognitive heuristics and groupthink.\nIn sum, the fusion of AI machine learning and human judgment to gauge an adversary's intentions (and predict escalation) for the purposes of planning and directing future wars for the pursuit of political objectives, is, therefore, a far less unlikely prospect in the near future than the use of AI to achieve tactical and operational ends (e.g. drone swarming and cyber defence).\n# Blurring the AI-cyber offence-defence line: (Glaser and Kaufmann Citation 1998 ; Lynn-Jones Citation 1995 ) Footnote 15\nSeveral US national security officials believe that AI, used as force multipliers for both defensive and offensive cyber weapons, will have a transformative impact on cybersecurity (Gartzke and Lindsay Citation 2015 ; Rid Citation 2012 ; Slayton Citation 2017 ; Hoffman Citation 2019). Footnote 16 Director of US National Intelligence, Daniel Coats, recently warned that AI could increase US vulnerability to cyberattacks, weaken its ability to attribute such attacks, improve the effectiveness and capabilities of adversaries weapon and intelligence systems, and cause accidents and related liability issues. To be sure, the line between AI cyber-offence and cyber-defence will likely remain an obscure one. Footnote 17 Bernard Brodie’s words of caution about the advent of nuclear weapons almost six decades ago resonate: ‘The [military] bias towards the offensive creates special problems in any technologically new situation where there is little or no relevant war experience to help one to reach a balanced judgment’ (Brodie Citation 1959).\nOn the one hand, AI might reduce a military’s vulnerability to cyberattacks. AI cyber-defence tools (or ‘counter-AI’), designed to recognise changes to patterns of behaviour in a network and detect anomalies, automatically detect software code vulnerabilities, and apply machine learning techniques (such as ‘deep learning’) to detect deviations and anomalies from normal network activity, Footnote 18 could form a more robust defence against cyber subversions (Johnson Citation 2019a). Footnote 19 According to the Pentagon’s new AI strategic report, ‘AI can enhance our ability to predict, identify, and respond to cyber and physical threats from a range of sources’ (US Department of Defense Citation 2019). In addition, the DoD’s Defense Innovation Unit (DIU) is prototyping an application (related to Project VOLTRON) that leverages AI to decipher high-level strategic questions, map probabilistic chains of events and develop alternative strategies in order to make DoD systems more resilient to AI-augmented cyberattacks and configure and fix errors more quickly than humans.\nOn the other hand, autonomy itself might increase a military’s vulnerability to cyberattacks, which rely on stealth, deception and stratagem. For example, an adversary could use malware to take control, manipulate or fool the behaviour and pattern recognition systems of autonomous systems, such as DoD’s Project Maven. Offensive attacks such as this would be relatively easy to execute, but very difficult to detect, attribute or effectively counter (Shachtman Citation 2011). Footnote 20 This problem set is compounded by the lack of an agreed framework or understanding of what constitutes escalatory behaviour (or ‘firebreaks’) in cyberspace. Thus, a cyber operation intended as a signal (i.e. for coercive diplomacy) could go undetected by the target, or worse, misinterpreted as an offensive attack. Even if information relating to an\noperation of this kind is accurately identified in a timely manner, the motives behind them could remain ambiguous, or misperceived. According to Robert Jervis, ‘it is likely that the country that is the object of the attack would assume that any effect was the intended one’ (Jervis Citation 2016).\nUS Cyber Fleet Command Commander, Michael Gilday, recently told the Senate Armed Services Committee that the US Navy must ‘improve an ability to proactively detect new and unknown malware … so we [the US] can act quickly using advanced analytics enabled by AI and machine learning’, which may give the US a ‘tactical advantage’ to identify malicious activity early on (Osborn Citation 2018). Even if analysts can obtain high-quality and reliable intelligence, however, they may not want to reveal it, because doing so could compromise a source, capability or tactic (Johnson Citation 2019a). Moreover, most observers now acknowledge that no strategy (i.e. combining offence and defence cyber operations) could realistically be expected to deter all malign cyber-attacks (Hoffman Citation 2019). Footnote 21 While AI-enhanced cyber capabilities can enhance deterrence, they can simultaneously incentivize others to attack thereby exacerbating the paradox of enhanced capabilities and increased vulnerabilities in the cyber domain (Gartzke and Lindsay Citation 2017; Libicki Citation 2016; Slayton Citation 2017). Footnote 22 As the historical record attests, this capability-vulnerability paradox is heightened when states are both dependent on a particular capability (such as AI and cyber tools) and their access or ability to use the capabilities is vulnerable to an adversary’s exploitation or subversion – creating first-mover incentives (Rid Citation 2013). Footnote 23\n# Cybersecurity and nuclear weapon systems: nuclear risk redux?\nAt a strategic level of conflict, AI applications designed to enhance cybersecurity for nuclear forces could simultaneously make cyber-dependent nuclear weapon systems (i.e. communications, data processing or early-warning sensors) more vulnerable to cyberattacks. It is now thought possible that a cyberattack (i.e. spoofing, hacking, manipulation and digital jamming) could infiltrate a nuclear weapons system, threaten the integrity of its communications, and ultimately (and possibly unbeknown to its target) gain control of its (nuclear and non-nuclear) command and control systems. Footnote 24 AI technology has not yet evolved to a point where it would allow nuclear-armed states to credibly threaten the survivability of each other’s nuclear second-strike capability. Thus, the development trajectory of AI (and its enabling technologies) means its impact on nuclear security will likely be theoretical and speculative for the foreseeable future.\nBecause of the intense time pressures that would likely loom large with the decision to use nuclear weapons – especially where a state maintains a launch-on-warning posture – AI-enhanced\ncyberattacks against nuclear systems would be almost impossible to detect and the warning signals difficult to authenticate, let alone attribute, within the short timeframe for initiating a nuclear strike. According to open sources, operators at the North American Aerospace Defence Command (NORAD) have less than three minutes to assess and confirm initial indications from early-warning systems of an incoming attack. This compressed decision-making time frame could put political leaders under intense pressure to make a decision to escalate during a crisis, with incomplete (and possibly false) information of a situation. Ironically, new technologies designed to enhance information (i.e. modernised nuclear C3I systems augmented by 5G networks, machine learning, big-data analytics and quantum computing) can also undermine clear and reliable information flow and communication, critical for effective deterrence (Gartzke and Lindsay Citation 2019). Footnote 25\nAdvances in AI could also exacerbate this cybersecurity challenge by enabling improvements to the cyber offence. Machine learning and AI by automating advanced persistent threat (APT) operations might dramatically reduce the extensive manpower resources and high levels of technical skill required to execute APT operations (or 'hunting for weaknesses'), especially against hardened nuclear targets. Footnote 26 The machine speed of AI-augmented cyber tools could enable a low-skilled and capital-restricted attacker to exploit a narrow window of opportunity to penetrate an adversary's cyber-defences or use APT tools to find new vulnerabilities. For example, when docked for maintenance air-gapped nuclear-powered submarines, considered secure when submerged, could become increasingly vulnerable to a new generation of low-cost – possibly black-market – and highly automated APT cyberattacks.\nAn attacker could also apply AI machine learning techniques to target autonomous dual-use early-warning and other operating systems (e.g. C3I, ISR, early-warning and robotic control networks) with 'weaponized software' such as hacking, subverting, spoofing or tricking, causing unpredictable and potentially undetectable errors, malfunctions and behavioural manipulation to weapons systems – or 'data-poisoning'. Footnote 27 Furthermore, as the linkages between digital and physical systems (or the 'Internet of Things') expand, the potential for to an adversary to use cyberattacks in both kinetic and non-kinetic attacks will increase. A significant risk variable in the operation of autonomous systems is the time that passes between a system failure (i.e. performing in a manner other than how the human operator intended) and the time it takes for a human operator to take corrective action. If the system failure is the result of a deliberate act, this time frame will be compressed (Johnson Citation 2019a).\nEven if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. This skewed assessment by states in the context of nuclear weapons, which are ready to launch nuclear weapons at a moment's notice, would likely precipitate worst-case scenario thinking that may spark an inadvertent escalation (Talmadge\nCitation 2017). Footnote 28 During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 29 For example, an AI-enabled third party generated 'deepfake' (audio and video manipulation), coupled with data-poisoning cyberattacks, could spark an escalatory crisis between two (or more) nuclear states (Fitzpatrick Citation 2019). Footnote 30\nHow could AI-augmented cyber capabilities create new pathways for accidental or inadvertent escalation? To illustrate these dynamics: State A launches a malicious AI-enhanced cyberattack to spoof State B's AI-enabled autonomous sensor platforms and automated target recognition systems, in such a way that the weapon system (e.g. a human-supervised ATR system) is fooled into interpreting a civilian object as a military target. B in response, based on manipulated or erroneous information, and the inability of human supervisors to detect the spoofed imagery that fooled the weapons' automated target recognition algorithm in time to take corrective action, accidentally (and unintentionally) escalates a situation (Libicki Citation 2018). In this example, the spoofing attack on the weapon systems' algorithm is executed in such a way that the imagery appears to the recognition system as indistinguishable from a valid military target (Rohrbach et al. Citation 2018) Footnote 31 escalating a situation based on a false premise that would unlikely to fool the human eye. Footnote 32\nAlso, the explainability (or 'black box') problem associated with AI applications could compound these potential escalation dynamics. Footnote 33 Insufficient understanding of how and why AI algorithms reach a particular judgment or decision might complicate the task of determining whether data sets had been deliberately compromised to manufacture false outcomes (e.g. attacking incorrect targets or misdirecting allies during combat). Footnote 34 Moreover, as humans and AI team up to accomplish particular missions, the opacity associated with how AI systems reach a decision may cause an operator to have either too much or too little confidence in a system's performance. As a result, unless the system's machine learning algorithm is terminated, at the end of the training phase, once deployed, it could potentially learn something it was not intended to, or even perform a task or mission that its human designers do not expect it to do. Footnote 35\nIn sum, technologies that improve the reliability and speed with which information is processed and disseminated by early-warning systems may paradoxically also increase the vulnerabilities of these networks, creating new first-mover advantages and escalation pathways during a crisis that spark unintentional (or intentional) warfare (Schneider Citation 2019). Footnote 36 Put another way, AI might be developed in order to enable a state to pursue a pre-determined escalatory path. Thus, increased escalation risk as a result of technology is not always inadvertent or accidental (Long and Rittenhouse Green Citation 2015). Footnote 37\n# 'Cyber guns' supercharged with AI machine learning\nWhile manipulations and system subversions are possible with existing cyber offence tools, rapid advances in AI and increasing degrees of military autonomy could amplify the potential speed, power and scale of future attacks in cyberspace. Footnote 38 There are three significant intersections between AI systems and cybersecurity that are most salient to the military domain: (1) advances in autonomy and machine learning techniques mean that a much broader range of physical systems are now vulnerable to cyberattacks (i.e. hacking and data-poisoning) (Greenberg Citation 2016); Footnote 39 (2) cyberattacks on AI systems can offer attackers access to machine learning algorithms, trained models being used by the application and a potential vast amounts of data from facial recognition and intelligence collection and analysis systems (e.g. satellite navigation and imagery systems used to cue precision munitions and supported intelligence, surveillance, reconnaissance missions); and (3) the use of AI systems in conjunction with existing cyber offence tools will enable sophisticated cyberattacks to be executed at larger scale (both geographically and across networks), at faster speeds, and simultaneously across multiple military domains with improved anonymity. In short, despite the relatively benign augmentation mechanisms used to develop cyber offensive capabilities with AI systems, the resultant impact on the speed and scope of AI cyber tools will likely have destabilizing effects, nonetheless. Footnote 40\nRecent advances in machine learning have significantly contributed to resolving several technical bottlenecks in several fields of AI, which could allow for significant qualitative improvements to a large variety of autonomous weapon systems and applications. In combination, machine learning and autonomy could intersect with nuclear security in a multitude of ways, with both positive and negative implications for strategic stability. For example, machine learning could enhance the detection capabilities of (nuclear and non-nuclear) early-warning systems and improve the collection and cross-analysis of ISR information, thus reducing the risk of inadvertent and accidental escalation. Machine learning may also bolster the cyber-security of command and control systems, create new and possibilities for arms control, testing, verification and monitoring nuclear stockpiles, and enhance conventional counterforce capabilities. Unmanned autonomous systems could be deployed in complex missions in hitherto inaccessible and cluttered environments (e.g. under-sea anti-submarine warfare) and aerial and underwater drones in swarms might eventually replace intercontinental ballistic missiles (ICBMs) and nuclear-armed submarines (SSBNs) for the delivery of nuclear weapons.\nIn response to these anticipated vulnerabilities, and to achieve the (perceived) first-mover advantages the increased speed in AI-augmented cyber tools could confer, China, Russia and the United States have continued to harden their AI cyber defences. Despite these parallel efforts, divergent views exist on how strategic communities perceive the nature of the risks posed by AI-enhanced cyberattacks against dual-use C3I systems. Thus, AI-augmented cyber intelligence-\ngathering tools targeting an adversary's command and control assets (especially dual-use facilities) during a crisis may be misinterpreted as a prelude to an imminent pre-emptive attack on its nuclear force and therefore as undermining strategic stability.\nMoreover, AI machine learning could enable new offensive concepts such as a so-called 'left-of-launch operation' (Ellison Citation 2015), Footnote  $^{41}$  thereby compounding the ambiguities and fear about pre-emptive attacks in cyberspace, and increasing first-mover incentives – or use-them-or-lose-them situations (Buchanan Citation 2017; Buchanan and Miller Citation 2017). Footnote  $^{42}$  In the cyber domain, it is generally unclear in the early stages of a cyber operation whether an adversary intends to collect intelligence or prepare for an offensive attack, which is more likely to be used early on in a conflict scenario. Open sources suggest, for example, that Chinese analysts view the vulnerability of China's nuclear command, control, and communications (C3I) systems to cyber infiltrations – even if an attacker's objective was limited to cyberespionage – as a highly escalatory national security threat. By contrast, Russian analysts tend to view Russia's nuclear C3I network as relatively isolated, and thus insulated from cyberattacks.\nIrrespective of these differences, the uncertainty caused by the possibility, actual or otherwise, that AI-enhanced offensive cyber tools might be used (or threatened) to undermine the credibility and reliability of states' nuclear forces would be destabilizing, both as result of the existence of enhanced cyber offensive capabilities and the potential efficacy of these tools against nuclear forces. As the historical record attests, rational actors have incentives to misrepresent what they know (i.e. lie and bluff) in order to improve the terms of any settlement that may emerge from the wartime bargaining process – or the risk-reward payoff. Even a modicum of uncertainty about the effectiveness of AI-augmented cyber capabilities during a crisis or conflict would, therefore, reduce both sides' risk tolerance and increase the incentive to strike pre-emptively as a hedging strategy.\nA virtuous cycle that might flow from enhanced reassurances premised on comprehensive intelligence would require information symmetry (i.e. equal access to intelligence and analysis systems) between great and rising powers, and shared confidence in the accuracy and credibility of these systems. Perhaps most challenging of all in a world of 'revisionist' rising powers, the intentions of all rival states would need to be genuinely benign for this optimistic view to occur. Under crisis conditions, for example, an offensive AI cyber tool that succeeds in compromising an adversary's nuclear weapon systems, resulting in an 'asymmetric information' situation, could cause either or both sides to overstate (or understate) its retaliatory capabilities, and in turn, be more inclined to act in a risky and escalatory fashion. In short, in a competitive strategic environment, where states are inclined to assume the worst of others' intentions, one state's efforts to enhance the survivability of its strategic forces may be viewed by others as a threat to their nuclear retaliatory capability – or second-strike capacity.\n# Manipulation of the information landscape\nWhile machine learning big-data analytics, sensing technology, networks supported by 5G networks, could alert commanders of incoming threats with increased speed and precision, and make accidents caused by human error potentially less likely (especially accidents and malfunctions caused by false warning), these advances could simultaneously amplify escalation risks in two ways. Footnote 43\nFirst, AI machine learning used as force multipliers for cyber offence (e.g. data poisoning spoofing, 'deepfakes', manipulation, hacking and digital jamming) would be considerably more difficult to detect – especially if an attacker used AI advanced persistent threat (APT) tools – or 'hunting for weaknesses' in spectrum-contested environments. Second, in the unlikely event an attack was successfully detected, threat identification (or attribution) at machine speed would be virtually impossible. In addition to the co-mingling and the speed of warfare issues, AI machine learning systems might also exacerbate the existing – well-established and non-kinetic – risk of inadvertent (and intentional) escalation from the manipulation of the information landscape in which decisions about nuclear weapons are located. For example, the use of deepfake videos built-in real-time and disseminated online for malicious purposes.\nHuman cognition, and thus effective deterrence and signalling intentions, is predicated on reliable and clear information; if an adversary is concerned that the information available to them is limited (or worse, inaccurate), they will likely assume the worst and act accordingly. Asymmetric information situations between rivals about the balance of military power could undermine crisis stability, and in turn, create rational incentives to escalate to nuclear confrontation. Consequently, states will be more inclined to assume the worst of others' intentions, especially in situations where the legitimacy of the status quo is contested (i.e. maritime Asia).\nIn the competitive strategic environment, it is easy to imagine unprovoked escalation caused by a malicious third-party (or state-proxy) clandestine action. Even if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 44 For example, in an effort to incite conflict between two rival states, State A uses proxy hackers to use a AI machine learning technique (i.e. generative adversarial networks GAN) to launch 'deepfake' video or audio material, depicting senior military commanders of State B conspiring to launch a pre-emptive strike on State C. Footnote 45 Then, this 'deepfake' footage is deliberately leaked into C's AI-augmented intelligence collection and analysis systems, provoking C to escalate the situation with strategic consequences. B responds in kind. Footnote 46\nThe current underdeveloped state of ‘counter-AI’ capabilities and other fail-safe mechanisms (e.g. circuit breakers) to de-escalate escalation in cyberspace, will make the unprovoked and unintentional escalation dynamics depicted in this scenario very challenging to mitigate. Moreover, in the emerging ‘deepfakes’ arms race (much like cybersecurity more generally), detection software will likely lag behind advances in offensive enabling solutions – or offence-dominant ones. According to computer science expert Hany Farid, there are probably 100–1000 times ‘more people developing the technology to manipulate content than there is to detect [it]’ (Fontaine and Frederick Citation 2019).\nThe overreliance on automation (or ‘automation bias’) in the deployment of increasing complex AI-augmented capabilities – such as cyber, C3I systems, AWS and precision missile munitions – designed to provide a distinct tactical advantage of machine-speed, will increase the vulnerability of these capabilities to exploitation. Moreover, the increasing substitution of human cognition for logic derived from machines will likely increase the opportunities for adversaries to exploit the limitations of narrow AI technology, i.e. lack of human intuition, brittleness in complex real-world situations and an inability to effectively detect or counter manipulation attacks (Libicki Citation 2018). Footnote 47 Therefore, until researchers unravel some of the unexplainable features of AI, human error and machine error will likely compound one another, with unpredictable results. Footnote 48 Simply put, we are at a critical crossroads in the parallel (and symbiotic) evolution of AI and cyberspace that national security communities, globally, will need to prepare for proactively (Johnson Citation 2019a).\n## Policy interventions\nA prominent theme that runs through this article – and central to understanding the potential impact AI for strategic stability and nuclear security more broadly – is the concern that AI systems operating at machine-speed will push the pace of combat to a point where the actions of machines surpass the (cognitive and physical) ability of human decision makers to control (or even comprehend) events.\nPossible multi-track policy responses to push back against the threat posed to stability from AI in a multipolar can be broadly categorized into: (1) those that focus on enhancing debate and discussion between researchers, global defence communities, decision makers, academics and other political and societal stakeholders; and (2) a range of specific policy recommendations and tools for great military powers to negotiate and implement.\n## Enhancing debate and dialogue\nFirst, to mitigate (or at least manage) the destabilizing and escalatory risks posed by the AI-cybersecurity nexus, great military powers must closely coordinate their confidence-building\nmeasures in an effort to pre-empt some of the risks to stability outlined above. Track-1 and track-2 discussions should include diplomats and military leaders (especially from China, Russian and the United States), industry experts, AI researchers and multi-disciplinary academics. Specifically, great military powers should establish an international framework for governance, norms, behaviour and the regulation of policy on machine learning-augmented cyber capabilities. Resistance to these efforts will likely come from states who worry that in revealing their cyber (especially offensive) capabilities, they could upend the deterrence utility of these tools.\nTo be sure, the challenges posed in the coordination and implementation of these policies will require bold and visionary leadership to circumvent the inevitable regional agendas, interdisciplinary resistance and burgeoning security dilemmas between rival states. Because of the rapid technological change in AI formal treaties, associated with arms-control agreements that require lengthy and complex negotiation and ratification processes, legal frameworks risk becoming obsolete before they come into effect. The historical record has demonstrated on several occasions that these kinds of challenges facing humanity can be overcome (Krimsky Citation 1962; Naur and Randell Citation 1968). Footnote 49\nSecond, the think tank community, academics and AI research experts should pool their resources to investigate the implications of the AI-cyber nexus for a range of potential security issues such as: the impact of AI bias on future military-use applications; how to prepare for (and react to) artificial general intelligence; and measures to mitigate, prevent and manage offensive uses of AI. Footnote 50 If necessary, steps could be taken to amend existing legal definitions of offensive cyber operations, such as hacking, data-poisoning attacks and spoofing, to account for the increased proliferation and potential damage caused by AI.\nThird, and related, due to the intrinsic dual-use nature of the AI-cyber security problem set, this dialogue should also be expanded to include other stakeholders such as private-sector AI and cybersecurity experts, the commercial sector, ethicists, philosophers, civil society and public opinion. Footnote 51 States should also collaborate on dual-use AI research to leverage AI's low-cost and scaling advantages (i.e. in autonomy and robotics). Further, a focus on the safety, testing and robustness of AI systems is a critical step in mitigating potential vulnerabilities and risks caused by errors, bias, and explainability in uncontrolled and complex environments. Footnote 52 In the case of offensive AI-related cybersecurity research, where public distribution might cause vulnerabilities and worsen security, publication could be confined to trusted organisations and entities.\nBest practices that exist in more mature methods for addressing dual-use concerns, like computer security, where applicable, might be applied to AI. For example, the wide use of red teaming exercises to enhance network security, organisation and practices. Specifically, AI-cyber red teaming – like DARPA's Cyber Grand Challenge – will enable engineers and operators to better\nunderstand the skills needed to execute particular offensive and defensive operations (especially using machine learning techniques), and to better manage system vulnerabilities, adversarial exploitation, stress-testing and social engineering challenges (Anderson, Woodbridge, and Filar Citation 2016).\nA recent study explored case studies of previous examples (biological weapons, cryptography and nuclear technology) of dual-use technologies – with civilian and military uses – to examine potential insights for AI dual-use risk management policies such as export controls and prepublication reviews. These insights also highlight the potential challenges of establishing regulatory, legal and normative frameworks for dual-use technologies – for example, the cautionary tale of ineffective efforts, in the late 1990s, to regulate cryptographic algorithms and cyber-network security tools through export controls.\nThe extent to which the integration of AI into dual-use weapon systems (such as cybersecurity) might influence actors' attitude to risk, the offence-defence balance and, in turn, perceptions of others' intentions and capabilities, could have profound implications for strategic deterrence and nuclear security. Footnote 53 In sum, cognizant that some states have deployed (or imminently plan to deploy) AI systems, experts generally agree that AI requires further experimentation, testing and development before being integrated into lethal weapon systems and their decision-making support systems.\n# Taking the lead from the cybersecurity community\nSome examples of cybersecurity-centered measures that might be further researched and implemented to pre-empt and mitigate some of risks posed by AI-augmented cyberattacks (especially involving safety-critical nuclear systems) highlighted in this paper include the following.\nFirst, coordinating AI-simulated war games, red teaming creative thinking exercises and creating redundancies (i.e. back-ups or fail-safes) in networks to detect errors, fix vulnerabilities and increase the reliability and robustness of military (especially nuclear-centric) systems. Specific procedures could be implemented to enable confidential reporting and fixing the vulnerabilities, subversions and other kinds of manipulations detected in AI systems. These findings could be used to track the proliferation of AI-related incursions and then countermeasures could be developed and standardised to manage these threats. Footnote 54\nSecond, states should formalise verification methods and protocols and consider issues such as: to what extent, under what circumstances, and for what types of AI systems can formal verification be implemented? Might other approaches be developed to achieve similar goals (e.g. machine learning and big data analysis augmented verification methods)? For example, DARPA's Assured Autonomy Program combines data-driven machine learning algorithms to assure the\nsafety of autonomous cyber physical systems (air, ground, sea and undersea unmanned vehicles), which continue to learn throughout their lifespans.\nThis learning characteristic makes assurance or verification using traditional methods especially challenging. Whether AI applications in the military domain can be formally verified, for now, is an unanswered question. Footnote 55 Much like that of cyber systems, the complexity of AI systems and the difficulty of defining their properties for formal verification makes them less amenable to verification compared to other types of technology.\nThird, the global defence communities should actively invest in the development of AI cyber-defence tools (e.g. analysing classification errors, automatic detection of remote vulnerability scanning and model extraction improvements), AI-centric secure hardware, and other fail-safe mechanisms, to allow for de-escalation and to prevent unintentional or accidental escalation.\nSeveral issues that need further exploration related to these measures include: the extent to which existing tools might be effective against vulnerabilities in AI systems? How can these tools be tailored for AI systems across multiple military domains? Is there an equivalent to 'patching' in military AI systems? What kinds of policies might incentivize, and ensure compliance with, meaningful reforms to existing hardware in the military sphere? While these questions are challenging, and necessarily speculative for now, answers will become more evident as the technology matures.\n# Disclosure statement\nNo potential conflict of interest was reported by the author."
    },
    "author_year": {
      "total": {
        "intext_total": 0,
        "success_occurrences": 0,
        "success_unique": 0,
        "bib_unique_total": 0,
        "occurrence_match_rate": 0.0,
        "bib_coverage_rate": 0.0,
        "success_percentage": 0.0,
        "style": "author_year"
      },
      "results": [],
      "flat_text": "# ABSTRACT\nHow could AI-infused cyber capabilities be used to subvert, or otherwise compromise, the reliability, control and use of states' nuclear forces? This article argues that a new generation of artificial intelligence (AI) enhanced cyber capabilities will amplify the risk of inadvertent escalation caused by the co-mingling of nuclear and strategic non-nuclear weapons and the increasing speed of warfare, thereby increasing the risk of nuclear confrontation. It examines the potential implications of cyber (offensive and defensive) capabilities augmented with AI applications for nuclear security. The article concludes that future iterations of AI-enhanced cyber counterforce capabilities will complicate the existing challenges of cyber defence, and in turn, compromise nuclear assets and increase the escalatory effects of offensive cyber capabilities.\n# KEYWORDS:\n- Artificial intelligence\n- cyber-security\n- US-China Relations\n- nuclear security\n- deterrence policy\n- emerging technology\n# Introduction\nGiven the hyperbole surrounding AI, it is easy to overstate the opportunities and challenges posed by the development and deployment of AI in the military sphere (Boulanin Citation 2019). Today, there remains a large amount of debate among AI researchers surrounding the significance of several significant technical and operational challenges in the deployment of AI-infused systems (e.g. drone swarming, command and control decision-making support systems and a broader range of autonomous weapon systems). Moreover, many of the risks posed by AI in the nuclear domain today are not necessarily new (Boulanin Citation 2019). Recent advances in AI (especially machine learning techniques) exacerbate existing risks to escalation and stability rather than generating entirely new ones. While AI could enable significant improvements in many military domains (including nuclear weapons), for the foreseeable future, developments in military AI will likely be far more prosaic than implied in popular culture. The main worry for nuclear stability, therefore, is that militaries will underestimate (or overstate) or ignore the potential shortcomings and risks associated with the use of the current generation of AI technology in the safety-critical military sphere – and especially the nuclear domain.\nBecause of the diverse approaches to AI-based research, there is no universally accepted definition of ‘artificial intelligence’, which is confusing when the term is applied generically to make grandiose claims about its revolutionary effects. Footnote  ${}^{1}$  AI can be understood as a universal term for improving the performance of automated (or autonomous) systems to solve a wide variety of complex tasks including: perception (sensors, computer vision, audio and image processing); reasoning and decision-making (problem solving, searching, planning and reasoning); learning and knowledge representation (machine learning, deep networks and modelling); Footnote  ${}^{2}$  communication (language processing); autonomy and robotics ; and human-AI collaboration (humans define the systems’ purpose, goals and context).\nIn a military context, as a potential enabler and force multiplier of advanced weapon systems, AI is more akin to electricity, radio, radar and intelligence, surveillance and reconnaissance (ISR) support systems than a 'weapon' per se. Much like these enabling support systems, AI is also strategically vital, vulnerable, inherently cross-domain and dependent on advanced technology. Thus, even if AI applications are unable to make better battlefield decisions than humans, Footnote  $^{3}$  militaries that use AI in human-machine teaming will doubtless gain significant advantages (e.g. remote-sensing, situational-awareness, battlefield-manoeuvres and a compressed decision-making loop), compared to those who depend on human judgment – and semi-autonomous technology – alone. This is particularly the case, in operating systems in operating environments that demand endurance and rapid decision-making across multiple combat zones (Ayoub and Payne Citation 2016).\nThis article argues that a new generation of AI-augmented offensive cyber capabilities will amplify the risk of inadvertent escalation posed by the co-mingling of nuclear and strategic (or counterforce) non-nuclear weapons and the increasing speed of warfare, and in turn, increase the risk of nuclear confrontation. The article's thesis is grounded in three core themes. First, AI does not exist a vacuum, that is, in isolation, AI will unlikely be a strategic game changer. Instead, it will likely mutually reinforce the destabilizing effects of existing advanced capabilities thereby increasing the speed of warfare and compressing the decision-making timeframe (Johnson Citation 2019a). AI-enabled and enhanced capabilities will have a more significant impact (positive or negative) on strategic stability than the sum of its parts. Furthermore, the intersection of AI with nuclear weapons, and a broader spectrum of strategic non-nuclear weapons, will likely accelerate the erosion of the survivability of nuclear arsenals associated with the 'computer revolution'. Put another way, military AI and the advanced capabilities it enables are a natural manifestation – rather than the cause or origin – of an established trend, which could lead states to adopt destabilizing launch postures due to the increasing speed of war and co-mingling.\nSecond, AI's impact on stability, deterrence and escalation will likely be determined as much (or more) by states' perception of its functionality as much as by what it is capable of doing. In the case of nuclear strategy and deterrence, the perception of an adversary's capabilities and\nintentions is as (or more) important than its actual capability. Thus, in addition to the importance of military force postures, capabilities and doctrine, the effects of AI will also have a strong cognitive element, increasing the risk of inadvertent escalation as a result of misperception and misunderstanding.\nFinally, and related to the above, the increasingly competitive and contested nuclear multipolar world order will compound the destabilizing effects of AI, and in turn, increase the escalation risks in future warfare between great military powers – especially China and the United States (Johnson Citation 2019b). Moreover, the potential operational and strategic advantages offered by AI-augmented capabilities could prove irresistible to nuclear-armed strategic rivals, causing them to eschew the limitations of AI, and compromise safety and verification standards to protect, or attempt to capture, technological superiority on the future digitised battlefield.\n# Military AI primer\nArtificial intelligence research began as early as the 1950s, as a broad concept concerned with the science and engineering of making intelligent machines. Since the 1950s, AI research has gone through several development phases – from early exploitations in the 1950s and 1960s and the 'AI Summer' during the 1970s, through to the early 1980s and the 'AI Winter' from the 1980s – each of which failed to live up to its initial, and often over-hyped, expectations. In the past decade, the explosion of interest in the field (or 'AI renaissance') occurred due to the convergence of four enabling developments: (1) the exponential growth in computing processing power; (2) expanded data sets (especially 'big data' sources) (Gray et al. Citation 2015); Footnote  $^{4}$  (3) advances in the implementation of machine learning techniques and algorithms (especially deep 'neural networks') (Schmidhuber Citation 2015); Footnote  $^{5}$  and (4) the rapid expansion of commercial interest and investment in AI (Hoadley and Nathan Citation 2018). Footnote  $^{6}$\nAI is concerned with machines that emulate capabilities that are usually associated with human intelligence, such as language, reasoning, learning, heuristics and observation. Today, all practical (i.e. technically feasible) AI applications fall into the 'narrow' category, and less so, artificial general intelligence (AGI) or 'superintelligence' (Reedy Citation 2017). Footnote  $^{7}$  'Narrow AI' has been widely used in a broad range of civilian and military tasks since the 1960s, (Nilsson Citation 2010; Russel and Norvig Citation 2014) and involves statistical algorithms (mostly based on machine learning techniques) that learn procedures through analysis of large training data sets designed to approximate and replicate human cognitive tasks (Russel and Norvig Citation 2014). Footnote  $^{8}$  'Narrow AI' is the category of artificial intelligence to which this article refers when it assesses the impact of this technology in a military context.\nGiven the diverse approaches to research in AI (Boden Citation 2016 ; Vernon Citation 2014), Footnote  $^{9}$  there is no universally accepted definition of AI. A recent US congressional report defines AI as follows: 'Any artificial system that performs tasks under varying and unpredictable circumstances, without significant human oversight, or that can learn from their experience and improve their performance ... they may solve tasks requiring human-like perception, cognition, planning, learning, communication, or physical action' (Hoadley and Nathan Citation 2018).\nToday, a large gap exists between the reality of what AI technology is capable of doing in a military context, and the expectations and fears of public opinion, policymakers and global defence communities. How will AI technologies influence public opinion about the use of military force and warfare? (Sechser, Narang, and Talmadge Citation 2019). The misrepresentations and misconceptions that exist today in the narratives surrounding AI (especially in societal, economic and national security debates) and are in large part caused by the hyperbole and exaggerated depictions of AI in science fiction (Zarkadakis Citation 2015). Footnote  $^{10}$  Misrepresentations of the potential opportunities and risks associated with this technology in the military sphere can obscure constructive and crucial debate on these topics. Specifically, this includes the challenge of balancing the potential operational, tactical and strategic benefits of leveraging AI while managing the risks posed to nuclear security in the (especially premature) pursuit of these advantages.\nThe historical record on technological change in a military context counsels scepticism against alarmist predictions and extrapolations from emerging trends: technologies rarely evolve in the way futurists predict and many applications have had countervailing or conditional effects that have ameliorated the dystopian predictions of naysayers (Metz Citation 2000 ; Metz and Kievit Citation 1995). In short, AI's impact on nuclear stability will, like previous generations of emerging technology (i.e. chemical and biological weapons, cyber and nuclear weapons), have both stabilising and destabilizing (and possibly contradictory) effects on strategic stability. For example, many expected that chemical weapons would instantly and dramatically change the nature of warfare and deterrence after the British used poison gas during World War I (Sechser, Narang, and Talmadge Citation 2019). However, chemical weapons proved far less practical, impactful, disruptive and relatively easier to defend against than conventional explosives. As a corollary, the degree to which military AI poses risks to future strategic stability will depend in large part on the pace and scope with which this technology facilitates new ways to improve the delivery of, and defence against, nuclear weapons and strategic non-nuclear weapons.\nConceptually, AI-augmented applications can be categorized into those that have predominately operational, tactical and strategic implications in future warfare. At the operational and tactical level, applications include: autonomy and robotics; multi-actor interaction red teaming wargaming; Footnote  $^{11}$  big data-driven modelling; Footnote  $^{12}$  intelligence collection and analysis (e.g. to locate and monitor mobile missiles, troops movement) (Conklin Citation 2018); Footnote\n$^{13}$ cybersecurity talent management; predictive maintenance, logistics, planning and forecasting; and vendor contract and budget management. In preparation for a new AI strategy, US Strategic Operations Command (SOCOM) recently crafted a roadmap that invests heavily in AI and machine learning technology based on several pilot projects with three overarching goals to create an algorithmic warfare multi-operational team: 'AI-ready workforce, AI-applications, and AI-outreach' (Lee Citation 2019).\nTherefore, AI's impact on strategic stability and escalation will also be shaped by broader and more nuanced factors which influence the trajectory of emerging technologies, including: military strategy and doctrine – that seeks to manipulate escalation risks – military culture and organisation, alliance structures, domestic politics and public opinion, to name but a few. The Cold War record demonstrates that emerging technologies act primarily to enable independent variables which, in combination with other endogenous factors, can heighten escalation risks between adversaries. Therefore, in isolation, technology is not a major exogenous cause of (inadvertent or intentional) military escalation.\nAt a strategic level of warfare, AI uses include: qualitative improvements to the nuclear command, control, communications, and intelligence (C3I) architecture; enhancing target acquisition, tracking, guidance systems and discrimination of missile and air defence systems; being force multipliers of both offensive and defensive machine-learning infused cyber capabilities; and qualitatively bolstering nuclear and non-nuclear missile delivery systems – including hypersonic variants. Footnote 14\nIn combination, AI machine learning algorithms fused with advances in sensor technology could be engineered to allow nuclear delivery systems to operate more autonomously and precisely, with less reliance on human operators calibrating navigation and guidance parameters and equipped with more robust countermeasures against jamming or spoofing attacks. At the strategic level, AI-augmented command and control systems could potentially mitigate many of the shortcomings inherent to human strategic decision-making during the 'fog of war', such as the susceptibility to invest in sunk costs, skewed risk judgment, cognitive heuristics and groupthink.\nIn sum, the fusion of AI machine learning and human judgment to gauge an adversary's intentions (and predict escalation) for the purposes of planning and directing future wars for the pursuit of political objectives, is, therefore, a far less unlikely prospect in the near future than the use of AI to achieve tactical and operational ends (e.g. drone swarming and cyber defence).\n# Blurring the AI-cyber offence-defence line: (Glaser and Kaufmann Citation 1998 ; Lynn-Jones Citation 1995 ) Footnote 15\nSeveral US national security officials believe that AI, used as force multipliers for both defensive and offensive cyber weapons, will have a transformative impact on cybersecurity (Gartzke and Lindsay Citation 2015 ; Rid Citation 2012 ; Slayton Citation 2017 ; Hoffman Citation 2019). Footnote 16 Director of US National Intelligence, Daniel Coats, recently warned that AI could increase US vulnerability to cyberattacks, weaken its ability to attribute such attacks, improve the effectiveness and capabilities of adversaries weapon and intelligence systems, and cause accidents and related liability issues. To be sure, the line between AI cyber-offence and cyber-defence will likely remain an obscure one. Footnote 17 Bernard Brodie’s words of caution about the advent of nuclear weapons almost six decades ago resonate: ‘The [military] bias towards the offensive creates special problems in any technologically new situation where there is little or no relevant war experience to help one to reach a balanced judgment’ (Brodie Citation 1959).\nOn the one hand, AI might reduce a military’s vulnerability to cyberattacks. AI cyber-defence tools (or ‘counter-AI’), designed to recognise changes to patterns of behaviour in a network and detect anomalies, automatically detect software code vulnerabilities, and apply machine learning techniques (such as ‘deep learning’) to detect deviations and anomalies from normal network activity, Footnote 18 could form a more robust defence against cyber subversions (Johnson Citation 2019a). Footnote 19 According to the Pentagon’s new AI strategic report, ‘AI can enhance our ability to predict, identify, and respond to cyber and physical threats from a range of sources’ (US Department of Defense Citation 2019). In addition, the DoD’s Defense Innovation Unit (DIU) is prototyping an application (related to Project VOLTRON) that leverages AI to decipher high-level strategic questions, map probabilistic chains of events and develop alternative strategies in order to make DoD systems more resilient to AI-augmented cyberattacks and configure and fix errors more quickly than humans.\nOn the other hand, autonomy itself might increase a military’s vulnerability to cyberattacks, which rely on stealth, deception and stratagem. For example, an adversary could use malware to take control, manipulate or fool the behaviour and pattern recognition systems of autonomous systems, such as DoD’s Project Maven. Offensive attacks such as this would be relatively easy to execute, but very difficult to detect, attribute or effectively counter (Shachtman Citation 2011). Footnote 20 This problem set is compounded by the lack of an agreed framework or understanding of what constitutes escalatory behaviour (or ‘firebreaks’) in cyberspace. Thus, a cyber operation intended as a signal (i.e. for coercive diplomacy) could go undetected by the target, or worse, misinterpreted as an offensive attack. Even if information relating to an\noperation of this kind is accurately identified in a timely manner, the motives behind them could remain ambiguous, or misperceived. According to Robert Jervis, ‘it is likely that the country that is the object of the attack would assume that any effect was the intended one’ (Jervis Citation 2016).\nUS Cyber Fleet Command Commander, Michael Gilday, recently told the Senate Armed Services Committee that the US Navy must ‘improve an ability to proactively detect new and unknown malware … so we [the US] can act quickly using advanced analytics enabled by AI and machine learning’, which may give the US a ‘tactical advantage’ to identify malicious activity early on (Osborn Citation 2018). Even if analysts can obtain high-quality and reliable intelligence, however, they may not want to reveal it, because doing so could compromise a source, capability or tactic (Johnson Citation 2019a). Moreover, most observers now acknowledge that no strategy (i.e. combining offence and defence cyber operations) could realistically be expected to deter all malign cyber-attacks (Hoffman Citation 2019). Footnote 21 While AI-enhanced cyber capabilities can enhance deterrence, they can simultaneously incentivize others to attack thereby exacerbating the paradox of enhanced capabilities and increased vulnerabilities in the cyber domain (Gartzke and Lindsay Citation 2017; Libicki Citation 2016; Slayton Citation 2017). Footnote 22 As the historical record attests, this capability-vulnerability paradox is heightened when states are both dependent on a particular capability (such as AI and cyber tools) and their access or ability to use the capabilities is vulnerable to an adversary’s exploitation or subversion – creating first-mover incentives (Rid Citation 2013). Footnote 23\n# Cybersecurity and nuclear weapon systems: nuclear risk redux?\nAt a strategic level of conflict, AI applications designed to enhance cybersecurity for nuclear forces could simultaneously make cyber-dependent nuclear weapon systems (i.e. communications, data processing or early-warning sensors) more vulnerable to cyberattacks. It is now thought possible that a cyberattack (i.e. spoofing, hacking, manipulation and digital jamming) could infiltrate a nuclear weapons system, threaten the integrity of its communications, and ultimately (and possibly unbeknown to its target) gain control of its (nuclear and non-nuclear) command and control systems. Footnote 24 AI technology has not yet evolved to a point where it would allow nuclear-armed states to credibly threaten the survivability of each other’s nuclear second-strike capability. Thus, the development trajectory of AI (and its enabling technologies) means its impact on nuclear security will likely be theoretical and speculative for the foreseeable future.\nBecause of the intense time pressures that would likely loom large with the decision to use nuclear weapons – especially where a state maintains a launch-on-warning posture – AI-enhanced\ncyberattacks against nuclear systems would be almost impossible to detect and the warning signals difficult to authenticate, let alone attribute, within the short timeframe for initiating a nuclear strike. According to open sources, operators at the North American Aerospace Defence Command (NORAD) have less than three minutes to assess and confirm initial indications from early-warning systems of an incoming attack. This compressed decision-making time frame could put political leaders under intense pressure to make a decision to escalate during a crisis, with incomplete (and possibly false) information of a situation. Ironically, new technologies designed to enhance information (i.e. modernised nuclear C3I systems augmented by 5G networks, machine learning, big-data analytics and quantum computing) can also undermine clear and reliable information flow and communication, critical for effective deterrence (Gartzke and Lindsay Citation 2019). Footnote 25\nAdvances in AI could also exacerbate this cybersecurity challenge by enabling improvements to the cyber offence. Machine learning and AI by automating advanced persistent threat (APT) operations might dramatically reduce the extensive manpower resources and high levels of technical skill required to execute APT operations (or 'hunting for weaknesses'), especially against hardened nuclear targets. Footnote 26 The machine speed of AI-augmented cyber tools could enable a low-skilled and capital-restricted attacker to exploit a narrow window of opportunity to penetrate an adversary's cyber-defences or use APT tools to find new vulnerabilities. For example, when docked for maintenance air-gapped nuclear-powered submarines, considered secure when submerged, could become increasingly vulnerable to a new generation of low-cost – possibly black-market – and highly automated APT cyberattacks.\nAn attacker could also apply AI machine learning techniques to target autonomous dual-use early-warning and other operating systems (e.g. C3I, ISR, early-warning and robotic control networks) with 'weaponized software' such as hacking, subverting, spoofing or tricking, causing unpredictable and potentially undetectable errors, malfunctions and behavioural manipulation to weapons systems – or 'data-poisoning'. Footnote 27 Furthermore, as the linkages between digital and physical systems (or the 'Internet of Things') expand, the potential for to an adversary to use cyberattacks in both kinetic and non-kinetic attacks will increase. A significant risk variable in the operation of autonomous systems is the time that passes between a system failure (i.e. performing in a manner other than how the human operator intended) and the time it takes for a human operator to take corrective action. If the system failure is the result of a deliberate act, this time frame will be compressed (Johnson Citation 2019a).\nEven if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. This skewed assessment by states in the context of nuclear weapons, which are ready to launch nuclear weapons at a moment's notice, would likely precipitate worst-case scenario thinking that may spark an inadvertent escalation (Talmadge\nCitation 2017). Footnote 28 During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 29 For example, an AI-enabled third party generated 'deepfake' (audio and video manipulation), coupled with data-poisoning cyberattacks, could spark an escalatory crisis between two (or more) nuclear states (Fitzpatrick Citation 2019). Footnote 30\nHow could AI-augmented cyber capabilities create new pathways for accidental or inadvertent escalation? To illustrate these dynamics: State A launches a malicious AI-enhanced cyberattack to spoof State B's AI-enabled autonomous sensor platforms and automated target recognition systems, in such a way that the weapon system (e.g. a human-supervised ATR system) is fooled into interpreting a civilian object as a military target. B in response, based on manipulated or erroneous information, and the inability of human supervisors to detect the spoofed imagery that fooled the weapons' automated target recognition algorithm in time to take corrective action, accidentally (and unintentionally) escalates a situation (Libicki Citation 2018). In this example, the spoofing attack on the weapon systems' algorithm is executed in such a way that the imagery appears to the recognition system as indistinguishable from a valid military target (Rohrbach et al. Citation 2018) Footnote 31 escalating a situation based on a false premise that would unlikely to fool the human eye. Footnote 32\nAlso, the explainability (or 'black box') problem associated with AI applications could compound these potential escalation dynamics. Footnote 33 Insufficient understanding of how and why AI algorithms reach a particular judgment or decision might complicate the task of determining whether data sets had been deliberately compromised to manufacture false outcomes (e.g. attacking incorrect targets or misdirecting allies during combat). Footnote 34 Moreover, as humans and AI team up to accomplish particular missions, the opacity associated with how AI systems reach a decision may cause an operator to have either too much or too little confidence in a system's performance. As a result, unless the system's machine learning algorithm is terminated, at the end of the training phase, once deployed, it could potentially learn something it was not intended to, or even perform a task or mission that its human designers do not expect it to do. Footnote 35\nIn sum, technologies that improve the reliability and speed with which information is processed and disseminated by early-warning systems may paradoxically also increase the vulnerabilities of these networks, creating new first-mover advantages and escalation pathways during a crisis that spark unintentional (or intentional) warfare (Schneider Citation 2019). Footnote 36 Put another way, AI might be developed in order to enable a state to pursue a pre-determined escalatory path. Thus, increased escalation risk as a result of technology is not always inadvertent or accidental (Long and Rittenhouse Green Citation 2015). Footnote 37\n# 'Cyber guns' supercharged with AI machine learning\nWhile manipulations and system subversions are possible with existing cyber offence tools, rapid advances in AI and increasing degrees of military autonomy could amplify the potential speed, power and scale of future attacks in cyberspace. Footnote 38 There are three significant intersections between AI systems and cybersecurity that are most salient to the military domain: (1) advances in autonomy and machine learning techniques mean that a much broader range of physical systems are now vulnerable to cyberattacks (i.e. hacking and data-poisoning) (Greenberg Citation 2016); Footnote 39 (2) cyberattacks on AI systems can offer attackers access to machine learning algorithms, trained models being used by the application and a potential vast amounts of data from facial recognition and intelligence collection and analysis systems (e.g. satellite navigation and imagery systems used to cue precision munitions and supported intelligence, surveillance, reconnaissance missions); and (3) the use of AI systems in conjunction with existing cyber offence tools will enable sophisticated cyberattacks to be executed at larger scale (both geographically and across networks), at faster speeds, and simultaneously across multiple military domains with improved anonymity. In short, despite the relatively benign augmentation mechanisms used to develop cyber offensive capabilities with AI systems, the resultant impact on the speed and scope of AI cyber tools will likely have destabilizing effects, nonetheless. Footnote 40\nRecent advances in machine learning have significantly contributed to resolving several technical bottlenecks in several fields of AI, which could allow for significant qualitative improvements to a large variety of autonomous weapon systems and applications. In combination, machine learning and autonomy could intersect with nuclear security in a multitude of ways, with both positive and negative implications for strategic stability. For example, machine learning could enhance the detection capabilities of (nuclear and non-nuclear) early-warning systems and improve the collection and cross-analysis of ISR information, thus reducing the risk of inadvertent and accidental escalation. Machine learning may also bolster the cyber-security of command and control systems, create new and possibilities for arms control, testing, verification and monitoring nuclear stockpiles, and enhance conventional counterforce capabilities. Unmanned autonomous systems could be deployed in complex missions in hitherto inaccessible and cluttered environments (e.g. under-sea anti-submarine warfare) and aerial and underwater drones in swarms might eventually replace intercontinental ballistic missiles (ICBMs) and nuclear-armed submarines (SSBNs) for the delivery of nuclear weapons.\nIn response to these anticipated vulnerabilities, and to achieve the (perceived) first-mover advantages the increased speed in AI-augmented cyber tools could confer, China, Russia and the United States have continued to harden their AI cyber defences. Despite these parallel efforts, divergent views exist on how strategic communities perceive the nature of the risks posed by AI-enhanced cyberattacks against dual-use C3I systems. Thus, AI-augmented cyber intelligence-\ngathering tools targeting an adversary's command and control assets (especially dual-use facilities) during a crisis may be misinterpreted as a prelude to an imminent pre-emptive attack on its nuclear force and therefore as undermining strategic stability.\nMoreover, AI machine learning could enable new offensive concepts such as a so-called 'left-of-launch operation' (Ellison Citation 2015), Footnote  $^{41}$  thereby compounding the ambiguities and fear about pre-emptive attacks in cyberspace, and increasing first-mover incentives – or use-them-or-lose-them situations (Buchanan Citation 2017; Buchanan and Miller Citation 2017). Footnote  $^{42}$  In the cyber domain, it is generally unclear in the early stages of a cyber operation whether an adversary intends to collect intelligence or prepare for an offensive attack, which is more likely to be used early on in a conflict scenario. Open sources suggest, for example, that Chinese analysts view the vulnerability of China's nuclear command, control, and communications (C3I) systems to cyber infiltrations – even if an attacker's objective was limited to cyberespionage – as a highly escalatory national security threat. By contrast, Russian analysts tend to view Russia's nuclear C3I network as relatively isolated, and thus insulated from cyberattacks.\nIrrespective of these differences, the uncertainty caused by the possibility, actual or otherwise, that AI-enhanced offensive cyber tools might be used (or threatened) to undermine the credibility and reliability of states' nuclear forces would be destabilizing, both as result of the existence of enhanced cyber offensive capabilities and the potential efficacy of these tools against nuclear forces. As the historical record attests, rational actors have incentives to misrepresent what they know (i.e. lie and bluff) in order to improve the terms of any settlement that may emerge from the wartime bargaining process – or the risk-reward payoff. Even a modicum of uncertainty about the effectiveness of AI-augmented cyber capabilities during a crisis or conflict would, therefore, reduce both sides' risk tolerance and increase the incentive to strike pre-emptively as a hedging strategy.\nA virtuous cycle that might flow from enhanced reassurances premised on comprehensive intelligence would require information symmetry (i.e. equal access to intelligence and analysis systems) between great and rising powers, and shared confidence in the accuracy and credibility of these systems. Perhaps most challenging of all in a world of 'revisionist' rising powers, the intentions of all rival states would need to be genuinely benign for this optimistic view to occur. Under crisis conditions, for example, an offensive AI cyber tool that succeeds in compromising an adversary's nuclear weapon systems, resulting in an 'asymmetric information' situation, could cause either or both sides to overstate (or understate) its retaliatory capabilities, and in turn, be more inclined to act in a risky and escalatory fashion. In short, in a competitive strategic environment, where states are inclined to assume the worst of others' intentions, one state's efforts to enhance the survivability of its strategic forces may be viewed by others as a threat to their nuclear retaliatory capability – or second-strike capacity.\n# Manipulation of the information landscape\nWhile machine learning big-data analytics, sensing technology, networks supported by 5G networks, could alert commanders of incoming threats with increased speed and precision, and make accidents caused by human error potentially less likely (especially accidents and malfunctions caused by false warning), these advances could simultaneously amplify escalation risks in two ways. Footnote 43\nFirst, AI machine learning used as force multipliers for cyber offence (e.g. data poisoning spoofing, 'deepfakes', manipulation, hacking and digital jamming) would be considerably more difficult to detect – especially if an attacker used AI advanced persistent threat (APT) tools – or 'hunting for weaknesses' in spectrum-contested environments. Second, in the unlikely event an attack was successfully detected, threat identification (or attribution) at machine speed would be virtually impossible. In addition to the co-mingling and the speed of warfare issues, AI machine learning systems might also exacerbate the existing – well-established and non-kinetic – risk of inadvertent (and intentional) escalation from the manipulation of the information landscape in which decisions about nuclear weapons are located. For example, the use of deepfake videos built-in real-time and disseminated online for malicious purposes.\nHuman cognition, and thus effective deterrence and signalling intentions, is predicated on reliable and clear information; if an adversary is concerned that the information available to them is limited (or worse, inaccurate), they will likely assume the worst and act accordingly. Asymmetric information situations between rivals about the balance of military power could undermine crisis stability, and in turn, create rational incentives to escalate to nuclear confrontation. Consequently, states will be more inclined to assume the worst of others' intentions, especially in situations where the legitimacy of the status quo is contested (i.e. maritime Asia).\nIn the competitive strategic environment, it is easy to imagine unprovoked escalation caused by a malicious third-party (or state-proxy) clandestine action. Even if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 44 For example, in an effort to incite conflict between two rival states, State A uses proxy hackers to use a AI machine learning technique (i.e. generative adversarial networks GAN) to launch 'deepfake' video or audio material, depicting senior military commanders of State B conspiring to launch a pre-emptive strike on State C. Footnote 45 Then, this 'deepfake' footage is deliberately leaked into C's AI-augmented intelligence collection and analysis systems, provoking C to escalate the situation with strategic consequences. B responds in kind. Footnote 46\nThe current underdeveloped state of ‘counter-AI’ capabilities and other fail-safe mechanisms (e.g. circuit breakers) to de-escalate escalation in cyberspace, will make the unprovoked and unintentional escalation dynamics depicted in this scenario very challenging to mitigate. Moreover, in the emerging ‘deepfakes’ arms race (much like cybersecurity more generally), detection software will likely lag behind advances in offensive enabling solutions – or offence-dominant ones. According to computer science expert Hany Farid, there are probably 100–1000 times ‘more people developing the technology to manipulate content than there is to detect [it]’ (Fontaine and Frederick Citation 2019).\nThe overreliance on automation (or ‘automation bias’) in the deployment of increasing complex AI-augmented capabilities – such as cyber, C3I systems, AWS and precision missile munitions – designed to provide a distinct tactical advantage of machine-speed, will increase the vulnerability of these capabilities to exploitation. Moreover, the increasing substitution of human cognition for logic derived from machines will likely increase the opportunities for adversaries to exploit the limitations of narrow AI technology, i.e. lack of human intuition, brittleness in complex real-world situations and an inability to effectively detect or counter manipulation attacks (Libicki Citation 2018). Footnote 47 Therefore, until researchers unravel some of the unexplainable features of AI, human error and machine error will likely compound one another, with unpredictable results. Footnote 48 Simply put, we are at a critical crossroads in the parallel (and symbiotic) evolution of AI and cyberspace that national security communities, globally, will need to prepare for proactively (Johnson Citation 2019a).\n## Policy interventions\nA prominent theme that runs through this article – and central to understanding the potential impact AI for strategic stability and nuclear security more broadly – is the concern that AI systems operating at machine-speed will push the pace of combat to a point where the actions of machines surpass the (cognitive and physical) ability of human decision makers to control (or even comprehend) events.\nPossible multi-track policy responses to push back against the threat posed to stability from AI in a multipolar can be broadly categorized into: (1) those that focus on enhancing debate and discussion between researchers, global defence communities, decision makers, academics and other political and societal stakeholders; and (2) a range of specific policy recommendations and tools for great military powers to negotiate and implement.\n## Enhancing debate and dialogue\nFirst, to mitigate (or at least manage) the destabilizing and escalatory risks posed by the AI-cybersecurity nexus, great military powers must closely coordinate their confidence-building\nmeasures in an effort to pre-empt some of the risks to stability outlined above. Track-1 and track-2 discussions should include diplomats and military leaders (especially from China, Russian and the United States), industry experts, AI researchers and multi-disciplinary academics. Specifically, great military powers should establish an international framework for governance, norms, behaviour and the regulation of policy on machine learning-augmented cyber capabilities. Resistance to these efforts will likely come from states who worry that in revealing their cyber (especially offensive) capabilities, they could upend the deterrence utility of these tools.\nTo be sure, the challenges posed in the coordination and implementation of these policies will require bold and visionary leadership to circumvent the inevitable regional agendas, interdisciplinary resistance and burgeoning security dilemmas between rival states. Because of the rapid technological change in AI formal treaties, associated with arms-control agreements that require lengthy and complex negotiation and ratification processes, legal frameworks risk becoming obsolete before they come into effect. The historical record has demonstrated on several occasions that these kinds of challenges facing humanity can be overcome (Krimsky Citation 1962; Naur and Randell Citation 1968). Footnote 49\nSecond, the think tank community, academics and AI research experts should pool their resources to investigate the implications of the AI-cyber nexus for a range of potential security issues such as: the impact of AI bias on future military-use applications; how to prepare for (and react to) artificial general intelligence; and measures to mitigate, prevent and manage offensive uses of AI. Footnote 50 If necessary, steps could be taken to amend existing legal definitions of offensive cyber operations, such as hacking, data-poisoning attacks and spoofing, to account for the increased proliferation and potential damage caused by AI.\nThird, and related, due to the intrinsic dual-use nature of the AI-cyber security problem set, this dialogue should also be expanded to include other stakeholders such as private-sector AI and cybersecurity experts, the commercial sector, ethicists, philosophers, civil society and public opinion. Footnote 51 States should also collaborate on dual-use AI research to leverage AI's low-cost and scaling advantages (i.e. in autonomy and robotics). Further, a focus on the safety, testing and robustness of AI systems is a critical step in mitigating potential vulnerabilities and risks caused by errors, bias, and explainability in uncontrolled and complex environments. Footnote 52 In the case of offensive AI-related cybersecurity research, where public distribution might cause vulnerabilities and worsen security, publication could be confined to trusted organisations and entities.\nBest practices that exist in more mature methods for addressing dual-use concerns, like computer security, where applicable, might be applied to AI. For example, the wide use of red teaming exercises to enhance network security, organisation and practices. Specifically, AI-cyber red teaming – like DARPA's Cyber Grand Challenge – will enable engineers and operators to better\nunderstand the skills needed to execute particular offensive and defensive operations (especially using machine learning techniques), and to better manage system vulnerabilities, adversarial exploitation, stress-testing and social engineering challenges (Anderson, Woodbridge, and Filar Citation 2016).\nA recent study explored case studies of previous examples (biological weapons, cryptography and nuclear technology) of dual-use technologies – with civilian and military uses – to examine potential insights for AI dual-use risk management policies such as export controls and prepublication reviews. These insights also highlight the potential challenges of establishing regulatory, legal and normative frameworks for dual-use technologies – for example, the cautionary tale of ineffective efforts, in the late 1990s, to regulate cryptographic algorithms and cyber-network security tools through export controls.\nThe extent to which the integration of AI into dual-use weapon systems (such as cybersecurity) might influence actors' attitude to risk, the offence-defence balance and, in turn, perceptions of others' intentions and capabilities, could have profound implications for strategic deterrence and nuclear security. Footnote 53 In sum, cognizant that some states have deployed (or imminently plan to deploy) AI systems, experts generally agree that AI requires further experimentation, testing and development before being integrated into lethal weapon systems and their decision-making support systems.\n# Taking the lead from the cybersecurity community\nSome examples of cybersecurity-centered measures that might be further researched and implemented to pre-empt and mitigate some of risks posed by AI-augmented cyberattacks (especially involving safety-critical nuclear systems) highlighted in this paper include the following.\nFirst, coordinating AI-simulated war games, red teaming creative thinking exercises and creating redundancies (i.e. back-ups or fail-safes) in networks to detect errors, fix vulnerabilities and increase the reliability and robustness of military (especially nuclear-centric) systems. Specific procedures could be implemented to enable confidential reporting and fixing the vulnerabilities, subversions and other kinds of manipulations detected in AI systems. These findings could be used to track the proliferation of AI-related incursions and then countermeasures could be developed and standardised to manage these threats. Footnote 54\nSecond, states should formalise verification methods and protocols and consider issues such as: to what extent, under what circumstances, and for what types of AI systems can formal verification be implemented? Might other approaches be developed to achieve similar goals (e.g. machine learning and big data analysis augmented verification methods)? For example, DARPA's Assured Autonomy Program combines data-driven machine learning algorithms to assure the\nsafety of autonomous cyber physical systems (air, ground, sea and undersea unmanned vehicles), which continue to learn throughout their lifespans.\nThis learning characteristic makes assurance or verification using traditional methods especially challenging. Whether AI applications in the military domain can be formally verified, for now, is an unanswered question. Footnote 55 Much like that of cyber systems, the complexity of AI systems and the difficulty of defining their properties for formal verification makes them less amenable to verification compared to other types of technology.\nThird, the global defence communities should actively invest in the development of AI cyber-defence tools (e.g. analysing classification errors, automatic detection of remote vulnerability scanning and model extraction improvements), AI-centric secure hardware, and other fail-safe mechanisms, to allow for de-escalation and to prevent unintentional or accidental escalation.\nSeveral issues that need further exploration related to these measures include: the extent to which existing tools might be effective against vulnerabilities in AI systems? How can these tools be tailored for AI systems across multiple military domains? Is there an equivalent to 'patching' in military AI systems? What kinds of policies might incentivize, and ensure compliance with, meaningful reforms to existing hardware in the military sphere? While these questions are challenging, and necessarily speculative for now, answers will become more evident as the technology matures.\n# Disclosure statement\nNo potential conflict of interest was reported by the author."
    }
  },
  "summary": {
    "full_text": {
      "words": 7024,
      "tokens": 9112
    },
    "flat_text": {
      "words": 7022,
      "tokens": 9102
    }
  },
  "payload": "## # Introduction\n\nGiven the hyperbole surrounding AI, it is easy to overstate the opportunities and challenges posed by the development and deployment of AI in the military sphere (Boulanin Citation 2019). Today, there remains a large amount of debate among AI researchers surrounding the significance of several significant technical and operational challenges in the deployment of AI-infused systems (e.g. drone swarming, command and control decision-making support systems and a broader range of autonomous weapon systems). Moreover, many of the risks posed by AI in the nuclear domain today are not necessarily new (Boulanin Citation 2019). Recent advances in AI (especially machine learning techniques) exacerbate existing risks to escalation and stability rather than generating entirely new ones. While AI could enable significant improvements in many military domains (including nuclear weapons), for the foreseeable future, developments in military AI will likely be far more prosaic than implied in popular culture. The main worry for nuclear stability, therefore, is that militaries will underestimate (or overstate) or ignore the potential shortcomings and risks associated with the use of the current generation of AI technology in the safety-critical military sphere – and especially the nuclear domain.\nBecause of the diverse approaches to AI-based research, there is no universally accepted definition of ‘artificial intelligence’, which is confusing when the term is applied generically to make grandiose claims about its revolutionary effects. Footnote    AI can be understood as a universal term for improving the performance of automated (or autonomous) systems to solve a wide variety of complex tasks including: perception (sensors, computer vision, audio and image processing); reasoning and decision-making (problem solving, searching, planning and reasoning); learning and knowledge representation (machine learning, deep networks and modelling); Footnote    communication (language processing); autonomy and robotics ; and human-AI collaboration (humans define the systems’ purpose, goals and context).\nIn a military context, as a potential enabler and force multiplier of advanced weapon systems, AI is more akin to electricity, radio, radar and intelligence, surveillance and reconnaissance (ISR) support systems than a 'weapon' per se. Much like these enabling support systems, AI is also strategically vital, vulnerable, inherently cross-domain and dependent on advanced technology. Thus, even if AI applications are unable to make better battlefield decisions than humans, Footnote  $^{3}$  militaries that use AI in human-machine teaming will doubtless gain significant advantages (e.g. remote-sensing, situational-awareness, battlefield-manoeuvres and a compressed decision-making loop), compared to those who depend on human judgment – and semi-autonomous technology – alone. This is particularly the case, in operating systems in operating environments that demand endurance and rapid decision-making across multiple combat zones (Ayoub and Payne Citation 2016).\nThis article argues that a new generation of AI-augmented offensive cyber capabilities will amplify the risk of inadvertent escalation posed by the co-mingling of nuclear and strategic (or counterforce) non-nuclear weapons and the increasing speed of warfare, and in turn, increase the risk of nuclear confrontation. The article's thesis is grounded in three core themes. First, AI does not exist a vacuum, that is, in isolation, AI will unlikely be a strategic game changer. Instead, it will likely mutually reinforce the destabilizing effects of existing advanced capabilities thereby increasing the speed of warfare and compressing the decision-making timeframe (Johnson Citation 2019a). AI-enabled and enhanced capabilities will have a more significant impact (positive or negative) on strategic stability than the sum of its parts. Furthermore, the intersection of AI with nuclear weapons, and a broader spectrum of strategic non-nuclear weapons, will likely accelerate the erosion of the survivability of nuclear arsenals associated with the 'computer revolution'. Put another way, military AI and the advanced capabilities it enables are a natural manifestation – rather than the cause or origin – of an established trend, which could lead states to adopt destabilizing launch postures due to the increasing speed of war and co-mingling.\nSecond, AI's impact on stability, deterrence and escalation will likely be determined as much (or more) by states' perception of its functionality as much as by what it is capable of doing. In the case of nuclear strategy and deterrence, the perception of an adversary's capabilities and\nintentions is as (or more) important than its actual capability. Thus, in addition to the importance of military force postures, capabilities and doctrine, the effects of AI will also have a strong cognitive element, increasing the risk of inadvertent escalation as a result of misperception and misunderstanding.\nFinally, and related to the above, the increasingly competitive and contested nuclear multipolar world order will compound the destabilizing effects of AI, and in turn, increase the escalation risks in future warfare between great military powers – especially China and the United States (Johnson Citation 2019b). Moreover, the potential operational and strategic advantages offered by AI-augmented capabilities could prove irresistible to nuclear-armed strategic rivals, causing them to eschew the limitations of AI, and compromise safety and verification standards to protect, or attempt to capture, technological superiority on the future digitised battlefield.\n\n---\n\n## # Taking the lead from the cybersecurity community\n\nSome examples of cybersecurity-centered measures that might be further researched and implemented to pre-empt and mitigate some of risks posed by AI-augmented cyberattacks (especially involving safety-critical nuclear systems) highlighted in this paper include the following.\nFirst, coordinating AI-simulated war games, red teaming creative thinking exercises and creating redundancies (i.e. back-ups or fail-safes) in networks to detect errors, fix vulnerabilities and increase the reliability and robustness of military (especially nuclear-centric) systems. Specific procedures could be implemented to enable confidential reporting and fixing the vulnerabilities, subversions and other kinds of manipulations detected in AI systems. These findings could be used to track the proliferation of AI-related incursions and then countermeasures could be developed and standardised to manage these threats. Footnote 54\nSecond, states should formalise verification methods and protocols and consider issues such as: to what extent, under what circumstances, and for what types of AI systems can formal verification be implemented? Might other approaches be developed to achieve similar goals (e.g. machine learning and big data analysis augmented verification methods)? For example, DARPA's Assured Autonomy Program combines data-driven machine learning algorithms to assure the\nsafety of autonomous cyber physical systems (air, ground, sea and undersea unmanned vehicles), which continue to learn throughout their lifespans.\nThis learning characteristic makes assurance or verification using traditional methods especially challenging. Whether AI applications in the military domain can be formally verified, for now, is an unanswered question. Footnote 55 Much like that of cyber systems, the complexity of AI systems and the difficulty of defining their properties for formal verification makes them less amenable to verification compared to other types of technology.\nThird, the global defence communities should actively invest in the development of AI cyber-defence tools (e.g. analysing classification errors, automatic detection of remote vulnerability scanning and model extraction improvements), AI-centric secure hardware, and other fail-safe mechanisms, to allow for de-escalation and to prevent unintentional or accidental escalation.\nSeveral issues that need further exploration related to these measures include: the extent to which existing tools might be effective against vulnerabilities in AI systems? How can these tools be tailored for AI systems across multiple military domains? Is there an equivalent to 'patching' in military AI systems? What kinds of policies might incentivize, and ensure compliance with, meaningful reforms to existing hardware in the military sphere? While these questions are challenging, and necessarily speculative for now, answers will become more evident as the technology matures.",
  "summary_log": "---LOG_SUMMARY_START---\ndoc_status:SUCCESS\nsections_raw:9\nsections_clean:9\nintro:FOUND\nconclusion:FOUND\npredefined_sections:None\nextra_sections:# Military AI primer|# Manipulation of the information landscape\npayload_tokens_before:5374\npayload_tokens_after:5374\ndropped_section:None\nadded_section:None\n---LOG_SUMMARY_END---",
  "pages_text": [
    "# ABSTRACT\n\nHow could AI-infused cyber capabilities be used to subvert, or otherwise compromise, the reliability, control and use of states' nuclear forces? This article argues that a new generation of artificial intelligence (AI) enhanced cyber capabilities will amplify the risk of inadvertent escalation caused by the co-mingling of nuclear and strategic non-nuclear weapons and the increasing speed of warfare, thereby increasing the risk of nuclear confrontation. It examines the potential implications of cyber (offensive and defensive) capabilities augmented with AI applications for nuclear security. The article concludes that future iterations of AI-enhanced cyber counterforce capabilities will complicate the existing challenges of cyber defence, and in turn, compromise nuclear assets and increase the escalatory effects of offensive cyber capabilities.\n\n# KEYWORDS:\n\n- Artificial intelligence\n- cyber-security\n- US-China Relations\n- nuclear security\n- deterrence policy\n- emerging technology\n\n# Introduction\n\nGiven the hyperbole surrounding AI, it is easy to overstate the opportunities and challenges posed by the development and deployment of AI in the military sphere (Boulanin Citation 2019). Today, there remains a large amount of debate among AI researchers surrounding the significance of several significant technical and operational challenges in the deployment of AI-infused systems (e.g. drone swarming, command and control decision-making support systems and a broader range of autonomous weapon systems). Moreover, many of the risks posed by AI in the nuclear domain today are not necessarily new (Boulanin Citation 2019). Recent advances in AI (especially machine learning techniques) exacerbate existing risks to escalation and stability rather than generating entirely new ones. While AI could enable significant improvements in many military domains (including nuclear weapons), for the foreseeable future, developments in military AI will likely be far more prosaic than implied in popular culture. The main worry for nuclear stability, therefore, is that militaries will underestimate (or overstate) or ignore the potential shortcomings and risks associated with the use of the current generation of AI technology in the safety-critical military sphere – and especially the nuclear domain.",
    "Because of the diverse approaches to AI-based research, there is no universally accepted definition of ‘artificial intelligence’, which is confusing when the term is applied generically to make grandiose claims about its revolutionary effects. Footnote  ${}^{1}$  AI can be understood as a universal term for improving the performance of automated (or autonomous) systems to solve a wide variety of complex tasks including: perception (sensors, computer vision, audio and image processing); reasoning and decision-making (problem solving, searching, planning and reasoning); learning and knowledge representation (machine learning, deep networks and modelling); Footnote  ${}^{2}$  communication (language processing); autonomy and robotics ; and human-AI collaboration (humans define the systems’ purpose, goals and context).\n\nIn a military context, as a potential enabler and force multiplier of advanced weapon systems, AI is more akin to electricity, radio, radar and intelligence, surveillance and reconnaissance (ISR) support systems than a 'weapon' per se. Much like these enabling support systems, AI is also strategically vital, vulnerable, inherently cross-domain and dependent on advanced technology. Thus, even if AI applications are unable to make better battlefield decisions than humans, Footnote  $^{3}$  militaries that use AI in human-machine teaming will doubtless gain significant advantages (e.g. remote-sensing, situational-awareness, battlefield-manoeuvres and a compressed decision-making loop), compared to those who depend on human judgment – and semi-autonomous technology – alone. This is particularly the case, in operating systems in operating environments that demand endurance and rapid decision-making across multiple combat zones (Ayoub and Payne Citation 2016).\n\nThis article argues that a new generation of AI-augmented offensive cyber capabilities will amplify the risk of inadvertent escalation posed by the co-mingling of nuclear and strategic (or counterforce) non-nuclear weapons and the increasing speed of warfare, and in turn, increase the risk of nuclear confrontation. The article's thesis is grounded in three core themes. First, AI does not exist a vacuum, that is, in isolation, AI will unlikely be a strategic game changer. Instead, it will likely mutually reinforce the destabilizing effects of existing advanced capabilities thereby increasing the speed of warfare and compressing the decision-making timeframe (Johnson Citation 2019a). AI-enabled and enhanced capabilities will have a more significant impact (positive or negative) on strategic stability than the sum of its parts. Furthermore, the intersection of AI with nuclear weapons, and a broader spectrum of strategic non-nuclear weapons, will likely accelerate the erosion of the survivability of nuclear arsenals associated with the 'computer revolution'. Put another way, military AI and the advanced capabilities it enables are a natural manifestation – rather than the cause or origin – of an established trend, which could lead states to adopt destabilizing launch postures due to the increasing speed of war and co-mingling.\n\nSecond, AI's impact on stability, deterrence and escalation will likely be determined as much (or more) by states' perception of its functionality as much as by what it is capable of doing. In the case of nuclear strategy and deterrence, the perception of an adversary's capabilities and",
    "intentions is as (or more) important than its actual capability. Thus, in addition to the importance of military force postures, capabilities and doctrine, the effects of AI will also have a strong cognitive element, increasing the risk of inadvertent escalation as a result of misperception and misunderstanding.\n\nFinally, and related to the above, the increasingly competitive and contested nuclear multipolar world order will compound the destabilizing effects of AI, and in turn, increase the escalation risks in future warfare between great military powers – especially China and the United States (Johnson Citation 2019b). Moreover, the potential operational and strategic advantages offered by AI-augmented capabilities could prove irresistible to nuclear-armed strategic rivals, causing them to eschew the limitations of AI, and compromise safety and verification standards to protect, or attempt to capture, technological superiority on the future digitised battlefield.\n\n# Military AI primer\n\nArtificial intelligence research began as early as the 1950s, as a broad concept concerned with the science and engineering of making intelligent machines. Since the 1950s, AI research has gone through several development phases – from early exploitations in the 1950s and 1960s and the 'AI Summer' during the 1970s, through to the early 1980s and the 'AI Winter' from the 1980s – each of which failed to live up to its initial, and often over-hyped, expectations. In the past decade, the explosion of interest in the field (or 'AI renaissance') occurred due to the convergence of four enabling developments: (1) the exponential growth in computing processing power; (2) expanded data sets (especially 'big data' sources) (Gray et al. Citation 2015); Footnote  $^{4}$  (3) advances in the implementation of machine learning techniques and algorithms (especially deep 'neural networks') (Schmidhuber Citation 2015); Footnote  $^{5}$  and (4) the rapid expansion of commercial interest and investment in AI (Hoadley and Nathan Citation 2018). Footnote  $^{6}$\n\nAI is concerned with machines that emulate capabilities that are usually associated with human intelligence, such as language, reasoning, learning, heuristics and observation. Today, all practical (i.e. technically feasible) AI applications fall into the 'narrow' category, and less so, artificial general intelligence (AGI) or 'superintelligence' (Reedy Citation 2017). Footnote  $^{7}$  'Narrow AI' has been widely used in a broad range of civilian and military tasks since the 1960s, (Nilsson Citation 2010; Russel and Norvig Citation 2014) and involves statistical algorithms (mostly based on machine learning techniques) that learn procedures through analysis of large training data sets designed to approximate and replicate human cognitive tasks (Russel and Norvig Citation 2014). Footnote  $^{8}$  'Narrow AI' is the category of artificial intelligence to which this article refers when it assesses the impact of this technology in a military context.",
    "Given the diverse approaches to research in AI (Boden Citation 2016 ; Vernon Citation 2014), Footnote  $^{9}$  there is no universally accepted definition of AI. A recent US congressional report defines AI as follows: 'Any artificial system that performs tasks under varying and unpredictable circumstances, without significant human oversight, or that can learn from their experience and improve their performance ... they may solve tasks requiring human-like perception, cognition, planning, learning, communication, or physical action' (Hoadley and Nathan Citation 2018).\n\nToday, a large gap exists between the reality of what AI technology is capable of doing in a military context, and the expectations and fears of public opinion, policymakers and global defence communities. How will AI technologies influence public opinion about the use of military force and warfare? (Sechser, Narang, and Talmadge Citation 2019). The misrepresentations and misconceptions that exist today in the narratives surrounding AI (especially in societal, economic and national security debates) and are in large part caused by the hyperbole and exaggerated depictions of AI in science fiction (Zarkadakis Citation 2015). Footnote  $^{10}$  Misrepresentations of the potential opportunities and risks associated with this technology in the military sphere can obscure constructive and crucial debate on these topics. Specifically, this includes the challenge of balancing the potential operational, tactical and strategic benefits of leveraging AI while managing the risks posed to nuclear security in the (especially premature) pursuit of these advantages.\n\nThe historical record on technological change in a military context counsels scepticism against alarmist predictions and extrapolations from emerging trends: technologies rarely evolve in the way futurists predict and many applications have had countervailing or conditional effects that have ameliorated the dystopian predictions of naysayers (Metz Citation 2000 ; Metz and Kievit Citation 1995). In short, AI's impact on nuclear stability will, like previous generations of emerging technology (i.e. chemical and biological weapons, cyber and nuclear weapons), have both stabilising and destabilizing (and possibly contradictory) effects on strategic stability. For example, many expected that chemical weapons would instantly and dramatically change the nature of warfare and deterrence after the British used poison gas during World War I (Sechser, Narang, and Talmadge Citation 2019). However, chemical weapons proved far less practical, impactful, disruptive and relatively easier to defend against than conventional explosives. As a corollary, the degree to which military AI poses risks to future strategic stability will depend in large part on the pace and scope with which this technology facilitates new ways to improve the delivery of, and defence against, nuclear weapons and strategic non-nuclear weapons.\n\nConceptually, AI-augmented applications can be categorized into those that have predominately operational, tactical and strategic implications in future warfare. At the operational and tactical level, applications include: autonomy and robotics; multi-actor interaction red teaming wargaming; Footnote  $^{11}$  big data-driven modelling; Footnote  $^{12}$  intelligence collection and analysis (e.g. to locate and monitor mobile missiles, troops movement) (Conklin Citation 2018); Footnote",
    "$^{13}$ cybersecurity talent management; predictive maintenance, logistics, planning and forecasting; and vendor contract and budget management. In preparation for a new AI strategy, US Strategic Operations Command (SOCOM) recently crafted a roadmap that invests heavily in AI and machine learning technology based on several pilot projects with three overarching goals to create an algorithmic warfare multi-operational team: 'AI-ready workforce, AI-applications, and AI-outreach' (Lee Citation 2019).\n\nTherefore, AI's impact on strategic stability and escalation will also be shaped by broader and more nuanced factors which influence the trajectory of emerging technologies, including: military strategy and doctrine – that seeks to manipulate escalation risks – military culture and organisation, alliance structures, domestic politics and public opinion, to name but a few. The Cold War record demonstrates that emerging technologies act primarily to enable independent variables which, in combination with other endogenous factors, can heighten escalation risks between adversaries. Therefore, in isolation, technology is not a major exogenous cause of (inadvertent or intentional) military escalation.\n\nAt a strategic level of warfare, AI uses include: qualitative improvements to the nuclear command, control, communications, and intelligence (C3I) architecture; enhancing target acquisition, tracking, guidance systems and discrimination of missile and air defence systems; being force multipliers of both offensive and defensive machine-learning infused cyber capabilities; and qualitatively bolstering nuclear and non-nuclear missile delivery systems – including hypersonic variants. Footnote 14\n\nIn combination, AI machine learning algorithms fused with advances in sensor technology could be engineered to allow nuclear delivery systems to operate more autonomously and precisely, with less reliance on human operators calibrating navigation and guidance parameters and equipped with more robust countermeasures against jamming or spoofing attacks. At the strategic level, AI-augmented command and control systems could potentially mitigate many of the shortcomings inherent to human strategic decision-making during the 'fog of war', such as the susceptibility to invest in sunk costs, skewed risk judgment, cognitive heuristics and groupthink.\n\nIn sum, the fusion of AI machine learning and human judgment to gauge an adversary's intentions (and predict escalation) for the purposes of planning and directing future wars for the pursuit of political objectives, is, therefore, a far less unlikely prospect in the near future than the use of AI to achieve tactical and operational ends (e.g. drone swarming and cyber defence).",
    "# Blurring the AI-cyber offence-defence line: (Glaser and Kaufmann Citation 1998 ; Lynn-Jones Citation 1995 ) Footnote 15\n\nSeveral US national security officials believe that AI, used as force multipliers for both defensive and offensive cyber weapons, will have a transformative impact on cybersecurity (Gartzke and Lindsay Citation 2015 ; Rid Citation 2012 ; Slayton Citation 2017 ; Hoffman Citation 2019). Footnote 16 Director of US National Intelligence, Daniel Coats, recently warned that AI could increase US vulnerability to cyberattacks, weaken its ability to attribute such attacks, improve the effectiveness and capabilities of adversaries weapon and intelligence systems, and cause accidents and related liability issues. To be sure, the line between AI cyber-offence and cyber-defence will likely remain an obscure one. Footnote 17 Bernard Brodie’s words of caution about the advent of nuclear weapons almost six decades ago resonate: ‘The [military] bias towards the offensive creates special problems in any technologically new situation where there is little or no relevant war experience to help one to reach a balanced judgment’ (Brodie Citation 1959).\n\nOn the one hand, AI might reduce a military’s vulnerability to cyberattacks. AI cyber-defence tools (or ‘counter-AI’), designed to recognise changes to patterns of behaviour in a network and detect anomalies, automatically detect software code vulnerabilities, and apply machine learning techniques (such as ‘deep learning’) to detect deviations and anomalies from normal network activity, Footnote 18 could form a more robust defence against cyber subversions (Johnson Citation 2019a). Footnote 19 According to the Pentagon’s new AI strategic report, ‘AI can enhance our ability to predict, identify, and respond to cyber and physical threats from a range of sources’ (US Department of Defense Citation 2019). In addition, the DoD’s Defense Innovation Unit (DIU) is prototyping an application (related to Project VOLTRON) that leverages AI to decipher high-level strategic questions, map probabilistic chains of events and develop alternative strategies in order to make DoD systems more resilient to AI-augmented cyberattacks and configure and fix errors more quickly than humans.\n\nOn the other hand, autonomy itself might increase a military’s vulnerability to cyberattacks, which rely on stealth, deception and stratagem. For example, an adversary could use malware to take control, manipulate or fool the behaviour and pattern recognition systems of autonomous systems, such as DoD’s Project Maven. Offensive attacks such as this would be relatively easy to execute, but very difficult to detect, attribute or effectively counter (Shachtman Citation 2011). Footnote 20 This problem set is compounded by the lack of an agreed framework or understanding of what constitutes escalatory behaviour (or ‘firebreaks’) in cyberspace. Thus, a cyber operation intended as a signal (i.e. for coercive diplomacy) could go undetected by the target, or worse, misinterpreted as an offensive attack. Even if information relating to an",
    "operation of this kind is accurately identified in a timely manner, the motives behind them could remain ambiguous, or misperceived. According to Robert Jervis, ‘it is likely that the country that is the object of the attack would assume that any effect was the intended one’ (Jervis Citation 2016).\n\nUS Cyber Fleet Command Commander, Michael Gilday, recently told the Senate Armed Services Committee that the US Navy must ‘improve an ability to proactively detect new and unknown malware … so we [the US] can act quickly using advanced analytics enabled by AI and machine learning’, which may give the US a ‘tactical advantage’ to identify malicious activity early on (Osborn Citation 2018). Even if analysts can obtain high-quality and reliable intelligence, however, they may not want to reveal it, because doing so could compromise a source, capability or tactic (Johnson Citation 2019a). Moreover, most observers now acknowledge that no strategy (i.e. combining offence and defence cyber operations) could realistically be expected to deter all malign cyber-attacks (Hoffman Citation 2019). Footnote 21 While AI-enhanced cyber capabilities can enhance deterrence, they can simultaneously incentivize others to attack thereby exacerbating the paradox of enhanced capabilities and increased vulnerabilities in the cyber domain (Gartzke and Lindsay Citation 2017; Libicki Citation 2016; Slayton Citation 2017). Footnote 22 As the historical record attests, this capability-vulnerability paradox is heightened when states are both dependent on a particular capability (such as AI and cyber tools) and their access or ability to use the capabilities is vulnerable to an adversary’s exploitation or subversion – creating first-mover incentives (Rid Citation 2013). Footnote 23\n\n# Cybersecurity and nuclear weapon systems: nuclear risk redux?\n\nAt a strategic level of conflict, AI applications designed to enhance cybersecurity for nuclear forces could simultaneously make cyber-dependent nuclear weapon systems (i.e. communications, data processing or early-warning sensors) more vulnerable to cyberattacks. It is now thought possible that a cyberattack (i.e. spoofing, hacking, manipulation and digital jamming) could infiltrate a nuclear weapons system, threaten the integrity of its communications, and ultimately (and possibly unbeknown to its target) gain control of its (nuclear and non-nuclear) command and control systems. Footnote 24 AI technology has not yet evolved to a point where it would allow nuclear-armed states to credibly threaten the survivability of each other’s nuclear second-strike capability. Thus, the development trajectory of AI (and its enabling technologies) means its impact on nuclear security will likely be theoretical and speculative for the foreseeable future.\n\nBecause of the intense time pressures that would likely loom large with the decision to use nuclear weapons – especially where a state maintains a launch-on-warning posture – AI-enhanced",
    "cyberattacks against nuclear systems would be almost impossible to detect and the warning signals difficult to authenticate, let alone attribute, within the short timeframe for initiating a nuclear strike. According to open sources, operators at the North American Aerospace Defence Command (NORAD) have less than three minutes to assess and confirm initial indications from early-warning systems of an incoming attack. This compressed decision-making time frame could put political leaders under intense pressure to make a decision to escalate during a crisis, with incomplete (and possibly false) information of a situation. Ironically, new technologies designed to enhance information (i.e. modernised nuclear C3I systems augmented by 5G networks, machine learning, big-data analytics and quantum computing) can also undermine clear and reliable information flow and communication, critical for effective deterrence (Gartzke and Lindsay Citation 2019). Footnote 25\n\nAdvances in AI could also exacerbate this cybersecurity challenge by enabling improvements to the cyber offence. Machine learning and AI by automating advanced persistent threat (APT) operations might dramatically reduce the extensive manpower resources and high levels of technical skill required to execute APT operations (or 'hunting for weaknesses'), especially against hardened nuclear targets. Footnote 26 The machine speed of AI-augmented cyber tools could enable a low-skilled and capital-restricted attacker to exploit a narrow window of opportunity to penetrate an adversary's cyber-defences or use APT tools to find new vulnerabilities. For example, when docked for maintenance air-gapped nuclear-powered submarines, considered secure when submerged, could become increasingly vulnerable to a new generation of low-cost – possibly black-market – and highly automated APT cyberattacks.\n\nAn attacker could also apply AI machine learning techniques to target autonomous dual-use early-warning and other operating systems (e.g. C3I, ISR, early-warning and robotic control networks) with 'weaponized software' such as hacking, subverting, spoofing or tricking, causing unpredictable and potentially undetectable errors, malfunctions and behavioural manipulation to weapons systems – or 'data-poisoning'. Footnote 27 Furthermore, as the linkages between digital and physical systems (or the 'Internet of Things') expand, the potential for to an adversary to use cyberattacks in both kinetic and non-kinetic attacks will increase. A significant risk variable in the operation of autonomous systems is the time that passes between a system failure (i.e. performing in a manner other than how the human operator intended) and the time it takes for a human operator to take corrective action. If the system failure is the result of a deliberate act, this time frame will be compressed (Johnson Citation 2019a).\n\nEven if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. This skewed assessment by states in the context of nuclear weapons, which are ready to launch nuclear weapons at a moment's notice, would likely precipitate worst-case scenario thinking that may spark an inadvertent escalation (Talmadge",
    "Citation 2017). Footnote 28 During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 29 For example, an AI-enabled third party generated 'deepfake' (audio and video manipulation), coupled with data-poisoning cyberattacks, could spark an escalatory crisis between two (or more) nuclear states (Fitzpatrick Citation 2019). Footnote 30\n\nHow could AI-augmented cyber capabilities create new pathways for accidental or inadvertent escalation? To illustrate these dynamics: State A launches a malicious AI-enhanced cyberattack to spoof State B's AI-enabled autonomous sensor platforms and automated target recognition systems, in such a way that the weapon system (e.g. a human-supervised ATR system) is fooled into interpreting a civilian object as a military target. B in response, based on manipulated or erroneous information, and the inability of human supervisors to detect the spoofed imagery that fooled the weapons' automated target recognition algorithm in time to take corrective action, accidentally (and unintentionally) escalates a situation (Libicki Citation 2018). In this example, the spoofing attack on the weapon systems' algorithm is executed in such a way that the imagery appears to the recognition system as indistinguishable from a valid military target (Rohrbach et al. Citation 2018) Footnote 31 escalating a situation based on a false premise that would unlikely to fool the human eye. Footnote 32\n\nAlso, the explainability (or 'black box') problem associated with AI applications could compound these potential escalation dynamics. Footnote 33 Insufficient understanding of how and why AI algorithms reach a particular judgment or decision might complicate the task of determining whether data sets had been deliberately compromised to manufacture false outcomes (e.g. attacking incorrect targets or misdirecting allies during combat). Footnote 34 Moreover, as humans and AI team up to accomplish particular missions, the opacity associated with how AI systems reach a decision may cause an operator to have either too much or too little confidence in a system's performance. As a result, unless the system's machine learning algorithm is terminated, at the end of the training phase, once deployed, it could potentially learn something it was not intended to, or even perform a task or mission that its human designers do not expect it to do. Footnote 35\n\nIn sum, technologies that improve the reliability and speed with which information is processed and disseminated by early-warning systems may paradoxically also increase the vulnerabilities of these networks, creating new first-mover advantages and escalation pathways during a crisis that spark unintentional (or intentional) warfare (Schneider Citation 2019). Footnote 36 Put another way, AI might be developed in order to enable a state to pursue a pre-determined escalatory path. Thus, increased escalation risk as a result of technology is not always inadvertent or accidental (Long and Rittenhouse Green Citation 2015). Footnote 37",
    "# 'Cyber guns' supercharged with AI machine learning\n\nWhile manipulations and system subversions are possible with existing cyber offence tools, rapid advances in AI and increasing degrees of military autonomy could amplify the potential speed, power and scale of future attacks in cyberspace. Footnote 38 There are three significant intersections between AI systems and cybersecurity that are most salient to the military domain: (1) advances in autonomy and machine learning techniques mean that a much broader range of physical systems are now vulnerable to cyberattacks (i.e. hacking and data-poisoning) (Greenberg Citation 2016); Footnote 39 (2) cyberattacks on AI systems can offer attackers access to machine learning algorithms, trained models being used by the application and a potential vast amounts of data from facial recognition and intelligence collection and analysis systems (e.g. satellite navigation and imagery systems used to cue precision munitions and supported intelligence, surveillance, reconnaissance missions); and (3) the use of AI systems in conjunction with existing cyber offence tools will enable sophisticated cyberattacks to be executed at larger scale (both geographically and across networks), at faster speeds, and simultaneously across multiple military domains with improved anonymity. In short, despite the relatively benign augmentation mechanisms used to develop cyber offensive capabilities with AI systems, the resultant impact on the speed and scope of AI cyber tools will likely have destabilizing effects, nonetheless. Footnote 40\n\nRecent advances in machine learning have significantly contributed to resolving several technical bottlenecks in several fields of AI, which could allow for significant qualitative improvements to a large variety of autonomous weapon systems and applications. In combination, machine learning and autonomy could intersect with nuclear security in a multitude of ways, with both positive and negative implications for strategic stability. For example, machine learning could enhance the detection capabilities of (nuclear and non-nuclear) early-warning systems and improve the collection and cross-analysis of ISR information, thus reducing the risk of inadvertent and accidental escalation. Machine learning may also bolster the cyber-security of command and control systems, create new and possibilities for arms control, testing, verification and monitoring nuclear stockpiles, and enhance conventional counterforce capabilities. Unmanned autonomous systems could be deployed in complex missions in hitherto inaccessible and cluttered environments (e.g. under-sea anti-submarine warfare) and aerial and underwater drones in swarms might eventually replace intercontinental ballistic missiles (ICBMs) and nuclear-armed submarines (SSBNs) for the delivery of nuclear weapons.\n\nIn response to these anticipated vulnerabilities, and to achieve the (perceived) first-mover advantages the increased speed in AI-augmented cyber tools could confer, China, Russia and the United States have continued to harden their AI cyber defences. Despite these parallel efforts, divergent views exist on how strategic communities perceive the nature of the risks posed by AI-enhanced cyberattacks against dual-use C3I systems. Thus, AI-augmented cyber intelligence-",
    "gathering tools targeting an adversary's command and control assets (especially dual-use facilities) during a crisis may be misinterpreted as a prelude to an imminent pre-emptive attack on its nuclear force and therefore as undermining strategic stability.\n\nMoreover, AI machine learning could enable new offensive concepts such as a so-called 'left-of-launch operation' (Ellison Citation 2015), Footnote  $^{41}$  thereby compounding the ambiguities and fear about pre-emptive attacks in cyberspace, and increasing first-mover incentives – or use-them-or-lose-them situations (Buchanan Citation 2017; Buchanan and Miller Citation 2017). Footnote  $^{42}$  In the cyber domain, it is generally unclear in the early stages of a cyber operation whether an adversary intends to collect intelligence or prepare for an offensive attack, which is more likely to be used early on in a conflict scenario. Open sources suggest, for example, that Chinese analysts view the vulnerability of China's nuclear command, control, and communications (C3I) systems to cyber infiltrations – even if an attacker's objective was limited to cyberespionage – as a highly escalatory national security threat. By contrast, Russian analysts tend to view Russia's nuclear C3I network as relatively isolated, and thus insulated from cyberattacks.\n\nIrrespective of these differences, the uncertainty caused by the possibility, actual or otherwise, that AI-enhanced offensive cyber tools might be used (or threatened) to undermine the credibility and reliability of states' nuclear forces would be destabilizing, both as result of the existence of enhanced cyber offensive capabilities and the potential efficacy of these tools against nuclear forces. As the historical record attests, rational actors have incentives to misrepresent what they know (i.e. lie and bluff) in order to improve the terms of any settlement that may emerge from the wartime bargaining process – or the risk-reward payoff. Even a modicum of uncertainty about the effectiveness of AI-augmented cyber capabilities during a crisis or conflict would, therefore, reduce both sides' risk tolerance and increase the incentive to strike pre-emptively as a hedging strategy.\n\nA virtuous cycle that might flow from enhanced reassurances premised on comprehensive intelligence would require information symmetry (i.e. equal access to intelligence and analysis systems) between great and rising powers, and shared confidence in the accuracy and credibility of these systems. Perhaps most challenging of all in a world of 'revisionist' rising powers, the intentions of all rival states would need to be genuinely benign for this optimistic view to occur. Under crisis conditions, for example, an offensive AI cyber tool that succeeds in compromising an adversary's nuclear weapon systems, resulting in an 'asymmetric information' situation, could cause either or both sides to overstate (or understate) its retaliatory capabilities, and in turn, be more inclined to act in a risky and escalatory fashion. In short, in a competitive strategic environment, where states are inclined to assume the worst of others' intentions, one state's efforts to enhance the survivability of its strategic forces may be viewed by others as a threat to their nuclear retaliatory capability – or second-strike capacity.",
    "# Manipulation of the information landscape\n\nWhile machine learning big-data analytics, sensing technology, networks supported by 5G networks, could alert commanders of incoming threats with increased speed and precision, and make accidents caused by human error potentially less likely (especially accidents and malfunctions caused by false warning), these advances could simultaneously amplify escalation risks in two ways. Footnote 43\n\nFirst, AI machine learning used as force multipliers for cyber offence (e.g. data poisoning spoofing, 'deepfakes', manipulation, hacking and digital jamming) would be considerably more difficult to detect – especially if an attacker used AI advanced persistent threat (APT) tools – or 'hunting for weaknesses' in spectrum-contested environments. Second, in the unlikely event an attack was successfully detected, threat identification (or attribution) at machine speed would be virtually impossible. In addition to the co-mingling and the speed of warfare issues, AI machine learning systems might also exacerbate the existing – well-established and non-kinetic – risk of inadvertent (and intentional) escalation from the manipulation of the information landscape in which decisions about nuclear weapons are located. For example, the use of deepfake videos built-in real-time and disseminated online for malicious purposes.\n\nHuman cognition, and thus effective deterrence and signalling intentions, is predicated on reliable and clear information; if an adversary is concerned that the information available to them is limited (or worse, inaccurate), they will likely assume the worst and act accordingly. Asymmetric information situations between rivals about the balance of military power could undermine crisis stability, and in turn, create rational incentives to escalate to nuclear confrontation. Consequently, states will be more inclined to assume the worst of others' intentions, especially in situations where the legitimacy of the status quo is contested (i.e. maritime Asia).\n\nIn the competitive strategic environment, it is easy to imagine unprovoked escalation caused by a malicious third-party (or state-proxy) clandestine action. Even if nuclear early-warning systems might eventually detect the subversion, heightened levels of uncertainty and tension caused by an alert may impel the respective militaries to put their nuclear weapons on high alert status. During a crisis, the inability of a state to determine an attacker's intent may lead an actor to conclude that an attack (threatened or actual) was intended to undermine its nuclear deterrent. Footnote 44 For example, in an effort to incite conflict between two rival states, State A uses proxy hackers to use a AI machine learning technique (i.e. generative adversarial networks GAN) to launch 'deepfake' video or audio material, depicting senior military commanders of State B conspiring to launch a pre-emptive strike on State C. Footnote 45 Then, this 'deepfake' footage is deliberately leaked into C's AI-augmented intelligence collection and analysis systems, provoking C to escalate the situation with strategic consequences. B responds in kind. Footnote 46",
    "The current underdeveloped state of ‘counter-AI’ capabilities and other fail-safe mechanisms (e.g. circuit breakers) to de-escalate escalation in cyberspace, will make the unprovoked and unintentional escalation dynamics depicted in this scenario very challenging to mitigate. Moreover, in the emerging ‘deepfakes’ arms race (much like cybersecurity more generally), detection software will likely lag behind advances in offensive enabling solutions – or offence-dominant ones. According to computer science expert Hany Farid, there are probably 100–1000 times ‘more people developing the technology to manipulate content than there is to detect [it]’ (Fontaine and Frederick Citation 2019).\n\nThe overreliance on automation (or ‘automation bias’) in the deployment of increasing complex AI-augmented capabilities – such as cyber, C3I systems, AWS and precision missile munitions – designed to provide a distinct tactical advantage of machine-speed, will increase the vulnerability of these capabilities to exploitation. Moreover, the increasing substitution of human cognition for logic derived from machines will likely increase the opportunities for adversaries to exploit the limitations of narrow AI technology, i.e. lack of human intuition, brittleness in complex real-world situations and an inability to effectively detect or counter manipulation attacks (Libicki Citation 2018). Footnote 47 Therefore, until researchers unravel some of the unexplainable features of AI, human error and machine error will likely compound one another, with unpredictable results. Footnote 48 Simply put, we are at a critical crossroads in the parallel (and symbiotic) evolution of AI and cyberspace that national security communities, globally, will need to prepare for proactively (Johnson Citation 2019a).\n\n## Policy interventions\n\nA prominent theme that runs through this article – and central to understanding the potential impact AI for strategic stability and nuclear security more broadly – is the concern that AI systems operating at machine-speed will push the pace of combat to a point where the actions of machines surpass the (cognitive and physical) ability of human decision makers to control (or even comprehend) events.\n\nPossible multi-track policy responses to push back against the threat posed to stability from AI in a multipolar can be broadly categorized into: (1) those that focus on enhancing debate and discussion between researchers, global defence communities, decision makers, academics and other political and societal stakeholders; and (2) a range of specific policy recommendations and tools for great military powers to negotiate and implement.\n\n## Enhancing debate and dialogue\n\nFirst, to mitigate (or at least manage) the destabilizing and escalatory risks posed by the AI-cybersecurity nexus, great military powers must closely coordinate their confidence-building",
    "measures in an effort to pre-empt some of the risks to stability outlined above. Track-1 and track-2 discussions should include diplomats and military leaders (especially from China, Russian and the United States), industry experts, AI researchers and multi-disciplinary academics. Specifically, great military powers should establish an international framework for governance, norms, behaviour and the regulation of policy on machine learning-augmented cyber capabilities. Resistance to these efforts will likely come from states who worry that in revealing their cyber (especially offensive) capabilities, they could upend the deterrence utility of these tools.\n\nTo be sure, the challenges posed in the coordination and implementation of these policies will require bold and visionary leadership to circumvent the inevitable regional agendas, interdisciplinary resistance and burgeoning security dilemmas between rival states. Because of the rapid technological change in AI formal treaties, associated with arms-control agreements that require lengthy and complex negotiation and ratification processes, legal frameworks risk becoming obsolete before they come into effect. The historical record has demonstrated on several occasions that these kinds of challenges facing humanity can be overcome (Krimsky Citation 1962; Naur and Randell Citation 1968). Footnote 49\n\nSecond, the think tank community, academics and AI research experts should pool their resources to investigate the implications of the AI-cyber nexus for a range of potential security issues such as: the impact of AI bias on future military-use applications; how to prepare for (and react to) artificial general intelligence; and measures to mitigate, prevent and manage offensive uses of AI. Footnote 50 If necessary, steps could be taken to amend existing legal definitions of offensive cyber operations, such as hacking, data-poisoning attacks and spoofing, to account for the increased proliferation and potential damage caused by AI.\n\nThird, and related, due to the intrinsic dual-use nature of the AI-cyber security problem set, this dialogue should also be expanded to include other stakeholders such as private-sector AI and cybersecurity experts, the commercial sector, ethicists, philosophers, civil society and public opinion. Footnote 51 States should also collaborate on dual-use AI research to leverage AI's low-cost and scaling advantages (i.e. in autonomy and robotics). Further, a focus on the safety, testing and robustness of AI systems is a critical step in mitigating potential vulnerabilities and risks caused by errors, bias, and explainability in uncontrolled and complex environments. Footnote 52 In the case of offensive AI-related cybersecurity research, where public distribution might cause vulnerabilities and worsen security, publication could be confined to trusted organisations and entities.\n\nBest practices that exist in more mature methods for addressing dual-use concerns, like computer security, where applicable, might be applied to AI. For example, the wide use of red teaming exercises to enhance network security, organisation and practices. Specifically, AI-cyber red teaming – like DARPA's Cyber Grand Challenge – will enable engineers and operators to better",
    "understand the skills needed to execute particular offensive and defensive operations (especially using machine learning techniques), and to better manage system vulnerabilities, adversarial exploitation, stress-testing and social engineering challenges (Anderson, Woodbridge, and Filar Citation 2016).\n\nA recent study explored case studies of previous examples (biological weapons, cryptography and nuclear technology) of dual-use technologies – with civilian and military uses – to examine potential insights for AI dual-use risk management policies such as export controls and prepublication reviews. These insights also highlight the potential challenges of establishing regulatory, legal and normative frameworks for dual-use technologies – for example, the cautionary tale of ineffective efforts, in the late 1990s, to regulate cryptographic algorithms and cyber-network security tools through export controls.\n\nThe extent to which the integration of AI into dual-use weapon systems (such as cybersecurity) might influence actors' attitude to risk, the offence-defence balance and, in turn, perceptions of others' intentions and capabilities, could have profound implications for strategic deterrence and nuclear security. Footnote 53 In sum, cognizant that some states have deployed (or imminently plan to deploy) AI systems, experts generally agree that AI requires further experimentation, testing and development before being integrated into lethal weapon systems and their decision-making support systems.\n\n# Taking the lead from the cybersecurity community\n\nSome examples of cybersecurity-centered measures that might be further researched and implemented to pre-empt and mitigate some of risks posed by AI-augmented cyberattacks (especially involving safety-critical nuclear systems) highlighted in this paper include the following.\n\nFirst, coordinating AI-simulated war games, red teaming creative thinking exercises and creating redundancies (i.e. back-ups or fail-safes) in networks to detect errors, fix vulnerabilities and increase the reliability and robustness of military (especially nuclear-centric) systems. Specific procedures could be implemented to enable confidential reporting and fixing the vulnerabilities, subversions and other kinds of manipulations detected in AI systems. These findings could be used to track the proliferation of AI-related incursions and then countermeasures could be developed and standardised to manage these threats. Footnote 54\n\nSecond, states should formalise verification methods and protocols and consider issues such as: to what extent, under what circumstances, and for what types of AI systems can formal verification be implemented? Might other approaches be developed to achieve similar goals (e.g. machine learning and big data analysis augmented verification methods)? For example, DARPA's Assured Autonomy Program combines data-driven machine learning algorithms to assure the",
    "safety of autonomous cyber physical systems (air, ground, sea and undersea unmanned vehicles), which continue to learn throughout their lifespans.\n\nThis learning characteristic makes assurance or verification using traditional methods especially challenging. Whether AI applications in the military domain can be formally verified, for now, is an unanswered question. Footnote 55 Much like that of cyber systems, the complexity of AI systems and the difficulty of defining their properties for formal verification makes them less amenable to verification compared to other types of technology.\n\nThird, the global defence communities should actively invest in the development of AI cyber-defence tools (e.g. analysing classification errors, automatic detection of remote vulnerability scanning and model extraction improvements), AI-centric secure hardware, and other fail-safe mechanisms, to allow for de-escalation and to prevent unintentional or accidental escalation.\n\nSeveral issues that need further exploration related to these measures include: the extent to which existing tools might be effective against vulnerabilities in AI systems? How can these tools be tailored for AI systems across multiple military domains? Is there an equivalent to 'patching' in military AI systems? What kinds of policies might incentivize, and ensure compliance with, meaningful reforms to existing hardware in the military sphere? While these questions are challenging, and necessarily speculative for now, answers will become more evident as the technology matures.\n\n# Disclosure statement\n\nNo potential conflict of interest was reported by the author."
  ],
  "metadata": {
    "title": "ABSTRACT",
    "subtitle": "",
    "document_type": "unknown",
    "venue": "",
    "publication_year": null,
    "authors": [],
    "affiliations": [],
    "emails": [],
    "orcids": [],
    "corresponding_author_line": "",
    "abstract": "How could AI-infused cyber capabilities be used to subvert, or otherwise compromise, the reliability, control and use of states' nuclear forces? This article argues that a new generation of artificial intelligence (AI) enhanced cyber capabilities will amplify the risk of inadvertent escalation caused by the co-mingling of nuclear and strategic non-nuclear weapons and the increasing speed of warfare, thereby increasing the risk of nuclear confrontation. It examines the potential implications of cyber (offensive and defensive) capabilities augmented with AI applications for nuclear security. The article concludes that future iterations of AI-enhanced cyber counterforce capabilities will complicate the existing challenges of cyber defence, and in turn, compromise nuclear assets and increase the escalatory effects of offensive cyber capabilities.",
    "keywords": [],
    "publication_dates": {},
    "identifiers": {
      "doi": [],
      "issn": [],
      "isbn": [],
      "arxiv": [],
      "pmid": [],
      "pmcid": [],
      "urls": []
    },
    "references_block_count": 0,
    "references_entries_estimated": 0,
    "heading_count": 12,
    "max_heading_level": 2,
    "partial_document": {
      "is_partial_document": false,
      "reasons": [
        "no_authors",
        "no_identifier"
      ],
      "toc_dot_lines": 0
    }
  },
  "validation": {
    "dominant_quality": {
      "intext_total": 1,
      "index_coverage": 1.0,
      "intext_citation_coverage": 1.0,
      "preceding_text_coverage": 1.0,
      "footnote_coverage": 1.0,
      "unique_index_count": 1
    },
    "footnotes_quality": {
      "intext_total": 15,
      "index_coverage": 1.0,
      "intext_citation_coverage": 1.0,
      "preceding_text_coverage": 1.0,
      "footnote_coverage": 0.0,
      "unique_index_count": 15,
      "items_total": 0
    },
    "style_validation": {
      "detected_style": "tex_superscript",
      "recommended_style": "unknown",
      "aligned": true,
      "signals": {
        "superscript_hits": 0,
        "superscript_definition_lines": 0,
        "numeric_bracket_hits": 0,
        "numeric_endnote_lines": 0,
        "author_year_hits": 0
      }
    },
    "coverage_validation": {
      "dominant_bib_total": 1.0,
      "dominant_bib_coverage_rate": 1.0,
      "dominant_link_target": "footnotes",
      "dominant_unresolved_flag": "unresolved_footnote_links"
    },
    "heading_validation": {
      "heading_count": 12,
      "max_heading_level": 2,
      "level_jump_violations": 0,
      "numbering_parent_violations": 0,
      "has_reference_heading": false,
      "has_subheadings": true
    },
    "metadata_validation": {
      "field_presence": {
        "title": true,
        "authors": false,
        "affiliations": false,
        "emails": false,
        "orcids": false,
        "abstract": true,
        "keywords": false,
        "venue": false,
        "persistent_identifier": false,
        "headings": true,
        "partial_document": false
      },
      "counts": {
        "authors": 0,
        "affiliations": 0,
        "emails": 0,
        "orcids": 0,
        "keywords": 0,
        "doi": 0,
        "issn": 0,
        "isbn": 0,
        "arxiv": 0,
        "pmid": 0,
        "pmcid": 0,
        "urls": 0
      },
      "coverage": {
        "core_coverage": 0.5,
        "email_author_link_rate": 0.0
      },
      "partial_document": {
        "is_partial_document": false,
        "reasons": [
          "no_authors",
          "no_identifier"
        ],
        "toc_dot_lines": 0
      },
      "flags": [
        "meta_missing_authors",
        "meta_missing_persistent_identifier"
      ]
    },
    "flags": [
      "footnotes_bucket_unresolved",
      "missing_reference_heading",
      "meta_missing_authors",
      "meta_missing_persistent_identifier"
    ]
  },
  "citation_summary": {
    "style": "tex_superscript",
    "dominant_bucket": "tex",
    "dominant": {
      "intext_total": 1.0,
      "success_occurrences": 1.0,
      "success_unique": 1.0,
      "bib_unique_total": 1.0,
      "occurrence_match_rate": 1.0,
      "bib_coverage_rate": 1.0,
      "success_percentage": 100.0,
      "missing_intext_expected_total": 12.0,
      "highest_intext_index": 13.0,
      "missing_footnotes_for_seen_total": 0.0,
      "uncited_footnote_total": 0.0,
      "style": "tex_superscript"
    },
    "buckets": {
      "footnotes": {
        "intext_total": 15.0,
        "success_occurrences": 0.0,
        "success_unique": 0.0,
        "bib_unique_total": 0.0,
        "occurrence_match_rate": 0.0,
        "bib_coverage_rate": 0.0,
        "success_percentage": 0.0,
        "missing_intext_expected_total": 27.0,
        "highest_intext_index": 42.0,
        "missing_footnotes_for_seen_total": 15.0,
        "uncited_footnote_total": 0.0,
        "style": "footnotes"
      },
      "tex": {
        "intext_total": 1.0,
        "success_occurrences": 1.0,
        "success_unique": 1.0,
        "bib_unique_total": 1.0,
        "occurrence_match_rate": 1.0,
        "bib_coverage_rate": 1.0,
        "success_percentage": 100.0,
        "missing_intext_expected_total": 12.0,
        "highest_intext_index": 13.0,
        "missing_footnotes_for_seen_total": 0.0,
        "uncited_footnote_total": 0.0,
        "style": "tex_superscript"
      },
      "numeric": {
        "intext_total": 0.0,
        "success_occurrences": 0.0,
        "success_unique": 0.0,
        "bib_unique_total": 0.0,
        "occurrence_match_rate": 0.0,
        "bib_coverage_rate": 0.0,
        "success_percentage": 0.0,
        "missing_intext_expected_total": 0,
        "highest_intext_index": 0,
        "missing_footnotes_for_seen_total": 0,
        "uncited_footnote_total": 0,
        "style": "numeric"
      },
      "author_year": {
        "intext_total": 0.0,
        "success_occurrences": 0.0,
        "success_unique": 0.0,
        "bib_unique_total": 0.0,
        "occurrence_match_rate": 0.0,
        "bib_coverage_rate": 0.0,
        "success_percentage": 0.0,
        "missing_intext_expected_total": 0,
        "highest_intext_index": 0,
        "missing_footnotes_for_seen_total": 0,
        "uncited_footnote_total": 0,
        "style": "author_year"
      }
    }
  },
  "updated_at_utc": "2026-02-14T08:16:54.676486+00:00"
}