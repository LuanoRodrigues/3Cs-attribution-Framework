{
  "pdf_path": "C:\\Users\\luano\\Zotero\\storage\\PGNDKI3W\\ssrn_id1928870_code427934.pdf",
  "custom_id": "373",
  "response": {
    "id": "batch-b6235e9e-374-759b8992-b652-4dfa-a14d-cd79218da2eb",
    "custom_id": "373",
    "response": {
      "status_code": 200,
      "body": {
        "pages": [
          {
            "index": 0,
            "markdown": "Electronic copy available at: https://ssrn.com/abstract=1928870\n\n# Leverage in cyberspace, without deterrence\n\nBenjamin D. Mazzotta\n\nThe Fletcher School of Law and Diplomacy, Tufts University\nbenmazzotta@alumni.tufts.edu\n\nMarch 19, 2011\n\nPaper presented to the Annual Convention of the International Studies Association.\nMontréal, QC.\n\n## Abstract\n\nDeterrence theory, which was of enormous value during the nuclear era, cannot adequately prepare defense thinkers for future cyber conflicts. Instead, the best states can hope for is robust cooperation with other states and with private actors to improve their circumstances in future conflicts. A number of key assumptions that supported nuclear deterrence do not hold in the cyber domain: immediately observable attacks; clear legal status under the law of armed conflict; states' preeminence for research and development; high financial and technical barriers to entry; cheap observation of delivery facilities; feasible second strike; and a stable MAD outcome. States can create several incentives for improved collaboration with allies and private sector partners. States can derive mutual benefits from shared intelligence; shared standards for risk assessment; joint exercises; and standby arrangements for crucial information, equipment, materials, and services. Apart from sanctions against access to these carrots, sticks include reconnaissance for cyber conflicts, cyber exploitation with the threat of disclosure, and policies explicitly linking cyber events to non-cyber diplomatic and security concerns. States have additional leverage over private actors through commercial regulation, law enforcement, and liability; although the burden of compliance may poison public-private cooperation.\n\n# 1 Introduction\n\nCyber attacks have generated enormous controversy in international relations along the lines of a recurrent *Late Show with David Letterman* segment, \"Is this anything?\" Cyber hawks point to the far-reaching effects and cost-efficacy of attacks (Bajaj, 2010; Borg, 2005). Skeptics accuse hawks of crying Chicken Little over simple web defacement (Singel, 2010). Cyber war has captured Hollywood's imagination at the strategic level as in *Die Hard IV: Live Free or Die Hard*; at the tactical level as in *Vantage Point*; and oftener as dystopian techno-singularity as in *Eagle Eye*, *The Matrix*, and",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 1,
            "markdown": "2 DETERRENCE THEORY\n\n2001: A Space Odyssey. Journalists (Gorman, 2010), freelancers (Thornburgh, 2005), think tanks (Krekel, 2009; Walton et al., 2009), business (Falliere et al., 2010; Langevin et al., 2008; Allen et al., 2005; Tsipenyuk et al., 2005), military doctrine (Joint Chiefs of Staff, 2007, 2000), government documents (White House, 2009, 1998) and IR theorists (Clarke and Knake, 2010; Lynn, 2010; Morgan, 2010; Nye, 2010; Clarke, 2009; Owens et al., 2009; Liang and Xiangsui, 2002; Rattray, 2001) describe broad classes of attacks whose objectives might include the disruption of civilian infrastructure; tactical advantage in conventional war; intelligence gathering; financial gain; and terrorism. This article assumes the feasibility of broad classes of attack as a moot point, and outlines the strategic consequences of those attacks as if they were known capabilities.\n\nFaced with similarly disruptive military technology in the 1940s, the discipline of international relations refined the concept of deterrence to devise a strategy for safe navigation of potentially catastrophic nuclear arms races. Nuclear theorists then and cyber theorists now pointed out strikingly similar aspects of the new technology: vast geographic reach of the attacks, revolutionary intensity of purported attack effects, and relative cost efficacy of the new weapons. Cyber attacks may promise a revolution in military affairs similar to the nuclear revolution. Nuclear theorists then focused on several features of the new technology and constructed a strategic equilibrium from which deviance was less likely. Cyber theorists now must look carefully at the features of the new technology to assess its strategic consequences.\n\nDeterrence in cyberspace is a mirage. Cyberspace itself bears little resemblance to the physical world around us, and efforts to reason by analogy with physical space often do more harm than good. Nuclear deterrence is a particularly poor strategic model for cyberspace. The best we can hope for in cyberspace is to enlist the aid of allies who may cooperate for mutual benefit. There are specific sources of leverage that we have in cyberspace, thanks to our unusual capabilities in government and the private sector. Only when taken out of context, may these levers of control be shoehorned into a purported model of deterrence. Faith in deterrence may trick us to trust our own defenses, and blind us to opportunities for alliance.\n\n## 2 Deterrence theory\n\n### 2.1 Incentives\n\nDeterrence works, in theory, because of a brutally simple set of incentives facing the state. State behavior under the security dilemma is the basis of deterrence. Aggressor states seek to gain power through aggressive wars. Strategic competitors promise retaliation for these wars that more than wipes out the gains from expansion.\n\nDeterrence theory takes as its point of departure that the state is a rational actor (Achen and Snidal, 1989; Brodie, 1978), subject to an inescapable calculus of self-interest (Waltz, 1990). Various critiques of the rational actor model of the state in international relations have been advanced, including unraveling of deterrence commitments (Morgan, 1977), well-established cognitive and behavioral deviance from simple economic self-interest (Kahneman and Tversky, 2000, 1979; Tversky and Kahneman, 1974); status-seeking behaviors in nuclear weapons development (Art, 1980);\n\nElectronic copy available at: https://ssrn.com/abstract=1928870",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 2,
            "markdown": "2 DETERRENCE THEORY\n\ntaboos in tactical deployment of nuclear weapons (Tannenwald, 1999; Dunn, 1994); and constructivist narratives of leadership and the national interest.\n\nJervis (1979) describes waves of innovation designed to address shortcomings of deterrence theory. The major features of these second-wave models are the aggressor’s political calculus; the possibility of rewards and negotiations as complements to simple retaliation; and constructivist interpretations of the aggressor’s goals in security. Many of these criticisms are further explicated in Taliaferro (2001), which organizes contemporary variants of realist international security theory.\n\nNecessary conditions for effective deterrence vary greatly from one variant of the theory to another. Waltz (1990) holds that the key to nuclear deterrence is a reliable second strike that eliminates potential gains from aggression. He further argues that the low cost and weight of nuclear weapons leads inexorably to defensive vulnerability. Brodie (1978) argued that deterrence applied mainly to the Soviet-American security dilemma, and largely on the basis of Soviet acquisition of nuclear weapons. Extended deterrence modeled the behavior of allies within this bilateral framework.\n\n### 2.2 Technology\n\nKey features of nuclear technology circa 1960 were ease of observing weapons deployment at various stages, including launch site construction, weapons launch, and successful attack sites; extreme lethality and blast radius; absence of viable defensive countermeasures; difficulty of developing nuclear weapons, measured by financial costs, knowledge, and manufacturing capabilities; and limited means of delivery, viz., aircraft, oceangoing vessels, and ballistic missile. These points are not controversial. Efforts to overcome these technical limits of nuclear weapons (e.g., anti-ballistic missile defenses) attracted opprobium, as they threatened the stability of the mutually assured destruction (MAD) equilibrium (Dunn, 1994).\n\nNuclear technology itself ensures that the adversary is immediately aware of any attacks; that a short list of suspects are considered capable of an attack; and that, in fact, at least one from the short list must have participated in the attack at some level. Dunn (1994) notes that American strategists ignored the possibility of nuclear weapons smuggling for just that reason. Satellite and aircraft observations were long considered sufficient to monitor nuclear weapons deployment. More exhaustive inspections of suspected facilities featured prominently in Iran, Iraq, and North Korea; but the nature of weapons development has changed considerably since the early nuclear age. Today critical materials are widely available, e.g., nuclear expertise, raw fuel, and industrial components for enrichment and delivery systems. This was absolutely not the case during the early nuclear age.\n\n### 2.3 Deterrence in cyberspace\n\nDeterrence, having served America so well during the Cold war, was an obvious place to begin discussions of cyber strategy. McConnell (2010), in an op-ed on cyber strategy, poses a choice between deterrence and preemption as the central strategic question in cyberspace. He further argues that deterrence rests on four necessary capabilities: a declared retaliation policy, attribution of the attack to the attacker, location of the\n\nElectronic copy available at: https://ssrn.com/abstract=1928870",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 3,
            "markdown": "attacker, and the counterstrike itself. If the layman’s definition of deterrence is that we can make ourselves safe in cyberspace without shooting first, then let us all hope that McConnell is correct. On the other hand, there is no consensus among experts that the four necessary conditions McConnell cites are even feasible, let alone likely to be achieved.\n\nThe National Research Council hosted an essay competition and conference in 2010 dedicated to the feasibility of deterrence in cyberspace. Patrick Morgan, a long-standing skeptic of deterrence theory, questions the applicability of nuclear deterrence to cyberspace *(Morgan, 2010)*. *Lukasik (2010)* points to serious flaws in a wide variety of declaratory policies on deterrence, such as the threat to retaliate with conventional lethal force. *Clark and Landau (2010)* explain stubborn technical problems with attribution. *Rattray and Healey (2010)* give an overview of offensive cyber weapons, some of which are plausible for retaliation. *Schmitt (2010)* covers the legality of national self-defense under cyber attack; and in a related paper *Sofaer et al. (2010)* outline hypothetical features of international institutions that would purport to ameliorate cyber security.\n\n## 3 A simple model\n\nSuccessful nuclear deterrence relies on two crucial actions that are trivial in early Cold War nuclear conflict; but that are well-established problems in cyber conflict. The adversary must observe that an attack has in fact occurred. The adversary must also identify the attacker. This section of the article develops a quick Bayesian model of the adversary’s problem, with plausible hypotheses about the likely identity of the attacker.\n\nThis framework leads to several simple questions about attack technology and the adversary’s decisions.\n\n1. How difficult is it to detect a given attack?\n2. How difficult is it to identify the attacker?\n3. How likely is the adversary to retaliate on mere suspicion of an attack?\n4. How likely is the adversary to retaliate upon a merely suspected attacker?\n5. How likely is the adversary to lash out at the wrong attacker?\n\nA convenient starting place is to model the expected utility of the attacker’s payoffs over the future states of the world: viz., attacks with retaliation, attacks without retaliation, and status quo.\n\n$U\\equiv u(\\mathbf{E}\\pi)$ (1)\n$\\mathbf{E}\\pi=p_{a}\\pi_{a}+p_{a}\\left(p_{r}\\pi_{r}+p_{\\sim r}\\pi_{\\sim r}\\right)+\\left(1-p_{a}\\right)\\pi_{s}$ (2)\n\nwhere",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 4,
            "markdown": "3 A SIMPLE MODEL\n\n|   | where  |\n| --- | --- |\n|  U: | expected utility  |\n|  a: | subscript for attack  |\n|  s: | subscript for status quo  |\n|  p: | probability  |\n|  π: | payoff  |\n|  r: | subscript for retaliation  |\n|  ~r: | subscript for nonretaliation  |\n\nWe can further decompose the probability of retaliation into separate component probabilities, as in (3). The adversary needs to detect that the attack has in fact taken place, attribute the source of the attack, and then decide to respond to the attack. Early Cold War deterrence proceeded as a special case of the equation below, where virtually any attack would lead to certain attribution. Today adversaries may only be able to suspect the adversary's identity; or suspect that an attack has in fact taken place; or have only a rough suspicion on both counts. $^{1}$\n\n$$\np _ {r} = \\underbrace {p _ {d , i} p _ {r | D \\cap I}} _ {\\text {certain attribution}} + \\underbrace {p _ {d , \\sim i} p _ {r | D \\cap \\sim I}} _ {\\text {unidentified attacker}} + \\underbrace {p _ {\\sim d , i} p _ {r | \\sim D \\cap I}} _ {\\text {suspected attack}} + \\underbrace {p _ {\\sim d , \\sim i} p _ {r | \\sim D \\cap \\sim I}} _ {\\text {rough suspicion}} \\tag {3}\n$$\n\nAnd when the adversary may mistake the identity of the attacker, as in (4), the attacker may also err in retaliation, with potentially (dis)adventaneous payoffs for the attacker.\n\n$$\n\\mathbf {E} \\pi = \\underbrace {p _ {\\mathrm {a}} \\pi_ {\\mathrm {a}}} _ {\\text {attack's benefit}} + \\underbrace {p _ {\\mathrm {a}} \\left(p _ {\\mathrm {r}} \\pi_ {\\mathrm {r}} + p _ {\\sim \\mathrm {r}} \\pi_ {\\sim \\mathrm {r}}\\right)} _ {\\text {retaliation's cost}} + \\underbrace {p _ {\\mathrm {d} , \\sim \\mathrm {i}} p _ {\\mathrm {m} | \\mathrm {D} \\cap \\sim \\mathrm {I}} + p _ {\\mathrm {s}} \\pi_ {\\mathrm {s}}} _ {\\text {mistaken counterattack}} \\tag {4}\n$$\n\n|   | where  |\n| --- | --- |\n|  pr: | probability of retaliation  |\n|  D: | successful detection of an attack  |\n|  ~D: | failure to detect an attack  |\n|  I: | successful identification of an attacker  |\n|  pr|D∩I: | probability of retaliation conditional on adversary's detection without identification  |\n|  pd: | probability of adversary's detecting attack  |\n|  pi: | probability of adversary's identifying attack  |\n|  pm: | probability of a mistaken counterattack  |\n\nThere are stark differences between nuclear attacks circa 1960, modeled in (5) and cyber attacks, modeled in (6). $^{2}$  Nuclear attacks are obvious by the scale of their destruction. Attribution was simple, due to the small number of states in possession of the technology. There are no serious barriers to acquisition of technical expertise or requisite hardware for cyber attack. There is no reason to suspect that the adversary\n\nElectronic copy available at: https://ssrn.com/abstract=1928870",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 5,
            "markdown": "3 A SIMPLE MODEL\n\nwill understand that an attack will be detected until long after it has begun. There is no reason that the adversary will immediately suspect whether the adversary in a given cyber attack is a state or not a state.\n\n$$\np _ {d i} \\gg p _ {d, - i} &gt; p _ {- d} \\tag {5}\n$$\n\n$$\np _ {d, - i} &gt; p _ {- d, i} &gt; p _ {d, i} &gt; p _ {- d, - i} \\tag {6}\n$$\n\nThe prior probabilities for identifying the sources of detected nuclear attacks are also much simpler than those of detected cyber attacks. There are many fewer likely suspects for a nuclear attack, as in (7) than for a nuclear attack, as in (8).\n\n$$\n\\text {P r i o r s}: \\left\\{ \\begin{array}{l l} \\sum p _ {H 1} \\dots p _ {H 1 0} = . 9 5 &amp; \\text {d e c l a r e d a n d k n o w n n u c l e a r s t a t e s} \\\\ \\sum p _ {H 1 1} \\dots p _ {H 2 0} = . 0 4 &amp; \\text {u n d e c l a r e d n u c l e a r s t a t e s} \\\\ \\sum p _ {H 2 1} \\dots p _ {H 1 0 0} = . 0 1 &amp; \\text {n o n - n u c l e a r s t a t e s , n o n - s t a t e a c t o r s} \\end{array} \\right. \\tag {7}\n$$\n\n$$\n\\text {P r i o r s}: \\left\\{ \\begin{array}{l l} \\sum p _ {H 1} \\dots p _ {H 1 0} = . 3 0 &amp; \\text {k n o w n s t a t e a d v e r s a r i e s} \\\\ \\sum p _ {H 1 1} \\dots p _ {H 2 0} = . 1 0 &amp; \\text {s u s p e c t e d s t a t e a d v e r s a r i e s} \\\\ \\sum p _ {H 2 1} \\dots p _ {H 1 0 0} = . 3 0 &amp; \\text {n o n - a d v e r s a r i a l s t a t e s} \\\\ \\sum p _ {H 1 0 1} \\dots p _ {H 5 0 0} = . 3 0 &amp; \\text {n o n - s t a t e a c t o r s} \\end{array} \\right. \\tag {8}\n$$\n\nwhere\n\nH1: Hypothesis attributing attack to country 1\n\n$\\mathbf{P}_{\\mathrm{H1}}$  : Probability that H1 is true\n\n$\\mathbf{P}_{\\mathrm{H1|E}}$  : Probability that H1 is true given evidence E\n\nThe identity of the attacking state is a hypothesis that the state will attempt to prove by means of evidence. Evidence distinguishes among candidate hypotheses. Absent any data, the states has trouble choosing which adversary to hold responsible for an attack. Evidence of one sort or another updates beliefs. In the Bayesian framework, the best evidence is that which is essentially incompatible with an otherwise-plausible theory. It forces the state to relinquish cherished presuppositions about looming conflicts.\n\nThe quality of the evidence can be characterized as the difference between the prior distribution of probability among available hypotheses (9) and the posterior distribution (10).\n\n$$\n\\text {P r i o r}: \\left\\{ \\begin{array}{l} p _ {H 1} = . 5 0 \\\\ p _ {H 2} = . 5 0 \\end{array} \\right. \\tag {9}\n$$\n\n$$\n\\text {P o s t e r i o r}: \\left\\{ \\begin{array}{l} p _ {H 1 | E} = . 9 0 \\\\ p _ {H 2 | E} = . 1 0 \\end{array} \\right. \\tag {10}\n$$\n\nEvidence plays a much greater role in cyberspace than it does in nuclear conflict.\n\nElectronic copy available at: https://ssrn.com/abstract=1928870",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 6,
            "markdown": "4 CYBERSPACE AND DETERRENCE\n7\n\nBecause the number of plausible candidates for an observed nuclear attack was fairly low, relatively cheap evidence could lead to a high degree of certainty in meaningfully short time horizon. With readily available telecommunications, satellite observations and corroboration by other states, states could achieve a reasonable degree of certainty that an attack had occurred, and that a particular state was responsible. This is ordinarily not the case in cyberspace. Cyber evidence is markedly more difficult to collect, to analyze, and to explain to non-specialists. States' networks have access points and data logs numerous like the grains of sand on a beach. Computer forensics involves sifting through these logs to gather insights that can be integrated with non-technical analysis; all of which takes time. The cost of some cyber attacks is well below that of nuclear weapons development. Most states and many non-state actors might perhaps be involved in any observed cyberattack.\n\nEvidence is thus a primary tool for shaping other states' behavior. States possessing rich knowledge about internet traffic, threat analysis, forensics of prior attacks, etc. can leverage that knowledge by selectively sharing information. As a trusted third party, states can choose to share evidence that helps neighbors identify attackers and verify that attacks have in fact taken place. This information can produce two specific, useful changes of state behavior: neighbors can mind their own security interests; and neighbors refrain from destructive mistakes driven by inaccurate suspicion.\n\nDeception is also possible; but only at the expense of the state's reputation for honest communication. Frequent deception encourages potential allies and adversaries to mistrust proffered information; so it dilutes the state's future influence for the sake of a fleeting advantage.\n\n## 4 Cyberspace and deterrence\n\n### 4.1 To observe attacks\n\nThe Wikileaks release of US State Department cables illustrates a particular feature of cyber attacks that changes the strategy of deterrence. Bradley Manning's alleged distribution of classified files to non-combatants outside the United States did enormous damage to the tactical and strategic positions of the United States.³ Until the moment of the attack's public disclosure, however, little if any harm at all came to individuals or property in the United States. The specific information security policies circumvented were controls on Manning's permission to copy files on uncontrolled media. Specifically, he smuggled the files, which were ultimately copied to Wikileaks servers in various countries, out of a secure facility on a CD-R of Lady Gaga music.⁴\n\nI dwell on this rogue-insider problem because it highlights the difference between the time of tactical cyber attack, such as it is, and that of observable attack effects. Rattray and Healey (2010) refer to cyberattacks as tactically quick but strategically\n\n³For brevity, I will drop further use of the word alleged, although Manning’s trial has not established the facts of the case or his guilt. Case facts as related here are taken directly from popular media, and not any first-hand knowledge.\n\n⁴For the uninitiated: one of her better known songs is a duet with Beyoncé, Telephone: “K-kinda busy/ K-kinda busy /Sorry, I cannot hear you, I’m kinda busy./ …Stop callin’, stop callin’,” I don’t wanna think anymore!/ I left my hand and my heart on the dance floor.”\n\nElectronic copy available at: https://ssrn.com/abstract=1928870",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 7,
            "markdown": "4 CYBERSPACE AND DETERRENCE\n8\n\nslow; meaning that cyberattacks often require years of preparation. Once Manning had the disk out of the secure facility, all of the remaining cyber operations happened off site and on computers not in control of the injured party. Though Manning's actions were both illegal and injurious to his country, none of his specific actions likely required him to circumvent electronic controls on his access. He did not need to crack encryption or authentication. He did not exploit flaws in software design. He used only the privileges granted to him in his official capacity. Prior to the dissemination of classified files, the only evidence that forensics could have turned up would be logs of file transfers to an optical drive by an authorized user. Had Manning been questioned about the incident, there is no reason to suspect that the investigation would have turned to public disclosure of the copied files. The act of disclosure was both the source of injury and the only likely moment when investigators would have begun to suspect foul play.\n\nThis distinction is hard to reconcile with consequentialist cyber attack definitions. Notably, Libicki (2009) draws a spectrum of consequence severity, with either financial losses or bloodshed as the standard for valid reprisals.⁵ Prohibited access to computer systems falls into a consequentialist gray area, neither nuisance nor grievance. Libicki explicitly considers the difference between attacks achieved by cyber means and simple deception, in order to conclude that it is the consequence of the attack and not its means that justifies *jus ad bello*. If we adopt a consequentialist standard for cyber attack, however, we cannot escape the conclusion that harmless enough tactical maneuvers may be used to prepare the ground for future attacks of devastating consequence. The attacker's tactical maneuver may occur well in advance of any reasonably observable consequence. Under some circumstances, the severity of consequences cannot be known even in principle until the attacker effectuates them.\n\nLin (2009) distinguishes between *cyberattack* and *cyberexploitation*, whereby one entails merely copying files, software and hardware configurations to remote locations. Cyber attack, on the other hand, aims to degrade, alter or destroy software and hardware belonging to the adversary. Lin's categories distinguish attacks primarily designed to benefit the attacker by yielding information about the adversary, from attacks designed to impair the target's systems themselves. Lin developed this framework about the same time that he edited a review of offensive cyberattack capabilities for the United States (Owens et al., 2009).\n\nMany other classifications of malware and cyber attack exist. Rattray and Healey (2010) describe and illustrate the tactical functions of cyber attack. Technical definitions from military doctrine are grounded in the role cyber operations play in military tactics: support of non-cyber troops, interdiction, reconnaissance, counter-cyber, suppression of cyber defenses, and a strategic-level cyber mission. Their hypothetical scenarios point to different military and political contexts: interstate conflict, piracy, irregular war, terrorism, covert operations, and intelligence gathering.\n\nTsipenyuk et al. (2005) describe seven categories of vulnerabilities in software design, which today and in the future will lead to unauthorized access (i.e., read) and execution (i.e., write, execute). These technical categories describe the means by which encryption, authentication, state variables, and controls against unauthorized execution\n\n⁵Technically, Libicki's scale distinguishes between mass casualty events and heightened risk of nuclear catastrophe. I have omitted the distinction here.\n\nElectronic copy available at: https://ssrn.com/abstract=1928870",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 8,
            "markdown": "can be prevented.\n\n### 4.2 To identify attackers\n\nAttributing cyber attacks to specific individuals, organizations, and IP addresses is a deep and abiding problem in cyber security. Although there is considerable difference of expert opinion on the feasibility of attribution, *Clark and Landau (2010)* problematize the issue succinctly. They discuss the different meanings of attribution based on the defender’s purpose: machine-level attribution identifies the origins and sources of packets, and often their contents. Individual level attribution links the machine’s user to official identities such as name, residence, place of business, and registration with various government and corporate entities. Clark and Landau point out that the main obstacle to cyber attribution is often jurisdiction, rather than technical capability. The deciding factor, often, is whether investigators have timely cooperation for success across jurisdictions.\n\n### 4.3 To retaliate\n\nSecond strike capability raises more problems than it solves. *Owens et al. (2009)* devote an entire book to the legal and political difficulties facing states that would develop cyber weapons. Developing an offensive capability, due to the nature of cyber weapons, is not at all similar to firing missile tests and taking satellite observations of prospective targets. Cyber weapons largely do not point and shoot, and the ones that states are likely to employ as a retaliatory strike are not simple botnet attacks that would take websites offline. Instead, offensive cyber capabilities should require long periods of active gathering of access to potential adversaries’ systems. Only after a long period of reconnaissance can attackers have confidence in the success of a timely counterattack.\n\nDefining proportional response to a cyber attack is difficult, since it is often the effects of the cyber attack on non-cyber systems that states wish to punish. This line of reasoning is a slippery slope for states that seek to mitigate the risk of of cyber conflict or mollify incipient conflicts. Detractors of cyber offense also point to America’s relatively greater reliance on information technology both in military and civilian life *(Schwartau, 2000)*. Since we have more to lose from successful IT attacks, restricting the notion of proportionality to pure cyber retaliation for cyber attacks is a self-defeating line of reasoning; not unlike Richard Clarke’s famous accusation that Donald Rumsfeld wanted to find a better adversary than Afghanistan, with better targets for the Air Force.\n\nKnock-on effects of declaratory policy in this area are also troublesome. If we could define standards of proportionality and achieve consensus thereon, cyber attacks are sufficiently flexible that clever attackers could adjust the lethality, destructiveness, and economic impact of cyber attacks to evade reprisals.\n\nIn fact a number of the problems with cyber deterrence as a strategy have to do with knock-on effects *(Morgan, 2010)*. Convincing adversaries that the state has no plans to use cyber attacks is difficult enough, without declaring a policy to boot that virtually necessitates years of covert operations to prepare the ground for those cyber retalia",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 9,
            "markdown": "4 CYBERSPACE AND DETERRENCE\n10\n\ntions. Declaration of intended lethal retaliation for pure cyber attacks could plausibly weaken credibility, if the promised reprisals are too strong.\n\n## 4.4 Time, the essential variable\n\nThe greatest single problem with deterrence in cyberspace is the anachronism of attack, effect, and retaliation. The central promise of deterrence is that the counterstrike will rob the attacker of the supposed fruits of the attack, through the promise of swift and certain retribution. For the most interesting cyber attacks, i.e., ones where the attacker will prepare the ground for days or years in advance of a tactical opportunity, the damage has long been done before the effects of the cyber attack become apparent. The promise of swift and certain retribution may simply ring hollow.\n\nSchwartau (2001) lays out a unique and relevant security model in layman’s terms. Schwartau points out that defensive security measures, in and of themselves, are largely all fallible. The most secure physical and cyber technologies that we can create are designed to keep things secret or off limits for a period of time, until a complementary response can be activated. Door locks and safes are measured by the duration of time for which they can withstand well defined classes of physical attack. Encryption algorithms are measured by the time required to crack passwords by brute force. Password tokens and password expiry dates are designed to ensure that passwords have a short shelf life (more on that below). Airgapped networks are designed to limit the opportunities that attackers have to access the system. By limiting the frequency of access, they effectively raise the time required to penetrate the system. Even the decision to maintain obscure, legacy operating systems (not Windows, Mac, or Unix) in particular firms demands time of would-be attackers, i.e., time spent developing the competence to crack access controls. In Schwartau’s model, every security measure is a method of buying time until the cavalry can arrive.\n\nSchwartau’s definition of security as a measure designed to gain time for appropriate response further complicates deterrence. The time to respond to an attack can only begin from the observation of the attack. A fully trusted computing platform, that could inform us when it had been breached—instead of when the attack’s effects were observed—is at best a distant prospect in the future.\n\nRoughly speaking, let us assume that the greater the time available for reconnaissance, the greater the scale of consequences that attackers can achieve and the higher the degree of confidence they can have in the result. If that is true, then there is an inexorable advantage among states under the security dilemma to conduct reconnaissance as early as possible. As a result, states know they must expect this behavior from their adversaries.\n\nThe start date of technical actions in cyber conflict occurs well in advance of the start date of political conflict, and well in advance of the start date of observed effects of cyber attacks. One of the main strategic goals of the national cyber defense strategy must be to take advantage of the planning phase of cyber conflict. The ways to take advantage of that phase are to develop situational awareness; to conduct appropriate counter-reconnaissance; to practice civil and military defense; to foster close interjurisdictional collaboration; and to gain the trust of likely allies.\n\nElectronic copy available at: https://ssrn.com/abstract=1928870",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 10,
            "markdown": "5 LEVERS IN CYBERSPACE\n11\n\n# 5 Levers in cyberspace\n\n## 5.1 Cooperation\n\nCooperation is one of the policy levers that can serve cybersecurity strategy. There are many domains where allies can cooperate on cyber issues: law enforcement, intelligence, military, and civil defense. In each of these areas overlapping capabilities can provide specific benefits to potential allies.\n\nThe rapid pace of technological development is one oft-cited reason that cooperation is vital to success in cyberspace; but is not the only one. As a rough analogy, the databases used by antivirus software have a short shelf life. Within days they are obsolete, as the variety of malware observed in the wild grows continuously. Nearly all cybersecurity problems evolve and require constant study due to changes in software, hardware, and business practices. Any small, isolated organization will likely benefit from exchange of information related to cybersecurity, whether or not that information is of a primarily technical nature.\n\nInstitutional collaboration is arguably more important than exchange of technical information. Computer crimes in peacetime frequently involve a morass of overlapping national and local laws, international agreements, and competing authorities for investigation, prosecution and jurisdiction (Westby, 2004). People, hardware, and communications may operate in several (or dozens of) countries, using communications infrastructure, services, and data facilities that are themselves spread across geographic and political boundaries. Forensics, investigation and prosecution require formal structures for timely collaboration across these various borders.\n\nIntelligence gathering and threat analysis require similar institutional agreements for interagency and international cooperation. Intelligence organizations are rightly designed to take care with sensitive and classified information. Intelligence organizations by default lock down access to information that could be valuable to strategic competitors.\n\nIn peacetime, we must assume that states, firms, and non-state actors have already undertaken reconnaissance activities. Indeed evidence of this abounds (Demchak and Dombrowski, 2011). Exchange of reconnaissance information, both technical and organizational in nature, has the potential to greatly increase the quality of each party's information. There may be no hard and fast line between reconnaissance activities in cyberspace, threat analysis, and situational awareness for national defense. Many pieces of information that are necessary for threat analysis and attack detection would ultimately be useful for countermeasures and counterstrikes to cyber attacks.\n\nFinally, joint exercises are an important precursor to engagement in notionally \"hot\" cyber wars. Cyber security simulations train military and civil authorities to detect, analyze, and adapt to cyber attacks in the context of political conflicts, conventional hostilities, and threats to vital national interests. These simulations align expectations among allies for best practices, standards of inference, norms of behavior, and thresholds for retaliation of various kinds. Given the paucity of global standards for cyber conflict (Hayden, 2011; Libicki, 2009), first-hand experience of other countries' behavior and analysis may prove indispensable. Countries excluded from these exercises lose valuable opportunities to train their own personnel, develop and test\n\nElectronic copy available at: https://ssrn.com/abstract=1928870",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 11,
            "markdown": "5 LEVERS IN CYBERSPACE\n12\n\nprocedures, learn about neighbors' practices, and advocate for favored practices with potential allies.\n\nGenerally, the larger the network of cooperating states, the greater is the potential benefit of access therein; and the greater the harm from exclusion. (Conversely, the larger the number of states collaborating, the more difficult it is to achieve consensus on exclusion, and to monitor and enforce such decisions.) Similarly, the greater the capabilities of the states collaborating in such a network, the greater is the benefit of inclusion in the network.\n\n## 5.2 Shared networks\n\nSpecific networks can themselves be assets offered to share with allies and withheld from enemies. These networks may be bounded by physical connections, communications protocols, or logical restrictions on access. As with the previous section, each of these levers can be wielded either as a carrot or a stick.\n\nSpecial diplomatic communications networks predate computers themselves. Telephone hotlines connected Moscow to Washington during the Cold War, in order to prevent missed calls during a nuclear crisis. SIPRNET limits unauthorized access to sensitive and classified information systems; and in theory it limits the temptation for personnel to copy sensitive information out to insecure hardware.\n\nIn the private sector, special financial networks connect banks, trading desks, market exchanges, and clearinghouses. Most firms and households do not have access to these information systems. Many other examples of communications networks for finance exist, in addition to those serving the largest OECD banks. Credit card payments networks, electronic funds transfers, wire transfers, Paypal, Western Union, and Islamic payments networks all provide special access to particular financial communications. Membership in these networks greatly reduces transaction costs for both the end users and the network operators themselves.\n\nSimilarly, telecommunications networks provide still further benefits. Telecommunications encompass a vast array of services, as simple and open as mobile telephone calls or shortwave radio broadcasts; and as complex as combat communications systems, which integrate secure streams of voice, positioning, imaging, targeting, and order-of-battle data.\n\nIn the future, as user identification protocols become more pervasive, participation in those registries will become another important source of leverage. Registries need not be centralized; but the ability to verify cryptographic signatures is central to a wide variety of authentication and encryption protocols for communications. Various levels of access could include permission to request an identity (either for an individual or for hardware); permission to request identities on behalf of others; permission to verify users' credentials against the registry of identities; and permission to create, alter, and delete identities in the registry. Trust is enacted in code. Software structures the choices users can make about extending trust to registries, users, and administrators. States, both by the grant (or revocation) of trust to various identity protocols and by the limits placed on identities created for users, can structure other states' access to legitimate telecommunications networks.\n\nElectronic copy available at: https://ssrn.com/abstract=1928870",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 12,
            "markdown": "### 5.3 Standards development\n\nStates will also want a voice in the development of global standards in IT. A dizzying array of standards bodies exist today, and these standards bodies are often located outside government; although more and more states seek to bring these institutions directly under their authority. Standards bodies govern the development of technologies such the internet protocol itself and elements of commonly used computer languages and file formats; the assignment of names and addresses; and many other related technical standards.\n\nStandards bodies also handle closely related non-technical matters, such as financial, accounting and risk standards for corporate information technology. Standards bodies may be located within the government, in international organizations (e.g., the International Telecommunications Union), within a single private company (e.g., Microsoft), in industry associations, in a non-profit entity with joint representation from industry and government (e.g., ICANN), or in a private non-profit (e.g., Mozilla Foundation). Government may also solicit bids or competitions for specific standards, such as encryption algorithms, which are designed privately and then adopted by the government for official business. Sofaer et al. (2010) discuss the bodies in the international system with jurisdiction over IT standards and telecommunications law.\n\nMultilateral standards bodies could potentially play a role in technical, financial, legal, or other standards. Multilateral bodies are notoriously slow and weak, particularly when consensus is required to undertake action (Tsebelis, 1995). Bilateral treaties can substitute for multilateral arrangements, as in the case of investment law. Where a strong state enacts a number of nearly identical treaties with weaker counterparties, it effectively reduces the power of the counterparties in negotiations, who otherwise may be excluded from advantageous economic and security relations with the stronger state. The bilateral approach is also more brittle. The strong state must replicate bilateral negotiations over proposed amendments with each weaker state. Weaker states have less incentive to remain in bilateral agreements when disputes arise; exiting a multilateral treaty or body entails severing a tie with all member states.\n\n### 5.4 Private industry\n\nStates wishing to influence adversaries’ behavior must consider markets and market regulations as part of cyber strategy. Commercial and industrial policies have far reaching consequences for the development of technology, industry and a skilled workforce. Similarly, the alternatives available to would-be attackers depend greatly on commercial markets for software and hardware. States have many modes of contact with private industry.\n\nStates can legislate or regulate business practices in strategically important industries. For instance, the United States has banned unauthorized export of advanced weapons systems. States can enact financial and risk management standards that influence business practices. Rice (2008) discusses extensively how standards of risk tolerated in software design differ from those in consumer products, and the consequences of risk and liability for product design and marketing. Anderson and Moore (2006) develop economic models of externalized costs that explain persistent problems",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 13,
            "markdown": "6 CONCLUSION\n\nwith software design, digital privacy, and insurance markets for cyber damages. States can also attempt to manage the development of standards for software, data, and communications; but Varian (2001) explains the particulars of how and why industry may not cooperate.\n\nStates can impose conditions on vendors. These conditions can involve criteria related to software license structure; geographic and political ties; adherence to particular data formats; performance criteria of services; features that create vendor lock-in; the components of software and service bundles; supply chain assurance; and a host of other features (Rosenweig, 2010). The purchasing power of the government may not be sufficient to appreciably alter the market shares of leading technology and standards; but it does provide the state with a lever of control over at commercial software developed for its own use.\n\nStates can develop standby arrangements for civil defense. These standby arrangements may be as simple as regular goodwill visits between (for example) the Department of Homeland Security industry liaisons and industry executives. They may take the form of immediate crisis response, either focused directly on monitoring and maintaining computer networks (e.g., Computer Emergency Readiness Teams) or on infrastructure assurance during cyber events. States have also developed laws governing corporate cooperation in law enforcement and national information policies; although these interactions entail potentially huge liability and reuptational risks to the corporation. Global corporations, for their part, face difficult decisions about whether corporate values and governance permit accommodation of unique local laws.\n\nStates can choose whether and when to subsidize and restrict public access to information assets, such GPS information and satellite imagery. Forbidding access to certain information assets may have the unintended consequence of encouraging competitors to acquire new skills and capabilities. Furthermore, the state can commission research on technologies that are not yet commercially available.\n\nStates that seek to gain strategic advantage in cyberspace must consider the unorthodox economics of information technology industries. Varian et al. (2004) describe features of supply and demand in information technology, competitive strategy and pricing dynamics, and relevant features of networks. Brafman and Beckstrom (2006), Tapscott and Williams (2006), and Barabási (2002) and give useful and engaging introductions to some newer theoretical approaches that help to explain why some networks don’t have leaders, what motivates apparently uncompensated collaborators on large projects, and how to think about the important features of networks. State policies on cyber security are fraught with pitfalls of unintended consequences; and economics is a good tool to demarcate the big ones.\n\n## 6 Conclusion\n\nThe basic condition of cyber conflict described in this article is that situational awareness is costly. States must assume that adversaries have begun to plumb the depths of the network long in advance of a conflict’s apparent start date. Most security measures that are designed to buy time for effective response must be implemented during peacetime. As a result, strategies that ignore the realities of daily business will not win\n\nElectronic copy available at: https://ssrn.com/abstract=1928870",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 14,
            "markdown": "REFERENCES\n\nthe full cooperation of key industry partners. Winning cyber conflicts will, in a very specific sense, require states to win the peace.\n\n# References\n\nAchen, C. H. and D. Snidal (1989, January). Rational deterrence theory and comparative case studies. *World Politics* 41(2), 143–169.\n\nAllen, J. H., S. Barnum, R. J. Ellison, G. McGraw, and N. Mead (2005). *Software Security Engineering*. Addison Wesley.\n\nAnderson, R. and T. Moore (2006, 27 October). The economics of information security. *Science* 314(5799), 610–613.\n\nArt, R. J. (1980, April). To what ends military power? *International Security* 4(4), 3–35.\n\nBajaj, K. (2010). Cybersecurity agenda: Mobilizing for international action. EastWest Institute.\n\nBarabási, A.-L. (2002). *Linked: The New Science of Networks*. Perseus Books Group.\n\nBorg, S. (2005). Economically complex cyberattacks. *IEEE security &amp; privacy* 3(6), 64–67.\n\nBrafman, O. and R. Beckstrom (2006). *The Starfish and the Spider: The Unstoppable Power of Leaderless Organizations*. Penguin Group USA.\n\nBrodie, B. (1978, April). The development of nuclear strategy. *International Security* 2(4), 65–83.\n\nClark, D. D. and S. Landau (2010). Untangling attribution. In *Proceedings of a Workshop on Deterring Cyberattacks: Informing Strategies and Developing Options for US Policy*. National Research Council: National Academies Press.\n\nClarke, R. (2009). War from cyberspace. *National Interest* 2009(104), 31–36.\n\nClarke, R. and R. Knake (2010). *Cyber War: The Next Threat to National Security and What to Do About It*. HarperCollins.\n\nDemchak, C. C. and P. Dombrowski (2011). Rise of a cybered Westphalian age. *Security Studies Quarterly* 5(1), 34–63.\n\nDunn, L. A. (1994). Rethinking the nuclear equation: The United States and the new nuclear powers. *The Washington Quarterly* 17(1), 5–25.\n\nFalliere, N., L. O. Murchu, and E. Chien (2010, November). W32.Stuxnet Dossier: Version 1.3. Symantec.\n\nGorman, S. (2010, 8 October). “US plans cyber shield for utilities, companies”. *Wall Street Journal*.\n\nElectronic copy available at: https://ssrn.com/abstract=1928870",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 15,
            "markdown": "REFERENCES\n\nHayden, M. (2011). The future of things “cyber”. *Strategic Studies Quarterly* 5(1), 3–7.\n\nJervis, R. (1979, January). Deterrence theory revisited. *World Politics* 31(2), 289–324.\n\nJoint Chiefs of Staff (2000). Joint Doctrine for Electronic Warfare. Joint Publication 3-51.\n\nJoint Chiefs of Staff (2007). Electronic Warfare. Joint Publication 3-13.1.\n\nKahneman, D. and A. Tversky (1979, March). Prospect theory: An analysis of decision under risk. *Econometrica* 47(2), 263–291.\n\nKahneman, D. and A. Tversky (2000). *Choices, Values, and Frames*. Cambridge University Press.\n\nKrekel, B. (2009, October). Capability of the people’s republic of china to conduct cyber warfare and computer network exploitation. The US-China Economic and Security Review Commission.\n\nLangevin, J. R., M. T. McCaul, S. Charney, and H. Raduege (2008, December). Securing cyberspace for the 44th presidency. A report of the CSIS commission on cyber-security for the 44th presidency, Washington, DC.\n\nLiang, Q. and W. Xiangsui (2002). *Unrestricted Warfare: China’s Master Plan to Destroy America*. Pan American Publishing Company.\n\nLibicki, M. C. (2009). *Cyberdeterrence and Cyberwar*. RAND.\n\nLin, H. (2009, July). Lifting the veil on cyber offense. *Security and Privacy IEEE* 7(4), 15–21.\n\nLukasik, S. (2010). A framework for thinking about cyber conflict and cyber deterrence with possible declaratory policies for these domains. In *Proceedings of a Workshop on Deterring Cyber Attacks: Informing Strategies and Developing Options for US Policy*, Washington, D.C. National Academies Press.\n\nLynn, William J., I. (2010, September). Defending a new domain: The Pentagon’s cyberstrategy. *Foreign Affairs*.\n\nMcConnell, M. (2010, 28 February). “We’re losing the cyber war. Here’s the strategy to win it.”. *The Washington Post*.\n\nMorgan, P. (1977). *Deterrence*. Sage.\n\nMorgan, P. (2010). Applicability of traditional deterrence concepts and theory to the cyber realm. In *Proceedings of a Workshop on Deterring Cyber Attacks: Informing Strategies and Developing Options for US Policy*, Washington, D.C. National Academies Press.\n\nNye, Joseph S., J. (2010). Cyber Power. Belfer Center for International Affairs, Harvard Kennedy School.\n\nhttp://www.ebi.org/ebi/ebi_0182\n\nElectronic copy available at: https://ssrn.com/abstract=1928870",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 16,
            "markdown": "REFERENCES\n\nOwens, W. A., K. W. Dam, and H. S. Lin (Eds.) (2009). Technology, policy, law and ethics regarding US acquisition and use of offensive cyberattack capabilities. Committee on Offensive Information Warfare: National Research Council.\n\nRattray, G. and J. Healey (2010). Categorizing and understanding offensive cyber capabilities and their use. In Proceedings of a Workshop on Deterring Cyber Attacks: Informing Strategies and Developing Options for US Policy. National Academies Press.\n\nRattray, G. J. (2001). Strategic War in Cyberspace. MIT Press.\n\nRice, D. (2008). Geekonomics: The Real Cost of Insecure Software. Addison Wesley Professional.\n\nRosenweig, P. (2010). The organization of the united states government and private sector for achieving cyber deterrence. In Proceedings of a Workshop on Deterring Cyber Attacks: Informing Strategies and Developing Options for US Policy, Washington, D.C. National Academies Press.\n\nSchmitt, M. N. (2010). Cyber operations in international law: the use of force, collective security, self-defense, and armed conflicts. In Proceedings of a Workshop on Deterring Cyberattacks: Informing Strategies and Developing Options for US Policy. National Research Council: National Academies Press.\n\nSchwartau, W. (2000). Looming security threats: Asymmetrical adversaries. *Orbis* 44(2), 197–205.\n\nSchwartau, W. (2001). Time Based Security (2nd ed.). Interact Press.\n\nSingel, R. (2010, 1 March). Cyberwar hype intended to destroy the open internet. Blog post, http://www.wired.com/threatlevel/2010/03/cyber-war-hype/.\n\nSofaer, A. D., D. Clark, and W. Diffie (2010). Cyber security and international agreements. In Proceedings of a Workshop on Deterring Cyber Attacks: Informing Strategies and Developing Options for US Policy. National Academies Press.\n\nTaliaferro, J. W. (2001). Security seeking under anarchy: Defensive realism revisited. International Security 25(3), 128–161.\n\nTannenwald, N. (1999). The nuclear taboo: The United States and the normative basis of nuclear non-use. International Organization 53, 433–468.\n\nTapscott, D. and A. D. Williams (2006). *Wikinomics*. Penguin.\n\nThornburgh, N. (2005, 29 August). “The invasion of the Chinese cyberspies (and the man who tried to stop them)”. *Time*.\n\nTsebelis, G. (1995). Decision making in political systems: Veto players in presidentialism, parliamentarism, multicameralism and multipartyism. *British Journal of Political Science* 25(03), 289–325.\n\nElectronic copy available at: https://ssrn.com/abstract=1928870",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          },
          {
            "index": 17,
            "markdown": "REFERENCES\n\nTsipenyuk, K., B. Chess, and G. McGraw (2005, November). Seven pernicious kingdoms: a taxonomy of software security errors. *Security and Privacy, IEEE* 3(6), 81–84.\n\nTversky, A. and D. Kahneman (1974). Judgment under uncertainty: Heuristics and biases. *Science* 185(1), 124–131.\n\nVarian, H. R. (2001, 15 November). Economics of information technology. Raffaele Mattioli Lecture, Bocconi University.\n\nVarian, H. R., J. Farrell, and C. Shapiro (2004). *The Economics of Information Technology*. Cambridge University Press.\n\nWalton, G., R. J. Deibert, A. Machanda, R. Rohozinski, and N. Villanueva (2009, 29 March). Tracking GhostNet: Investigating a cyber espionage network. Citizen Lab, Munck Centre for International Studies, University of Toronto.\n\nWaltz, K. N. (1990). Nuclear myths and political realities. *The American Political Science Review* 84(3), 731–745.\n\nWestby, J. (2004). *International Guide to Cyber Security*. American Bar Association.\n\nWhite House (1998, 22 May). Critical infrastructure protection. Presidential decision directive / NSC 63.\n\nWhite House (2009). Cyberspace policy review: Assuring a trusted and resilient information and communications infrastructure.\n\nElectronic copy available at: https://ssrn.com/abstract=1928870",
            "images": [],
            "dimensions": {
              "dpi": 200,
              "height": 2200,
              "width": 1700
            }
          }
        ],
        "model": "mistral-ocr-latest",
        "usage_info": {
          "pages_processed": 18,
          "doc_size_bytes": 241465
        },
        "document_annotation": null
      }
    },
    "error": null
  },
  "full_text": "Electronic copy available at: https://ssrn.com/abstract=1928870\n# Leverage in cyberspace, without deterrence\nBenjamin D. Mazzotta\nThe Fletcher School of Law and Diplomacy, Tufts University\nbenmazzotta@alumni.tufts.edu\nMarch 19, 2011\nPaper presented to the Annual Convention of the International Studies Association.\nMontréal, QC.\n## Abstract\nDeterrence theory, which was of enormous value during the nuclear era, cannot adequately prepare defense thinkers for future cyber conflicts. Instead, the best states can hope for is robust cooperation with other states and with private actors to improve their circumstances in future conflicts. A number of key assumptions that supported nuclear deterrence do not hold in the cyber domain: immediately observable attacks; clear legal status under the law of armed conflict; states' preeminence for research and development; high financial and technical barriers to entry; cheap observation of delivery facilities; feasible second strike; and a stable MAD outcome. States can create several incentives for improved collaboration with allies and private sector partners. States can derive mutual benefits from shared intelligence; shared standards for risk assessment; joint exercises; and standby arrangements for crucial information, equipment, materials, and services. Apart from sanctions against access to these carrots, sticks include reconnaissance for cyber conflicts, cyber exploitation with the threat of disclosure, and policies explicitly linking cyber events to non-cyber diplomatic and security concerns. States have additional leverage over private actors through commercial regulation, law enforcement, and liability; although the burden of compliance may poison public-private cooperation.\n# 1 Introduction\nCyber attacks have generated enormous controversy in international relations along the lines of a recurrent *Late Show with David Letterman* segment, \"Is this anything?\" Cyber hawks point to the far-reaching effects and cost-efficacy of attacks (Bajaj, 2010; Borg, 2005). Skeptics accuse hawks of crying Chicken Little over simple web defacement (Singel, 2010). Cyber war has captured Hollywood's imagination at the strategic level as in *Die Hard IV: Live Free or Die Hard*; at the tactical level as in *Vantage Point*; and oftener as dystopian techno-singularity as in *Eagle Eye*, *The Matrix*, and\n2 DETERRENCE THEORY\n2001: A Space Odyssey. Journalists (Gorman, 2010), freelancers (Thornburgh, 2005), think tanks (Krekel, 2009; Walton et al., 2009), business (Falliere et al., 2010; Langevin et al., 2008; Allen et al., 2005; Tsipenyuk et al., 2005), military doctrine (Joint Chiefs of Staff, 2007, 2000), government documents (White House, 2009, 1998) and IR theorists (Clarke and Knake, 2010; Lynn, 2010; Morgan, 2010; Nye, 2010; Clarke, 2009; Owens et al., 2009; Liang and Xiangsui, 2002; Rattray, 2001) describe broad classes of attacks whose objectives might include the disruption of civilian infrastructure; tactical advantage in conventional war; intelligence gathering; financial gain; and terrorism. This article assumes the feasibility of broad classes of attack as a moot point, and outlines the strategic consequences of those attacks as if they were known capabilities.\nFaced with similarly disruptive military technology in the 1940s, the discipline of international relations refined the concept of deterrence to devise a strategy for safe navigation of potentially catastrophic nuclear arms races. Nuclear theorists then and cyber theorists now pointed out strikingly similar aspects of the new technology: vast geographic reach of the attacks, revolutionary intensity of purported attack effects, and relative cost efficacy of the new weapons. Cyber attacks may promise a revolution in military affairs similar to the nuclear revolution. Nuclear theorists then focused on several features of the new technology and constructed a strategic equilibrium from which deviance was less likely. Cyber theorists now must look carefully at the features of the new technology to assess its strategic consequences.\nDeterrence in cyberspace is a mirage. Cyberspace itself bears little resemblance to the physical world around us, and efforts to reason by analogy with physical space often do more harm than good. Nuclear deterrence is a particularly poor strategic model for cyberspace. The best we can hope for in cyberspace is to enlist the aid of allies who may cooperate for mutual benefit. There are specific sources of leverage that we have in cyberspace, thanks to our unusual capabilities in government and the private sector. Only when taken out of context, may these levers of control be shoehorned into a purported model of deterrence. Faith in deterrence may trick us to trust our own defenses, and blind us to opportunities for alliance.\n## 2 Deterrence theory\n### 2.1 Incentives\nDeterrence works, in theory, because of a brutally simple set of incentives facing the state. State behavior under the security dilemma is the basis of deterrence. Aggressor states seek to gain power through aggressive wars. Strategic competitors promise retaliation for these wars that more than wipes out the gains from expansion.\nDeterrence theory takes as its point of departure that the state is a rational actor (Achen and Snidal, 1989; Brodie, 1978), subject to an inescapable calculus of self-interest (Waltz, 1990). Various critiques of the rational actor model of the state in international relations have been advanced, including unraveling of deterrence commitments (Morgan, 1977), well-established cognitive and behavioral deviance from simple economic self-interest (Kahneman and Tversky, 2000, 1979; Tversky and Kahneman, 1974); status-seeking behaviors in nuclear weapons development (Art, 1980);\nElectronic copy available at: https://ssrn.com/abstract=1928870\n2 DETERRENCE THEORY\ntaboos in tactical deployment of nuclear weapons (Tannenwald, 1999; Dunn, 1994); and constructivist narratives of leadership and the national interest.\nJervis (1979) describes waves of innovation designed to address shortcomings of deterrence theory. The major features of these second-wave models are the aggressor’s political calculus; the possibility of rewards and negotiations as complements to simple retaliation; and constructivist interpretations of the aggressor’s goals in security. Many of these criticisms are further explicated in Taliaferro (2001), which organizes contemporary variants of realist international security theory.\nNecessary conditions for effective deterrence vary greatly from one variant of the theory to another. Waltz (1990) holds that the key to nuclear deterrence is a reliable second strike that eliminates potential gains from aggression. He further argues that the low cost and weight of nuclear weapons leads inexorably to defensive vulnerability. Brodie (1978) argued that deterrence applied mainly to the Soviet-American security dilemma, and largely on the basis of Soviet acquisition of nuclear weapons. Extended deterrence modeled the behavior of allies within this bilateral framework.\n### 2.2 Technology\nKey features of nuclear technology circa 1960 were ease of observing weapons deployment at various stages, including launch site construction, weapons launch, and successful attack sites; extreme lethality and blast radius; absence of viable defensive countermeasures; difficulty of developing nuclear weapons, measured by financial costs, knowledge, and manufacturing capabilities; and limited means of delivery, viz., aircraft, oceangoing vessels, and ballistic missile. These points are not controversial. Efforts to overcome these technical limits of nuclear weapons (e.g., anti-ballistic missile defenses) attracted opprobium, as they threatened the stability of the mutually assured destruction (MAD) equilibrium (Dunn, 1994).\nNuclear technology itself ensures that the adversary is immediately aware of any attacks; that a short list of suspects are considered capable of an attack; and that, in fact, at least one from the short list must have participated in the attack at some level. Dunn (1994) notes that American strategists ignored the possibility of nuclear weapons smuggling for just that reason. Satellite and aircraft observations were long considered sufficient to monitor nuclear weapons deployment. More exhaustive inspections of suspected facilities featured prominently in Iran, Iraq, and North Korea; but the nature of weapons development has changed considerably since the early nuclear age. Today critical materials are widely available, e.g., nuclear expertise, raw fuel, and industrial components for enrichment and delivery systems. This was absolutely not the case during the early nuclear age.\n### 2.3 Deterrence in cyberspace\nDeterrence, having served America so well during the Cold war, was an obvious place to begin discussions of cyber strategy. McConnell (2010), in an op-ed on cyber strategy, poses a choice between deterrence and preemption as the central strategic question in cyberspace. He further argues that deterrence rests on four necessary capabilities: a declared retaliation policy, attribution of the attack to the attacker, location of the\nElectronic copy available at: https://ssrn.com/abstract=1928870\nattacker, and the counterstrike itself. If the layman’s definition of deterrence is that we can make ourselves safe in cyberspace without shooting first, then let us all hope that McConnell is correct. On the other hand, there is no consensus among experts that the four necessary conditions McConnell cites are even feasible, let alone likely to be achieved.\nThe National Research Council hosted an essay competition and conference in 2010 dedicated to the feasibility of deterrence in cyberspace. Patrick Morgan, a long-standing skeptic of deterrence theory, questions the applicability of nuclear deterrence to cyberspace *(Morgan, 2010)*. *Lukasik (2010)* points to serious flaws in a wide variety of declaratory policies on deterrence, such as the threat to retaliate with conventional lethal force. *Clark and Landau (2010)* explain stubborn technical problems with attribution. *Rattray and Healey (2010)* give an overview of offensive cyber weapons, some of which are plausible for retaliation. *Schmitt (2010)* covers the legality of national self-defense under cyber attack; and in a related paper *Sofaer et al. (2010)* outline hypothetical features of international institutions that would purport to ameliorate cyber security.\n## 3 A simple model\nSuccessful nuclear deterrence relies on two crucial actions that are trivial in early Cold War nuclear conflict; but that are well-established problems in cyber conflict. The adversary must observe that an attack has in fact occurred. The adversary must also identify the attacker. This section of the article develops a quick Bayesian model of the adversary’s problem, with plausible hypotheses about the likely identity of the attacker.\nThis framework leads to several simple questions about attack technology and the adversary’s decisions.\n1. How difficult is it to detect a given attack?\n2. How difficult is it to identify the attacker?\n3. How likely is the adversary to retaliate on mere suspicion of an attack?\n4. How likely is the adversary to retaliate upon a merely suspected attacker?\n5. How likely is the adversary to lash out at the wrong attacker?\nA convenient starting place is to model the expected utility of the attacker’s payoffs over the future states of the world: viz., attacks with retaliation, attacks without retaliation, and status quo.\n$U\\equiv u(\\mathbf{E}\\pi)$ (1)\n$\\mathbf{E}\\pi=p_{a}\\pi_{a}+p_{a}\\left(p_{r}\\pi_{r}+p_{\\sim r}\\pi_{\\sim r}\\right)+\\left(1-p_{a}\\right)\\pi_{s}$ (2)\nwhere\n3 A SIMPLE MODEL\n|   | where  |\n| --- | --- |\n|  U: | expected utility  |\n|  a: | subscript for attack  |\n|  s: | subscript for status quo  |\n|  p: | probability  |\n|  π: | payoff  |\n|  r: | subscript for retaliation  |\n|  ~r: | subscript for nonretaliation  |\nWe can further decompose the probability of retaliation into separate component probabilities, as in (3). The adversary needs to detect that the attack has in fact taken place, attribute the source of the attack, and then decide to respond to the attack. Early Cold War deterrence proceeded as a special case of the equation below, where virtually any attack would lead to certain attribution. Today adversaries may only be able to suspect the adversary's identity; or suspect that an attack has in fact taken place; or have only a rough suspicion on both counts. $^{1}$\n$$\np _ {r} = \\underbrace {p _ {d , i} p _ {r | D \\cap I}} _ {\\text {certain attribution}} + \\underbrace {p _ {d , \\sim i} p _ {r | D \\cap \\sim I}} _ {\\text {unidentified attacker}} + \\underbrace {p _ {\\sim d , i} p _ {r | \\sim D \\cap I}} _ {\\text {suspected attack}} + \\underbrace {p _ {\\sim d , \\sim i} p _ {r | \\sim D \\cap \\sim I}} _ {\\text {rough suspicion}} \\tag {3}\n$$\nAnd when the adversary may mistake the identity of the attacker, as in (4), the attacker may also err in retaliation, with potentially (dis)adventaneous payoffs for the attacker.\n$$\n\\mathbf {E} \\pi = \\underbrace {p _ {\\mathrm {a}} \\pi_ {\\mathrm {a}}} _ {\\text {attack's benefit}} + \\underbrace {p _ {\\mathrm {a}} \\left(p _ {\\mathrm {r}} \\pi_ {\\mathrm {r}} + p _ {\\sim \\mathrm {r}} \\pi_ {\\sim \\mathrm {r}}\\right)} _ {\\text {retaliation's cost}} + \\underbrace {p _ {\\mathrm {d} , \\sim \\mathrm {i}} p _ {\\mathrm {m} | \\mathrm {D} \\cap \\sim \\mathrm {I}} + p _ {\\mathrm {s}} \\pi_ {\\mathrm {s}}} _ {\\text {mistaken counterattack}} \\tag {4}\n$$\n|   | where  |\n| --- | --- |\n|  pr: | probability of retaliation  |\n|  D: | successful detection of an attack  |\n|  ~D: | failure to detect an attack  |\n|  I: | successful identification of an attacker  |\n|  pr|D∩I: | probability of retaliation conditional on adversary's detection without identification  |\n|  pd: | probability of adversary's detecting attack  |\n|  pi: | probability of adversary's identifying attack  |\n|  pm: | probability of a mistaken counterattack  |\nThere are stark differences between nuclear attacks circa 1960, modeled in (5) and cyber attacks, modeled in (6). $^{2}$  Nuclear attacks are obvious by the scale of their destruction. Attribution was simple, due to the small number of states in possession of the technology. There are no serious barriers to acquisition of technical expertise or requisite hardware for cyber attack. There is no reason to suspect that the adversary\nElectronic copy available at: https://ssrn.com/abstract=1928870\n3 A SIMPLE MODEL\nwill understand that an attack will be detected until long after it has begun. There is no reason that the adversary will immediately suspect whether the adversary in a given cyber attack is a state or not a state.\n$$\np _ {d i} \\gg p _ {d, - i} &gt; p _ {- d} \\tag {5}\n$$\n$$\np _ {d, - i} &gt; p _ {- d, i} &gt; p _ {d, i} &gt; p _ {- d, - i} \\tag {6}\n$$\nThe prior probabilities for identifying the sources of detected nuclear attacks are also much simpler than those of detected cyber attacks. There are many fewer likely suspects for a nuclear attack, as in (7) than for a nuclear attack, as in (8).\n$$\n\\text {P r i o r s}: \\left\\{ \\begin{array}{l l} \\sum p _ {H 1} \\dots p _ {H 1 0} = . 9 5 &amp; \\text {d e c l a r e d a n d k n o w n n u c l e a r s t a t e s} \\\\ \\sum p _ {H 1 1} \\dots p _ {H 2 0} = . 0 4 &amp; \\text {u n d e c l a r e d n u c l e a r s t a t e s} \\\\ \\sum p _ {H 2 1} \\dots p _ {H 1 0 0} = . 0 1 &amp; \\text {n o n - n u c l e a r s t a t e s , n o n - s t a t e a c t o r s} \\end{array} \\right. \\tag {7}\n$$\n$$\n\\text {P r i o r s}: \\left\\{ \\begin{array}{l l} \\sum p _ {H 1} \\dots p _ {H 1 0} = . 3 0 &amp; \\text {k n o w n s t a t e a d v e r s a r i e s} \\\\ \\sum p _ {H 1 1} \\dots p _ {H 2 0} = . 1 0 &amp; \\text {s u s p e c t e d s t a t e a d v e r s a r i e s} \\\\ \\sum p _ {H 2 1} \\dots p _ {H 1 0 0} = . 3 0 &amp; \\text {n o n - a d v e r s a r i a l s t a t e s} \\\\ \\sum p _ {H 1 0 1} \\dots p _ {H 5 0 0} = . 3 0 &amp; \\text {n o n - s t a t e a c t o r s} \\end{array} \\right. \\tag {8}\n$$\nwhere\nH1: Hypothesis attributing attack to country 1\n$\\mathbf{P}_{\\mathrm{H1}}$  : Probability that H1 is true\n$\\mathbf{P}_{\\mathrm{H1|E}}$  : Probability that H1 is true given evidence E\nThe identity of the attacking state is a hypothesis that the state will attempt to prove by means of evidence. Evidence distinguishes among candidate hypotheses. Absent any data, the states has trouble choosing which adversary to hold responsible for an attack. Evidence of one sort or another updates beliefs. In the Bayesian framework, the best evidence is that which is essentially incompatible with an otherwise-plausible theory. It forces the state to relinquish cherished presuppositions about looming conflicts.\nThe quality of the evidence can be characterized as the difference between the prior distribution of probability among available hypotheses (9) and the posterior distribution (10).\n$$\n\\text {P r i o r}: \\left\\{ \\begin{array}{l} p _ {H 1} = . 5 0 \\\\ p _ {H 2} = . 5 0 \\end{array} \\right. \\tag {9}\n$$\n$$\n\\text {P o s t e r i o r}: \\left\\{ \\begin{array}{l} p _ {H 1 | E} = . 9 0 \\\\ p _ {H 2 | E} = . 1 0 \\end{array} \\right. \\tag {10}\n$$\nEvidence plays a much greater role in cyberspace than it does in nuclear conflict.\nElectronic copy available at: https://ssrn.com/abstract=1928870\n4 CYBERSPACE AND DETERRENCE\n7\nBecause the number of plausible candidates for an observed nuclear attack was fairly low, relatively cheap evidence could lead to a high degree of certainty in meaningfully short time horizon. With readily available telecommunications, satellite observations and corroboration by other states, states could achieve a reasonable degree of certainty that an attack had occurred, and that a particular state was responsible. This is ordinarily not the case in cyberspace. Cyber evidence is markedly more difficult to collect, to analyze, and to explain to non-specialists. States' networks have access points and data logs numerous like the grains of sand on a beach. Computer forensics involves sifting through these logs to gather insights that can be integrated with non-technical analysis; all of which takes time. The cost of some cyber attacks is well below that of nuclear weapons development. Most states and many non-state actors might perhaps be involved in any observed cyberattack.\nEvidence is thus a primary tool for shaping other states' behavior. States possessing rich knowledge about internet traffic, threat analysis, forensics of prior attacks, etc. can leverage that knowledge by selectively sharing information. As a trusted third party, states can choose to share evidence that helps neighbors identify attackers and verify that attacks have in fact taken place. This information can produce two specific, useful changes of state behavior: neighbors can mind their own security interests; and neighbors refrain from destructive mistakes driven by inaccurate suspicion.\nDeception is also possible; but only at the expense of the state's reputation for honest communication. Frequent deception encourages potential allies and adversaries to mistrust proffered information; so it dilutes the state's future influence for the sake of a fleeting advantage.\n## 4 Cyberspace and deterrence\n### 4.1 To observe attacks\nThe Wikileaks release of US State Department cables illustrates a particular feature of cyber attacks that changes the strategy of deterrence. Bradley Manning's alleged distribution of classified files to non-combatants outside the United States did enormous damage to the tactical and strategic positions of the United States.³ Until the moment of the attack's public disclosure, however, little if any harm at all came to individuals or property in the United States. The specific information security policies circumvented were controls on Manning's permission to copy files on uncontrolled media. Specifically, he smuggled the files, which were ultimately copied to Wikileaks servers in various countries, out of a secure facility on a CD-R of Lady Gaga music.⁴\nI dwell on this rogue-insider problem because it highlights the difference between the time of tactical cyber attack, such as it is, and that of observable attack effects. Rattray and Healey (2010) refer to cyberattacks as tactically quick but strategically\n³For brevity, I will drop further use of the word alleged, although Manning’s trial has not established the facts of the case or his guilt. Case facts as related here are taken directly from popular media, and not any first-hand knowledge.\n⁴For the uninitiated: one of her better known songs is a duet with Beyoncé, Telephone: “K-kinda busy/ K-kinda busy /Sorry, I cannot hear you, I’m kinda busy./ …Stop callin’, stop callin’,” I don’t wanna think anymore!/ I left my hand and my heart on the dance floor.”\nElectronic copy available at: https://ssrn.com/abstract=1928870\n4 CYBERSPACE AND DETERRENCE\n8\nslow; meaning that cyberattacks often require years of preparation. Once Manning had the disk out of the secure facility, all of the remaining cyber operations happened off site and on computers not in control of the injured party. Though Manning's actions were both illegal and injurious to his country, none of his specific actions likely required him to circumvent electronic controls on his access. He did not need to crack encryption or authentication. He did not exploit flaws in software design. He used only the privileges granted to him in his official capacity. Prior to the dissemination of classified files, the only evidence that forensics could have turned up would be logs of file transfers to an optical drive by an authorized user. Had Manning been questioned about the incident, there is no reason to suspect that the investigation would have turned to public disclosure of the copied files. The act of disclosure was both the source of injury and the only likely moment when investigators would have begun to suspect foul play.\nThis distinction is hard to reconcile with consequentialist cyber attack definitions. Notably, Libicki (2009) draws a spectrum of consequence severity, with either financial losses or bloodshed as the standard for valid reprisals.⁵ Prohibited access to computer systems falls into a consequentialist gray area, neither nuisance nor grievance. Libicki explicitly considers the difference between attacks achieved by cyber means and simple deception, in order to conclude that it is the consequence of the attack and not its means that justifies *jus ad bello*. If we adopt a consequentialist standard for cyber attack, however, we cannot escape the conclusion that harmless enough tactical maneuvers may be used to prepare the ground for future attacks of devastating consequence. The attacker's tactical maneuver may occur well in advance of any reasonably observable consequence. Under some circumstances, the severity of consequences cannot be known even in principle until the attacker effectuates them.\nLin (2009) distinguishes between *cyberattack* and *cyberexploitation*, whereby one entails merely copying files, software and hardware configurations to remote locations. Cyber attack, on the other hand, aims to degrade, alter or destroy software and hardware belonging to the adversary. Lin's categories distinguish attacks primarily designed to benefit the attacker by yielding information about the adversary, from attacks designed to impair the target's systems themselves. Lin developed this framework about the same time that he edited a review of offensive cyberattack capabilities for the United States (Owens et al., 2009).\nMany other classifications of malware and cyber attack exist. Rattray and Healey (2010) describe and illustrate the tactical functions of cyber attack. Technical definitions from military doctrine are grounded in the role cyber operations play in military tactics: support of non-cyber troops, interdiction, reconnaissance, counter-cyber, suppression of cyber defenses, and a strategic-level cyber mission. Their hypothetical scenarios point to different military and political contexts: interstate conflict, piracy, irregular war, terrorism, covert operations, and intelligence gathering.\nTsipenyuk et al. (2005) describe seven categories of vulnerabilities in software design, which today and in the future will lead to unauthorized access (i.e., read) and execution (i.e., write, execute). These technical categories describe the means by which encryption, authentication, state variables, and controls against unauthorized execution\n⁵Technically, Libicki's scale distinguishes between mass casualty events and heightened risk of nuclear catastrophe. I have omitted the distinction here.\nElectronic copy available at: https://ssrn.com/abstract=1928870\ncan be prevented.\n### 4.2 To identify attackers\nAttributing cyber attacks to specific individuals, organizations, and IP addresses is a deep and abiding problem in cyber security. Although there is considerable difference of expert opinion on the feasibility of attribution, *Clark and Landau (2010)* problematize the issue succinctly. They discuss the different meanings of attribution based on the defender’s purpose: machine-level attribution identifies the origins and sources of packets, and often their contents. Individual level attribution links the machine’s user to official identities such as name, residence, place of business, and registration with various government and corporate entities. Clark and Landau point out that the main obstacle to cyber attribution is often jurisdiction, rather than technical capability. The deciding factor, often, is whether investigators have timely cooperation for success across jurisdictions.\n### 4.3 To retaliate\nSecond strike capability raises more problems than it solves. *Owens et al. (2009)* devote an entire book to the legal and political difficulties facing states that would develop cyber weapons. Developing an offensive capability, due to the nature of cyber weapons, is not at all similar to firing missile tests and taking satellite observations of prospective targets. Cyber weapons largely do not point and shoot, and the ones that states are likely to employ as a retaliatory strike are not simple botnet attacks that would take websites offline. Instead, offensive cyber capabilities should require long periods of active gathering of access to potential adversaries’ systems. Only after a long period of reconnaissance can attackers have confidence in the success of a timely counterattack.\nDefining proportional response to a cyber attack is difficult, since it is often the effects of the cyber attack on non-cyber systems that states wish to punish. This line of reasoning is a slippery slope for states that seek to mitigate the risk of of cyber conflict or mollify incipient conflicts. Detractors of cyber offense also point to America’s relatively greater reliance on information technology both in military and civilian life *(Schwartau, 2000)*. Since we have more to lose from successful IT attacks, restricting the notion of proportionality to pure cyber retaliation for cyber attacks is a self-defeating line of reasoning; not unlike Richard Clarke’s famous accusation that Donald Rumsfeld wanted to find a better adversary than Afghanistan, with better targets for the Air Force.\nKnock-on effects of declaratory policy in this area are also troublesome. If we could define standards of proportionality and achieve consensus thereon, cyber attacks are sufficiently flexible that clever attackers could adjust the lethality, destructiveness, and economic impact of cyber attacks to evade reprisals.\nIn fact a number of the problems with cyber deterrence as a strategy have to do with knock-on effects *(Morgan, 2010)*. Convincing adversaries that the state has no plans to use cyber attacks is difficult enough, without declaring a policy to boot that virtually necessitates years of covert operations to prepare the ground for those cyber retalia\n4 CYBERSPACE AND DETERRENCE\n10\ntions. Declaration of intended lethal retaliation for pure cyber attacks could plausibly weaken credibility, if the promised reprisals are too strong.\n## 4.4 Time, the essential variable\nThe greatest single problem with deterrence in cyberspace is the anachronism of attack, effect, and retaliation. The central promise of deterrence is that the counterstrike will rob the attacker of the supposed fruits of the attack, through the promise of swift and certain retribution. For the most interesting cyber attacks, i.e., ones where the attacker will prepare the ground for days or years in advance of a tactical opportunity, the damage has long been done before the effects of the cyber attack become apparent. The promise of swift and certain retribution may simply ring hollow.\nSchwartau (2001) lays out a unique and relevant security model in layman’s terms. Schwartau points out that defensive security measures, in and of themselves, are largely all fallible. The most secure physical and cyber technologies that we can create are designed to keep things secret or off limits for a period of time, until a complementary response can be activated. Door locks and safes are measured by the duration of time for which they can withstand well defined classes of physical attack. Encryption algorithms are measured by the time required to crack passwords by brute force. Password tokens and password expiry dates are designed to ensure that passwords have a short shelf life (more on that below). Airgapped networks are designed to limit the opportunities that attackers have to access the system. By limiting the frequency of access, they effectively raise the time required to penetrate the system. Even the decision to maintain obscure, legacy operating systems (not Windows, Mac, or Unix) in particular firms demands time of would-be attackers, i.e., time spent developing the competence to crack access controls. In Schwartau’s model, every security measure is a method of buying time until the cavalry can arrive.\nSchwartau’s definition of security as a measure designed to gain time for appropriate response further complicates deterrence. The time to respond to an attack can only begin from the observation of the attack. A fully trusted computing platform, that could inform us when it had been breached—instead of when the attack’s effects were observed—is at best a distant prospect in the future.\nRoughly speaking, let us assume that the greater the time available for reconnaissance, the greater the scale of consequences that attackers can achieve and the higher the degree of confidence they can have in the result. If that is true, then there is an inexorable advantage among states under the security dilemma to conduct reconnaissance as early as possible. As a result, states know they must expect this behavior from their adversaries.\nThe start date of technical actions in cyber conflict occurs well in advance of the start date of political conflict, and well in advance of the start date of observed effects of cyber attacks. One of the main strategic goals of the national cyber defense strategy must be to take advantage of the planning phase of cyber conflict. The ways to take advantage of that phase are to develop situational awareness; to conduct appropriate counter-reconnaissance; to practice civil and military defense; to foster close interjurisdictional collaboration; and to gain the trust of likely allies.\nElectronic copy available at: https://ssrn.com/abstract=1928870\n5 LEVERS IN CYBERSPACE\n11\n# 5 Levers in cyberspace\n## 5.1 Cooperation\nCooperation is one of the policy levers that can serve cybersecurity strategy. There are many domains where allies can cooperate on cyber issues: law enforcement, intelligence, military, and civil defense. In each of these areas overlapping capabilities can provide specific benefits to potential allies.\nThe rapid pace of technological development is one oft-cited reason that cooperation is vital to success in cyberspace; but is not the only one. As a rough analogy, the databases used by antivirus software have a short shelf life. Within days they are obsolete, as the variety of malware observed in the wild grows continuously. Nearly all cybersecurity problems evolve and require constant study due to changes in software, hardware, and business practices. Any small, isolated organization will likely benefit from exchange of information related to cybersecurity, whether or not that information is of a primarily technical nature.\nInstitutional collaboration is arguably more important than exchange of technical information. Computer crimes in peacetime frequently involve a morass of overlapping national and local laws, international agreements, and competing authorities for investigation, prosecution and jurisdiction (Westby, 2004). People, hardware, and communications may operate in several (or dozens of) countries, using communications infrastructure, services, and data facilities that are themselves spread across geographic and political boundaries. Forensics, investigation and prosecution require formal structures for timely collaboration across these various borders.\nIntelligence gathering and threat analysis require similar institutional agreements for interagency and international cooperation. Intelligence organizations are rightly designed to take care with sensitive and classified information. Intelligence organizations by default lock down access to information that could be valuable to strategic competitors.\nIn peacetime, we must assume that states, firms, and non-state actors have already undertaken reconnaissance activities. Indeed evidence of this abounds (Demchak and Dombrowski, 2011). Exchange of reconnaissance information, both technical and organizational in nature, has the potential to greatly increase the quality of each party's information. There may be no hard and fast line between reconnaissance activities in cyberspace, threat analysis, and situational awareness for national defense. Many pieces of information that are necessary for threat analysis and attack detection would ultimately be useful for countermeasures and counterstrikes to cyber attacks.\nFinally, joint exercises are an important precursor to engagement in notionally \"hot\" cyber wars. Cyber security simulations train military and civil authorities to detect, analyze, and adapt to cyber attacks in the context of political conflicts, conventional hostilities, and threats to vital national interests. These simulations align expectations among allies for best practices, standards of inference, norms of behavior, and thresholds for retaliation of various kinds. Given the paucity of global standards for cyber conflict (Hayden, 2011; Libicki, 2009), first-hand experience of other countries' behavior and analysis may prove indispensable. Countries excluded from these exercises lose valuable opportunities to train their own personnel, develop and test\nElectronic copy available at: https://ssrn.com/abstract=1928870\n5 LEVERS IN CYBERSPACE\n12\nprocedures, learn about neighbors' practices, and advocate for favored practices with potential allies.\nGenerally, the larger the network of cooperating states, the greater is the potential benefit of access therein; and the greater the harm from exclusion. (Conversely, the larger the number of states collaborating, the more difficult it is to achieve consensus on exclusion, and to monitor and enforce such decisions.) Similarly, the greater the capabilities of the states collaborating in such a network, the greater is the benefit of inclusion in the network.\n## 5.2 Shared networks\nSpecific networks can themselves be assets offered to share with allies and withheld from enemies. These networks may be bounded by physical connections, communications protocols, or logical restrictions on access. As with the previous section, each of these levers can be wielded either as a carrot or a stick.\nSpecial diplomatic communications networks predate computers themselves. Telephone hotlines connected Moscow to Washington during the Cold War, in order to prevent missed calls during a nuclear crisis. SIPRNET limits unauthorized access to sensitive and classified information systems; and in theory it limits the temptation for personnel to copy sensitive information out to insecure hardware.\nIn the private sector, special financial networks connect banks, trading desks, market exchanges, and clearinghouses. Most firms and households do not have access to these information systems. Many other examples of communications networks for finance exist, in addition to those serving the largest OECD banks. Credit card payments networks, electronic funds transfers, wire transfers, Paypal, Western Union, and Islamic payments networks all provide special access to particular financial communications. Membership in these networks greatly reduces transaction costs for both the end users and the network operators themselves.\nSimilarly, telecommunications networks provide still further benefits. Telecommunications encompass a vast array of services, as simple and open as mobile telephone calls or shortwave radio broadcasts; and as complex as combat communications systems, which integrate secure streams of voice, positioning, imaging, targeting, and order-of-battle data.\nIn the future, as user identification protocols become more pervasive, participation in those registries will become another important source of leverage. Registries need not be centralized; but the ability to verify cryptographic signatures is central to a wide variety of authentication and encryption protocols for communications. Various levels of access could include permission to request an identity (either for an individual or for hardware); permission to request identities on behalf of others; permission to verify users' credentials against the registry of identities; and permission to create, alter, and delete identities in the registry. Trust is enacted in code. Software structures the choices users can make about extending trust to registries, users, and administrators. States, both by the grant (or revocation) of trust to various identity protocols and by the limits placed on identities created for users, can structure other states' access to legitimate telecommunications networks.\nElectronic copy available at: https://ssrn.com/abstract=1928870\n### 5.3 Standards development\nStates will also want a voice in the development of global standards in IT. A dizzying array of standards bodies exist today, and these standards bodies are often located outside government; although more and more states seek to bring these institutions directly under their authority. Standards bodies govern the development of technologies such the internet protocol itself and elements of commonly used computer languages and file formats; the assignment of names and addresses; and many other related technical standards.\nStandards bodies also handle closely related non-technical matters, such as financial, accounting and risk standards for corporate information technology. Standards bodies may be located within the government, in international organizations (e.g., the International Telecommunications Union), within a single private company (e.g., Microsoft), in industry associations, in a non-profit entity with joint representation from industry and government (e.g., ICANN), or in a private non-profit (e.g., Mozilla Foundation). Government may also solicit bids or competitions for specific standards, such as encryption algorithms, which are designed privately and then adopted by the government for official business. Sofaer et al. (2010) discuss the bodies in the international system with jurisdiction over IT standards and telecommunications law.\nMultilateral standards bodies could potentially play a role in technical, financial, legal, or other standards. Multilateral bodies are notoriously slow and weak, particularly when consensus is required to undertake action (Tsebelis, 1995). Bilateral treaties can substitute for multilateral arrangements, as in the case of investment law. Where a strong state enacts a number of nearly identical treaties with weaker counterparties, it effectively reduces the power of the counterparties in negotiations, who otherwise may be excluded from advantageous economic and security relations with the stronger state. The bilateral approach is also more brittle. The strong state must replicate bilateral negotiations over proposed amendments with each weaker state. Weaker states have less incentive to remain in bilateral agreements when disputes arise; exiting a multilateral treaty or body entails severing a tie with all member states.\n### 5.4 Private industry\nStates wishing to influence adversaries’ behavior must consider markets and market regulations as part of cyber strategy. Commercial and industrial policies have far reaching consequences for the development of technology, industry and a skilled workforce. Similarly, the alternatives available to would-be attackers depend greatly on commercial markets for software and hardware. States have many modes of contact with private industry.\nStates can legislate or regulate business practices in strategically important industries. For instance, the United States has banned unauthorized export of advanced weapons systems. States can enact financial and risk management standards that influence business practices. Rice (2008) discusses extensively how standards of risk tolerated in software design differ from those in consumer products, and the consequences of risk and liability for product design and marketing. Anderson and Moore (2006) develop economic models of externalized costs that explain persistent problems\n6 CONCLUSION\nwith software design, digital privacy, and insurance markets for cyber damages. States can also attempt to manage the development of standards for software, data, and communications; but Varian (2001) explains the particulars of how and why industry may not cooperate.\nStates can impose conditions on vendors. These conditions can involve criteria related to software license structure; geographic and political ties; adherence to particular data formats; performance criteria of services; features that create vendor lock-in; the components of software and service bundles; supply chain assurance; and a host of other features (Rosenweig, 2010). The purchasing power of the government may not be sufficient to appreciably alter the market shares of leading technology and standards; but it does provide the state with a lever of control over at commercial software developed for its own use.\nStates can develop standby arrangements for civil defense. These standby arrangements may be as simple as regular goodwill visits between (for example) the Department of Homeland Security industry liaisons and industry executives. They may take the form of immediate crisis response, either focused directly on monitoring and maintaining computer networks (e.g., Computer Emergency Readiness Teams) or on infrastructure assurance during cyber events. States have also developed laws governing corporate cooperation in law enforcement and national information policies; although these interactions entail potentially huge liability and reuptational risks to the corporation. Global corporations, for their part, face difficult decisions about whether corporate values and governance permit accommodation of unique local laws.\nStates can choose whether and when to subsidize and restrict public access to information assets, such GPS information and satellite imagery. Forbidding access to certain information assets may have the unintended consequence of encouraging competitors to acquire new skills and capabilities. Furthermore, the state can commission research on technologies that are not yet commercially available.\nStates that seek to gain strategic advantage in cyberspace must consider the unorthodox economics of information technology industries. Varian et al. (2004) describe features of supply and demand in information technology, competitive strategy and pricing dynamics, and relevant features of networks. Brafman and Beckstrom (2006), Tapscott and Williams (2006), and Barabási (2002) and give useful and engaging introductions to some newer theoretical approaches that help to explain why some networks don’t have leaders, what motivates apparently uncompensated collaborators on large projects, and how to think about the important features of networks. State policies on cyber security are fraught with pitfalls of unintended consequences; and economics is a good tool to demarcate the big ones.\n## 6 Conclusion\nThe basic condition of cyber conflict described in this article is that situational awareness is costly. States must assume that adversaries have begun to plumb the depths of the network long in advance of a conflict’s apparent start date. Most security measures that are designed to buy time for effective response must be implemented during peacetime. As a result, strategies that ignore the realities of daily business will not win\nElectronic copy available at: https://ssrn.com/abstract=1928870\nREFERENCES\nthe full cooperation of key industry partners. Winning cyber conflicts will, in a very specific sense, require states to win the peace.",
  "references": [
    "# References\nAchen, C. H. and D. Snidal (1989, January). Rational deterrence theory and comparative case studies. *World Politics* 41(2), 143–169.\nAllen, J. H., S. Barnum, R. J. Ellison, G. McGraw, and N. Mead (2005). *Software Security Engineering*. Addison Wesley.\nAnderson, R. and T. Moore (2006, 27 October). The economics of information security. *Science* 314(5799), 610–613.\nArt, R. J. (1980, April). To what ends military power? *International Security* 4(4), 3–35.\nBajaj, K. (2010). Cybersecurity agenda: Mobilizing for international action. EastWest Institute.\nBarabási, A.-L. (2002). *Linked: The New Science of Networks*. Perseus Books Group.\nBorg, S. (2005). Economically complex cyberattacks. *IEEE security &amp; privacy* 3(6), 64–67.\nBrafman, O. and R. Beckstrom (2006). *The Starfish and the Spider: The Unstoppable Power of Leaderless Organizations*. Penguin Group USA.\nBrodie, B. (1978, April). The development of nuclear strategy. *International Security* 2(4), 65–83.\nClark, D. D. and S. Landau (2010). Untangling attribution. In *Proceedings of a Workshop on Deterring Cyberattacks: Informing Strategies and Developing Options for US Policy*. National Research Council: National Academies Press.\nClarke, R. (2009). War from cyberspace. *National Interest* 2009(104), 31–36.\nClarke, R. and R. Knake (2010). *Cyber War: The Next Threat to National Security and What to Do About It*. HarperCollins.\nDemchak, C. C. and P. Dombrowski (2011). Rise of a cybered Westphalian age. *Security Studies Quarterly* 5(1), 34–63.\nDunn, L. A. (1994). Rethinking the nuclear equation: The United States and the new nuclear powers. *The Washington Quarterly* 17(1), 5–25.\nFalliere, N., L. O. Murchu, and E. Chien (2010, November). W32.Stuxnet Dossier: Version 1.3. Symantec.\nGorman, S. (2010, 8 October). “US plans cyber shield for utilities, companies”. *Wall Street Journal*.\nElectronic copy available at: https://ssrn.com/abstract=1928870\nREFERENCES\nHayden, M. (2011). The future of things “cyber”. *Strategic Studies Quarterly* 5(1), 3–7.\nJervis, R. (1979, January). Deterrence theory revisited. *World Politics* 31(2), 289–324.\nJoint Chiefs of Staff (2000). Joint Doctrine for Electronic Warfare. Joint Publication 3-51.\nJoint Chiefs of Staff (2007). Electronic Warfare. Joint Publication 3-13.1.\nKahneman, D. and A. Tversky (1979, March). Prospect theory: An analysis of decision under risk. *Econometrica* 47(2), 263–291.\nKahneman, D. and A. Tversky (2000). *Choices, Values, and Frames*. Cambridge University Press.\nKrekel, B. (2009, October). Capability of the people’s republic of china to conduct cyber warfare and computer network exploitation. The US-China Economic and Security Review Commission.\nLangevin, J. R., M. T. McCaul, S. Charney, and H. Raduege (2008, December). Securing cyberspace for the 44th presidency. A report of the CSIS commission on cyber-security for the 44th presidency, Washington, DC.\nLiang, Q. and W. Xiangsui (2002). *Unrestricted Warfare: China’s Master Plan to Destroy America*. Pan American Publishing Company.\nLibicki, M. C. (2009). *Cyberdeterrence and Cyberwar*. RAND.\nLin, H. (2009, July). Lifting the veil on cyber offense. *Security and Privacy IEEE* 7(4), 15–21.\nLukasik, S. (2010). A framework for thinking about cyber conflict and cyber deterrence with possible declaratory policies for these domains. In *Proceedings of a Workshop on Deterring Cyber Attacks: Informing Strategies and Developing Options for US Policy*, Washington, D.C. National Academies Press.\nLynn, William J., I. (2010, September). Defending a new domain: The Pentagon’s cyberstrategy. *Foreign Affairs*.\nMcConnell, M. (2010, 28 February). “We’re losing the cyber war. Here’s the strategy to win it.”. *The Washington Post*.\nMorgan, P. (1977). *Deterrence*. Sage.\nMorgan, P. (2010). Applicability of traditional deterrence concepts and theory to the cyber realm. In *Proceedings of a Workshop on Deterring Cyber Attacks: Informing Strategies and Developing Options for US Policy*, Washington, D.C. National Academies Press.\nNye, Joseph S., J. (2010). Cyber Power. Belfer Center for International Affairs, Harvard Kennedy School.\nhttp://www.ebi.org/ebi/ebi_0182\nElectronic copy available at: https://ssrn.com/abstract=1928870\nREFERENCES\nOwens, W. A., K. W. Dam, and H. S. Lin (Eds.) (2009). Technology, policy, law and ethics regarding US acquisition and use of offensive cyberattack capabilities. Committee on Offensive Information Warfare: National Research Council.\nRattray, G. and J. Healey (2010). Categorizing and understanding offensive cyber capabilities and their use. In Proceedings of a Workshop on Deterring Cyber Attacks: Informing Strategies and Developing Options for US Policy. National Academies Press.\nRattray, G. J. (2001). Strategic War in Cyberspace. MIT Press.\nRice, D. (2008). Geekonomics: The Real Cost of Insecure Software. Addison Wesley Professional.\nRosenweig, P. (2010). The organization of the united states government and private sector for achieving cyber deterrence. In Proceedings of a Workshop on Deterring Cyber Attacks: Informing Strategies and Developing Options for US Policy, Washington, D.C. National Academies Press.\nSchmitt, M. N. (2010). Cyber operations in international law: the use of force, collective security, self-defense, and armed conflicts. In Proceedings of a Workshop on Deterring Cyberattacks: Informing Strategies and Developing Options for US Policy. National Research Council: National Academies Press.\nSchwartau, W. (2000). Looming security threats: Asymmetrical adversaries. *Orbis* 44(2), 197–205.\nSchwartau, W. (2001). Time Based Security (2nd ed.). Interact Press.\nSingel, R. (2010, 1 March). Cyberwar hype intended to destroy the open internet. Blog post, http://www.wired.com/threatlevel/2010/03/cyber-war-hype/.\nSofaer, A. D., D. Clark, and W. Diffie (2010). Cyber security and international agreements. In Proceedings of a Workshop on Deterring Cyber Attacks: Informing Strategies and Developing Options for US Policy. National Academies Press.\nTaliaferro, J. W. (2001). Security seeking under anarchy: Defensive realism revisited. International Security 25(3), 128–161.\nTannenwald, N. (1999). The nuclear taboo: The United States and the normative basis of nuclear non-use. International Organization 53, 433–468.\nTapscott, D. and A. D. Williams (2006). *Wikinomics*. Penguin.\nThornburgh, N. (2005, 29 August). “The invasion of the Chinese cyberspies (and the man who tried to stop them)”. *Time*.\nTsebelis, G. (1995). Decision making in political systems: Veto players in presidentialism, parliamentarism, multicameralism and multipartyism. *British Journal of Political Science* 25(03), 289–325.\nElectronic copy available at: https://ssrn.com/abstract=1928870\nREFERENCES\nTsipenyuk, K., B. Chess, and G. McGraw (2005, November). Seven pernicious kingdoms: a taxonomy of software security errors. *Security and Privacy, IEEE* 3(6), 81–84.\nTversky, A. and D. Kahneman (1974). Judgment under uncertainty: Heuristics and biases. *Science* 185(1), 124–131.\nVarian, H. R. (2001, 15 November). Economics of information technology. Raffaele Mattioli Lecture, Bocconi University.\nVarian, H. R., J. Farrell, and C. Shapiro (2004). *The Economics of Information Technology*. Cambridge University Press.\nWalton, G., R. J. Deibert, A. Machanda, R. Rohozinski, and N. Villanueva (2009, 29 March). Tracking GhostNet: Investigating a cyber espionage network. Citizen Lab, Munck Centre for International Studies, University of Toronto.\nWaltz, K. N. (1990). Nuclear myths and political realities. *The American Political Science Review* 84(3), 731–745.\nWestby, J. (2004). *International Guide to Cyber Security*. American Bar Association.\nWhite House (1998, 22 May). Critical infrastructure protection. Presidential decision directive / NSC 63.\nWhite House (2009). Cyberspace policy review: Assuring a trusted and resilient information and communications infrastructure.\nElectronic copy available at: https://ssrn.com/abstract=1928870"
  ],
  "flat_text": "Electronic copy available at: https://ssrn.com/abstract=1928870 # Leverage in cyberspace, without deterrence Benjamin D. Mazzotta The Fletcher School of Law and Diplomacy, Tufts University benmazzotta@alumni.tufts.edu March 19, 2011 Paper presented to the Annual Convention of the International Studies Association.\nMontréal, QC.\n## Abstract Deterrence theory, which was of enormous value during the nuclear era, cannot adequately prepare defense thinkers for future cyber conflicts. Instead, the best states can hope for is robust cooperation with other states and with private actors to improve their circumstances in future conflicts. A number of key assumptions that supported nuclear deterrence do not hold in the cyber domain: immediately observable attacks; clear legal status under the law of armed conflict; states' preeminence for research and development; high financial and technical barriers to entry; cheap observation of delivery facilities; feasible second strike; and a stable MAD outcome. States can create several incentives for improved collaboration with allies and private sector partners. States can derive mutual benefits from shared intelligence; shared standards for risk assessment; joint exercises; and standby arrangements for crucial information, equipment, materials, and services. Apart from sanctions against access to these carrots, sticks include reconnaissance for cyber conflicts, cyber exploitation with the threat of disclosure, and policies explicitly linking cyber events to non-cyber diplomatic and security concerns. States have additional leverage over private actors through commercial regulation, law enforcement, and liability; although the burden of compliance may poison public-private cooperation.\n# 1 Introduction Cyber attacks have generated enormous controversy in international relations along the lines of a recurrent *Late Show with David Letterman* segment,\"Is this anything?\"Cyber hawks point to the far-reaching effects and cost-efficacy of attacks. Skeptics accuse hawks of crying Chicken Little over simple web defacement. Cyber war has captured Hollywood's imagination at the strategic level as in *Die Hard IV: Live Free or Die Hard*; at the tactical level as in *Vantage Point*; and oftener as dystopian techno-singularity as in *Eagle Eye*, *The Matrix*, and 2001: A Space Odyssey. Journalists, freelancers, think tanks, business, military doctrine (Joint Chiefs of Staff, 2007, 2000), government documents (White House, 2009, 1998) and IR theorists describe broad classes of attacks whose objectives might include the disruption of civilian infrastructure; tactical advantage in conventional war; intelligence gathering; financial gain; and terrorism. This article assumes the feasibility of broad classes of attack as a moot point, and outlines the strategic consequences of those attacks as if they were known capabilities.\nFaced with similarly disruptive military technology in the 1940s, the discipline of international relations refined the concept of deterrence to devise a strategy for safe navigation of potentially catastrophic nuclear arms races. Nuclear theorists then and cyber theorists now pointed out strikingly similar aspects of the new technology: vast geographic reach of the attacks, revolutionary intensity of purported attack effects, and relative cost efficacy of the new weapons. Cyber attacks may promise a revolution in military affairs similar to the nuclear revolution. Nuclear theorists then focused on several features of the new technology and constructed a strategic equilibrium from which deviance was less likely. Cyber theorists now must look carefully at the features of the new technology to assess its strategic consequences.\nDeterrence in cyberspace is a mirage. Cyberspace itself bears little resemblance to the physical world around us, and efforts to reason by analogy with physical space often do more harm than good. Nuclear deterrence is a particularly poor strategic model for cyberspace. The best we can hope for in cyberspace is to enlist the aid of allies who may cooperate for mutual benefit. There are specific sources of leverage that we have in cyberspace, thanks to our unusual capabilities in government and the private sector. Only when taken out of context, may these levers of control be shoehorned into a purported model of deterrence. Faith in deterrence may trick us to trust our own defenses, and blind us to opportunities for alliance.\n## 2 Deterrence theory ### 2.1 Incentives Deterrence works, in theory, because of a brutally simple set of incentives facing the state. State behavior under the security dilemma is the basis of deterrence. Aggressor states seek to gain power through aggressive wars. Strategic competitors promise retaliation for these wars that more than wipes out the gains from expansion.\nDeterrence theory takes as its point of departure that the state is a rational actor, subject to an inescapable calculus of self-interest. Various critiques of the rational actor model of the state in international relations have been advanced, including unraveling of deterrence commitments, well-established cognitive and behavioral deviance from simple economic self-interest; status-seeking behaviors in nuclear weapons development; Electronic copy available at: https://ssrn.com/abstract=1928870 taboos in tactical deployment of nuclear weapons; and constructivist narratives of leadership and the national interest.\nJervis (1979) describes waves of innovation designed to address shortcomings of deterrence theory. The major features of these second-wave models are the aggressor’s political calculus; the possibility of rewards and negotiations as complements to simple retaliation; and constructivist interpretations of the aggressor’s goals in security. Many of these criticisms are further explicated in Taliaferro (2001), which organizes contemporary variants of realist international security theory.\nNecessary conditions for effective deterrence vary greatly from one variant of the theory to another. Waltz (1990) holds that the key to nuclear deterrence is a reliable second strike that eliminates potential gains from aggression. He further argues that the low cost and weight of nuclear weapons leads inexorably to defensive vulnerability. Brodie (1978) argued that deterrence applied mainly to the Soviet-American security dilemma, and largely on the basis of Soviet acquisition of nuclear weapons. Extended deterrence modeled the behavior of allies within this bilateral framework.\n### 2.2 Technology Key features of nuclear technology circa 1960 were ease of observing weapons deployment at various stages, including launch site construction, weapons launch, and successful attack sites; extreme lethality and blast radius; absence of viable defensive countermeasures; difficulty of developing nuclear weapons, measured by financial costs, knowledge, and manufacturing capabilities; and limited means of delivery, viz., aircraft, oceangoing vessels, and ballistic missile. These points are not controversial. Efforts to overcome these technical limits of nuclear weapons (e.g., anti-ballistic missile defenses) attracted opprobium, as they threatened the stability of the mutually assured destruction (MAD) equilibrium.\nNuclear technology itself ensures that the adversary is immediately aware of any attacks; that a short list of suspects are considered capable of an attack; and that, in fact, at least one from the short list must have participated in the attack at some level. Dunn (1994) notes that American strategists ignored the possibility of nuclear weapons smuggling for just that reason. Satellite and aircraft observations were long considered sufficient to monitor nuclear weapons deployment. More exhaustive inspections of suspected facilities featured prominently in Iran, Iraq, and North Korea; but the nature of weapons development has changed considerably since the early nuclear age. Today critical materials are widely available, e.g., nuclear expertise, raw fuel, and industrial components for enrichment and delivery systems. This was absolutely not the case during the early nuclear age.\n### 2.3 Deterrence in cyberspace Deterrence, having served America so well during the Cold war, was an obvious place to begin discussions of cyber strategy. McConnell (2010), in an op-ed on cyber strategy, poses a choice between deterrence and preemption as the central strategic question in cyberspace. He further argues that deterrence rests on four necessary capabilities: a declared retaliation policy, attribution of the attack to the attacker, location of the Electronic copy available at: https://ssrn.com/abstract=1928870 attacker, and the counterstrike itself. If the layman’s definition of deterrence is that we can make ourselves safe in cyberspace without shooting first, then let us all hope that McConnell is correct. On the other hand, there is no consensus among experts that the four necessary conditions McConnell cites are even feasible, let alone likely to be achieved.\nThe National Research Council hosted an essay competition and conference in 2010 dedicated to the feasibility of deterrence in cyberspace. Patrick Morgan, a long-standing skeptic of deterrence theory, questions the applicability of nuclear deterrence to cyberspace **. *Lukasik (2010)* points to serious flaws in a wide variety of declaratory policies on deterrence, such as the threat to retaliate with conventional lethal force. *Clark and Landau (2010)* explain stubborn technical problems with attribution. *Rattray and Healey (2010)* give an overview of offensive cyber weapons, some of which are plausible for retaliation. *Schmitt (2010)* covers the legality of national self-defense under cyber attack; and in a related paper *Sofaer et al. (2010)* outline hypothetical features of international institutions that would purport to ameliorate cyber security.\n## 3 A simple model Successful nuclear deterrence relies on two crucial actions that are trivial in early Cold War nuclear conflict; but that are well-established problems in cyber conflict. The adversary must observe that an attack has in fact occurred. The adversary must also identify the attacker. This section of the article develops a quick Bayesian model of the adversary’s problem, with plausible hypotheses about the likely identity of the attacker.\nThis framework leads to several simple questions about attack technology and the adversary’s decisions.\n\nA convenient starting place is to model the expected utility of the attacker’s payoffs over the future states of the world: viz., attacks with retaliation, attacks without retaliation, and status quo.\n$U\\equiv u(\\mathbf{E}\\pi)$ (1) $\\mathbf{E}\\pi=p_{a}\\pi_{a}+p_{a}\\left(p_{r}\\pi_{r}+p_{\\sim r}\\pi_{\\sim r}\\right)+\\left(1-p_{a}\\right)\\pi_{s}$ (2) where\n\n|   | where  |\n| --- | --- |\n|  U: | expected utility  |\n|  a: | subscript for attack  |\n|  s: | subscript for status quo  |\n|  p: | probability  |\n|  π: | payoff  |\n|  r: | subscript for retaliation  |\n|  ~r: | subscript for nonretaliation  |\nWe can further decompose the probability of retaliation into separate component probabilities, as in (3). The adversary needs to detect that the attack has in fact taken place, attribute the source of the attack, and then decide to respond to the attack. Early Cold War deterrence proceeded as a special case of the equation below, where virtually any attack would lead to certain attribution. Today adversaries may only be able to suspect the adversary's identity; or suspect that an attack has in fact taken place; or have only a rough suspicion on both counts. $^{1}$\n$$\np _ {r} = \\underbrace {p _ {d , i} p _ {r | D \\cap I}} _ {\\text {certain attribution}} + \\underbrace {p _ {d , \\sim i} p _ {r | D \\cap \\sim I}} _ {\\text {unidentified attacker}} + \\underbrace {p _ {\\sim d , i} p _ {r | \\sim D \\cap I}} _ {\\text {suspected attack}} + \\underbrace {p _ {\\sim d , \\sim i} p _ {r | \\sim D \\cap \\sim I}} _ {\\text {rough suspicion}} \\tag {3}\n$$\nAnd when the adversary may mistake the identity of the attacker, as in (4), the attacker may also err in retaliation, with potentially (dis)adventaneous payoffs for the attacker.\n$$\n\\mathbf {E} \\pi = \\underbrace {p _ {\\mathrm {a}} \\pi_ {\\mathrm {a}}} _ {\\text {attack's benefit}} + \\underbrace {p _ {\\mathrm {a}} \\left(p _ {\\mathrm {r}} \\pi_ {\\mathrm {r}} + p _ {\\sim \\mathrm {r}} \\pi_ {\\sim \\mathrm {r}}\\right)} _ {\\text {retaliation's cost}} + \\underbrace {p _ {\\mathrm {d} , \\sim \\mathrm {i}} p _ {\\mathrm {m} | \\mathrm {D} \\cap \\sim \\mathrm {I}} + p _ {\\mathrm {s}} \\pi_ {\\mathrm {s}}} _ {\\text {mistaken counterattack}} \\tag {4}\n$$\n|   | where  |\n| --- | --- |\n|  pr: | probability of retaliation  |\n|  D: | successful detection of an attack  |\n|  ~D: | failure to detect an attack  |\n|  I: | successful identification of an attacker  |\n|  pr|D∩I: | probability of retaliation conditional on adversary's detection without identification  |\n|  pd: | probability of adversary's detecting attack  |\n|  pi: | probability of adversary's identifying attack  |\n|  pm: | probability of a mistaken counterattack  |\nThere are stark differences between nuclear attacks circa 1960, modeled in (5) and cyber attacks, modeled in (6). $^{2}$  Nuclear attacks are obvious by the scale of their destruction. Attribution was simple, due to the small number of states in possession of the technology. There are no serious barriers to acquisition of technical expertise or requisite hardware for cyber attack. There is no reason to suspect that the adversary\nElectronic copy available at: https://ssrn.com/abstract=1928870\n\nwill understand that an attack will be detected until long after it has begun. There is no reason that the adversary will immediately suspect whether the adversary in a given cyber attack is a state or not a state.\n$$ p _ {d i} \\gg p _ {d, - i} &gt; p _ {- d} \\tag {5} $$ $$ p _ {d, - i} &gt; p _ {- d, i} &gt; p _ {d, i} &gt; p _ {- d, - i} \\tag {6} $$ The prior probabilities for identifying the sources of detected nuclear attacks are also much simpler than those of detected cyber attacks. There are many fewer likely suspects for a nuclear attack, as in (7) than for a nuclear attack, as in (8).\n$$ \\text {P r i o r s}: \\left\\{ \\begin{array}{l l} \\sum p _ {H 1} \\dots p _ {H 1 0} =. 9 5 &amp; \\text {d e c l a r e d a n d k n o w n n u c l e a r s t a t e s} \\\\ \\sum p _ {H 1 1} \\dots p _ {H 2 0} =. 0 4 &amp; \\text {u n d e c l a r e d n u c l e a r s t a t e s} \\\\ \\sum p _ {H 2 1} \\dots p _ {H 1 0 0} =. 0 1 &amp; \\text {n o n - n u c l e a r s t a t e s, n o n - s t a t e a c t o r s} \\end{array} \\right. \\tag {7} $$ $$ \\text {P r i o r s}: \\left\\{ \\begin{array}{l l} \\sum p _ {H 1} \\dots p _ {H 1 0} =. 3 0 &amp; \\text {k n o w n s t a t e a d v e r s a r i e s} \\\\ \\sum p _ {H 1 1} \\dots p _ {H 2 0} =. 1 0 &amp; \\text {s u s p e c t e d s t a t e a d v e r s a r i e s} \\\\ \\sum p _ {H 2 1} \\dots p _ {H 1 0 0} =. 3 0 &amp; \\text {n o n - a d v e r s a r i a l s t a t e s} \\\\ \\sum p _ {H 1 0 1} \\dots p _ {H 5 0 0} =. 3 0 &amp; \\text {n o n - s t a t e a c t o r s} \\end{array} \\right. \\tag {8} $$ where H1: Hypothesis attributing attack to country 1 $\\mathbf{P}_{\\mathrm{H1}}$: Probability that H1 is true $\\mathbf{P}_{\\mathrm{H1|E}}$: Probability that H1 is true given evidence E The identity of the attacking state is a hypothesis that the state will attempt to prove by means of evidence. Evidence distinguishes among candidate hypotheses. Absent any data, the states has trouble choosing which adversary to hold responsible for an attack. Evidence of one sort or another updates beliefs. In the Bayesian framework, the best evidence is that which is essentially incompatible with an otherwise-plausible theory. It forces the state to relinquish cherished presuppositions about looming conflicts.\nThe quality of the evidence can be characterized as the difference between the prior distribution of probability among available hypotheses (9) and the posterior distribution (10).\n$$ \\text {P r i o r}: \\left\\{ \\begin{array}{l} p _ {H 1} =. 5 0 \\\\ p _ {H 2} =. 5 0 \\end{array} \\right. \\tag {9} $$ $$ \\text {P o s t e r i o r}: \\left\\{ \\begin{array}{l} p _ {H 1 | E} =. 9 0 \\\\ p _ {H 2 | E} =. 1 0 \\end{array} \\right. \\tag {10} $$ Evidence plays a much greater role in cyberspace than it does in nuclear conflict.\nElectronic copy available at: https://ssrn.com/abstract=1928870 Evidence is thus a primary tool for shaping other states' behavior. States possessing rich knowledge about internet traffic, threat analysis, forensics of prior attacks, etc. can leverage that knowledge by selectively sharing information. As a trusted third party, states can choose to share evidence that helps neighbors identify attackers and verify that attacks have in fact taken place. This information can produce two specific, useful changes of state behavior: neighbors can mind their own security interests; and neighbors refrain from destructive mistakes driven by inaccurate suspicion.\nDeception is also possible; but only at the expense of the state's reputation for honest communication. Frequent deception encourages potential allies and adversaries to mistrust proffered information; so it dilutes the state's future influence for the sake of a fleeting advantage.\n## 4 Cyberspace and deterrence ### 4.1 To observe attacks The Wikileaks release of US State Department cables illustrates a particular feature of cyber attacks that changes the strategy of deterrence. Bradley Manning's alleged distribution of classified files to non-combatants outside the United States did enormous damage to the tactical and strategic positions of the United States.³ Until the moment of the attack's public disclosure, however, little if any harm at all came to individuals or property in the United States. The specific information security policies circumvented were controls on Manning's permission to copy files on uncontrolled media. Specifically, he smuggled the files, which were ultimately copied to Wikileaks servers in various countries, out of a secure facility on a CD-R of Lady Gaga music.⁴ I dwell on this rogue-insider problem because it highlights the difference between the time of tactical cyber attack, such as it is, and that of observable attack effects. Rattray and Healey (2010) refer to cyberattacks as tactically quick but strategically ³For brevity, I will drop further use of the word alleged, although Manning’s trial has not established the facts of the case or his guilt. Case facts as related here are taken directly from popular media, and not any first-hand knowledge.\n⁴For the uninitiated: one of her better known songs is a duet with Beyoncé, Telephone: “K-kinda busy/ K-kinda busy /Sorry, I cannot hear you, I’m kinda busy./ …Stop callin’, stop callin’,” I don’t wanna think anymore!/ I left my hand and my heart on the dance floor.”\nElectronic copy available at: https://ssrn.com/abstract=1928870 This distinction is hard to reconcile with consequentialist cyber attack definitions. Notably, Libicki (2009) draws a spectrum of consequence severity, with either financial losses or bloodshed as the standard for valid reprisals.⁵ Prohibited access to computer systems falls into a consequentialist gray area, neither nuisance nor grievance. Libicki explicitly considers the difference between attacks achieved by cyber means and simple deception, in order to conclude that it is the consequence of the attack and not its means that justifies *jus ad bello*. If we adopt a consequentialist standard for cyber attack, however, we cannot escape the conclusion that harmless enough tactical maneuvers may be used to prepare the ground for future attacks of devastating consequence. The attacker's tactical maneuver may occur well in advance of any reasonably observable consequence. Under some circumstances, the severity of consequences cannot be known even in principle until the attacker effectuates them.\nLin (2009) distinguishes between *cyberattack* and *cyberexploitation*, whereby one entails merely copying files, software and hardware configurations to remote locations. Cyber attack, on the other hand, aims to degrade, alter or destroy software and hardware belonging to the adversary. Lin's categories distinguish attacks primarily designed to benefit the attacker by yielding information about the adversary, from attacks designed to impair the target's systems themselves. Lin developed this framework about the same time that he edited a review of offensive cyberattack capabilities for the United States.\nMany other classifications of malware and cyber attack exist. Rattray and Healey (2010) describe and illustrate the tactical functions of cyber attack. Technical definitions from military doctrine are grounded in the role cyber operations play in military tactics: support of non-cyber troops, interdiction, reconnaissance, counter-cyber, suppression of cyber defenses, and a strategic-level cyber mission. Their hypothetical scenarios point to different military and political contexts: interstate conflict, piracy, irregular war, terrorism, covert operations, and intelligence gathering.\nTsipenyuk et al. (2005) describe seven categories of vulnerabilities in software design, which today and in the future will lead to unauthorized access (i.e., read) and execution (i.e., write, execute). These technical categories describe the means by which encryption, authentication, state variables, and controls against unauthorized execution ⁵Technically, Libicki's scale distinguishes between mass casualty events and heightened risk of nuclear catastrophe. I have omitted the distinction here.\nElectronic copy available at: https://ssrn.com/abstract=1928870 can be prevented.\n### 4.2 To identify attackers Attributing cyber attacks to specific individuals, organizations, and IP addresses is a deep and abiding problem in cyber security. Although there is considerable difference of expert opinion on the feasibility of attribution, *Clark and Landau (2010)* problematize the issue succinctly. They discuss the different meanings of attribution based on the defender’s purpose: machine-level attribution identifies the origins and sources of packets, and often their contents. Individual level attribution links the machine’s user to official identities such as name, residence, place of business, and registration with various government and corporate entities. Clark and Landau point out that the main obstacle to cyber attribution is often jurisdiction, rather than technical capability. The deciding factor, often, is whether investigators have timely cooperation for success across jurisdictions.\n### 4.3 To retaliate Second strike capability raises more problems than it solves. *Owens et al. (2009)* devote an entire book to the legal and political difficulties facing states that would develop cyber weapons. Developing an offensive capability, due to the nature of cyber weapons, is not at all similar to firing missile tests and taking satellite observations of prospective targets. Cyber weapons largely do not point and shoot, and the ones that states are likely to employ as a retaliatory strike are not simple botnet attacks that would take websites offline. Instead, offensive cyber capabilities should require long periods of active gathering of access to potential adversaries’ systems. Only after a long period of reconnaissance can attackers have confidence in the success of a timely counterattack.\nDefining proportional response to a cyber attack is difficult, since it is often the effects of the cyber attack on non-cyber systems that states wish to punish. This line of reasoning is a slippery slope for states that seek to mitigate the risk of of cyber conflict or mollify incipient conflicts. Detractors of cyber offense also point to America’s relatively greater reliance on information technology both in military and civilian life **. Since we have more to lose from successful IT attacks, restricting the notion of proportionality to pure cyber retaliation for cyber attacks is a self-defeating line of reasoning; not unlike Richard Clarke’s famous accusation that Donald Rumsfeld wanted to find a better adversary than Afghanistan, with better targets for the Air Force.\nKnock-on effects of declaratory policy in this area are also troublesome. If we could define standards of proportionality and achieve consensus thereon, cyber attacks are sufficiently flexible that clever attackers could adjust the lethality, destructiveness, and economic impact of cyber attacks to evade reprisals.\nIn fact a number of the problems with cyber deterrence as a strategy have to do with knock-on effects **. Convincing adversaries that the state has no plans to use cyber attacks is difficult enough, without declaring a policy to boot that virtually necessitates years of covert operations to prepare the ground for those cyber retalia\n\n## 4.4 Time, the essential variable\nThe greatest single problem with deterrence in cyberspace is the anachronism of attack, effect, and retaliation. The central promise of deterrence is that the counterstrike will rob the attacker of the supposed fruits of the attack, through the promise of swift and certain retribution. For the most interesting cyber attacks, i.e., ones where the attacker will prepare the ground for days or years in advance of a tactical opportunity, the damage has long been done before the effects of the cyber attack become apparent. The promise of swift and certain retribution may simply ring hollow.\nSchwartau (2001) lays out a unique and relevant security model in layman’s terms. Schwartau points out that defensive security measures, in and of themselves, are largely all fallible. The most secure physical and cyber technologies that we can create are designed to keep things secret or off limits for a period of time, until a complementary response can be activated. Door locks and safes are measured by the duration of time for which they can withstand well defined classes of physical attack. Encryption algorithms are measured by the time required to crack passwords by brute force. Password tokens and password expiry dates are designed to ensure that passwords have a short shelf life (more on that below). Airgapped networks are designed to limit the opportunities that attackers have to access the system. By limiting the frequency of access, they effectively raise the time required to penetrate the system. Even the decision to maintain obscure, legacy operating systems (not Windows, Mac, or Unix) in particular firms demands time of would-be attackers, i.e., time spent developing the competence to crack access controls. In Schwartau’s model, every security measure is a method of buying time until the cavalry can arrive.\nSchwartau’s definition of security as a measure designed to gain time for appropriate response further complicates deterrence. The time to respond to an attack can only begin from the observation of the attack. A fully trusted computing platform, that could inform us when it had been breached—instead of when the attack’s effects were observed—is at best a distant prospect in the future.\nRoughly speaking, let us assume that the greater the time available for reconnaissance, the greater the scale of consequences that attackers can achieve and the higher the degree of confidence they can have in the result. If that is true, then there is an inexorable advantage among states under the security dilemma to conduct reconnaissance as early as possible. As a result, states know they must expect this behavior from their adversaries.\nThe start date of technical actions in cyber conflict occurs well in advance of the start date of political conflict, and well in advance of the start date of observed effects of cyber attacks. One of the main strategic goals of the national cyber defense strategy must be to take advantage of the planning phase of cyber conflict. The ways to take advantage of that phase are to develop situational awareness; to conduct appropriate counter-reconnaissance; to practice civil and military defense; to foster close interjurisdictional collaboration; and to gain the trust of likely allies.\nElectronic copy available at: https://ssrn.com/abstract=1928870\n\n## 5.1 Cooperation\nCooperation is one of the policy levers that can serve cybersecurity strategy. There are many domains where allies can cooperate on cyber issues: law enforcement, intelligence, military, and civil defense. In each of these areas overlapping capabilities can provide specific benefits to potential allies.\nThe rapid pace of technological development is one oft-cited reason that cooperation is vital to success in cyberspace; but is not the only one. As a rough analogy, the databases used by antivirus software have a short shelf life. Within days they are obsolete, as the variety of malware observed in the wild grows continuously. Nearly all cybersecurity problems evolve and require constant study due to changes in software, hardware, and business practices. Any small, isolated organization will likely benefit from exchange of information related to cybersecurity, whether or not that information is of a primarily technical nature.\nInstitutional collaboration is arguably more important than exchange of technical information. Computer crimes in peacetime frequently involve a morass of overlapping national and local laws, international agreements, and competing authorities for investigation, prosecution and jurisdiction . People, hardware, and communications may operate in several (or dozens of) countries, using communications infrastructure, services, and data facilities that are themselves spread across geographic and political boundaries. Forensics, investigation and prosecution require formal structures for timely collaboration across these various borders.\nIntelligence gathering and threat analysis require similar institutional agreements for interagency and international cooperation. Intelligence organizations are rightly designed to take care with sensitive and classified information. Intelligence organizations by default lock down access to information that could be valuable to strategic competitors.\nIn peacetime, we must assume that states, firms, and non-state actors have already undertaken reconnaissance activities. Indeed evidence of this abounds . Exchange of reconnaissance information, both technical and organizational in nature, has the potential to greatly increase the quality of each party's information. There may be no hard and fast line between reconnaissance activities in cyberspace, threat analysis, and situational awareness for national defense. Many pieces of information that are necessary for threat analysis and attack detection would ultimately be useful for countermeasures and counterstrikes to cyber attacks.\nFinally, joint exercises are an important precursor to engagement in notionally \"hot\" cyber wars. Cyber security simulations train military and civil authorities to detect, analyze, and adapt to cyber attacks in the context of political conflicts, conventional hostilities, and threats to vital national interests. These simulations align expectations among allies for best practices, standards of inference, norms of behavior, and thresholds for retaliation of various kinds. Given the paucity of global standards for cyber conflict , first-hand experience of other countries' behavior and analysis may prove indispensable. Countries excluded from these exercises lose valuable opportunities to train their own personnel, develop and test\nElectronic copy available at: https://ssrn.com/abstract=1928870\n\nGenerally, the larger the network of cooperating states, the greater is the potential benefit of access therein; and the greater the harm from exclusion. (Conversely, the larger the number of states collaborating, the more difficult it is to achieve consensus on exclusion, and to monitor and enforce such decisions.) Similarly, the greater the capabilities of the states collaborating in such a network, the greater is the benefit of inclusion in the network.\n## 5.2 Shared networks Specific networks can themselves be assets offered to share with allies and withheld from enemies. These networks may be bounded by physical connections, communications protocols, or logical restrictions on access. As with the previous section, each of these levers can be wielded either as a carrot or a stick.\nSpecial diplomatic communications networks predate computers themselves. Telephone hotlines connected Moscow to Washington during the Cold War, in order to prevent missed calls during a nuclear crisis. SIPRNET limits unauthorized access to sensitive and classified information systems; and in theory it limits the temptation for personnel to copy sensitive information out to insecure hardware.\nIn the private sector, special financial networks connect banks, trading desks, market exchanges, and clearinghouses. Most firms and households do not have access to these information systems. Many other examples of communications networks for finance exist, in addition to those serving the largest OECD banks. Credit card payments networks, electronic funds transfers, wire transfers, Paypal, Western Union, and Islamic payments networks all provide special access to particular financial communications. Membership in these networks greatly reduces transaction costs for both the end users and the network operators themselves.\nSimilarly, telecommunications networks provide still further benefits. Telecommunications encompass a vast array of services, as simple and open as mobile telephone calls or shortwave radio broadcasts; and as complex as combat communications systems, which integrate secure streams of voice, positioning, imaging, targeting, and order-of-battle data.\nIn the future, as user identification protocols become more pervasive, participation in those registries will become another important source of leverage. Registries need not be centralized; but the ability to verify cryptographic signatures is central to a wide variety of authentication and encryption protocols for communications. Various levels of access could include permission to request an identity (either for an individual or for hardware); permission to request identities on behalf of others; permission to verify users' credentials against the registry of identities; and permission to create, alter, and delete identities in the registry. Trust is enacted in code. Software structures the choices users can make about extending trust to registries, users, and administrators. States, both by the grant (or revocation) of trust to various identity protocols and by the limits placed on identities created for users, can structure other states' access to legitimate telecommunications networks.\nElectronic copy available at: https://ssrn.com/abstract=1928870 ### 5.3 Standards development States will also want a voice in the development of global standards in IT. A dizzying array of standards bodies exist today, and these standards bodies are often located outside government; although more and more states seek to bring these institutions directly under their authority. Standards bodies govern the development of technologies such the internet protocol itself and elements of commonly used computer languages and file formats; the assignment of names and addresses; and many other related technical standards.\nStandards bodies also handle closely related non-technical matters, such as financial, accounting and risk standards for corporate information technology. Standards bodies may be located within the government, in international organizations (e.g., the International Telecommunications Union), within a single private company (e.g., Microsoft), in industry associations, in a non-profit entity with joint representation from industry and government (e.g., ICANN), or in a private non-profit (e.g., Mozilla Foundation). Government may also solicit bids or competitions for specific standards, such as encryption algorithms, which are designed privately and then adopted by the government for official business. Sofaer et al. (2010) discuss the bodies in the international system with jurisdiction over IT standards and telecommunications law.\nMultilateral standards bodies could potentially play a role in technical, financial, legal, or other standards. Multilateral bodies are notoriously slow and weak, particularly when consensus is required to undertake action. Bilateral treaties can substitute for multilateral arrangements, as in the case of investment law. Where a strong state enacts a number of nearly identical treaties with weaker counterparties, it effectively reduces the power of the counterparties in negotiations, who otherwise may be excluded from advantageous economic and security relations with the stronger state. The bilateral approach is also more brittle. The strong state must replicate bilateral negotiations over proposed amendments with each weaker state. Weaker states have less incentive to remain in bilateral agreements when disputes arise; exiting a multilateral treaty or body entails severing a tie with all member states.\n### 5.4 Private industry States wishing to influence adversaries’ behavior must consider markets and market regulations as part of cyber strategy. Commercial and industrial policies have far reaching consequences for the development of technology, industry and a skilled workforce. Similarly, the alternatives available to would-be attackers depend greatly on commercial markets for software and hardware. States have many modes of contact with private industry.\nStates can legislate or regulate business practices in strategically important industries. For instance, the United States has banned unauthorized export of advanced weapons systems. States can enact financial and risk management standards that influence business practices. Rice (2008) discusses extensively how standards of risk tolerated in software design differ from those in consumer products, and the consequences of risk and liability for product design and marketing. Anderson and Moore (2006) develop economic models of externalized costs that explain persistent problems with software design, digital privacy, and insurance markets for cyber damages. States can also attempt to manage the development of standards for software, data, and communications; but Varian (2001) explains the particulars of how and why industry may not cooperate.\nStates can impose conditions on vendors. These conditions can involve criteria related to software license structure; geographic and political ties; adherence to particular data formats; performance criteria of services; features that create vendor lock-in; the components of software and service bundles; supply chain assurance; and a host of other features. The purchasing power of the government may not be sufficient to appreciably alter the market shares of leading technology and standards; but it does provide the state with a lever of control over at commercial software developed for its own use.\nStates can develop standby arrangements for civil defense. These standby arrangements may be as simple as regular goodwill visits between (for example) the Department of Homeland Security industry liaisons and industry executives. They may take the form of immediate crisis response, either focused directly on monitoring and maintaining computer networks (e.g., Computer Emergency Readiness Teams) or on infrastructure assurance during cyber events. States have also developed laws governing corporate cooperation in law enforcement and national information policies; although these interactions entail potentially huge liability and reuptational risks to the corporation. Global corporations, for their part, face difficult decisions about whether corporate values and governance permit accommodation of unique local laws.\nStates can choose whether and when to subsidize and restrict public access to information assets, such GPS information and satellite imagery. Forbidding access to certain information assets may have the unintended consequence of encouraging competitors to acquire new skills and capabilities. Furthermore, the state can commission research on technologies that are not yet commercially available.\nStates that seek to gain strategic advantage in cyberspace must consider the unorthodox economics of information technology industries. Varian et al. (2004) describe features of supply and demand in information technology, competitive strategy and pricing dynamics, and relevant features of networks. Brafman and Beckstrom (2006), Tapscott and Williams (2006), and Barabási (2002) and give useful and engaging introductions to some newer theoretical approaches that help to explain why some networks don’t have leaders, what motivates apparently uncompensated collaborators on large projects, and how to think about the important features of networks. State policies on cyber security are fraught with pitfalls of unintended consequences; and economics is a good tool to demarcate the big ones.\n## 6 Conclusion The basic condition of cyber conflict described in this article is that situational awareness is costly. States must assume that adversaries have begun to plumb the depths of the network long in advance of a conflict’s apparent start date. Most security measures that are designed to buy time for effective response must be implemented during peacetime. As a result, strategies that ignore the realities of daily business will not win Electronic copy available at: https://ssrn.com/abstract=1928870 REFERENCES the full cooperation of key industry partners. Winning cyber conflicts will, in a very specific sense, require states to win the peace.",
  "citations": {
    "style": "author_year",
    "flat_text": "Electronic copy available at: https://ssrn.com/abstract=1928870 # Leverage in cyberspace, without deterrence Benjamin D. Mazzotta The Fletcher School of Law and Diplomacy, Tufts University benmazzotta@alumni.tufts.edu March 19, 2011 Paper presented to the Annual Convention of the International Studies Association.\nMontréal, QC.\n## Abstract Deterrence theory, which was of enormous value during the nuclear era, cannot adequately prepare defense thinkers for future cyber conflicts. Instead, the best states can hope for is robust cooperation with other states and with private actors to improve their circumstances in future conflicts. A number of key assumptions that supported nuclear deterrence do not hold in the cyber domain: immediately observable attacks; clear legal status under the law of armed conflict; states' preeminence for research and development; high financial and technical barriers to entry; cheap observation of delivery facilities; feasible second strike; and a stable MAD outcome. States can create several incentives for improved collaboration with allies and private sector partners. States can derive mutual benefits from shared intelligence; shared standards for risk assessment; joint exercises; and standby arrangements for crucial information, equipment, materials, and services. Apart from sanctions against access to these carrots, sticks include reconnaissance for cyber conflicts, cyber exploitation with the threat of disclosure, and policies explicitly linking cyber events to non-cyber diplomatic and security concerns. States have additional leverage over private actors through commercial regulation, law enforcement, and liability; although the burden of compliance may poison public-private cooperation.\n# 1 Introduction Cyber attacks have generated enormous controversy in international relations along the lines of a recurrent *Late Show with David Letterman* segment,\"Is this anything?\"Cyber hawks point to the far-reaching effects and cost-efficacy of attacks. Skeptics accuse hawks of crying Chicken Little over simple web defacement. Cyber war has captured Hollywood's imagination at the strategic level as in *Die Hard IV: Live Free or Die Hard*; at the tactical level as in *Vantage Point*; and oftener as dystopian techno-singularity as in *Eagle Eye*, *The Matrix*, and 2001: A Space Odyssey. Journalists, freelancers, think tanks, business, military doctrine (Joint Chiefs of Staff, 2007, 2000), government documents (White House, 2009, 1998) and IR theorists describe broad classes of attacks whose objectives might include the disruption of civilian infrastructure; tactical advantage in conventional war; intelligence gathering; financial gain; and terrorism. This article assumes the feasibility of broad classes of attack as a moot point, and outlines the strategic consequences of those attacks as if they were known capabilities.\nFaced with similarly disruptive military technology in the 1940s, the discipline of international relations refined the concept of deterrence to devise a strategy for safe navigation of potentially catastrophic nuclear arms races. Nuclear theorists then and cyber theorists now pointed out strikingly similar aspects of the new technology: vast geographic reach of the attacks, revolutionary intensity of purported attack effects, and relative cost efficacy of the new weapons. Cyber attacks may promise a revolution in military affairs similar to the nuclear revolution. Nuclear theorists then focused on several features of the new technology and constructed a strategic equilibrium from which deviance was less likely. Cyber theorists now must look carefully at the features of the new technology to assess its strategic consequences.\nDeterrence in cyberspace is a mirage. Cyberspace itself bears little resemblance to the physical world around us, and efforts to reason by analogy with physical space often do more harm than good. Nuclear deterrence is a particularly poor strategic model for cyberspace. The best we can hope for in cyberspace is to enlist the aid of allies who may cooperate for mutual benefit. There are specific sources of leverage that we have in cyberspace, thanks to our unusual capabilities in government and the private sector. Only when taken out of context, may these levers of control be shoehorned into a purported model of deterrence. Faith in deterrence may trick us to trust our own defenses, and blind us to opportunities for alliance.\n## 2 Deterrence theory ### 2.1 Incentives Deterrence works, in theory, because of a brutally simple set of incentives facing the state. State behavior under the security dilemma is the basis of deterrence. Aggressor states seek to gain power through aggressive wars. Strategic competitors promise retaliation for these wars that more than wipes out the gains from expansion.\nDeterrence theory takes as its point of departure that the state is a rational actor, subject to an inescapable calculus of self-interest. Various critiques of the rational actor model of the state in international relations have been advanced, including unraveling of deterrence commitments, well-established cognitive and behavioral deviance from simple economic self-interest; status-seeking behaviors in nuclear weapons development; Electronic copy available at: https://ssrn.com/abstract=1928870 taboos in tactical deployment of nuclear weapons; and constructivist narratives of leadership and the national interest.\nJervis (1979) describes waves of innovation designed to address shortcomings of deterrence theory. The major features of these second-wave models are the aggressor’s political calculus; the possibility of rewards and negotiations as complements to simple retaliation; and constructivist interpretations of the aggressor’s goals in security. Many of these criticisms are further explicated in Taliaferro (2001), which organizes contemporary variants of realist international security theory.\nNecessary conditions for effective deterrence vary greatly from one variant of the theory to another. Waltz (1990) holds that the key to nuclear deterrence is a reliable second strike that eliminates potential gains from aggression. He further argues that the low cost and weight of nuclear weapons leads inexorably to defensive vulnerability. Brodie (1978) argued that deterrence applied mainly to the Soviet-American security dilemma, and largely on the basis of Soviet acquisition of nuclear weapons. Extended deterrence modeled the behavior of allies within this bilateral framework.\n### 2.2 Technology Key features of nuclear technology circa 1960 were ease of observing weapons deployment at various stages, including launch site construction, weapons launch, and successful attack sites; extreme lethality and blast radius; absence of viable defensive countermeasures; difficulty of developing nuclear weapons, measured by financial costs, knowledge, and manufacturing capabilities; and limited means of delivery, viz., aircraft, oceangoing vessels, and ballistic missile. These points are not controversial. Efforts to overcome these technical limits of nuclear weapons (e.g., anti-ballistic missile defenses) attracted opprobium, as they threatened the stability of the mutually assured destruction (MAD) equilibrium.\nNuclear technology itself ensures that the adversary is immediately aware of any attacks; that a short list of suspects are considered capable of an attack; and that, in fact, at least one from the short list must have participated in the attack at some level. Dunn (1994) notes that American strategists ignored the possibility of nuclear weapons smuggling for just that reason. Satellite and aircraft observations were long considered sufficient to monitor nuclear weapons deployment. More exhaustive inspections of suspected facilities featured prominently in Iran, Iraq, and North Korea; but the nature of weapons development has changed considerably since the early nuclear age. Today critical materials are widely available, e.g., nuclear expertise, raw fuel, and industrial components for enrichment and delivery systems. This was absolutely not the case during the early nuclear age.\n### 2.3 Deterrence in cyberspace Deterrence, having served America so well during the Cold war, was an obvious place to begin discussions of cyber strategy. McConnell (2010), in an op-ed on cyber strategy, poses a choice between deterrence and preemption as the central strategic question in cyberspace. He further argues that deterrence rests on four necessary capabilities: a declared retaliation policy, attribution of the attack to the attacker, location of the Electronic copy available at: https://ssrn.com/abstract=1928870 attacker, and the counterstrike itself. If the layman’s definition of deterrence is that we can make ourselves safe in cyberspace without shooting first, then let us all hope that McConnell is correct. On the other hand, there is no consensus among experts that the four necessary conditions McConnell cites are even feasible, let alone likely to be achieved.\nThe National Research Council hosted an essay competition and conference in 2010 dedicated to the feasibility of deterrence in cyberspace. Patrick Morgan, a long-standing skeptic of deterrence theory, questions the applicability of nuclear deterrence to cyberspace **. *Lukasik (2010)* points to serious flaws in a wide variety of declaratory policies on deterrence, such as the threat to retaliate with conventional lethal force. *Clark and Landau (2010)* explain stubborn technical problems with attribution. *Rattray and Healey (2010)* give an overview of offensive cyber weapons, some of which are plausible for retaliation. *Schmitt (2010)* covers the legality of national self-defense under cyber attack; and in a related paper *Sofaer et al. (2010)* outline hypothetical features of international institutions that would purport to ameliorate cyber security.\n## 3 A simple model Successful nuclear deterrence relies on two crucial actions that are trivial in early Cold War nuclear conflict; but that are well-established problems in cyber conflict. The adversary must observe that an attack has in fact occurred. The adversary must also identify the attacker. This section of the article develops a quick Bayesian model of the adversary’s problem, with plausible hypotheses about the likely identity of the attacker.\nThis framework leads to several simple questions about attack technology and the adversary’s decisions.\n\nA convenient starting place is to model the expected utility of the attacker’s payoffs over the future states of the world: viz., attacks with retaliation, attacks without retaliation, and status quo.\n$U\\equiv u(\\mathbf{E}\\pi)$ (1) $\\mathbf{E}\\pi=p_{a}\\pi_{a}+p_{a}\\left(p_{r}\\pi_{r}+p_{\\sim r}\\pi_{\\sim r}\\right)+\\left(1-p_{a}\\right)\\pi_{s}$ (2) where\n\n|   | where  |\n| --- | --- |\n|  U: | expected utility  |\n|  a: | subscript for attack  |\n|  s: | subscript for status quo  |\n|  p: | probability  |\n|  π: | payoff  |\n|  r: | subscript for retaliation  |\n|  ~r: | subscript for nonretaliation  |\nWe can further decompose the probability of retaliation into separate component probabilities, as in (3). The adversary needs to detect that the attack has in fact taken place, attribute the source of the attack, and then decide to respond to the attack. Early Cold War deterrence proceeded as a special case of the equation below, where virtually any attack would lead to certain attribution. Today adversaries may only be able to suspect the adversary's identity; or suspect that an attack has in fact taken place; or have only a rough suspicion on both counts. $^{1}$\n$$\np _ {r} = \\underbrace {p _ {d , i} p _ {r | D \\cap I}} _ {\\text {certain attribution}} + \\underbrace {p _ {d , \\sim i} p _ {r | D \\cap \\sim I}} _ {\\text {unidentified attacker}} + \\underbrace {p _ {\\sim d , i} p _ {r | \\sim D \\cap I}} _ {\\text {suspected attack}} + \\underbrace {p _ {\\sim d , \\sim i} p _ {r | \\sim D \\cap \\sim I}} _ {\\text {rough suspicion}} \\tag {3}\n$$\nAnd when the adversary may mistake the identity of the attacker, as in (4), the attacker may also err in retaliation, with potentially (dis)adventaneous payoffs for the attacker.\n$$\n\\mathbf {E} \\pi = \\underbrace {p _ {\\mathrm {a}} \\pi_ {\\mathrm {a}}} _ {\\text {attack's benefit}} + \\underbrace {p _ {\\mathrm {a}} \\left(p _ {\\mathrm {r}} \\pi_ {\\mathrm {r}} + p _ {\\sim \\mathrm {r}} \\pi_ {\\sim \\mathrm {r}}\\right)} _ {\\text {retaliation's cost}} + \\underbrace {p _ {\\mathrm {d} , \\sim \\mathrm {i}} p _ {\\mathrm {m} | \\mathrm {D} \\cap \\sim \\mathrm {I}} + p _ {\\mathrm {s}} \\pi_ {\\mathrm {s}}} _ {\\text {mistaken counterattack}} \\tag {4}\n$$\n|   | where  |\n| --- | --- |\n|  pr: | probability of retaliation  |\n|  D: | successful detection of an attack  |\n|  ~D: | failure to detect an attack  |\n|  I: | successful identification of an attacker  |\n|  pr|D∩I: | probability of retaliation conditional on adversary's detection without identification  |\n|  pd: | probability of adversary's detecting attack  |\n|  pi: | probability of adversary's identifying attack  |\n|  pm: | probability of a mistaken counterattack  |\nThere are stark differences between nuclear attacks circa 1960, modeled in (5) and cyber attacks, modeled in (6). $^{2}$  Nuclear attacks are obvious by the scale of their destruction. Attribution was simple, due to the small number of states in possession of the technology. There are no serious barriers to acquisition of technical expertise or requisite hardware for cyber attack. There is no reason to suspect that the adversary\nElectronic copy available at: https://ssrn.com/abstract=1928870\n\nwill understand that an attack will be detected until long after it has begun. There is no reason that the adversary will immediately suspect whether the adversary in a given cyber attack is a state or not a state.\n$$ p _ {d i} \\gg p _ {d, - i} &gt; p _ {- d} \\tag {5} $$ $$ p _ {d, - i} &gt; p _ {- d, i} &gt; p _ {d, i} &gt; p _ {- d, - i} \\tag {6} $$ The prior probabilities for identifying the sources of detected nuclear attacks are also much simpler than those of detected cyber attacks. There are many fewer likely suspects for a nuclear attack, as in (7) than for a nuclear attack, as in (8).\n$$ \\text {P r i o r s}: \\left\\{ \\begin{array}{l l} \\sum p _ {H 1} \\dots p _ {H 1 0} =. 9 5 &amp; \\text {d e c l a r e d a n d k n o w n n u c l e a r s t a t e s} \\\\ \\sum p _ {H 1 1} \\dots p _ {H 2 0} =. 0 4 &amp; \\text {u n d e c l a r e d n u c l e a r s t a t e s} \\\\ \\sum p _ {H 2 1} \\dots p _ {H 1 0 0} =. 0 1 &amp; \\text {n o n - n u c l e a r s t a t e s, n o n - s t a t e a c t o r s} \\end{array} \\right. \\tag {7} $$ $$ \\text {P r i o r s}: \\left\\{ \\begin{array}{l l} \\sum p _ {H 1} \\dots p _ {H 1 0} =. 3 0 &amp; \\text {k n o w n s t a t e a d v e r s a r i e s} \\\\ \\sum p _ {H 1 1} \\dots p _ {H 2 0} =. 1 0 &amp; \\text {s u s p e c t e d s t a t e a d v e r s a r i e s} \\\\ \\sum p _ {H 2 1} \\dots p _ {H 1 0 0} =. 3 0 &amp; \\text {n o n - a d v e r s a r i a l s t a t e s} \\\\ \\sum p _ {H 1 0 1} \\dots p _ {H 5 0 0} =. 3 0 &amp; \\text {n o n - s t a t e a c t o r s} \\end{array} \\right. \\tag {8} $$ where H1: Hypothesis attributing attack to country 1 $\\mathbf{P}_{\\mathrm{H1}}$: Probability that H1 is true $\\mathbf{P}_{\\mathrm{H1|E}}$: Probability that H1 is true given evidence E The identity of the attacking state is a hypothesis that the state will attempt to prove by means of evidence. Evidence distinguishes among candidate hypotheses. Absent any data, the states has trouble choosing which adversary to hold responsible for an attack. Evidence of one sort or another updates beliefs. In the Bayesian framework, the best evidence is that which is essentially incompatible with an otherwise-plausible theory. It forces the state to relinquish cherished presuppositions about looming conflicts.\nThe quality of the evidence can be characterized as the difference between the prior distribution of probability among available hypotheses (9) and the posterior distribution (10).\n$$ \\text {P r i o r}: \\left\\{ \\begin{array}{l} p _ {H 1} =. 5 0 \\\\ p _ {H 2} =. 5 0 \\end{array} \\right. \\tag {9} $$ $$ \\text {P o s t e r i o r}: \\left\\{ \\begin{array}{l} p _ {H 1 | E} =. 9 0 \\\\ p _ {H 2 | E} =. 1 0 \\end{array} \\right. \\tag {10} $$ Evidence plays a much greater role in cyberspace than it does in nuclear conflict.\nElectronic copy available at: https://ssrn.com/abstract=1928870 Evidence is thus a primary tool for shaping other states' behavior. States possessing rich knowledge about internet traffic, threat analysis, forensics of prior attacks, etc. can leverage that knowledge by selectively sharing information. As a trusted third party, states can choose to share evidence that helps neighbors identify attackers and verify that attacks have in fact taken place. This information can produce two specific, useful changes of state behavior: neighbors can mind their own security interests; and neighbors refrain from destructive mistakes driven by inaccurate suspicion.\nDeception is also possible; but only at the expense of the state's reputation for honest communication. Frequent deception encourages potential allies and adversaries to mistrust proffered information; so it dilutes the state's future influence for the sake of a fleeting advantage.\n## 4 Cyberspace and deterrence ### 4.1 To observe attacks The Wikileaks release of US State Department cables illustrates a particular feature of cyber attacks that changes the strategy of deterrence. Bradley Manning's alleged distribution of classified files to non-combatants outside the United States did enormous damage to the tactical and strategic positions of the United States.³ Until the moment of the attack's public disclosure, however, little if any harm at all came to individuals or property in the United States. The specific information security policies circumvented were controls on Manning's permission to copy files on uncontrolled media. Specifically, he smuggled the files, which were ultimately copied to Wikileaks servers in various countries, out of a secure facility on a CD-R of Lady Gaga music.⁴ I dwell on this rogue-insider problem because it highlights the difference between the time of tactical cyber attack, such as it is, and that of observable attack effects. Rattray and Healey (2010) refer to cyberattacks as tactically quick but strategically ³For brevity, I will drop further use of the word alleged, although Manning’s trial has not established the facts of the case or his guilt. Case facts as related here are taken directly from popular media, and not any first-hand knowledge.\n⁴For the uninitiated: one of her better known songs is a duet with Beyoncé, Telephone: “K-kinda busy/ K-kinda busy /Sorry, I cannot hear you, I’m kinda busy./ …Stop callin’, stop callin’,” I don’t wanna think anymore!/ I left my hand and my heart on the dance floor.”\nElectronic copy available at: https://ssrn.com/abstract=1928870 This distinction is hard to reconcile with consequentialist cyber attack definitions. Notably, Libicki (2009) draws a spectrum of consequence severity, with either financial losses or bloodshed as the standard for valid reprisals.⁵ Prohibited access to computer systems falls into a consequentialist gray area, neither nuisance nor grievance. Libicki explicitly considers the difference between attacks achieved by cyber means and simple deception, in order to conclude that it is the consequence of the attack and not its means that justifies *jus ad bello*. If we adopt a consequentialist standard for cyber attack, however, we cannot escape the conclusion that harmless enough tactical maneuvers may be used to prepare the ground for future attacks of devastating consequence. The attacker's tactical maneuver may occur well in advance of any reasonably observable consequence. Under some circumstances, the severity of consequences cannot be known even in principle until the attacker effectuates them.\nLin (2009) distinguishes between *cyberattack* and *cyberexploitation*, whereby one entails merely copying files, software and hardware configurations to remote locations. Cyber attack, on the other hand, aims to degrade, alter or destroy software and hardware belonging to the adversary. Lin's categories distinguish attacks primarily designed to benefit the attacker by yielding information about the adversary, from attacks designed to impair the target's systems themselves. Lin developed this framework about the same time that he edited a review of offensive cyberattack capabilities for the United States.\nMany other classifications of malware and cyber attack exist. Rattray and Healey (2010) describe and illustrate the tactical functions of cyber attack. Technical definitions from military doctrine are grounded in the role cyber operations play in military tactics: support of non-cyber troops, interdiction, reconnaissance, counter-cyber, suppression of cyber defenses, and a strategic-level cyber mission. Their hypothetical scenarios point to different military and political contexts: interstate conflict, piracy, irregular war, terrorism, covert operations, and intelligence gathering.\nTsipenyuk et al. (2005) describe seven categories of vulnerabilities in software design, which today and in the future will lead to unauthorized access (i.e., read) and execution (i.e., write, execute). These technical categories describe the means by which encryption, authentication, state variables, and controls against unauthorized execution ⁵Technically, Libicki's scale distinguishes between mass casualty events and heightened risk of nuclear catastrophe. I have omitted the distinction here.\nElectronic copy available at: https://ssrn.com/abstract=1928870 can be prevented.\n### 4.2 To identify attackers Attributing cyber attacks to specific individuals, organizations, and IP addresses is a deep and abiding problem in cyber security. Although there is considerable difference of expert opinion on the feasibility of attribution, *Clark and Landau (2010)* problematize the issue succinctly. They discuss the different meanings of attribution based on the defender’s purpose: machine-level attribution identifies the origins and sources of packets, and often their contents. Individual level attribution links the machine’s user to official identities such as name, residence, place of business, and registration with various government and corporate entities. Clark and Landau point out that the main obstacle to cyber attribution is often jurisdiction, rather than technical capability. The deciding factor, often, is whether investigators have timely cooperation for success across jurisdictions.\n### 4.3 To retaliate Second strike capability raises more problems than it solves. *Owens et al. (2009)* devote an entire book to the legal and political difficulties facing states that would develop cyber weapons. Developing an offensive capability, due to the nature of cyber weapons, is not at all similar to firing missile tests and taking satellite observations of prospective targets. Cyber weapons largely do not point and shoot, and the ones that states are likely to employ as a retaliatory strike are not simple botnet attacks that would take websites offline. Instead, offensive cyber capabilities should require long periods of active gathering of access to potential adversaries’ systems. Only after a long period of reconnaissance can attackers have confidence in the success of a timely counterattack.\nDefining proportional response to a cyber attack is difficult, since it is often the effects of the cyber attack on non-cyber systems that states wish to punish. This line of reasoning is a slippery slope for states that seek to mitigate the risk of of cyber conflict or mollify incipient conflicts. Detractors of cyber offense also point to America’s relatively greater reliance on information technology both in military and civilian life **. Since we have more to lose from successful IT attacks, restricting the notion of proportionality to pure cyber retaliation for cyber attacks is a self-defeating line of reasoning; not unlike Richard Clarke’s famous accusation that Donald Rumsfeld wanted to find a better adversary than Afghanistan, with better targets for the Air Force.\nKnock-on effects of declaratory policy in this area are also troublesome. If we could define standards of proportionality and achieve consensus thereon, cyber attacks are sufficiently flexible that clever attackers could adjust the lethality, destructiveness, and economic impact of cyber attacks to evade reprisals.\nIn fact a number of the problems with cyber deterrence as a strategy have to do with knock-on effects **. Convincing adversaries that the state has no plans to use cyber attacks is difficult enough, without declaring a policy to boot that virtually necessitates years of covert operations to prepare the ground for those cyber retalia\n\n## 4.4 Time, the essential variable\nThe greatest single problem with deterrence in cyberspace is the anachronism of attack, effect, and retaliation. The central promise of deterrence is that the counterstrike will rob the attacker of the supposed fruits of the attack, through the promise of swift and certain retribution. For the most interesting cyber attacks, i.e., ones where the attacker will prepare the ground for days or years in advance of a tactical opportunity, the damage has long been done before the effects of the cyber attack become apparent. The promise of swift and certain retribution may simply ring hollow.\nSchwartau (2001) lays out a unique and relevant security model in layman’s terms. Schwartau points out that defensive security measures, in and of themselves, are largely all fallible. The most secure physical and cyber technologies that we can create are designed to keep things secret or off limits for a period of time, until a complementary response can be activated. Door locks and safes are measured by the duration of time for which they can withstand well defined classes of physical attack. Encryption algorithms are measured by the time required to crack passwords by brute force. Password tokens and password expiry dates are designed to ensure that passwords have a short shelf life (more on that below). Airgapped networks are designed to limit the opportunities that attackers have to access the system. By limiting the frequency of access, they effectively raise the time required to penetrate the system. Even the decision to maintain obscure, legacy operating systems (not Windows, Mac, or Unix) in particular firms demands time of would-be attackers, i.e., time spent developing the competence to crack access controls. In Schwartau’s model, every security measure is a method of buying time until the cavalry can arrive.\nSchwartau’s definition of security as a measure designed to gain time for appropriate response further complicates deterrence. The time to respond to an attack can only begin from the observation of the attack. A fully trusted computing platform, that could inform us when it had been breached—instead of when the attack’s effects were observed—is at best a distant prospect in the future.\nRoughly speaking, let us assume that the greater the time available for reconnaissance, the greater the scale of consequences that attackers can achieve and the higher the degree of confidence they can have in the result. If that is true, then there is an inexorable advantage among states under the security dilemma to conduct reconnaissance as early as possible. As a result, states know they must expect this behavior from their adversaries.\nThe start date of technical actions in cyber conflict occurs well in advance of the start date of political conflict, and well in advance of the start date of observed effects of cyber attacks. One of the main strategic goals of the national cyber defense strategy must be to take advantage of the planning phase of cyber conflict. The ways to take advantage of that phase are to develop situational awareness; to conduct appropriate counter-reconnaissance; to practice civil and military defense; to foster close interjurisdictional collaboration; and to gain the trust of likely allies.\nElectronic copy available at: https://ssrn.com/abstract=1928870\n\n## 5.1 Cooperation\nCooperation is one of the policy levers that can serve cybersecurity strategy. There are many domains where allies can cooperate on cyber issues: law enforcement, intelligence, military, and civil defense. In each of these areas overlapping capabilities can provide specific benefits to potential allies.\nThe rapid pace of technological development is one oft-cited reason that cooperation is vital to success in cyberspace; but is not the only one. As a rough analogy, the databases used by antivirus software have a short shelf life. Within days they are obsolete, as the variety of malware observed in the wild grows continuously. Nearly all cybersecurity problems evolve and require constant study due to changes in software, hardware, and business practices. Any small, isolated organization will likely benefit from exchange of information related to cybersecurity, whether or not that information is of a primarily technical nature.\nInstitutional collaboration is arguably more important than exchange of technical information. Computer crimes in peacetime frequently involve a morass of overlapping national and local laws, international agreements, and competing authorities for investigation, prosecution and jurisdiction . People, hardware, and communications may operate in several (or dozens of) countries, using communications infrastructure, services, and data facilities that are themselves spread across geographic and political boundaries. Forensics, investigation and prosecution require formal structures for timely collaboration across these various borders.\nIntelligence gathering and threat analysis require similar institutional agreements for interagency and international cooperation. Intelligence organizations are rightly designed to take care with sensitive and classified information. Intelligence organizations by default lock down access to information that could be valuable to strategic competitors.\nIn peacetime, we must assume that states, firms, and non-state actors have already undertaken reconnaissance activities. Indeed evidence of this abounds . Exchange of reconnaissance information, both technical and organizational in nature, has the potential to greatly increase the quality of each party's information. There may be no hard and fast line between reconnaissance activities in cyberspace, threat analysis, and situational awareness for national defense. Many pieces of information that are necessary for threat analysis and attack detection would ultimately be useful for countermeasures and counterstrikes to cyber attacks.\nFinally, joint exercises are an important precursor to engagement in notionally \"hot\" cyber wars. Cyber security simulations train military and civil authorities to detect, analyze, and adapt to cyber attacks in the context of political conflicts, conventional hostilities, and threats to vital national interests. These simulations align expectations among allies for best practices, standards of inference, norms of behavior, and thresholds for retaliation of various kinds. Given the paucity of global standards for cyber conflict , first-hand experience of other countries' behavior and analysis may prove indispensable. Countries excluded from these exercises lose valuable opportunities to train their own personnel, develop and test\nElectronic copy available at: https://ssrn.com/abstract=1928870\n\nGenerally, the larger the network of cooperating states, the greater is the potential benefit of access therein; and the greater the harm from exclusion. (Conversely, the larger the number of states collaborating, the more difficult it is to achieve consensus on exclusion, and to monitor and enforce such decisions.) Similarly, the greater the capabilities of the states collaborating in such a network, the greater is the benefit of inclusion in the network.\n## 5.2 Shared networks Specific networks can themselves be assets offered to share with allies and withheld from enemies. These networks may be bounded by physical connections, communications protocols, or logical restrictions on access. As with the previous section, each of these levers can be wielded either as a carrot or a stick.\nSpecial diplomatic communications networks predate computers themselves. Telephone hotlines connected Moscow to Washington during the Cold War, in order to prevent missed calls during a nuclear crisis. SIPRNET limits unauthorized access to sensitive and classified information systems; and in theory it limits the temptation for personnel to copy sensitive information out to insecure hardware.\nIn the private sector, special financial networks connect banks, trading desks, market exchanges, and clearinghouses. Most firms and households do not have access to these information systems. Many other examples of communications networks for finance exist, in addition to those serving the largest OECD banks. Credit card payments networks, electronic funds transfers, wire transfers, Paypal, Western Union, and Islamic payments networks all provide special access to particular financial communications. Membership in these networks greatly reduces transaction costs for both the end users and the network operators themselves.\nSimilarly, telecommunications networks provide still further benefits. Telecommunications encompass a vast array of services, as simple and open as mobile telephone calls or shortwave radio broadcasts; and as complex as combat communications systems, which integrate secure streams of voice, positioning, imaging, targeting, and order-of-battle data.\nIn the future, as user identification protocols become more pervasive, participation in those registries will become another important source of leverage. Registries need not be centralized; but the ability to verify cryptographic signatures is central to a wide variety of authentication and encryption protocols for communications. Various levels of access could include permission to request an identity (either for an individual or for hardware); permission to request identities on behalf of others; permission to verify users' credentials against the registry of identities; and permission to create, alter, and delete identities in the registry. Trust is enacted in code. Software structures the choices users can make about extending trust to registries, users, and administrators. States, both by the grant (or revocation) of trust to various identity protocols and by the limits placed on identities created for users, can structure other states' access to legitimate telecommunications networks.\nElectronic copy available at: https://ssrn.com/abstract=1928870 ### 5.3 Standards development States will also want a voice in the development of global standards in IT. A dizzying array of standards bodies exist today, and these standards bodies are often located outside government; although more and more states seek to bring these institutions directly under their authority. Standards bodies govern the development of technologies such the internet protocol itself and elements of commonly used computer languages and file formats; the assignment of names and addresses; and many other related technical standards.\nStandards bodies also handle closely related non-technical matters, such as financial, accounting and risk standards for corporate information technology. Standards bodies may be located within the government, in international organizations (e.g., the International Telecommunications Union), within a single private company (e.g., Microsoft), in industry associations, in a non-profit entity with joint representation from industry and government (e.g., ICANN), or in a private non-profit (e.g., Mozilla Foundation). Government may also solicit bids or competitions for specific standards, such as encryption algorithms, which are designed privately and then adopted by the government for official business. Sofaer et al. (2010) discuss the bodies in the international system with jurisdiction over IT standards and telecommunications law.\nMultilateral standards bodies could potentially play a role in technical, financial, legal, or other standards. Multilateral bodies are notoriously slow and weak, particularly when consensus is required to undertake action. Bilateral treaties can substitute for multilateral arrangements, as in the case of investment law. Where a strong state enacts a number of nearly identical treaties with weaker counterparties, it effectively reduces the power of the counterparties in negotiations, who otherwise may be excluded from advantageous economic and security relations with the stronger state. The bilateral approach is also more brittle. The strong state must replicate bilateral negotiations over proposed amendments with each weaker state. Weaker states have less incentive to remain in bilateral agreements when disputes arise; exiting a multilateral treaty or body entails severing a tie with all member states.\n### 5.4 Private industry States wishing to influence adversaries’ behavior must consider markets and market regulations as part of cyber strategy. Commercial and industrial policies have far reaching consequences for the development of technology, industry and a skilled workforce. Similarly, the alternatives available to would-be attackers depend greatly on commercial markets for software and hardware. States have many modes of contact with private industry.\nStates can legislate or regulate business practices in strategically important industries. For instance, the United States has banned unauthorized export of advanced weapons systems. States can enact financial and risk management standards that influence business practices. Rice (2008) discusses extensively how standards of risk tolerated in software design differ from those in consumer products, and the consequences of risk and liability for product design and marketing. Anderson and Moore (2006) develop economic models of externalized costs that explain persistent problems with software design, digital privacy, and insurance markets for cyber damages. States can also attempt to manage the development of standards for software, data, and communications; but Varian (2001) explains the particulars of how and why industry may not cooperate.\nStates can impose conditions on vendors. These conditions can involve criteria related to software license structure; geographic and political ties; adherence to particular data formats; performance criteria of services; features that create vendor lock-in; the components of software and service bundles; supply chain assurance; and a host of other features. The purchasing power of the government may not be sufficient to appreciably alter the market shares of leading technology and standards; but it does provide the state with a lever of control over at commercial software developed for its own use.\nStates can develop standby arrangements for civil defense. These standby arrangements may be as simple as regular goodwill visits between (for example) the Department of Homeland Security industry liaisons and industry executives. They may take the form of immediate crisis response, either focused directly on monitoring and maintaining computer networks (e.g., Computer Emergency Readiness Teams) or on infrastructure assurance during cyber events. States have also developed laws governing corporate cooperation in law enforcement and national information policies; although these interactions entail potentially huge liability and reuptational risks to the corporation. Global corporations, for their part, face difficult decisions about whether corporate values and governance permit accommodation of unique local laws.\nStates can choose whether and when to subsidize and restrict public access to information assets, such GPS information and satellite imagery. Forbidding access to certain information assets may have the unintended consequence of encouraging competitors to acquire new skills and capabilities. Furthermore, the state can commission research on technologies that are not yet commercially available.\nStates that seek to gain strategic advantage in cyberspace must consider the unorthodox economics of information technology industries. Varian et al. (2004) describe features of supply and demand in information technology, competitive strategy and pricing dynamics, and relevant features of networks. Brafman and Beckstrom (2006), Tapscott and Williams (2006), and Barabási (2002) and give useful and engaging introductions to some newer theoretical approaches that help to explain why some networks don’t have leaders, what motivates apparently uncompensated collaborators on large projects, and how to think about the important features of networks. State policies on cyber security are fraught with pitfalls of unintended consequences; and economics is a good tool to demarcate the big ones.\n## 6 Conclusion The basic condition of cyber conflict described in this article is that situational awareness is costly. States must assume that adversaries have begun to plumb the depths of the network long in advance of a conflict’s apparent start date. Most security measures that are designed to buy time for effective response must be implemented during peacetime. As a result, strategies that ignore the realities of daily business will not win Electronic copy available at: https://ssrn.com/abstract=1928870 REFERENCES the full cooperation of key industry partners. Winning cyber conflicts will, in a very specific sense, require states to win the peace.",
    "footnotes": {
      "items": {},
      "intext": [
        {
          "index": "1",
          "intext_citation": "$^{1}$",
          "preceding_text": "",
          "footnote": null
        },
        {
          "index": "2",
          "intext_citation": "$^{2}$",
          "preceding_text": "",
          "footnote": null
        }
      ],
      "stats": {
        "intext_total": 2,
        "success_occurrences": 0,
        "success_unique": 0,
        "bib_unique_total": 0,
        "occurrence_match_rate": 0.0,
        "bib_coverage_rate": 0.0,
        "success_percentage": 0.0,
        "missing_intext_expected_total": 0,
        "missing_intext_indices": [],
        "highest_intext_index": 2,
        "missing_footnotes_for_seen_total": 2,
        "missing_footnotes_for_seen_intext": [
          1,
          2
        ],
        "uncited_footnote_total": 0,
        "uncited_footnote_indices": [],
        "style": "footnotes"
      }
    },
    "tex": {
      "total": {
        "intext_total": 2,
        "success_occurrences": 2,
        "success_unique": 2,
        "bib_unique_total": 2,
        "occurrence_match_rate": 1.0,
        "bib_coverage_rate": 1.0,
        "success_percentage": 100.0,
        "missing_intext_expected_total": 0,
        "missing_intext_indices": [],
        "highest_intext_index": 2,
        "missing_footnotes_for_seen_total": 0,
        "missing_footnotes_for_seen_intext": [],
        "uncited_footnote_total": 0,
        "uncited_footnote_indices": [],
        "style": "tex_superscript"
      },
      "results": [
        {
          "index": "1",
          "intext_citation": "${ }^{1}$",
          "preceding_text": "oceeded as a special case of the equation below, where virtually any attack would lead to certain attribution. Today adversaries may only be able to suspect the adversary's identity; or suspect that an attack has in fact taken place; or have only a rough suspicion on both counts.",
          "footnote": "How difficult is it to detect a given attack?"
        },
        {
          "index": "2",
          "intext_citation": "${ }^{2}$",
          "preceding_text": "tion | | pd: | probability of adversary's detecting attack | | pi: | probability of adversary's identifying attack | | pm: | probability of a mistaken counterattack | There are stark differences between nuclear attacks circa 1960, modeled in (5) and cyber attacks, modeled in (6).",
          "footnote": "DETERRENCE THEORY"
        }
      ],
      "flat_text": "Electronic copy available at: https://ssrn.com/abstract=1928870 # Leverage in cyberspace, without deterrence Benjamin D. Mazzotta The Fletcher School of Law and Diplomacy, Tufts University benmazzotta@alumni.tufts.edu March 19, 2011 Paper presented to the Annual Convention of the International Studies Association.\nMontréal, QC.\n## Abstract Deterrence theory, which was of enormous value during the nuclear era, cannot adequately prepare defense thinkers for future cyber conflicts. Instead, the best states can hope for is robust cooperation with other states and with private actors to improve their circumstances in future conflicts. A number of key assumptions that supported nuclear deterrence do not hold in the cyber domain: immediately observable attacks; clear legal status under the law of armed conflict; states' preeminence for research and development; high financial and technical barriers to entry; cheap observation of delivery facilities; feasible second strike; and a stable MAD outcome. States can create several incentives for improved collaboration with allies and private sector partners. States can derive mutual benefits from shared intelligence; shared standards for risk assessment; joint exercises; and standby arrangements for crucial information, equipment, materials, and services. Apart from sanctions against access to these carrots, sticks include reconnaissance for cyber conflicts, cyber exploitation with the threat of disclosure, and policies explicitly linking cyber events to non-cyber diplomatic and security concerns. States have additional leverage over private actors through commercial regulation, law enforcement, and liability; although the burden of compliance may poison public-private cooperation.\n# 1 Introduction Cyber attacks have generated enormous controversy in international relations along the lines of a recurrent *Late Show with David Letterman* segment,\"Is this anything?\"Cyber hawks point to the far-reaching effects and cost-efficacy of attacks (Bajaj, 2010; Borg, 2005). Skeptics accuse hawks of crying Chicken Little over simple web defacement (Singel, 2010). Cyber war has captured Hollywood's imagination at the strategic level as in *Die Hard IV: Live Free or Die Hard*; at the tactical level as in *Vantage Point*; and oftener as dystopian techno-singularity as in *Eagle Eye*, *The Matrix*, and 2001: A Space Odyssey. Journalists (Gorman, 2010), freelancers (Thornburgh, 2005), think tanks (Krekel, 2009; Walton et al., 2009), business (Falliere et al., 2010; Langevin et al., 2008; Allen et al., 2005; Tsipenyuk et al., 2005), military doctrine (Joint Chiefs of Staff, 2007, 2000), government documents (White House, 2009, 1998) and IR theorists (Clarke and Knake, 2010; Lynn, 2010; Morgan, 2010; Nye, 2010; Clarke, 2009; Owens et al., 2009; Liang and Xiangsui, 2002; Rattray, 2001) describe broad classes of attacks whose objectives might include the disruption of civilian infrastructure; tactical advantage in conventional war; intelligence gathering; financial gain; and terrorism. This article assumes the feasibility of broad classes of attack as a moot point, and outlines the strategic consequences of those attacks as if they were known capabilities.\nFaced with similarly disruptive military technology in the 1940s, the discipline of international relations refined the concept of deterrence to devise a strategy for safe navigation of potentially catastrophic nuclear arms races. Nuclear theorists then and cyber theorists now pointed out strikingly similar aspects of the new technology: vast geographic reach of the attacks, revolutionary intensity of purported attack effects, and relative cost efficacy of the new weapons. Cyber attacks may promise a revolution in military affairs similar to the nuclear revolution. Nuclear theorists then focused on several features of the new technology and constructed a strategic equilibrium from which deviance was less likely. Cyber theorists now must look carefully at the features of the new technology to assess its strategic consequences.\nDeterrence in cyberspace is a mirage. Cyberspace itself bears little resemblance to the physical world around us, and efforts to reason by analogy with physical space often do more harm than good. Nuclear deterrence is a particularly poor strategic model for cyberspace. The best we can hope for in cyberspace is to enlist the aid of allies who may cooperate for mutual benefit. There are specific sources of leverage that we have in cyberspace, thanks to our unusual capabilities in government and the private sector. Only when taken out of context, may these levers of control be shoehorned into a purported model of deterrence. Faith in deterrence may trick us to trust our own defenses, and blind us to opportunities for alliance.\n## 2 Deterrence theory ### 2.1 Incentives Deterrence works, in theory, because of a brutally simple set of incentives facing the state. State behavior under the security dilemma is the basis of deterrence. Aggressor states seek to gain power through aggressive wars. Strategic competitors promise retaliation for these wars that more than wipes out the gains from expansion.\nDeterrence theory takes as its point of departure that the state is a rational actor (Achen and Snidal, 1989; Brodie, 1978), subject to an inescapable calculus of self-interest (Waltz, 1990). Various critiques of the rational actor model of the state in international relations have been advanced, including unraveling of deterrence commitments (Morgan, 1977), well-established cognitive and behavioral deviance from simple economic self-interest (Kahneman and Tversky, 2000, 1979; Tversky and Kahneman, 1974); status-seeking behaviors in nuclear weapons development (Art, 1980); Electronic copy available at: https://ssrn.com/abstract=1928870 taboos in tactical deployment of nuclear weapons (Tannenwald, 1999; Dunn, 1994); and constructivist narratives of leadership and the national interest.\nJervis (1979) describes waves of innovation designed to address shortcomings of deterrence theory. The major features of these second-wave models are the aggressor’s political calculus; the possibility of rewards and negotiations as complements to simple retaliation; and constructivist interpretations of the aggressor’s goals in security. Many of these criticisms are further explicated in Taliaferro (2001), which organizes contemporary variants of realist international security theory.\nNecessary conditions for effective deterrence vary greatly from one variant of the theory to another. Waltz (1990) holds that the key to nuclear deterrence is a reliable second strike that eliminates potential gains from aggression. He further argues that the low cost and weight of nuclear weapons leads inexorably to defensive vulnerability. Brodie (1978) argued that deterrence applied mainly to the Soviet-American security dilemma, and largely on the basis of Soviet acquisition of nuclear weapons. Extended deterrence modeled the behavior of allies within this bilateral framework.\n### 2.2 Technology Key features of nuclear technology circa 1960 were ease of observing weapons deployment at various stages, including launch site construction, weapons launch, and successful attack sites; extreme lethality and blast radius; absence of viable defensive countermeasures; difficulty of developing nuclear weapons, measured by financial costs, knowledge, and manufacturing capabilities; and limited means of delivery, viz., aircraft, oceangoing vessels, and ballistic missile. These points are not controversial. Efforts to overcome these technical limits of nuclear weapons (e.g., anti-ballistic missile defenses) attracted opprobium, as they threatened the stability of the mutually assured destruction (MAD) equilibrium (Dunn, 1994).\nNuclear technology itself ensures that the adversary is immediately aware of any attacks; that a short list of suspects are considered capable of an attack; and that, in fact, at least one from the short list must have participated in the attack at some level. Dunn (1994) notes that American strategists ignored the possibility of nuclear weapons smuggling for just that reason. Satellite and aircraft observations were long considered sufficient to monitor nuclear weapons deployment. More exhaustive inspections of suspected facilities featured prominently in Iran, Iraq, and North Korea; but the nature of weapons development has changed considerably since the early nuclear age. Today critical materials are widely available, e.g., nuclear expertise, raw fuel, and industrial components for enrichment and delivery systems. This was absolutely not the case during the early nuclear age.\n### 2.3 Deterrence in cyberspace Deterrence, having served America so well during the Cold war, was an obvious place to begin discussions of cyber strategy. McConnell (2010), in an op-ed on cyber strategy, poses a choice between deterrence and preemption as the central strategic question in cyberspace. He further argues that deterrence rests on four necessary capabilities: a declared retaliation policy, attribution of the attack to the attacker, location of the Electronic copy available at: https://ssrn.com/abstract=1928870 attacker, and the counterstrike itself. If the layman’s definition of deterrence is that we can make ourselves safe in cyberspace without shooting first, then let us all hope that McConnell is correct. On the other hand, there is no consensus among experts that the four necessary conditions McConnell cites are even feasible, let alone likely to be achieved.\nThe National Research Council hosted an essay competition and conference in 2010 dedicated to the feasibility of deterrence in cyberspace. Patrick Morgan, a long-standing skeptic of deterrence theory, questions the applicability of nuclear deterrence to cyberspace *(Morgan, 2010)*. *Lukasik (2010)* points to serious flaws in a wide variety of declaratory policies on deterrence, such as the threat to retaliate with conventional lethal force. *Clark and Landau (2010)* explain stubborn technical problems with attribution. *Rattray and Healey (2010)* give an overview of offensive cyber weapons, some of which are plausible for retaliation. *Schmitt (2010)* covers the legality of national self-defense under cyber attack; and in a related paper *Sofaer et al. (2010)* outline hypothetical features of international institutions that would purport to ameliorate cyber security.\n## 3 A simple model Successful nuclear deterrence relies on two crucial actions that are trivial in early Cold War nuclear conflict; but that are well-established problems in cyber conflict. The adversary must observe that an attack has in fact occurred. The adversary must also identify the attacker. This section of the article develops a quick Bayesian model of the adversary’s problem, with plausible hypotheses about the likely identity of the attacker.\nThis framework leads to several simple questions about attack technology and the adversary’s decisions.\n\nA convenient starting place is to model the expected utility of the attacker’s payoffs over the future states of the world: viz., attacks with retaliation, attacks without retaliation, and status quo.\n$U\\equiv u(\\mathbf{E}\\pi)$ (1) $\\mathbf{E}\\pi=p_{a}\\pi_{a}+p_{a}\\left(p_{r}\\pi_{r}+p_{\\sim r}\\pi_{\\sim r}\\right)+\\left(1-p_{a}\\right)\\pi_{s}$ (2) where\n\n|   | where  |\n| --- | --- |\n|  U: | expected utility  |\n|  a: | subscript for attack  |\n|  s: | subscript for status quo  |\n|  p: | probability  |\n|  π: | payoff  |\n|  r: | subscript for retaliation  |\n|  ~r: | subscript for nonretaliation  |\nWe can further decompose the probability of retaliation into separate component probabilities, as in (3). The adversary needs to detect that the attack has in fact taken place, attribute the source of the attack, and then decide to respond to the attack. Early Cold War deterrence proceeded as a special case of the equation below, where virtually any attack would lead to certain attribution. Today adversaries may only be able to suspect the adversary's identity; or suspect that an attack has in fact taken place; or have only a rough suspicion on both counts. $^{1}$\n$$\np _ {r} = \\underbrace {p _ {d , i} p _ {r | D \\cap I}} _ {\\text {certain attribution}} + \\underbrace {p _ {d , \\sim i} p _ {r | D \\cap \\sim I}} _ {\\text {unidentified attacker}} + \\underbrace {p _ {\\sim d , i} p _ {r | \\sim D \\cap I}} _ {\\text {suspected attack}} + \\underbrace {p _ {\\sim d , \\sim i} p _ {r | \\sim D \\cap \\sim I}} _ {\\text {rough suspicion}} \\tag {3}\n$$\nAnd when the adversary may mistake the identity of the attacker, as in (4), the attacker may also err in retaliation, with potentially (dis)adventaneous payoffs for the attacker.\n$$\n\\mathbf {E} \\pi = \\underbrace {p _ {\\mathrm {a}} \\pi_ {\\mathrm {a}}} _ {\\text {attack's benefit}} + \\underbrace {p _ {\\mathrm {a}} \\left(p _ {\\mathrm {r}} \\pi_ {\\mathrm {r}} + p _ {\\sim \\mathrm {r}} \\pi_ {\\sim \\mathrm {r}}\\right)} _ {\\text {retaliation's cost}} + \\underbrace {p _ {\\mathrm {d} , \\sim \\mathrm {i}} p _ {\\mathrm {m} | \\mathrm {D} \\cap \\sim \\mathrm {I}} + p _ {\\mathrm {s}} \\pi_ {\\mathrm {s}}} _ {\\text {mistaken counterattack}} \\tag {4}\n$$\n|   | where  |\n| --- | --- |\n|  pr: | probability of retaliation  |\n|  D: | successful detection of an attack  |\n|  ~D: | failure to detect an attack  |\n|  I: | successful identification of an attacker  |\n|  pr|D∩I: | probability of retaliation conditional on adversary's detection without identification  |\n|  pd: | probability of adversary's detecting attack  |\n|  pi: | probability of adversary's identifying attack  |\n|  pm: | probability of a mistaken counterattack  |\nThere are stark differences between nuclear attacks circa 1960, modeled in (5) and cyber attacks, modeled in (6). $^{2}$  Nuclear attacks are obvious by the scale of their destruction. Attribution was simple, due to the small number of states in possession of the technology. There are no serious barriers to acquisition of technical expertise or requisite hardware for cyber attack. There is no reason to suspect that the adversary\nElectronic copy available at: https://ssrn.com/abstract=1928870\n\nwill understand that an attack will be detected until long after it has begun. There is no reason that the adversary will immediately suspect whether the adversary in a given cyber attack is a state or not a state.\n$$ p _ {d i} \\gg p _ {d, - i} &gt; p _ {- d} \\tag {5} $$ $$ p _ {d, - i} &gt; p _ {- d, i} &gt; p _ {d, i} &gt; p _ {- d, - i} \\tag {6} $$ The prior probabilities for identifying the sources of detected nuclear attacks are also much simpler than those of detected cyber attacks. There are many fewer likely suspects for a nuclear attack, as in (7) than for a nuclear attack, as in (8).\n$$ \\text {P r i o r s}: \\left\\{ \\begin{array}{l l} \\sum p _ {H 1} \\dots p _ {H 1 0} =. 9 5 &amp; \\text {d e c l a r e d a n d k n o w n n u c l e a r s t a t e s} \\\\ \\sum p _ {H 1 1} \\dots p _ {H 2 0} =. 0 4 &amp; \\text {u n d e c l a r e d n u c l e a r s t a t e s} \\\\ \\sum p _ {H 2 1} \\dots p _ {H 1 0 0} =. 0 1 &amp; \\text {n o n - n u c l e a r s t a t e s, n o n - s t a t e a c t o r s} \\end{array} \\right. \\tag {7} $$ $$ \\text {P r i o r s}: \\left\\{ \\begin{array}{l l} \\sum p _ {H 1} \\dots p _ {H 1 0} =. 3 0 &amp; \\text {k n o w n s t a t e a d v e r s a r i e s} \\\\ \\sum p _ {H 1 1} \\dots p _ {H 2 0} =. 1 0 &amp; \\text {s u s p e c t e d s t a t e a d v e r s a r i e s} \\\\ \\sum p _ {H 2 1} \\dots p _ {H 1 0 0} =. 3 0 &amp; \\text {n o n - a d v e r s a r i a l s t a t e s} \\\\ \\sum p _ {H 1 0 1} \\dots p _ {H 5 0 0} =. 3 0 &amp; \\text {n o n - s t a t e a c t o r s} \\end{array} \\right. \\tag {8} $$ where H1: Hypothesis attributing attack to country 1 $\\mathbf{P}_{\\mathrm{H1}}$: Probability that H1 is true $\\mathbf{P}_{\\mathrm{H1|E}}$: Probability that H1 is true given evidence E The identity of the attacking state is a hypothesis that the state will attempt to prove by means of evidence. Evidence distinguishes among candidate hypotheses. Absent any data, the states has trouble choosing which adversary to hold responsible for an attack. Evidence of one sort or another updates beliefs. In the Bayesian framework, the best evidence is that which is essentially incompatible with an otherwise-plausible theory. It forces the state to relinquish cherished presuppositions about looming conflicts.\nThe quality of the evidence can be characterized as the difference between the prior distribution of probability among available hypotheses (9) and the posterior distribution (10).\n$$ \\text {P r i o r}: \\left\\{ \\begin{array}{l} p _ {H 1} =. 5 0 \\\\ p _ {H 2} =. 5 0 \\end{array} \\right. \\tag {9} $$ $$ \\text {P o s t e r i o r}: \\left\\{ \\begin{array}{l} p _ {H 1 | E} =. 9 0 \\\\ p _ {H 2 | E} =. 1 0 \\end{array} \\right. \\tag {10} $$ Evidence plays a much greater role in cyberspace than it does in nuclear conflict.\nElectronic copy available at: https://ssrn.com/abstract=1928870 Evidence is thus a primary tool for shaping other states' behavior. States possessing rich knowledge about internet traffic, threat analysis, forensics of prior attacks, etc. can leverage that knowledge by selectively sharing information. As a trusted third party, states can choose to share evidence that helps neighbors identify attackers and verify that attacks have in fact taken place. This information can produce two specific, useful changes of state behavior: neighbors can mind their own security interests; and neighbors refrain from destructive mistakes driven by inaccurate suspicion.\nDeception is also possible; but only at the expense of the state's reputation for honest communication. Frequent deception encourages potential allies and adversaries to mistrust proffered information; so it dilutes the state's future influence for the sake of a fleeting advantage.\n## 4 Cyberspace and deterrence ### 4.1 To observe attacks The Wikileaks release of US State Department cables illustrates a particular feature of cyber attacks that changes the strategy of deterrence. Bradley Manning's alleged distribution of classified files to non-combatants outside the United States did enormous damage to the tactical and strategic positions of the United States.³ Until the moment of the attack's public disclosure, however, little if any harm at all came to individuals or property in the United States. The specific information security policies circumvented were controls on Manning's permission to copy files on uncontrolled media. Specifically, he smuggled the files, which were ultimately copied to Wikileaks servers in various countries, out of a secure facility on a CD-R of Lady Gaga music.⁴ I dwell on this rogue-insider problem because it highlights the difference between the time of tactical cyber attack, such as it is, and that of observable attack effects. Rattray and Healey (2010) refer to cyberattacks as tactically quick but strategically ³For brevity, I will drop further use of the word alleged, although Manning’s trial has not established the facts of the case or his guilt. Case facts as related here are taken directly from popular media, and not any first-hand knowledge.\n⁴For the uninitiated: one of her better known songs is a duet with Beyoncé, Telephone: “K-kinda busy/ K-kinda busy /Sorry, I cannot hear you, I’m kinda busy./ …Stop callin’, stop callin’,” I don’t wanna think anymore!/ I left my hand and my heart on the dance floor.”\nElectronic copy available at: https://ssrn.com/abstract=1928870 This distinction is hard to reconcile with consequentialist cyber attack definitions. Notably, Libicki (2009) draws a spectrum of consequence severity, with either financial losses or bloodshed as the standard for valid reprisals.⁵ Prohibited access to computer systems falls into a consequentialist gray area, neither nuisance nor grievance. Libicki explicitly considers the difference between attacks achieved by cyber means and simple deception, in order to conclude that it is the consequence of the attack and not its means that justifies *jus ad bello*. If we adopt a consequentialist standard for cyber attack, however, we cannot escape the conclusion that harmless enough tactical maneuvers may be used to prepare the ground for future attacks of devastating consequence. The attacker's tactical maneuver may occur well in advance of any reasonably observable consequence. Under some circumstances, the severity of consequences cannot be known even in principle until the attacker effectuates them.\nLin (2009) distinguishes between *cyberattack* and *cyberexploitation*, whereby one entails merely copying files, software and hardware configurations to remote locations. Cyber attack, on the other hand, aims to degrade, alter or destroy software and hardware belonging to the adversary. Lin's categories distinguish attacks primarily designed to benefit the attacker by yielding information about the adversary, from attacks designed to impair the target's systems themselves. Lin developed this framework about the same time that he edited a review of offensive cyberattack capabilities for the United States (Owens et al., 2009).\nMany other classifications of malware and cyber attack exist. Rattray and Healey (2010) describe and illustrate the tactical functions of cyber attack. Technical definitions from military doctrine are grounded in the role cyber operations play in military tactics: support of non-cyber troops, interdiction, reconnaissance, counter-cyber, suppression of cyber defenses, and a strategic-level cyber mission. Their hypothetical scenarios point to different military and political contexts: interstate conflict, piracy, irregular war, terrorism, covert operations, and intelligence gathering.\nTsipenyuk et al. (2005) describe seven categories of vulnerabilities in software design, which today and in the future will lead to unauthorized access (i.e., read) and execution (i.e., write, execute). These technical categories describe the means by which encryption, authentication, state variables, and controls against unauthorized execution ⁵Technically, Libicki's scale distinguishes between mass casualty events and heightened risk of nuclear catastrophe. I have omitted the distinction here.\nElectronic copy available at: https://ssrn.com/abstract=1928870 can be prevented.\n### 4.2 To identify attackers Attributing cyber attacks to specific individuals, organizations, and IP addresses is a deep and abiding problem in cyber security. Although there is considerable difference of expert opinion on the feasibility of attribution, *Clark and Landau (2010)* problematize the issue succinctly. They discuss the different meanings of attribution based on the defender’s purpose: machine-level attribution identifies the origins and sources of packets, and often their contents. Individual level attribution links the machine’s user to official identities such as name, residence, place of business, and registration with various government and corporate entities. Clark and Landau point out that the main obstacle to cyber attribution is often jurisdiction, rather than technical capability. The deciding factor, often, is whether investigators have timely cooperation for success across jurisdictions.\n### 4.3 To retaliate Second strike capability raises more problems than it solves. *Owens et al. (2009)* devote an entire book to the legal and political difficulties facing states that would develop cyber weapons. Developing an offensive capability, due to the nature of cyber weapons, is not at all similar to firing missile tests and taking satellite observations of prospective targets. Cyber weapons largely do not point and shoot, and the ones that states are likely to employ as a retaliatory strike are not simple botnet attacks that would take websites offline. Instead, offensive cyber capabilities should require long periods of active gathering of access to potential adversaries’ systems. Only after a long period of reconnaissance can attackers have confidence in the success of a timely counterattack.\nDefining proportional response to a cyber attack is difficult, since it is often the effects of the cyber attack on non-cyber systems that states wish to punish. This line of reasoning is a slippery slope for states that seek to mitigate the risk of of cyber conflict or mollify incipient conflicts. Detractors of cyber offense also point to America’s relatively greater reliance on information technology both in military and civilian life *(Schwartau, 2000)*. Since we have more to lose from successful IT attacks, restricting the notion of proportionality to pure cyber retaliation for cyber attacks is a self-defeating line of reasoning; not unlike Richard Clarke’s famous accusation that Donald Rumsfeld wanted to find a better adversary than Afghanistan, with better targets for the Air Force.\nKnock-on effects of declaratory policy in this area are also troublesome. If we could define standards of proportionality and achieve consensus thereon, cyber attacks are sufficiently flexible that clever attackers could adjust the lethality, destructiveness, and economic impact of cyber attacks to evade reprisals.\nIn fact a number of the problems with cyber deterrence as a strategy have to do with knock-on effects *(Morgan, 2010)*. Convincing adversaries that the state has no plans to use cyber attacks is difficult enough, without declaring a policy to boot that virtually necessitates years of covert operations to prepare the ground for those cyber retalia\n\n## 4.4 Time, the essential variable\nThe greatest single problem with deterrence in cyberspace is the anachronism of attack, effect, and retaliation. The central promise of deterrence is that the counterstrike will rob the attacker of the supposed fruits of the attack, through the promise of swift and certain retribution. For the most interesting cyber attacks, i.e., ones where the attacker will prepare the ground for days or years in advance of a tactical opportunity, the damage has long been done before the effects of the cyber attack become apparent. The promise of swift and certain retribution may simply ring hollow.\nSchwartau (2001) lays out a unique and relevant security model in layman’s terms. Schwartau points out that defensive security measures, in and of themselves, are largely all fallible. The most secure physical and cyber technologies that we can create are designed to keep things secret or off limits for a period of time, until a complementary response can be activated. Door locks and safes are measured by the duration of time for which they can withstand well defined classes of physical attack. Encryption algorithms are measured by the time required to crack passwords by brute force. Password tokens and password expiry dates are designed to ensure that passwords have a short shelf life (more on that below). Airgapped networks are designed to limit the opportunities that attackers have to access the system. By limiting the frequency of access, they effectively raise the time required to penetrate the system. Even the decision to maintain obscure, legacy operating systems (not Windows, Mac, or Unix) in particular firms demands time of would-be attackers, i.e., time spent developing the competence to crack access controls. In Schwartau’s model, every security measure is a method of buying time until the cavalry can arrive.\nSchwartau’s definition of security as a measure designed to gain time for appropriate response further complicates deterrence. The time to respond to an attack can only begin from the observation of the attack. A fully trusted computing platform, that could inform us when it had been breached—instead of when the attack’s effects were observed—is at best a distant prospect in the future.\nRoughly speaking, let us assume that the greater the time available for reconnaissance, the greater the scale of consequences that attackers can achieve and the higher the degree of confidence they can have in the result. If that is true, then there is an inexorable advantage among states under the security dilemma to conduct reconnaissance as early as possible. As a result, states know they must expect this behavior from their adversaries.\nThe start date of technical actions in cyber conflict occurs well in advance of the start date of political conflict, and well in advance of the start date of observed effects of cyber attacks. One of the main strategic goals of the national cyber defense strategy must be to take advantage of the planning phase of cyber conflict. The ways to take advantage of that phase are to develop situational awareness; to conduct appropriate counter-reconnaissance; to practice civil and military defense; to foster close interjurisdictional collaboration; and to gain the trust of likely allies.\nElectronic copy available at: https://ssrn.com/abstract=1928870\n\n## 5.1 Cooperation\nCooperation is one of the policy levers that can serve cybersecurity strategy. There are many domains where allies can cooperate on cyber issues: law enforcement, intelligence, military, and civil defense. In each of these areas overlapping capabilities can provide specific benefits to potential allies.\nThe rapid pace of technological development is one oft-cited reason that cooperation is vital to success in cyberspace; but is not the only one. As a rough analogy, the databases used by antivirus software have a short shelf life. Within days they are obsolete, as the variety of malware observed in the wild grows continuously. Nearly all cybersecurity problems evolve and require constant study due to changes in software, hardware, and business practices. Any small, isolated organization will likely benefit from exchange of information related to cybersecurity, whether or not that information is of a primarily technical nature.\nInstitutional collaboration is arguably more important than exchange of technical information. Computer crimes in peacetime frequently involve a morass of overlapping national and local laws, international agreements, and competing authorities for investigation, prosecution and jurisdiction (Westby, 2004). People, hardware, and communications may operate in several (or dozens of) countries, using communications infrastructure, services, and data facilities that are themselves spread across geographic and political boundaries. Forensics, investigation and prosecution require formal structures for timely collaboration across these various borders.\nIntelligence gathering and threat analysis require similar institutional agreements for interagency and international cooperation. Intelligence organizations are rightly designed to take care with sensitive and classified information. Intelligence organizations by default lock down access to information that could be valuable to strategic competitors.\nIn peacetime, we must assume that states, firms, and non-state actors have already undertaken reconnaissance activities. Indeed evidence of this abounds (Demchak and Dombrowski, 2011). Exchange of reconnaissance information, both technical and organizational in nature, has the potential to greatly increase the quality of each party's information. There may be no hard and fast line between reconnaissance activities in cyberspace, threat analysis, and situational awareness for national defense. Many pieces of information that are necessary for threat analysis and attack detection would ultimately be useful for countermeasures and counterstrikes to cyber attacks.\nFinally, joint exercises are an important precursor to engagement in notionally \"hot\" cyber wars. Cyber security simulations train military and civil authorities to detect, analyze, and adapt to cyber attacks in the context of political conflicts, conventional hostilities, and threats to vital national interests. These simulations align expectations among allies for best practices, standards of inference, norms of behavior, and thresholds for retaliation of various kinds. Given the paucity of global standards for cyber conflict (Hayden, 2011; Libicki, 2009), first-hand experience of other countries' behavior and analysis may prove indispensable. Countries excluded from these exercises lose valuable opportunities to train their own personnel, develop and test\nElectronic copy available at: https://ssrn.com/abstract=1928870\n\nGenerally, the larger the network of cooperating states, the greater is the potential benefit of access therein; and the greater the harm from exclusion. (Conversely, the larger the number of states collaborating, the more difficult it is to achieve consensus on exclusion, and to monitor and enforce such decisions.) Similarly, the greater the capabilities of the states collaborating in such a network, the greater is the benefit of inclusion in the network.\n## 5.2 Shared networks Specific networks can themselves be assets offered to share with allies and withheld from enemies. These networks may be bounded by physical connections, communications protocols, or logical restrictions on access. As with the previous section, each of these levers can be wielded either as a carrot or a stick.\nSpecial diplomatic communications networks predate computers themselves. Telephone hotlines connected Moscow to Washington during the Cold War, in order to prevent missed calls during a nuclear crisis. SIPRNET limits unauthorized access to sensitive and classified information systems; and in theory it limits the temptation for personnel to copy sensitive information out to insecure hardware.\nIn the private sector, special financial networks connect banks, trading desks, market exchanges, and clearinghouses. Most firms and households do not have access to these information systems. Many other examples of communications networks for finance exist, in addition to those serving the largest OECD banks. Credit card payments networks, electronic funds transfers, wire transfers, Paypal, Western Union, and Islamic payments networks all provide special access to particular financial communications. Membership in these networks greatly reduces transaction costs for both the end users and the network operators themselves.\nSimilarly, telecommunications networks provide still further benefits. Telecommunications encompass a vast array of services, as simple and open as mobile telephone calls or shortwave radio broadcasts; and as complex as combat communications systems, which integrate secure streams of voice, positioning, imaging, targeting, and order-of-battle data.\nIn the future, as user identification protocols become more pervasive, participation in those registries will become another important source of leverage. Registries need not be centralized; but the ability to verify cryptographic signatures is central to a wide variety of authentication and encryption protocols for communications. Various levels of access could include permission to request an identity (either for an individual or for hardware); permission to request identities on behalf of others; permission to verify users' credentials against the registry of identities; and permission to create, alter, and delete identities in the registry. Trust is enacted in code. Software structures the choices users can make about extending trust to registries, users, and administrators. States, both by the grant (or revocation) of trust to various identity protocols and by the limits placed on identities created for users, can structure other states' access to legitimate telecommunications networks.\nElectronic copy available at: https://ssrn.com/abstract=1928870 ### 5.3 Standards development States will also want a voice in the development of global standards in IT. A dizzying array of standards bodies exist today, and these standards bodies are often located outside government; although more and more states seek to bring these institutions directly under their authority. Standards bodies govern the development of technologies such the internet protocol itself and elements of commonly used computer languages and file formats; the assignment of names and addresses; and many other related technical standards.\nStandards bodies also handle closely related non-technical matters, such as financial, accounting and risk standards for corporate information technology. Standards bodies may be located within the government, in international organizations (e.g., the International Telecommunications Union), within a single private company (e.g., Microsoft), in industry associations, in a non-profit entity with joint representation from industry and government (e.g., ICANN), or in a private non-profit (e.g., Mozilla Foundation). Government may also solicit bids or competitions for specific standards, such as encryption algorithms, which are designed privately and then adopted by the government for official business. Sofaer et al. (2010) discuss the bodies in the international system with jurisdiction over IT standards and telecommunications law.\nMultilateral standards bodies could potentially play a role in technical, financial, legal, or other standards. Multilateral bodies are notoriously slow and weak, particularly when consensus is required to undertake action (Tsebelis, 1995). Bilateral treaties can substitute for multilateral arrangements, as in the case of investment law. Where a strong state enacts a number of nearly identical treaties with weaker counterparties, it effectively reduces the power of the counterparties in negotiations, who otherwise may be excluded from advantageous economic and security relations with the stronger state. The bilateral approach is also more brittle. The strong state must replicate bilateral negotiations over proposed amendments with each weaker state. Weaker states have less incentive to remain in bilateral agreements when disputes arise; exiting a multilateral treaty or body entails severing a tie with all member states.\n### 5.4 Private industry States wishing to influence adversaries’ behavior must consider markets and market regulations as part of cyber strategy. Commercial and industrial policies have far reaching consequences for the development of technology, industry and a skilled workforce. Similarly, the alternatives available to would-be attackers depend greatly on commercial markets for software and hardware. States have many modes of contact with private industry.\nStates can legislate or regulate business practices in strategically important industries. For instance, the United States has banned unauthorized export of advanced weapons systems. States can enact financial and risk management standards that influence business practices. Rice (2008) discusses extensively how standards of risk tolerated in software design differ from those in consumer products, and the consequences of risk and liability for product design and marketing. Anderson and Moore (2006) develop economic models of externalized costs that explain persistent problems with software design, digital privacy, and insurance markets for cyber damages. States can also attempt to manage the development of standards for software, data, and communications; but Varian (2001) explains the particulars of how and why industry may not cooperate.\nStates can impose conditions on vendors. These conditions can involve criteria related to software license structure; geographic and political ties; adherence to particular data formats; performance criteria of services; features that create vendor lock-in; the components of software and service bundles; supply chain assurance; and a host of other features (Rosenweig, 2010). The purchasing power of the government may not be sufficient to appreciably alter the market shares of leading technology and standards; but it does provide the state with a lever of control over at commercial software developed for its own use.\nStates can develop standby arrangements for civil defense. These standby arrangements may be as simple as regular goodwill visits between (for example) the Department of Homeland Security industry liaisons and industry executives. They may take the form of immediate crisis response, either focused directly on monitoring and maintaining computer networks (e.g., Computer Emergency Readiness Teams) or on infrastructure assurance during cyber events. States have also developed laws governing corporate cooperation in law enforcement and national information policies; although these interactions entail potentially huge liability and reuptational risks to the corporation. Global corporations, for their part, face difficult decisions about whether corporate values and governance permit accommodation of unique local laws.\nStates can choose whether and when to subsidize and restrict public access to information assets, such GPS information and satellite imagery. Forbidding access to certain information assets may have the unintended consequence of encouraging competitors to acquire new skills and capabilities. Furthermore, the state can commission research on technologies that are not yet commercially available.\nStates that seek to gain strategic advantage in cyberspace must consider the unorthodox economics of information technology industries. Varian et al. (2004) describe features of supply and demand in information technology, competitive strategy and pricing dynamics, and relevant features of networks. Brafman and Beckstrom (2006), Tapscott and Williams (2006), and Barabási (2002) and give useful and engaging introductions to some newer theoretical approaches that help to explain why some networks don’t have leaders, what motivates apparently uncompensated collaborators on large projects, and how to think about the important features of networks. State policies on cyber security are fraught with pitfalls of unintended consequences; and economics is a good tool to demarcate the big ones.\n## 6 Conclusion The basic condition of cyber conflict described in this article is that situational awareness is costly. States must assume that adversaries have begun to plumb the depths of the network long in advance of a conflict’s apparent start date. Most security measures that are designed to buy time for effective response must be implemented during peacetime. As a result, strategies that ignore the realities of daily business will not win Electronic copy available at: https://ssrn.com/abstract=1928870 REFERENCES the full cooperation of key industry partners. Winning cyber conflicts will, in a very specific sense, require states to win the peace."
    },
    "numeric": {
      "total": {
        "intext_total": 0,
        "success_occurrences": 0,
        "success_unique": 0,
        "bib_unique_total": 0,
        "occurrence_match_rate": 0.0,
        "bib_coverage_rate": 0.0,
        "success_percentage": 0.0,
        "style": "numeric"
      },
      "results": [],
      "flat_text": "Electronic copy available at: https://ssrn.com/abstract=1928870\n# Leverage in cyberspace, without deterrence\nBenjamin D. Mazzotta\nThe Fletcher School of Law and Diplomacy, Tufts University\nbenmazzotta@alumni.tufts.edu\nMarch 19, 2011\nPaper presented to the Annual Convention of the International Studies Association.\nMontréal, QC.\n## Abstract\nDeterrence theory, which was of enormous value during the nuclear era, cannot adequately prepare defense thinkers for future cyber conflicts. Instead, the best states can hope for is robust cooperation with other states and with private actors to improve their circumstances in future conflicts. A number of key assumptions that supported nuclear deterrence do not hold in the cyber domain: immediately observable attacks; clear legal status under the law of armed conflict; states' preeminence for research and development; high financial and technical barriers to entry; cheap observation of delivery facilities; feasible second strike; and a stable MAD outcome. States can create several incentives for improved collaboration with allies and private sector partners. States can derive mutual benefits from shared intelligence; shared standards for risk assessment; joint exercises; and standby arrangements for crucial information, equipment, materials, and services. Apart from sanctions against access to these carrots, sticks include reconnaissance for cyber conflicts, cyber exploitation with the threat of disclosure, and policies explicitly linking cyber events to non-cyber diplomatic and security concerns. States have additional leverage over private actors through commercial regulation, law enforcement, and liability; although the burden of compliance may poison public-private cooperation.\n# 1 Introduction\nCyber attacks have generated enormous controversy in international relations along the lines of a recurrent *Late Show with David Letterman* segment, \"Is this anything?\" Cyber hawks point to the far-reaching effects and cost-efficacy of attacks (Bajaj, 2010; Borg, 2005). Skeptics accuse hawks of crying Chicken Little over simple web defacement (Singel, 2010). Cyber war has captured Hollywood's imagination at the strategic level as in *Die Hard IV: Live Free or Die Hard*; at the tactical level as in *Vantage Point*; and oftener as dystopian techno-singularity as in *Eagle Eye*, *The Matrix*, and\n2 DETERRENCE THEORY\n2001: A Space Odyssey. Journalists (Gorman, 2010), freelancers (Thornburgh, 2005), think tanks (Krekel, 2009; Walton et al., 2009), business (Falliere et al., 2010; Langevin et al., 2008; Allen et al., 2005; Tsipenyuk et al., 2005), military doctrine (Joint Chiefs of Staff, 2007, 2000), government documents (White House, 2009, 1998) and IR theorists (Clarke and Knake, 2010; Lynn, 2010; Morgan, 2010; Nye, 2010; Clarke, 2009; Owens et al., 2009; Liang and Xiangsui, 2002; Rattray, 2001) describe broad classes of attacks whose objectives might include the disruption of civilian infrastructure; tactical advantage in conventional war; intelligence gathering; financial gain; and terrorism. This article assumes the feasibility of broad classes of attack as a moot point, and outlines the strategic consequences of those attacks as if they were known capabilities.\nFaced with similarly disruptive military technology in the 1940s, the discipline of international relations refined the concept of deterrence to devise a strategy for safe navigation of potentially catastrophic nuclear arms races. Nuclear theorists then and cyber theorists now pointed out strikingly similar aspects of the new technology: vast geographic reach of the attacks, revolutionary intensity of purported attack effects, and relative cost efficacy of the new weapons. Cyber attacks may promise a revolution in military affairs similar to the nuclear revolution. Nuclear theorists then focused on several features of the new technology and constructed a strategic equilibrium from which deviance was less likely. Cyber theorists now must look carefully at the features of the new technology to assess its strategic consequences.\nDeterrence in cyberspace is a mirage. Cyberspace itself bears little resemblance to the physical world around us, and efforts to reason by analogy with physical space often do more harm than good. Nuclear deterrence is a particularly poor strategic model for cyberspace. The best we can hope for in cyberspace is to enlist the aid of allies who may cooperate for mutual benefit. There are specific sources of leverage that we have in cyberspace, thanks to our unusual capabilities in government and the private sector. Only when taken out of context, may these levers of control be shoehorned into a purported model of deterrence. Faith in deterrence may trick us to trust our own defenses, and blind us to opportunities for alliance.\n## 2 Deterrence theory\n### 2.1 Incentives\nDeterrence works, in theory, because of a brutally simple set of incentives facing the state. State behavior under the security dilemma is the basis of deterrence. Aggressor states seek to gain power through aggressive wars. Strategic competitors promise retaliation for these wars that more than wipes out the gains from expansion.\nDeterrence theory takes as its point of departure that the state is a rational actor (Achen and Snidal, 1989; Brodie, 1978), subject to an inescapable calculus of self-interest (Waltz, 1990). Various critiques of the rational actor model of the state in international relations have been advanced, including unraveling of deterrence commitments (Morgan, 1977), well-established cognitive and behavioral deviance from simple economic self-interest (Kahneman and Tversky, 2000, 1979; Tversky and Kahneman, 1974); status-seeking behaviors in nuclear weapons development (Art, 1980);\nElectronic copy available at: https://ssrn.com/abstract=1928870\n2 DETERRENCE THEORY\ntaboos in tactical deployment of nuclear weapons (Tannenwald, 1999; Dunn, 1994); and constructivist narratives of leadership and the national interest.\nJervis (1979) describes waves of innovation designed to address shortcomings of deterrence theory. The major features of these second-wave models are the aggressor’s political calculus; the possibility of rewards and negotiations as complements to simple retaliation; and constructivist interpretations of the aggressor’s goals in security. Many of these criticisms are further explicated in Taliaferro (2001), which organizes contemporary variants of realist international security theory.\nNecessary conditions for effective deterrence vary greatly from one variant of the theory to another. Waltz (1990) holds that the key to nuclear deterrence is a reliable second strike that eliminates potential gains from aggression. He further argues that the low cost and weight of nuclear weapons leads inexorably to defensive vulnerability. Brodie (1978) argued that deterrence applied mainly to the Soviet-American security dilemma, and largely on the basis of Soviet acquisition of nuclear weapons. Extended deterrence modeled the behavior of allies within this bilateral framework.\n### 2.2 Technology\nKey features of nuclear technology circa 1960 were ease of observing weapons deployment at various stages, including launch site construction, weapons launch, and successful attack sites; extreme lethality and blast radius; absence of viable defensive countermeasures; difficulty of developing nuclear weapons, measured by financial costs, knowledge, and manufacturing capabilities; and limited means of delivery, viz., aircraft, oceangoing vessels, and ballistic missile. These points are not controversial. Efforts to overcome these technical limits of nuclear weapons (e.g., anti-ballistic missile defenses) attracted opprobium, as they threatened the stability of the mutually assured destruction (MAD) equilibrium (Dunn, 1994).\nNuclear technology itself ensures that the adversary is immediately aware of any attacks; that a short list of suspects are considered capable of an attack; and that, in fact, at least one from the short list must have participated in the attack at some level. Dunn (1994) notes that American strategists ignored the possibility of nuclear weapons smuggling for just that reason. Satellite and aircraft observations were long considered sufficient to monitor nuclear weapons deployment. More exhaustive inspections of suspected facilities featured prominently in Iran, Iraq, and North Korea; but the nature of weapons development has changed considerably since the early nuclear age. Today critical materials are widely available, e.g., nuclear expertise, raw fuel, and industrial components for enrichment and delivery systems. This was absolutely not the case during the early nuclear age.\n### 2.3 Deterrence in cyberspace\nDeterrence, having served America so well during the Cold war, was an obvious place to begin discussions of cyber strategy. McConnell (2010), in an op-ed on cyber strategy, poses a choice between deterrence and preemption as the central strategic question in cyberspace. He further argues that deterrence rests on four necessary capabilities: a declared retaliation policy, attribution of the attack to the attacker, location of the\nElectronic copy available at: https://ssrn.com/abstract=1928870\nattacker, and the counterstrike itself. If the layman’s definition of deterrence is that we can make ourselves safe in cyberspace without shooting first, then let us all hope that McConnell is correct. On the other hand, there is no consensus among experts that the four necessary conditions McConnell cites are even feasible, let alone likely to be achieved.\nThe National Research Council hosted an essay competition and conference in 2010 dedicated to the feasibility of deterrence in cyberspace. Patrick Morgan, a long-standing skeptic of deterrence theory, questions the applicability of nuclear deterrence to cyberspace *(Morgan, 2010)*. *Lukasik (2010)* points to serious flaws in a wide variety of declaratory policies on deterrence, such as the threat to retaliate with conventional lethal force. *Clark and Landau (2010)* explain stubborn technical problems with attribution. *Rattray and Healey (2010)* give an overview of offensive cyber weapons, some of which are plausible for retaliation. *Schmitt (2010)* covers the legality of national self-defense under cyber attack; and in a related paper *Sofaer et al. (2010)* outline hypothetical features of international institutions that would purport to ameliorate cyber security.\n## 3 A simple model\nSuccessful nuclear deterrence relies on two crucial actions that are trivial in early Cold War nuclear conflict; but that are well-established problems in cyber conflict. The adversary must observe that an attack has in fact occurred. The adversary must also identify the attacker. This section of the article develops a quick Bayesian model of the adversary’s problem, with plausible hypotheses about the likely identity of the attacker.\nThis framework leads to several simple questions about attack technology and the adversary’s decisions.\n1. How difficult is it to detect a given attack?\n2. How difficult is it to identify the attacker?\n3. How likely is the adversary to retaliate on mere suspicion of an attack?\n4. How likely is the adversary to retaliate upon a merely suspected attacker?\n5. How likely is the adversary to lash out at the wrong attacker?\nA convenient starting place is to model the expected utility of the attacker’s payoffs over the future states of the world: viz., attacks with retaliation, attacks without retaliation, and status quo.\n$U\\equiv u(\\mathbf{E}\\pi)$ (1)\n$\\mathbf{E}\\pi=p_{a}\\pi_{a}+p_{a}\\left(p_{r}\\pi_{r}+p_{\\sim r}\\pi_{\\sim r}\\right)+\\left(1-p_{a}\\right)\\pi_{s}$ (2)\nwhere\n3 A SIMPLE MODEL\n|   | where  |\n| --- | --- |\n|  U: | expected utility  |\n|  a: | subscript for attack  |\n|  s: | subscript for status quo  |\n|  p: | probability  |\n|  π: | payoff  |\n|  r: | subscript for retaliation  |\n|  ~r: | subscript for nonretaliation  |\nWe can further decompose the probability of retaliation into separate component probabilities, as in (3). The adversary needs to detect that the attack has in fact taken place, attribute the source of the attack, and then decide to respond to the attack. Early Cold War deterrence proceeded as a special case of the equation below, where virtually any attack would lead to certain attribution. Today adversaries may only be able to suspect the adversary's identity; or suspect that an attack has in fact taken place; or have only a rough suspicion on both counts. $^{1}$\n$$\np _ {r} = \\underbrace {p _ {d , i} p _ {r | D \\cap I}} _ {\\text {certain attribution}} + \\underbrace {p _ {d , \\sim i} p _ {r | D \\cap \\sim I}} _ {\\text {unidentified attacker}} + \\underbrace {p _ {\\sim d , i} p _ {r | \\sim D \\cap I}} _ {\\text {suspected attack}} + \\underbrace {p _ {\\sim d , \\sim i} p _ {r | \\sim D \\cap \\sim I}} _ {\\text {rough suspicion}} \\tag {3}\n$$\nAnd when the adversary may mistake the identity of the attacker, as in (4), the attacker may also err in retaliation, with potentially (dis)adventaneous payoffs for the attacker.\n$$\n\\mathbf {E} \\pi = \\underbrace {p _ {\\mathrm {a}} \\pi_ {\\mathrm {a}}} _ {\\text {attack's benefit}} + \\underbrace {p _ {\\mathrm {a}} \\left(p _ {\\mathrm {r}} \\pi_ {\\mathrm {r}} + p _ {\\sim \\mathrm {r}} \\pi_ {\\sim \\mathrm {r}}\\right)} _ {\\text {retaliation's cost}} + \\underbrace {p _ {\\mathrm {d} , \\sim \\mathrm {i}} p _ {\\mathrm {m} | \\mathrm {D} \\cap \\sim \\mathrm {I}} + p _ {\\mathrm {s}} \\pi_ {\\mathrm {s}}} _ {\\text {mistaken counterattack}} \\tag {4}\n$$\n|   | where  |\n| --- | --- |\n|  pr: | probability of retaliation  |\n|  D: | successful detection of an attack  |\n|  ~D: | failure to detect an attack  |\n|  I: | successful identification of an attacker  |\n|  pr|D∩I: | probability of retaliation conditional on adversary's detection without identification  |\n|  pd: | probability of adversary's detecting attack  |\n|  pi: | probability of adversary's identifying attack  |\n|  pm: | probability of a mistaken counterattack  |\nThere are stark differences between nuclear attacks circa 1960, modeled in (5) and cyber attacks, modeled in (6). $^{2}$  Nuclear attacks are obvious by the scale of their destruction. Attribution was simple, due to the small number of states in possession of the technology. There are no serious barriers to acquisition of technical expertise or requisite hardware for cyber attack. There is no reason to suspect that the adversary\nElectronic copy available at: https://ssrn.com/abstract=1928870\n3 A SIMPLE MODEL\nwill understand that an attack will be detected until long after it has begun. There is no reason that the adversary will immediately suspect whether the adversary in a given cyber attack is a state or not a state.\n$$\np _ {d i} \\gg p _ {d, - i} &gt; p _ {- d} \\tag {5}\n$$\n$$\np _ {d, - i} &gt; p _ {- d, i} &gt; p _ {d, i} &gt; p _ {- d, - i} \\tag {6}\n$$\nThe prior probabilities for identifying the sources of detected nuclear attacks are also much simpler than those of detected cyber attacks. There are many fewer likely suspects for a nuclear attack, as in (7) than for a nuclear attack, as in (8).\n$$\n\\text {P r i o r s}: \\left\\{ \\begin{array}{l l} \\sum p _ {H 1} \\dots p _ {H 1 0} = . 9 5 &amp; \\text {d e c l a r e d a n d k n o w n n u c l e a r s t a t e s} \\\\ \\sum p _ {H 1 1} \\dots p _ {H 2 0} = . 0 4 &amp; \\text {u n d e c l a r e d n u c l e a r s t a t e s} \\\\ \\sum p _ {H 2 1} \\dots p _ {H 1 0 0} = . 0 1 &amp; \\text {n o n - n u c l e a r s t a t e s , n o n - s t a t e a c t o r s} \\end{array} \\right. \\tag {7}\n$$\n$$\n\\text {P r i o r s}: \\left\\{ \\begin{array}{l l} \\sum p _ {H 1} \\dots p _ {H 1 0} = . 3 0 &amp; \\text {k n o w n s t a t e a d v e r s a r i e s} \\\\ \\sum p _ {H 1 1} \\dots p _ {H 2 0} = . 1 0 &amp; \\text {s u s p e c t e d s t a t e a d v e r s a r i e s} \\\\ \\sum p _ {H 2 1} \\dots p _ {H 1 0 0} = . 3 0 &amp; \\text {n o n - a d v e r s a r i a l s t a t e s} \\\\ \\sum p _ {H 1 0 1} \\dots p _ {H 5 0 0} = . 3 0 &amp; \\text {n o n - s t a t e a c t o r s} \\end{array} \\right. \\tag {8}\n$$\nwhere\nH1: Hypothesis attributing attack to country 1\n$\\mathbf{P}_{\\mathrm{H1}}$  : Probability that H1 is true\n$\\mathbf{P}_{\\mathrm{H1|E}}$  : Probability that H1 is true given evidence E\nThe identity of the attacking state is a hypothesis that the state will attempt to prove by means of evidence. Evidence distinguishes among candidate hypotheses. Absent any data, the states has trouble choosing which adversary to hold responsible for an attack. Evidence of one sort or another updates beliefs. In the Bayesian framework, the best evidence is that which is essentially incompatible with an otherwise-plausible theory. It forces the state to relinquish cherished presuppositions about looming conflicts.\nThe quality of the evidence can be characterized as the difference between the prior distribution of probability among available hypotheses (9) and the posterior distribution (10).\n$$\n\\text {P r i o r}: \\left\\{ \\begin{array}{l} p _ {H 1} = . 5 0 \\\\ p _ {H 2} = . 5 0 \\end{array} \\right. \\tag {9}\n$$\n$$\n\\text {P o s t e r i o r}: \\left\\{ \\begin{array}{l} p _ {H 1 | E} = . 9 0 \\\\ p _ {H 2 | E} = . 1 0 \\end{array} \\right. \\tag {10}\n$$\nEvidence plays a much greater role in cyberspace than it does in nuclear conflict.\nElectronic copy available at: https://ssrn.com/abstract=1928870\n4 CYBERSPACE AND DETERRENCE\n7\nBecause the number of plausible candidates for an observed nuclear attack was fairly low, relatively cheap evidence could lead to a high degree of certainty in meaningfully short time horizon. With readily available telecommunications, satellite observations and corroboration by other states, states could achieve a reasonable degree of certainty that an attack had occurred, and that a particular state was responsible. This is ordinarily not the case in cyberspace. Cyber evidence is markedly more difficult to collect, to analyze, and to explain to non-specialists. States' networks have access points and data logs numerous like the grains of sand on a beach. Computer forensics involves sifting through these logs to gather insights that can be integrated with non-technical analysis; all of which takes time. The cost of some cyber attacks is well below that of nuclear weapons development. Most states and many non-state actors might perhaps be involved in any observed cyberattack.\nEvidence is thus a primary tool for shaping other states' behavior. States possessing rich knowledge about internet traffic, threat analysis, forensics of prior attacks, etc. can leverage that knowledge by selectively sharing information. As a trusted third party, states can choose to share evidence that helps neighbors identify attackers and verify that attacks have in fact taken place. This information can produce two specific, useful changes of state behavior: neighbors can mind their own security interests; and neighbors refrain from destructive mistakes driven by inaccurate suspicion.\nDeception is also possible; but only at the expense of the state's reputation for honest communication. Frequent deception encourages potential allies and adversaries to mistrust proffered information; so it dilutes the state's future influence for the sake of a fleeting advantage.\n## 4 Cyberspace and deterrence\n### 4.1 To observe attacks\nThe Wikileaks release of US State Department cables illustrates a particular feature of cyber attacks that changes the strategy of deterrence. Bradley Manning's alleged distribution of classified files to non-combatants outside the United States did enormous damage to the tactical and strategic positions of the United States.³ Until the moment of the attack's public disclosure, however, little if any harm at all came to individuals or property in the United States. The specific information security policies circumvented were controls on Manning's permission to copy files on uncontrolled media. Specifically, he smuggled the files, which were ultimately copied to Wikileaks servers in various countries, out of a secure facility on a CD-R of Lady Gaga music.⁴\nI dwell on this rogue-insider problem because it highlights the difference between the time of tactical cyber attack, such as it is, and that of observable attack effects. Rattray and Healey (2010) refer to cyberattacks as tactically quick but strategically\n³For brevity, I will drop further use of the word alleged, although Manning’s trial has not established the facts of the case or his guilt. Case facts as related here are taken directly from popular media, and not any first-hand knowledge.\n⁴For the uninitiated: one of her better known songs is a duet with Beyoncé, Telephone: “K-kinda busy/ K-kinda busy /Sorry, I cannot hear you, I’m kinda busy./ …Stop callin’, stop callin’,” I don’t wanna think anymore!/ I left my hand and my heart on the dance floor.”\nElectronic copy available at: https://ssrn.com/abstract=1928870\n4 CYBERSPACE AND DETERRENCE\n8\nslow; meaning that cyberattacks often require years of preparation. Once Manning had the disk out of the secure facility, all of the remaining cyber operations happened off site and on computers not in control of the injured party. Though Manning's actions were both illegal and injurious to his country, none of his specific actions likely required him to circumvent electronic controls on his access. He did not need to crack encryption or authentication. He did not exploit flaws in software design. He used only the privileges granted to him in his official capacity. Prior to the dissemination of classified files, the only evidence that forensics could have turned up would be logs of file transfers to an optical drive by an authorized user. Had Manning been questioned about the incident, there is no reason to suspect that the investigation would have turned to public disclosure of the copied files. The act of disclosure was both the source of injury and the only likely moment when investigators would have begun to suspect foul play.\nThis distinction is hard to reconcile with consequentialist cyber attack definitions. Notably, Libicki (2009) draws a spectrum of consequence severity, with either financial losses or bloodshed as the standard for valid reprisals.⁵ Prohibited access to computer systems falls into a consequentialist gray area, neither nuisance nor grievance. Libicki explicitly considers the difference between attacks achieved by cyber means and simple deception, in order to conclude that it is the consequence of the attack and not its means that justifies *jus ad bello*. If we adopt a consequentialist standard for cyber attack, however, we cannot escape the conclusion that harmless enough tactical maneuvers may be used to prepare the ground for future attacks of devastating consequence. The attacker's tactical maneuver may occur well in advance of any reasonably observable consequence. Under some circumstances, the severity of consequences cannot be known even in principle until the attacker effectuates them.\nLin (2009) distinguishes between *cyberattack* and *cyberexploitation*, whereby one entails merely copying files, software and hardware configurations to remote locations. Cyber attack, on the other hand, aims to degrade, alter or destroy software and hardware belonging to the adversary. Lin's categories distinguish attacks primarily designed to benefit the attacker by yielding information about the adversary, from attacks designed to impair the target's systems themselves. Lin developed this framework about the same time that he edited a review of offensive cyberattack capabilities for the United States (Owens et al., 2009).\nMany other classifications of malware and cyber attack exist. Rattray and Healey (2010) describe and illustrate the tactical functions of cyber attack. Technical definitions from military doctrine are grounded in the role cyber operations play in military tactics: support of non-cyber troops, interdiction, reconnaissance, counter-cyber, suppression of cyber defenses, and a strategic-level cyber mission. Their hypothetical scenarios point to different military and political contexts: interstate conflict, piracy, irregular war, terrorism, covert operations, and intelligence gathering.\nTsipenyuk et al. (2005) describe seven categories of vulnerabilities in software design, which today and in the future will lead to unauthorized access (i.e., read) and execution (i.e., write, execute). These technical categories describe the means by which encryption, authentication, state variables, and controls against unauthorized execution\n⁵Technically, Libicki's scale distinguishes between mass casualty events and heightened risk of nuclear catastrophe. I have omitted the distinction here.\nElectronic copy available at: https://ssrn.com/abstract=1928870\ncan be prevented.\n### 4.2 To identify attackers\nAttributing cyber attacks to specific individuals, organizations, and IP addresses is a deep and abiding problem in cyber security. Although there is considerable difference of expert opinion on the feasibility of attribution, *Clark and Landau (2010)* problematize the issue succinctly. They discuss the different meanings of attribution based on the defender’s purpose: machine-level attribution identifies the origins and sources of packets, and often their contents. Individual level attribution links the machine’s user to official identities such as name, residence, place of business, and registration with various government and corporate entities. Clark and Landau point out that the main obstacle to cyber attribution is often jurisdiction, rather than technical capability. The deciding factor, often, is whether investigators have timely cooperation for success across jurisdictions.\n### 4.3 To retaliate\nSecond strike capability raises more problems than it solves. *Owens et al. (2009)* devote an entire book to the legal and political difficulties facing states that would develop cyber weapons. Developing an offensive capability, due to the nature of cyber weapons, is not at all similar to firing missile tests and taking satellite observations of prospective targets. Cyber weapons largely do not point and shoot, and the ones that states are likely to employ as a retaliatory strike are not simple botnet attacks that would take websites offline. Instead, offensive cyber capabilities should require long periods of active gathering of access to potential adversaries’ systems. Only after a long period of reconnaissance can attackers have confidence in the success of a timely counterattack.\nDefining proportional response to a cyber attack is difficult, since it is often the effects of the cyber attack on non-cyber systems that states wish to punish. This line of reasoning is a slippery slope for states that seek to mitigate the risk of of cyber conflict or mollify incipient conflicts. Detractors of cyber offense also point to America’s relatively greater reliance on information technology both in military and civilian life *(Schwartau, 2000)*. Since we have more to lose from successful IT attacks, restricting the notion of proportionality to pure cyber retaliation for cyber attacks is a self-defeating line of reasoning; not unlike Richard Clarke’s famous accusation that Donald Rumsfeld wanted to find a better adversary than Afghanistan, with better targets for the Air Force.\nKnock-on effects of declaratory policy in this area are also troublesome. If we could define standards of proportionality and achieve consensus thereon, cyber attacks are sufficiently flexible that clever attackers could adjust the lethality, destructiveness, and economic impact of cyber attacks to evade reprisals.\nIn fact a number of the problems with cyber deterrence as a strategy have to do with knock-on effects *(Morgan, 2010)*. Convincing adversaries that the state has no plans to use cyber attacks is difficult enough, without declaring a policy to boot that virtually necessitates years of covert operations to prepare the ground for those cyber retalia\n4 CYBERSPACE AND DETERRENCE\n10\ntions. Declaration of intended lethal retaliation for pure cyber attacks could plausibly weaken credibility, if the promised reprisals are too strong.\n## 4.4 Time, the essential variable\nThe greatest single problem with deterrence in cyberspace is the anachronism of attack, effect, and retaliation. The central promise of deterrence is that the counterstrike will rob the attacker of the supposed fruits of the attack, through the promise of swift and certain retribution. For the most interesting cyber attacks, i.e., ones where the attacker will prepare the ground for days or years in advance of a tactical opportunity, the damage has long been done before the effects of the cyber attack become apparent. The promise of swift and certain retribution may simply ring hollow.\nSchwartau (2001) lays out a unique and relevant security model in layman’s terms. Schwartau points out that defensive security measures, in and of themselves, are largely all fallible. The most secure physical and cyber technologies that we can create are designed to keep things secret or off limits for a period of time, until a complementary response can be activated. Door locks and safes are measured by the duration of time for which they can withstand well defined classes of physical attack. Encryption algorithms are measured by the time required to crack passwords by brute force. Password tokens and password expiry dates are designed to ensure that passwords have a short shelf life (more on that below). Airgapped networks are designed to limit the opportunities that attackers have to access the system. By limiting the frequency of access, they effectively raise the time required to penetrate the system. Even the decision to maintain obscure, legacy operating systems (not Windows, Mac, or Unix) in particular firms demands time of would-be attackers, i.e., time spent developing the competence to crack access controls. In Schwartau’s model, every security measure is a method of buying time until the cavalry can arrive.\nSchwartau’s definition of security as a measure designed to gain time for appropriate response further complicates deterrence. The time to respond to an attack can only begin from the observation of the attack. A fully trusted computing platform, that could inform us when it had been breached—instead of when the attack’s effects were observed—is at best a distant prospect in the future.\nRoughly speaking, let us assume that the greater the time available for reconnaissance, the greater the scale of consequences that attackers can achieve and the higher the degree of confidence they can have in the result. If that is true, then there is an inexorable advantage among states under the security dilemma to conduct reconnaissance as early as possible. As a result, states know they must expect this behavior from their adversaries.\nThe start date of technical actions in cyber conflict occurs well in advance of the start date of political conflict, and well in advance of the start date of observed effects of cyber attacks. One of the main strategic goals of the national cyber defense strategy must be to take advantage of the planning phase of cyber conflict. The ways to take advantage of that phase are to develop situational awareness; to conduct appropriate counter-reconnaissance; to practice civil and military defense; to foster close interjurisdictional collaboration; and to gain the trust of likely allies.\nElectronic copy available at: https://ssrn.com/abstract=1928870\n5 LEVERS IN CYBERSPACE\n11\n# 5 Levers in cyberspace\n## 5.1 Cooperation\nCooperation is one of the policy levers that can serve cybersecurity strategy. There are many domains where allies can cooperate on cyber issues: law enforcement, intelligence, military, and civil defense. In each of these areas overlapping capabilities can provide specific benefits to potential allies.\nThe rapid pace of technological development is one oft-cited reason that cooperation is vital to success in cyberspace; but is not the only one. As a rough analogy, the databases used by antivirus software have a short shelf life. Within days they are obsolete, as the variety of malware observed in the wild grows continuously. Nearly all cybersecurity problems evolve and require constant study due to changes in software, hardware, and business practices. Any small, isolated organization will likely benefit from exchange of information related to cybersecurity, whether or not that information is of a primarily technical nature.\nInstitutional collaboration is arguably more important than exchange of technical information. Computer crimes in peacetime frequently involve a morass of overlapping national and local laws, international agreements, and competing authorities for investigation, prosecution and jurisdiction (Westby, 2004). People, hardware, and communications may operate in several (or dozens of) countries, using communications infrastructure, services, and data facilities that are themselves spread across geographic and political boundaries. Forensics, investigation and prosecution require formal structures for timely collaboration across these various borders.\nIntelligence gathering and threat analysis require similar institutional agreements for interagency and international cooperation. Intelligence organizations are rightly designed to take care with sensitive and classified information. Intelligence organizations by default lock down access to information that could be valuable to strategic competitors.\nIn peacetime, we must assume that states, firms, and non-state actors have already undertaken reconnaissance activities. Indeed evidence of this abounds (Demchak and Dombrowski, 2011). Exchange of reconnaissance information, both technical and organizational in nature, has the potential to greatly increase the quality of each party's information. There may be no hard and fast line between reconnaissance activities in cyberspace, threat analysis, and situational awareness for national defense. Many pieces of information that are necessary for threat analysis and attack detection would ultimately be useful for countermeasures and counterstrikes to cyber attacks.\nFinally, joint exercises are an important precursor to engagement in notionally \"hot\" cyber wars. Cyber security simulations train military and civil authorities to detect, analyze, and adapt to cyber attacks in the context of political conflicts, conventional hostilities, and threats to vital national interests. These simulations align expectations among allies for best practices, standards of inference, norms of behavior, and thresholds for retaliation of various kinds. Given the paucity of global standards for cyber conflict (Hayden, 2011; Libicki, 2009), first-hand experience of other countries' behavior and analysis may prove indispensable. Countries excluded from these exercises lose valuable opportunities to train their own personnel, develop and test\nElectronic copy available at: https://ssrn.com/abstract=1928870\n5 LEVERS IN CYBERSPACE\n12\nprocedures, learn about neighbors' practices, and advocate for favored practices with potential allies.\nGenerally, the larger the network of cooperating states, the greater is the potential benefit of access therein; and the greater the harm from exclusion. (Conversely, the larger the number of states collaborating, the more difficult it is to achieve consensus on exclusion, and to monitor and enforce such decisions.) Similarly, the greater the capabilities of the states collaborating in such a network, the greater is the benefit of inclusion in the network.\n## 5.2 Shared networks\nSpecific networks can themselves be assets offered to share with allies and withheld from enemies. These networks may be bounded by physical connections, communications protocols, or logical restrictions on access. As with the previous section, each of these levers can be wielded either as a carrot or a stick.\nSpecial diplomatic communications networks predate computers themselves. Telephone hotlines connected Moscow to Washington during the Cold War, in order to prevent missed calls during a nuclear crisis. SIPRNET limits unauthorized access to sensitive and classified information systems; and in theory it limits the temptation for personnel to copy sensitive information out to insecure hardware.\nIn the private sector, special financial networks connect banks, trading desks, market exchanges, and clearinghouses. Most firms and households do not have access to these information systems. Many other examples of communications networks for finance exist, in addition to those serving the largest OECD banks. Credit card payments networks, electronic funds transfers, wire transfers, Paypal, Western Union, and Islamic payments networks all provide special access to particular financial communications. Membership in these networks greatly reduces transaction costs for both the end users and the network operators themselves.\nSimilarly, telecommunications networks provide still further benefits. Telecommunications encompass a vast array of services, as simple and open as mobile telephone calls or shortwave radio broadcasts; and as complex as combat communications systems, which integrate secure streams of voice, positioning, imaging, targeting, and order-of-battle data.\nIn the future, as user identification protocols become more pervasive, participation in those registries will become another important source of leverage. Registries need not be centralized; but the ability to verify cryptographic signatures is central to a wide variety of authentication and encryption protocols for communications. Various levels of access could include permission to request an identity (either for an individual or for hardware); permission to request identities on behalf of others; permission to verify users' credentials against the registry of identities; and permission to create, alter, and delete identities in the registry. Trust is enacted in code. Software structures the choices users can make about extending trust to registries, users, and administrators. States, both by the grant (or revocation) of trust to various identity protocols and by the limits placed on identities created for users, can structure other states' access to legitimate telecommunications networks.\nElectronic copy available at: https://ssrn.com/abstract=1928870\n### 5.3 Standards development\nStates will also want a voice in the development of global standards in IT. A dizzying array of standards bodies exist today, and these standards bodies are often located outside government; although more and more states seek to bring these institutions directly under their authority. Standards bodies govern the development of technologies such the internet protocol itself and elements of commonly used computer languages and file formats; the assignment of names and addresses; and many other related technical standards.\nStandards bodies also handle closely related non-technical matters, such as financial, accounting and risk standards for corporate information technology. Standards bodies may be located within the government, in international organizations (e.g., the International Telecommunications Union), within a single private company (e.g., Microsoft), in industry associations, in a non-profit entity with joint representation from industry and government (e.g., ICANN), or in a private non-profit (e.g., Mozilla Foundation). Government may also solicit bids or competitions for specific standards, such as encryption algorithms, which are designed privately and then adopted by the government for official business. Sofaer et al. (2010) discuss the bodies in the international system with jurisdiction over IT standards and telecommunications law.\nMultilateral standards bodies could potentially play a role in technical, financial, legal, or other standards. Multilateral bodies are notoriously slow and weak, particularly when consensus is required to undertake action (Tsebelis, 1995). Bilateral treaties can substitute for multilateral arrangements, as in the case of investment law. Where a strong state enacts a number of nearly identical treaties with weaker counterparties, it effectively reduces the power of the counterparties in negotiations, who otherwise may be excluded from advantageous economic and security relations with the stronger state. The bilateral approach is also more brittle. The strong state must replicate bilateral negotiations over proposed amendments with each weaker state. Weaker states have less incentive to remain in bilateral agreements when disputes arise; exiting a multilateral treaty or body entails severing a tie with all member states.\n### 5.4 Private industry\nStates wishing to influence adversaries’ behavior must consider markets and market regulations as part of cyber strategy. Commercial and industrial policies have far reaching consequences for the development of technology, industry and a skilled workforce. Similarly, the alternatives available to would-be attackers depend greatly on commercial markets for software and hardware. States have many modes of contact with private industry.\nStates can legislate or regulate business practices in strategically important industries. For instance, the United States has banned unauthorized export of advanced weapons systems. States can enact financial and risk management standards that influence business practices. Rice (2008) discusses extensively how standards of risk tolerated in software design differ from those in consumer products, and the consequences of risk and liability for product design and marketing. Anderson and Moore (2006) develop economic models of externalized costs that explain persistent problems\n6 CONCLUSION\nwith software design, digital privacy, and insurance markets for cyber damages. States can also attempt to manage the development of standards for software, data, and communications; but Varian (2001) explains the particulars of how and why industry may not cooperate.\nStates can impose conditions on vendors. These conditions can involve criteria related to software license structure; geographic and political ties; adherence to particular data formats; performance criteria of services; features that create vendor lock-in; the components of software and service bundles; supply chain assurance; and a host of other features (Rosenweig, 2010). The purchasing power of the government may not be sufficient to appreciably alter the market shares of leading technology and standards; but it does provide the state with a lever of control over at commercial software developed for its own use.\nStates can develop standby arrangements for civil defense. These standby arrangements may be as simple as regular goodwill visits between (for example) the Department of Homeland Security industry liaisons and industry executives. They may take the form of immediate crisis response, either focused directly on monitoring and maintaining computer networks (e.g., Computer Emergency Readiness Teams) or on infrastructure assurance during cyber events. States have also developed laws governing corporate cooperation in law enforcement and national information policies; although these interactions entail potentially huge liability and reuptational risks to the corporation. Global corporations, for their part, face difficult decisions about whether corporate values and governance permit accommodation of unique local laws.\nStates can choose whether and when to subsidize and restrict public access to information assets, such GPS information and satellite imagery. Forbidding access to certain information assets may have the unintended consequence of encouraging competitors to acquire new skills and capabilities. Furthermore, the state can commission research on technologies that are not yet commercially available.\nStates that seek to gain strategic advantage in cyberspace must consider the unorthodox economics of information technology industries. Varian et al. (2004) describe features of supply and demand in information technology, competitive strategy and pricing dynamics, and relevant features of networks. Brafman and Beckstrom (2006), Tapscott and Williams (2006), and Barabási (2002) and give useful and engaging introductions to some newer theoretical approaches that help to explain why some networks don’t have leaders, what motivates apparently uncompensated collaborators on large projects, and how to think about the important features of networks. State policies on cyber security are fraught with pitfalls of unintended consequences; and economics is a good tool to demarcate the big ones.\n## 6 Conclusion\nThe basic condition of cyber conflict described in this article is that situational awareness is costly. States must assume that adversaries have begun to plumb the depths of the network long in advance of a conflict’s apparent start date. Most security measures that are designed to buy time for effective response must be implemented during peacetime. As a result, strategies that ignore the realities of daily business will not win\nElectronic copy available at: https://ssrn.com/abstract=1928870\nREFERENCES\nthe full cooperation of key industry partners. Winning cyber conflicts will, in a very specific sense, require states to win the peace."
    },
    "author_year": {
      "total": {
        "intext_total": 23,
        "success_occurrences": 23,
        "success_unique": 22,
        "bib_unique_total": 50,
        "occurrence_match_rate": 1.0,
        "bib_coverage_rate": 0.44,
        "success_percentage": 100.0,
        "style": "author_year"
      },
      "results": [
        {
          "index": "bajaj|2010",
          "intext_citation": "(Bajaj, 2010; Borg, 2005)",
          "preceding_text": "Cyber attacks have generated enormous controversy in international relations along the lines of a recurrent *Late Show with David Letterman* segment, \"Is this anything?\" Cyber hawks point to the far-reaching effects and cost-efficacy of attacks",
          "footnote": "Bajaj, K. (2010). Cybersecurity agenda: Mobilizing for international action. EastWest Institute."
        },
        {
          "index": "singel|2010",
          "intext_citation": "(Singel, 2010)",
          "preceding_text": "Skeptics accuse hawks of crying Chicken Little over simple web defacement",
          "footnote": "Singel, R. (2010, 1 March). Cyberwar hype intended to destroy the open internet. Blog post, http://www.wired.com/threatlevel/2010/03/cyber-war-hype/."
        },
        {
          "index": "gorman|2010",
          "intext_citation": "(Gorman, 2010)",
          "preceding_text": "Journalists",
          "footnote": "Gorman, S. (2010, 8 October). “US plans cyber shield for utilities, companies”. *Wall Street Journal*."
        },
        {
          "index": "thornburgh|2005",
          "intext_citation": "(Thornburgh, 2005)",
          "preceding_text": "Journalists (Gorman, 2010), freelancers",
          "footnote": "Thornburgh, N. (2005, 29 August). “The invasion of the Chinese cyberspies (and the man who tried to stop them)”. *Time*."
        },
        {
          "index": "krekel|2009",
          "intext_citation": "(Krekel, 2009; Walton et al., 2009)",
          "preceding_text": "Journalists (Gorman, 2010), freelancers (Thornburgh, 2005), think tanks",
          "footnote": "Krekel, B. (2009, October). Capability of the people’s republic of china to conduct cyber warfare and computer network exploitation. The US-China Economic and Security Review Commission."
        },
        {
          "index": "falliere|2010",
          "intext_citation": "(Falliere et al., 2010; Langevin et al., 2008; Allen et al., 2005; Tsipenyuk et al., 2005)",
          "preceding_text": "Journalists (Gorman, 2010), freelancers (Thornburgh, 2005), think tanks (Krekel, 2009; Walton et al., 2009), business",
          "footnote": "Falliere, N., L. O. Murchu, and E. Chien (2010, November). W32.Stuxnet Dossier: Version 1.3. Symantec."
        },
        {
          "index": "clarke|2010",
          "intext_citation": "(Clarke and Knake, 2010; Lynn, 2010; Morgan, 2010; Nye, 2010; Clarke, 2009; Owens et al., 2009; Liang and Xiangsui, 2002; Rattray, 2001)",
          "preceding_text": "ks (Krekel, 2009; Walton et al., 2009), business (Falliere et al., 2010; Langevin et al., 2008; Allen et al., 2005; Tsipenyuk et al., 2005), military doctrine (Joint Chiefs of Staff, 2007, 2000), government documents (White House, 2009, 1998) and IR theorists",
          "footnote": "Clarke, R. and R. Knake (2010). *Cyber War: The Next Threat to National Security and What to Do About It*. HarperCollins."
        },
        {
          "index": "achen|1989",
          "intext_citation": "(Achen and Snidal, 1989; Brodie, 1978)",
          "preceding_text": "Deterrence theory takes as its point of departure that the state is a rational actor",
          "footnote": "Achen, C. H. and D. Snidal (1989, January). Rational deterrence theory and comparative case studies. *World Politics* 41(2), 143–169."
        },
        {
          "index": "waltz|1990",
          "intext_citation": "(Waltz, 1990)",
          "preceding_text": "Deterrence theory takes as its point of departure that the state is a rational actor (Achen and Snidal, 1989; Brodie, 1978), subject to an inescapable calculus of self-interest",
          "footnote": "Waltz, K. N. (1990). Nuclear myths and political realities. *The American Political Science Review* 84(3), 731–745."
        },
        {
          "index": "morgan|1977",
          "intext_citation": "(Morgan, 1977)",
          "preceding_text": "Various critiques of the rational actor model of the state in international relations have been advanced, including unraveling of deterrence commitments",
          "footnote": "Morgan, P. (1977). *Deterrence*. Sage."
        },
        {
          "index": "kahneman|2000",
          "intext_citation": "(Kahneman and Tversky, 2000, 1979; Tversky and Kahneman, 1974)",
          "preceding_text": "90). Various critiques of the rational actor model of the state in international relations have been advanced, including unraveling of deterrence commitments (Morgan, 1977), well-established cognitive and behavioral deviance from simple economic self-interest",
          "footnote": "Kahneman, D. and A. Tversky (2000). *Choices, Values, and Frames*. Cambridge University Press."
        },
        {
          "index": "art|1980",
          "intext_citation": "(Art, 1980)",
          "preceding_text": "unraveling of deterrence commitments (Morgan, 1977), well-established cognitive and behavioral deviance from simple economic self-interest (Kahneman and Tversky, 2000, 1979; Tversky and Kahneman, 1974); status-seeking behaviors in nuclear weapons development",
          "footnote": "Art, R. J. (1980, April). To what ends military power? *International Security* 4(4), 3–35."
        },
        {
          "index": "tannenwald|1999",
          "intext_citation": "(Tannenwald, 1999; Dunn, 1994)",
          "preceding_text": "taboos in tactical deployment of nuclear weapons",
          "footnote": "Tannenwald, N. (1999). The nuclear taboo: The United States and the normative basis of nuclear non-use. International Organization 53, 433–468."
        },
        {
          "index": "dunn|1994",
          "intext_citation": "(Dunn, 1994)",
          "preceding_text": "stic missile. These points are not controversial. Efforts to overcome these technical limits of nuclear weapons (e.g., anti-ballistic missile defenses) attracted opprobium, as they threatened the stability of the mutually assured destruction (MAD) equilibrium",
          "footnote": "Dunn, L. A. (1994). Rethinking the nuclear equation: The United States and the new nuclear powers. *The Washington Quarterly* 17(1), 5–25."
        },
        {
          "index": "morgan|2010",
          "intext_citation": "(Morgan, 2010)",
          "preceding_text": "Patrick Morgan, a long-standing skeptic of deterrence theory, questions the applicability of nuclear deterrence to cyberspace *",
          "footnote": "Morgan, P. (2010). Applicability of traditional deterrence concepts and theory to the cyber realm. In *Proceedings of a Workshop on Deterring Cyber Attacks: Informing Strategies and Developing Options for US Policy*, Washington, D.C. National Academies Press."
        },
        {
          "index": "owens|2009",
          "intext_citation": "(Owens et al., 2009)",
          "preceding_text": "Lin developed this framework about the same time that he edited a review of offensive cyberattack capabilities for the United States",
          "footnote": "Owens, W. A., K. W. Dam, and H. S. Lin (Eds.) (2009). Technology, policy, law and ethics regarding US acquisition and use of offensive cyberattack capabilities. Committee on Offensive Information Warfare: National Research Council."
        },
        {
          "index": "schwartau|2000",
          "intext_citation": "(Schwartau, 2000)",
          "preceding_text": "Detractors of cyber offense also point to America’s relatively greater reliance on information technology both in military and civilian life *",
          "footnote": "Schwartau, W. (2000). Looming security threats: Asymmetrical adversaries. *Orbis* 44(2), 197–205."
        },
        {
          "index": "morgan|2010",
          "intext_citation": "(Morgan, 2010)",
          "preceding_text": "In fact a number of the problems with cyber deterrence as a strategy have to do with knock-on effects *",
          "footnote": "Morgan, P. (2010). Applicability of traditional deterrence concepts and theory to the cyber realm. In *Proceedings of a Workshop on Deterring Cyber Attacks: Informing Strategies and Developing Options for US Policy*, Washington, D.C. National Academies Press."
        },
        {
          "index": "westby|2004",
          "intext_citation": "(Westby, 2004)",
          "preceding_text": "Computer crimes in peacetime frequently involve a morass of overlapping national and local laws, international agreements, and competing authorities for investigation, prosecution and jurisdiction",
          "footnote": "Westby, J. (2004). *International Guide to Cyber Security*. American Bar Association."
        },
        {
          "index": "demchak|2011",
          "intext_citation": "(Demchak and Dombrowski, 2011)",
          "preceding_text": "In peacetime, we must assume that states, firms, and non-state actors have already undertaken reconnaissance activities. Indeed evidence of this abounds",
          "footnote": "Demchak, C. C. and P. Dombrowski (2011). Rise of a cybered Westphalian age. *Security Studies Quarterly* 5(1), 34–63."
        },
        {
          "index": "hayden|2011",
          "intext_citation": "(Hayden, 2011; Libicki, 2009)",
          "preceding_text": "Given the paucity of global standards for cyber conflict",
          "footnote": "Hayden, M. (2011). The future of things “cyber”. *Strategic Studies Quarterly* 5(1), 3–7."
        },
        {
          "index": "tsebelis|1995",
          "intext_citation": "(Tsebelis, 1995)",
          "preceding_text": "Multilateral bodies are notoriously slow and weak, particularly when consensus is required to undertake action",
          "footnote": "Tsebelis, G. (1995). Decision making in political systems: Veto players in presidentialism, parliamentarism, multicameralism and multipartyism. *British Journal of Political Science* 25(03), 289–325."
        },
        {
          "index": "rosenweig|2010",
          "intext_citation": "(Rosenweig, 2010)",
          "preceding_text": "license structure; geographic and political ties; adherence to particular data formats; performance criteria of services; features that create vendor lock-in; the components of software and service bundles; supply chain assurance; and a host of other features",
          "footnote": "Rosenweig, P. (2010). The organization of the united states government and private sector for achieving cyber deterrence. In Proceedings of a Workshop on Deterring Cyber Attacks: Informing Strategies and Developing Options for US Policy, Washington, D.C. National Academies Press."
        }
      ],
      "flat_text": "Electronic copy available at: https://ssrn.com/abstract=1928870 # Leverage in cyberspace, without deterrence Benjamin D. Mazzotta The Fletcher School of Law and Diplomacy, Tufts University benmazzotta@alumni.tufts.edu March 19, 2011 Paper presented to the Annual Convention of the International Studies Association.\nMontréal, QC.\n## Abstract Deterrence theory, which was of enormous value during the nuclear era, cannot adequately prepare defense thinkers for future cyber conflicts. Instead, the best states can hope for is robust cooperation with other states and with private actors to improve their circumstances in future conflicts. A number of key assumptions that supported nuclear deterrence do not hold in the cyber domain: immediately observable attacks; clear legal status under the law of armed conflict; states' preeminence for research and development; high financial and technical barriers to entry; cheap observation of delivery facilities; feasible second strike; and a stable MAD outcome. States can create several incentives for improved collaboration with allies and private sector partners. States can derive mutual benefits from shared intelligence; shared standards for risk assessment; joint exercises; and standby arrangements for crucial information, equipment, materials, and services. Apart from sanctions against access to these carrots, sticks include reconnaissance for cyber conflicts, cyber exploitation with the threat of disclosure, and policies explicitly linking cyber events to non-cyber diplomatic and security concerns. States have additional leverage over private actors through commercial regulation, law enforcement, and liability; although the burden of compliance may poison public-private cooperation.\n# 1 Introduction Cyber attacks have generated enormous controversy in international relations along the lines of a recurrent *Late Show with David Letterman* segment,\"Is this anything?\"Cyber hawks point to the far-reaching effects and cost-efficacy of attacks. Skeptics accuse hawks of crying Chicken Little over simple web defacement. Cyber war has captured Hollywood's imagination at the strategic level as in *Die Hard IV: Live Free or Die Hard*; at the tactical level as in *Vantage Point*; and oftener as dystopian techno-singularity as in *Eagle Eye*, *The Matrix*, and 2001: A Space Odyssey. Journalists, freelancers, think tanks, business, military doctrine (Joint Chiefs of Staff, 2007, 2000), government documents (White House, 2009, 1998) and IR theorists describe broad classes of attacks whose objectives might include the disruption of civilian infrastructure; tactical advantage in conventional war; intelligence gathering; financial gain; and terrorism. This article assumes the feasibility of broad classes of attack as a moot point, and outlines the strategic consequences of those attacks as if they were known capabilities.\nFaced with similarly disruptive military technology in the 1940s, the discipline of international relations refined the concept of deterrence to devise a strategy for safe navigation of potentially catastrophic nuclear arms races. Nuclear theorists then and cyber theorists now pointed out strikingly similar aspects of the new technology: vast geographic reach of the attacks, revolutionary intensity of purported attack effects, and relative cost efficacy of the new weapons. Cyber attacks may promise a revolution in military affairs similar to the nuclear revolution. Nuclear theorists then focused on several features of the new technology and constructed a strategic equilibrium from which deviance was less likely. Cyber theorists now must look carefully at the features of the new technology to assess its strategic consequences.\nDeterrence in cyberspace is a mirage. Cyberspace itself bears little resemblance to the physical world around us, and efforts to reason by analogy with physical space often do more harm than good. Nuclear deterrence is a particularly poor strategic model for cyberspace. The best we can hope for in cyberspace is to enlist the aid of allies who may cooperate for mutual benefit. There are specific sources of leverage that we have in cyberspace, thanks to our unusual capabilities in government and the private sector. Only when taken out of context, may these levers of control be shoehorned into a purported model of deterrence. Faith in deterrence may trick us to trust our own defenses, and blind us to opportunities for alliance.\n## 2 Deterrence theory ### 2.1 Incentives Deterrence works, in theory, because of a brutally simple set of incentives facing the state. State behavior under the security dilemma is the basis of deterrence. Aggressor states seek to gain power through aggressive wars. Strategic competitors promise retaliation for these wars that more than wipes out the gains from expansion.\nDeterrence theory takes as its point of departure that the state is a rational actor, subject to an inescapable calculus of self-interest. Various critiques of the rational actor model of the state in international relations have been advanced, including unraveling of deterrence commitments, well-established cognitive and behavioral deviance from simple economic self-interest; status-seeking behaviors in nuclear weapons development; Electronic copy available at: https://ssrn.com/abstract=1928870 taboos in tactical deployment of nuclear weapons; and constructivist narratives of leadership and the national interest.\nJervis (1979) describes waves of innovation designed to address shortcomings of deterrence theory. The major features of these second-wave models are the aggressor’s political calculus; the possibility of rewards and negotiations as complements to simple retaliation; and constructivist interpretations of the aggressor’s goals in security. Many of these criticisms are further explicated in Taliaferro (2001), which organizes contemporary variants of realist international security theory.\nNecessary conditions for effective deterrence vary greatly from one variant of the theory to another. Waltz (1990) holds that the key to nuclear deterrence is a reliable second strike that eliminates potential gains from aggression. He further argues that the low cost and weight of nuclear weapons leads inexorably to defensive vulnerability. Brodie (1978) argued that deterrence applied mainly to the Soviet-American security dilemma, and largely on the basis of Soviet acquisition of nuclear weapons. Extended deterrence modeled the behavior of allies within this bilateral framework.\n### 2.2 Technology Key features of nuclear technology circa 1960 were ease of observing weapons deployment at various stages, including launch site construction, weapons launch, and successful attack sites; extreme lethality and blast radius; absence of viable defensive countermeasures; difficulty of developing nuclear weapons, measured by financial costs, knowledge, and manufacturing capabilities; and limited means of delivery, viz., aircraft, oceangoing vessels, and ballistic missile. These points are not controversial. Efforts to overcome these technical limits of nuclear weapons (e.g., anti-ballistic missile defenses) attracted opprobium, as they threatened the stability of the mutually assured destruction (MAD) equilibrium.\nNuclear technology itself ensures that the adversary is immediately aware of any attacks; that a short list of suspects are considered capable of an attack; and that, in fact, at least one from the short list must have participated in the attack at some level. Dunn (1994) notes that American strategists ignored the possibility of nuclear weapons smuggling for just that reason. Satellite and aircraft observations were long considered sufficient to monitor nuclear weapons deployment. More exhaustive inspections of suspected facilities featured prominently in Iran, Iraq, and North Korea; but the nature of weapons development has changed considerably since the early nuclear age. Today critical materials are widely available, e.g., nuclear expertise, raw fuel, and industrial components for enrichment and delivery systems. This was absolutely not the case during the early nuclear age.\n### 2.3 Deterrence in cyberspace Deterrence, having served America so well during the Cold war, was an obvious place to begin discussions of cyber strategy. McConnell (2010), in an op-ed on cyber strategy, poses a choice between deterrence and preemption as the central strategic question in cyberspace. He further argues that deterrence rests on four necessary capabilities: a declared retaliation policy, attribution of the attack to the attacker, location of the Electronic copy available at: https://ssrn.com/abstract=1928870 attacker, and the counterstrike itself. If the layman’s definition of deterrence is that we can make ourselves safe in cyberspace without shooting first, then let us all hope that McConnell is correct. On the other hand, there is no consensus among experts that the four necessary conditions McConnell cites are even feasible, let alone likely to be achieved.\nThe National Research Council hosted an essay competition and conference in 2010 dedicated to the feasibility of deterrence in cyberspace. Patrick Morgan, a long-standing skeptic of deterrence theory, questions the applicability of nuclear deterrence to cyberspace **. *Lukasik (2010)* points to serious flaws in a wide variety of declaratory policies on deterrence, such as the threat to retaliate with conventional lethal force. *Clark and Landau (2010)* explain stubborn technical problems with attribution. *Rattray and Healey (2010)* give an overview of offensive cyber weapons, some of which are plausible for retaliation. *Schmitt (2010)* covers the legality of national self-defense under cyber attack; and in a related paper *Sofaer et al. (2010)* outline hypothetical features of international institutions that would purport to ameliorate cyber security.\n## 3 A simple model Successful nuclear deterrence relies on two crucial actions that are trivial in early Cold War nuclear conflict; but that are well-established problems in cyber conflict. The adversary must observe that an attack has in fact occurred. The adversary must also identify the attacker. This section of the article develops a quick Bayesian model of the adversary’s problem, with plausible hypotheses about the likely identity of the attacker.\nThis framework leads to several simple questions about attack technology and the adversary’s decisions.\n\nA convenient starting place is to model the expected utility of the attacker’s payoffs over the future states of the world: viz., attacks with retaliation, attacks without retaliation, and status quo.\n$U\\equiv u(\\mathbf{E}\\pi)$ (1) $\\mathbf{E}\\pi=p_{a}\\pi_{a}+p_{a}\\left(p_{r}\\pi_{r}+p_{\\sim r}\\pi_{\\sim r}\\right)+\\left(1-p_{a}\\right)\\pi_{s}$ (2) where\n\n|   | where  |\n| --- | --- |\n|  U: | expected utility  |\n|  a: | subscript for attack  |\n|  s: | subscript for status quo  |\n|  p: | probability  |\n|  π: | payoff  |\n|  r: | subscript for retaliation  |\n|  ~r: | subscript for nonretaliation  |\nWe can further decompose the probability of retaliation into separate component probabilities, as in (3). The adversary needs to detect that the attack has in fact taken place, attribute the source of the attack, and then decide to respond to the attack. Early Cold War deterrence proceeded as a special case of the equation below, where virtually any attack would lead to certain attribution. Today adversaries may only be able to suspect the adversary's identity; or suspect that an attack has in fact taken place; or have only a rough suspicion on both counts. $^{1}$\n$$\np _ {r} = \\underbrace {p _ {d , i} p _ {r | D \\cap I}} _ {\\text {certain attribution}} + \\underbrace {p _ {d , \\sim i} p _ {r | D \\cap \\sim I}} _ {\\text {unidentified attacker}} + \\underbrace {p _ {\\sim d , i} p _ {r | \\sim D \\cap I}} _ {\\text {suspected attack}} + \\underbrace {p _ {\\sim d , \\sim i} p _ {r | \\sim D \\cap \\sim I}} _ {\\text {rough suspicion}} \\tag {3}\n$$\nAnd when the adversary may mistake the identity of the attacker, as in (4), the attacker may also err in retaliation, with potentially (dis)adventaneous payoffs for the attacker.\n$$\n\\mathbf {E} \\pi = \\underbrace {p _ {\\mathrm {a}} \\pi_ {\\mathrm {a}}} _ {\\text {attack's benefit}} + \\underbrace {p _ {\\mathrm {a}} \\left(p _ {\\mathrm {r}} \\pi_ {\\mathrm {r}} + p _ {\\sim \\mathrm {r}} \\pi_ {\\sim \\mathrm {r}}\\right)} _ {\\text {retaliation's cost}} + \\underbrace {p _ {\\mathrm {d} , \\sim \\mathrm {i}} p _ {\\mathrm {m} | \\mathrm {D} \\cap \\sim \\mathrm {I}} + p _ {\\mathrm {s}} \\pi_ {\\mathrm {s}}} _ {\\text {mistaken counterattack}} \\tag {4}\n$$\n|   | where  |\n| --- | --- |\n|  pr: | probability of retaliation  |\n|  D: | successful detection of an attack  |\n|  ~D: | failure to detect an attack  |\n|  I: | successful identification of an attacker  |\n|  pr|D∩I: | probability of retaliation conditional on adversary's detection without identification  |\n|  pd: | probability of adversary's detecting attack  |\n|  pi: | probability of adversary's identifying attack  |\n|  pm: | probability of a mistaken counterattack  |\nThere are stark differences between nuclear attacks circa 1960, modeled in (5) and cyber attacks, modeled in (6). $^{2}$  Nuclear attacks are obvious by the scale of their destruction. Attribution was simple, due to the small number of states in possession of the technology. There are no serious barriers to acquisition of technical expertise or requisite hardware for cyber attack. There is no reason to suspect that the adversary\nElectronic copy available at: https://ssrn.com/abstract=1928870\n\nwill understand that an attack will be detected until long after it has begun. There is no reason that the adversary will immediately suspect whether the adversary in a given cyber attack is a state or not a state.\n$$ p _ {d i} \\gg p _ {d, - i} &gt; p _ {- d} \\tag {5} $$ $$ p _ {d, - i} &gt; p _ {- d, i} &gt; p _ {d, i} &gt; p _ {- d, - i} \\tag {6} $$ The prior probabilities for identifying the sources of detected nuclear attacks are also much simpler than those of detected cyber attacks. There are many fewer likely suspects for a nuclear attack, as in (7) than for a nuclear attack, as in (8).\n$$ \\text {P r i o r s}: \\left\\{ \\begin{array}{l l} \\sum p _ {H 1} \\dots p _ {H 1 0} =. 9 5 &amp; \\text {d e c l a r e d a n d k n o w n n u c l e a r s t a t e s} \\\\ \\sum p _ {H 1 1} \\dots p _ {H 2 0} =. 0 4 &amp; \\text {u n d e c l a r e d n u c l e a r s t a t e s} \\\\ \\sum p _ {H 2 1} \\dots p _ {H 1 0 0} =. 0 1 &amp; \\text {n o n - n u c l e a r s t a t e s, n o n - s t a t e a c t o r s} \\end{array} \\right. \\tag {7} $$ $$ \\text {P r i o r s}: \\left\\{ \\begin{array}{l l} \\sum p _ {H 1} \\dots p _ {H 1 0} =. 3 0 &amp; \\text {k n o w n s t a t e a d v e r s a r i e s} \\\\ \\sum p _ {H 1 1} \\dots p _ {H 2 0} =. 1 0 &amp; \\text {s u s p e c t e d s t a t e a d v e r s a r i e s} \\\\ \\sum p _ {H 2 1} \\dots p _ {H 1 0 0} =. 3 0 &amp; \\text {n o n - a d v e r s a r i a l s t a t e s} \\\\ \\sum p _ {H 1 0 1} \\dots p _ {H 5 0 0} =. 3 0 &amp; \\text {n o n - s t a t e a c t o r s} \\end{array} \\right. \\tag {8} $$ where H1: Hypothesis attributing attack to country 1 $\\mathbf{P}_{\\mathrm{H1}}$: Probability that H1 is true $\\mathbf{P}_{\\mathrm{H1|E}}$: Probability that H1 is true given evidence E The identity of the attacking state is a hypothesis that the state will attempt to prove by means of evidence. Evidence distinguishes among candidate hypotheses. Absent any data, the states has trouble choosing which adversary to hold responsible for an attack. Evidence of one sort or another updates beliefs. In the Bayesian framework, the best evidence is that which is essentially incompatible with an otherwise-plausible theory. It forces the state to relinquish cherished presuppositions about looming conflicts.\nThe quality of the evidence can be characterized as the difference between the prior distribution of probability among available hypotheses (9) and the posterior distribution (10).\n$$ \\text {P r i o r}: \\left\\{ \\begin{array}{l} p _ {H 1} =. 5 0 \\\\ p _ {H 2} =. 5 0 \\end{array} \\right. \\tag {9} $$ $$ \\text {P o s t e r i o r}: \\left\\{ \\begin{array}{l} p _ {H 1 | E} =. 9 0 \\\\ p _ {H 2 | E} =. 1 0 \\end{array} \\right. \\tag {10} $$ Evidence plays a much greater role in cyberspace than it does in nuclear conflict.\nElectronic copy available at: https://ssrn.com/abstract=1928870 Evidence is thus a primary tool for shaping other states' behavior. States possessing rich knowledge about internet traffic, threat analysis, forensics of prior attacks, etc. can leverage that knowledge by selectively sharing information. As a trusted third party, states can choose to share evidence that helps neighbors identify attackers and verify that attacks have in fact taken place. This information can produce two specific, useful changes of state behavior: neighbors can mind their own security interests; and neighbors refrain from destructive mistakes driven by inaccurate suspicion.\nDeception is also possible; but only at the expense of the state's reputation for honest communication. Frequent deception encourages potential allies and adversaries to mistrust proffered information; so it dilutes the state's future influence for the sake of a fleeting advantage.\n## 4 Cyberspace and deterrence ### 4.1 To observe attacks The Wikileaks release of US State Department cables illustrates a particular feature of cyber attacks that changes the strategy of deterrence. Bradley Manning's alleged distribution of classified files to non-combatants outside the United States did enormous damage to the tactical and strategic positions of the United States.³ Until the moment of the attack's public disclosure, however, little if any harm at all came to individuals or property in the United States. The specific information security policies circumvented were controls on Manning's permission to copy files on uncontrolled media. Specifically, he smuggled the files, which were ultimately copied to Wikileaks servers in various countries, out of a secure facility on a CD-R of Lady Gaga music.⁴ I dwell on this rogue-insider problem because it highlights the difference between the time of tactical cyber attack, such as it is, and that of observable attack effects. Rattray and Healey (2010) refer to cyberattacks as tactically quick but strategically ³For brevity, I will drop further use of the word alleged, although Manning’s trial has not established the facts of the case or his guilt. Case facts as related here are taken directly from popular media, and not any first-hand knowledge.\n⁴For the uninitiated: one of her better known songs is a duet with Beyoncé, Telephone: “K-kinda busy/ K-kinda busy /Sorry, I cannot hear you, I’m kinda busy./ …Stop callin’, stop callin’,” I don’t wanna think anymore!/ I left my hand and my heart on the dance floor.”\nElectronic copy available at: https://ssrn.com/abstract=1928870 This distinction is hard to reconcile with consequentialist cyber attack definitions. Notably, Libicki (2009) draws a spectrum of consequence severity, with either financial losses or bloodshed as the standard for valid reprisals.⁵ Prohibited access to computer systems falls into a consequentialist gray area, neither nuisance nor grievance. Libicki explicitly considers the difference between attacks achieved by cyber means and simple deception, in order to conclude that it is the consequence of the attack and not its means that justifies *jus ad bello*. If we adopt a consequentialist standard for cyber attack, however, we cannot escape the conclusion that harmless enough tactical maneuvers may be used to prepare the ground for future attacks of devastating consequence. The attacker's tactical maneuver may occur well in advance of any reasonably observable consequence. Under some circumstances, the severity of consequences cannot be known even in principle until the attacker effectuates them.\nLin (2009) distinguishes between *cyberattack* and *cyberexploitation*, whereby one entails merely copying files, software and hardware configurations to remote locations. Cyber attack, on the other hand, aims to degrade, alter or destroy software and hardware belonging to the adversary. Lin's categories distinguish attacks primarily designed to benefit the attacker by yielding information about the adversary, from attacks designed to impair the target's systems themselves. Lin developed this framework about the same time that he edited a review of offensive cyberattack capabilities for the United States.\nMany other classifications of malware and cyber attack exist. Rattray and Healey (2010) describe and illustrate the tactical functions of cyber attack. Technical definitions from military doctrine are grounded in the role cyber operations play in military tactics: support of non-cyber troops, interdiction, reconnaissance, counter-cyber, suppression of cyber defenses, and a strategic-level cyber mission. Their hypothetical scenarios point to different military and political contexts: interstate conflict, piracy, irregular war, terrorism, covert operations, and intelligence gathering.\nTsipenyuk et al. (2005) describe seven categories of vulnerabilities in software design, which today and in the future will lead to unauthorized access (i.e., read) and execution (i.e., write, execute). These technical categories describe the means by which encryption, authentication, state variables, and controls against unauthorized execution ⁵Technically, Libicki's scale distinguishes between mass casualty events and heightened risk of nuclear catastrophe. I have omitted the distinction here.\nElectronic copy available at: https://ssrn.com/abstract=1928870 can be prevented.\n### 4.2 To identify attackers Attributing cyber attacks to specific individuals, organizations, and IP addresses is a deep and abiding problem in cyber security. Although there is considerable difference of expert opinion on the feasibility of attribution, *Clark and Landau (2010)* problematize the issue succinctly. They discuss the different meanings of attribution based on the defender’s purpose: machine-level attribution identifies the origins and sources of packets, and often their contents. Individual level attribution links the machine’s user to official identities such as name, residence, place of business, and registration with various government and corporate entities. Clark and Landau point out that the main obstacle to cyber attribution is often jurisdiction, rather than technical capability. The deciding factor, often, is whether investigators have timely cooperation for success across jurisdictions.\n### 4.3 To retaliate Second strike capability raises more problems than it solves. *Owens et al. (2009)* devote an entire book to the legal and political difficulties facing states that would develop cyber weapons. Developing an offensive capability, due to the nature of cyber weapons, is not at all similar to firing missile tests and taking satellite observations of prospective targets. Cyber weapons largely do not point and shoot, and the ones that states are likely to employ as a retaliatory strike are not simple botnet attacks that would take websites offline. Instead, offensive cyber capabilities should require long periods of active gathering of access to potential adversaries’ systems. Only after a long period of reconnaissance can attackers have confidence in the success of a timely counterattack.\nDefining proportional response to a cyber attack is difficult, since it is often the effects of the cyber attack on non-cyber systems that states wish to punish. This line of reasoning is a slippery slope for states that seek to mitigate the risk of of cyber conflict or mollify incipient conflicts. Detractors of cyber offense also point to America’s relatively greater reliance on information technology both in military and civilian life **. Since we have more to lose from successful IT attacks, restricting the notion of proportionality to pure cyber retaliation for cyber attacks is a self-defeating line of reasoning; not unlike Richard Clarke’s famous accusation that Donald Rumsfeld wanted to find a better adversary than Afghanistan, with better targets for the Air Force.\nKnock-on effects of declaratory policy in this area are also troublesome. If we could define standards of proportionality and achieve consensus thereon, cyber attacks are sufficiently flexible that clever attackers could adjust the lethality, destructiveness, and economic impact of cyber attacks to evade reprisals.\nIn fact a number of the problems with cyber deterrence as a strategy have to do with knock-on effects **. Convincing adversaries that the state has no plans to use cyber attacks is difficult enough, without declaring a policy to boot that virtually necessitates years of covert operations to prepare the ground for those cyber retalia\n\n## 4.4 Time, the essential variable\nThe greatest single problem with deterrence in cyberspace is the anachronism of attack, effect, and retaliation. The central promise of deterrence is that the counterstrike will rob the attacker of the supposed fruits of the attack, through the promise of swift and certain retribution. For the most interesting cyber attacks, i.e., ones where the attacker will prepare the ground for days or years in advance of a tactical opportunity, the damage has long been done before the effects of the cyber attack become apparent. The promise of swift and certain retribution may simply ring hollow.\nSchwartau (2001) lays out a unique and relevant security model in layman’s terms. Schwartau points out that defensive security measures, in and of themselves, are largely all fallible. The most secure physical and cyber technologies that we can create are designed to keep things secret or off limits for a period of time, until a complementary response can be activated. Door locks and safes are measured by the duration of time for which they can withstand well defined classes of physical attack. Encryption algorithms are measured by the time required to crack passwords by brute force. Password tokens and password expiry dates are designed to ensure that passwords have a short shelf life (more on that below). Airgapped networks are designed to limit the opportunities that attackers have to access the system. By limiting the frequency of access, they effectively raise the time required to penetrate the system. Even the decision to maintain obscure, legacy operating systems (not Windows, Mac, or Unix) in particular firms demands time of would-be attackers, i.e., time spent developing the competence to crack access controls. In Schwartau’s model, every security measure is a method of buying time until the cavalry can arrive.\nSchwartau’s definition of security as a measure designed to gain time for appropriate response further complicates deterrence. The time to respond to an attack can only begin from the observation of the attack. A fully trusted computing platform, that could inform us when it had been breached—instead of when the attack’s effects were observed—is at best a distant prospect in the future.\nRoughly speaking, let us assume that the greater the time available for reconnaissance, the greater the scale of consequences that attackers can achieve and the higher the degree of confidence they can have in the result. If that is true, then there is an inexorable advantage among states under the security dilemma to conduct reconnaissance as early as possible. As a result, states know they must expect this behavior from their adversaries.\nThe start date of technical actions in cyber conflict occurs well in advance of the start date of political conflict, and well in advance of the start date of observed effects of cyber attacks. One of the main strategic goals of the national cyber defense strategy must be to take advantage of the planning phase of cyber conflict. The ways to take advantage of that phase are to develop situational awareness; to conduct appropriate counter-reconnaissance; to practice civil and military defense; to foster close interjurisdictional collaboration; and to gain the trust of likely allies.\nElectronic copy available at: https://ssrn.com/abstract=1928870\n\n## 5.1 Cooperation\nCooperation is one of the policy levers that can serve cybersecurity strategy. There are many domains where allies can cooperate on cyber issues: law enforcement, intelligence, military, and civil defense. In each of these areas overlapping capabilities can provide specific benefits to potential allies.\nThe rapid pace of technological development is one oft-cited reason that cooperation is vital to success in cyberspace; but is not the only one. As a rough analogy, the databases used by antivirus software have a short shelf life. Within days they are obsolete, as the variety of malware observed in the wild grows continuously. Nearly all cybersecurity problems evolve and require constant study due to changes in software, hardware, and business practices. Any small, isolated organization will likely benefit from exchange of information related to cybersecurity, whether or not that information is of a primarily technical nature.\nInstitutional collaboration is arguably more important than exchange of technical information. Computer crimes in peacetime frequently involve a morass of overlapping national and local laws, international agreements, and competing authorities for investigation, prosecution and jurisdiction . People, hardware, and communications may operate in several (or dozens of) countries, using communications infrastructure, services, and data facilities that are themselves spread across geographic and political boundaries. Forensics, investigation and prosecution require formal structures for timely collaboration across these various borders.\nIntelligence gathering and threat analysis require similar institutional agreements for interagency and international cooperation. Intelligence organizations are rightly designed to take care with sensitive and classified information. Intelligence organizations by default lock down access to information that could be valuable to strategic competitors.\nIn peacetime, we must assume that states, firms, and non-state actors have already undertaken reconnaissance activities. Indeed evidence of this abounds . Exchange of reconnaissance information, both technical and organizational in nature, has the potential to greatly increase the quality of each party's information. There may be no hard and fast line between reconnaissance activities in cyberspace, threat analysis, and situational awareness for national defense. Many pieces of information that are necessary for threat analysis and attack detection would ultimately be useful for countermeasures and counterstrikes to cyber attacks.\nFinally, joint exercises are an important precursor to engagement in notionally \"hot\" cyber wars. Cyber security simulations train military and civil authorities to detect, analyze, and adapt to cyber attacks in the context of political conflicts, conventional hostilities, and threats to vital national interests. These simulations align expectations among allies for best practices, standards of inference, norms of behavior, and thresholds for retaliation of various kinds. Given the paucity of global standards for cyber conflict , first-hand experience of other countries' behavior and analysis may prove indispensable. Countries excluded from these exercises lose valuable opportunities to train their own personnel, develop and test\nElectronic copy available at: https://ssrn.com/abstract=1928870\n\nGenerally, the larger the network of cooperating states, the greater is the potential benefit of access therein; and the greater the harm from exclusion. (Conversely, the larger the number of states collaborating, the more difficult it is to achieve consensus on exclusion, and to monitor and enforce such decisions.) Similarly, the greater the capabilities of the states collaborating in such a network, the greater is the benefit of inclusion in the network.\n## 5.2 Shared networks Specific networks can themselves be assets offered to share with allies and withheld from enemies. These networks may be bounded by physical connections, communications protocols, or logical restrictions on access. As with the previous section, each of these levers can be wielded either as a carrot or a stick.\nSpecial diplomatic communications networks predate computers themselves. Telephone hotlines connected Moscow to Washington during the Cold War, in order to prevent missed calls during a nuclear crisis. SIPRNET limits unauthorized access to sensitive and classified information systems; and in theory it limits the temptation for personnel to copy sensitive information out to insecure hardware.\nIn the private sector, special financial networks connect banks, trading desks, market exchanges, and clearinghouses. Most firms and households do not have access to these information systems. Many other examples of communications networks for finance exist, in addition to those serving the largest OECD banks. Credit card payments networks, electronic funds transfers, wire transfers, Paypal, Western Union, and Islamic payments networks all provide special access to particular financial communications. Membership in these networks greatly reduces transaction costs for both the end users and the network operators themselves.\nSimilarly, telecommunications networks provide still further benefits. Telecommunications encompass a vast array of services, as simple and open as mobile telephone calls or shortwave radio broadcasts; and as complex as combat communications systems, which integrate secure streams of voice, positioning, imaging, targeting, and order-of-battle data.\nIn the future, as user identification protocols become more pervasive, participation in those registries will become another important source of leverage. Registries need not be centralized; but the ability to verify cryptographic signatures is central to a wide variety of authentication and encryption protocols for communications. Various levels of access could include permission to request an identity (either for an individual or for hardware); permission to request identities on behalf of others; permission to verify users' credentials against the registry of identities; and permission to create, alter, and delete identities in the registry. Trust is enacted in code. Software structures the choices users can make about extending trust to registries, users, and administrators. States, both by the grant (or revocation) of trust to various identity protocols and by the limits placed on identities created for users, can structure other states' access to legitimate telecommunications networks.\nElectronic copy available at: https://ssrn.com/abstract=1928870 ### 5.3 Standards development States will also want a voice in the development of global standards in IT. A dizzying array of standards bodies exist today, and these standards bodies are often located outside government; although more and more states seek to bring these institutions directly under their authority. Standards bodies govern the development of technologies such the internet protocol itself and elements of commonly used computer languages and file formats; the assignment of names and addresses; and many other related technical standards.\nStandards bodies also handle closely related non-technical matters, such as financial, accounting and risk standards for corporate information technology. Standards bodies may be located within the government, in international organizations (e.g., the International Telecommunications Union), within a single private company (e.g., Microsoft), in industry associations, in a non-profit entity with joint representation from industry and government (e.g., ICANN), or in a private non-profit (e.g., Mozilla Foundation). Government may also solicit bids or competitions for specific standards, such as encryption algorithms, which are designed privately and then adopted by the government for official business. Sofaer et al. (2010) discuss the bodies in the international system with jurisdiction over IT standards and telecommunications law.\nMultilateral standards bodies could potentially play a role in technical, financial, legal, or other standards. Multilateral bodies are notoriously slow and weak, particularly when consensus is required to undertake action. Bilateral treaties can substitute for multilateral arrangements, as in the case of investment law. Where a strong state enacts a number of nearly identical treaties with weaker counterparties, it effectively reduces the power of the counterparties in negotiations, who otherwise may be excluded from advantageous economic and security relations with the stronger state. The bilateral approach is also more brittle. The strong state must replicate bilateral negotiations over proposed amendments with each weaker state. Weaker states have less incentive to remain in bilateral agreements when disputes arise; exiting a multilateral treaty or body entails severing a tie with all member states.\n### 5.4 Private industry States wishing to influence adversaries’ behavior must consider markets and market regulations as part of cyber strategy. Commercial and industrial policies have far reaching consequences for the development of technology, industry and a skilled workforce. Similarly, the alternatives available to would-be attackers depend greatly on commercial markets for software and hardware. States have many modes of contact with private industry.\nStates can legislate or regulate business practices in strategically important industries. For instance, the United States has banned unauthorized export of advanced weapons systems. States can enact financial and risk management standards that influence business practices. Rice (2008) discusses extensively how standards of risk tolerated in software design differ from those in consumer products, and the consequences of risk and liability for product design and marketing. Anderson and Moore (2006) develop economic models of externalized costs that explain persistent problems with software design, digital privacy, and insurance markets for cyber damages. States can also attempt to manage the development of standards for software, data, and communications; but Varian (2001) explains the particulars of how and why industry may not cooperate.\nStates can impose conditions on vendors. These conditions can involve criteria related to software license structure; geographic and political ties; adherence to particular data formats; performance criteria of services; features that create vendor lock-in; the components of software and service bundles; supply chain assurance; and a host of other features. The purchasing power of the government may not be sufficient to appreciably alter the market shares of leading technology and standards; but it does provide the state with a lever of control over at commercial software developed for its own use.\nStates can develop standby arrangements for civil defense. These standby arrangements may be as simple as regular goodwill visits between (for example) the Department of Homeland Security industry liaisons and industry executives. They may take the form of immediate crisis response, either focused directly on monitoring and maintaining computer networks (e.g., Computer Emergency Readiness Teams) or on infrastructure assurance during cyber events. States have also developed laws governing corporate cooperation in law enforcement and national information policies; although these interactions entail potentially huge liability and reuptational risks to the corporation. Global corporations, for their part, face difficult decisions about whether corporate values and governance permit accommodation of unique local laws.\nStates can choose whether and when to subsidize and restrict public access to information assets, such GPS information and satellite imagery. Forbidding access to certain information assets may have the unintended consequence of encouraging competitors to acquire new skills and capabilities. Furthermore, the state can commission research on technologies that are not yet commercially available.\nStates that seek to gain strategic advantage in cyberspace must consider the unorthodox economics of information technology industries. Varian et al. (2004) describe features of supply and demand in information technology, competitive strategy and pricing dynamics, and relevant features of networks. Brafman and Beckstrom (2006), Tapscott and Williams (2006), and Barabási (2002) and give useful and engaging introductions to some newer theoretical approaches that help to explain why some networks don’t have leaders, what motivates apparently uncompensated collaborators on large projects, and how to think about the important features of networks. State policies on cyber security are fraught with pitfalls of unintended consequences; and economics is a good tool to demarcate the big ones.\n## 6 Conclusion The basic condition of cyber conflict described in this article is that situational awareness is costly. States must assume that adversaries have begun to plumb the depths of the network long in advance of a conflict’s apparent start date. Most security measures that are designed to buy time for effective response must be implemented during peacetime. As a result, strategies that ignore the realities of daily business will not win Electronic copy available at: https://ssrn.com/abstract=1928870 REFERENCES the full cooperation of key industry partners. Winning cyber conflicts will, in a very specific sense, require states to win the peace."
    }
  },
  "citation_summary": {
    "style": "author_year",
    "dominant_bucket": "author_year",
    "dominant": {
      "intext_total": 23.0,
      "success_occurrences": 23.0,
      "success_unique": 22.0,
      "bib_unique_total": 50.0,
      "occurrence_match_rate": 1.0,
      "bib_coverage_rate": 0.44,
      "success_percentage": 100.0,
      "missing_intext_expected_total": 0,
      "highest_intext_index": 0,
      "missing_footnotes_for_seen_total": 0,
      "uncited_footnote_total": 0,
      "style": "author_year"
    },
    "buckets": {
      "footnotes": {
        "intext_total": 2.0,
        "success_occurrences": 0.0,
        "success_unique": 0.0,
        "bib_unique_total": 0.0,
        "occurrence_match_rate": 0.0,
        "bib_coverage_rate": 0.0,
        "success_percentage": 0.0,
        "missing_intext_expected_total": 0.0,
        "highest_intext_index": 2.0,
        "missing_footnotes_for_seen_total": 2.0,
        "uncited_footnote_total": 0.0,
        "style": "footnotes"
      },
      "tex": {
        "intext_total": 2.0,
        "success_occurrences": 2.0,
        "success_unique": 2.0,
        "bib_unique_total": 2.0,
        "occurrence_match_rate": 1.0,
        "bib_coverage_rate": 1.0,
        "success_percentage": 100.0,
        "missing_intext_expected_total": 0.0,
        "highest_intext_index": 2.0,
        "missing_footnotes_for_seen_total": 0.0,
        "uncited_footnote_total": 0.0,
        "style": "tex_superscript"
      },
      "numeric": {
        "intext_total": 0.0,
        "success_occurrences": 0.0,
        "success_unique": 0.0,
        "bib_unique_total": 0.0,
        "occurrence_match_rate": 0.0,
        "bib_coverage_rate": 0.0,
        "success_percentage": 0.0,
        "missing_intext_expected_total": 0,
        "highest_intext_index": 0,
        "missing_footnotes_for_seen_total": 0,
        "uncited_footnote_total": 0,
        "style": "numeric"
      },
      "author_year": {
        "intext_total": 23.0,
        "success_occurrences": 23.0,
        "success_unique": 22.0,
        "bib_unique_total": 50.0,
        "occurrence_match_rate": 1.0,
        "bib_coverage_rate": 0.44,
        "success_percentage": 100.0,
        "missing_intext_expected_total": 0,
        "highest_intext_index": 0,
        "missing_footnotes_for_seen_total": 0,
        "uncited_footnote_total": 0,
        "style": "author_year"
      }
    }
  },
  "metadata": {
    "title": "Leverage in cyberspace, without deterrence",
    "subtitle": "The Fletcher School of Law and Diplomacy, Tufts University",
    "document_type": "journal_article",
    "venue": "",
    "publication_year": 2011,
    "authors": [
      "Benjamin D. Mazzotta"
    ],
    "affiliations": [
      "The Fletcher School of Law and Diplomacy, Tufts University"
    ],
    "emails": [
      "benmazzotta@alumni.tufts.edu"
    ],
    "orcids": [],
    "corresponding_author_line": "",
    "abstract": "Deterrence theory, which was of enormous value during the nuclear era, cannot adequately prepare defense thinkers for future cyber conflicts. Instead, the best states can hope for is robust cooperation with other states and with private actors to improve their circumstances in future conflicts. A number of key assumptions that supported nuclear deterrence do not hold in the cyber domain: immediately observable attacks; clear legal status under the law of armed conflict; states' preeminence for research and development; high financial and technical barriers to entry; cheap observation of delivery facilities; feasible second strike; and a stable MAD outcome. States can create several incentives for improved collaboration with allies and private sector partners. States can derive mutual benefits from shared intelligence; shared standards for risk assessment; joint exercises; and standby arrangements for crucial information, equipment, materials, and services. Apart from sanctions against access to these carrots, sticks include reconnaissance for cyber conflicts, cyber exploitation with the threat of disclosure, and policies explicitly linking cyber events to non-cyber diplomatic and security concerns. States have additional leverage over private actors through commercial regulation, law enforcement, and liability; although the burden of compliance may poison public-private cooperation.",
    "keywords": [],
    "publication_dates": {},
    "identifiers": {
      "doi": [],
      "issn": [],
      "isbn": [],
      "arxiv": [],
      "pmid": [],
      "pmcid": [],
      "urls": [
        "https://ssrn.com/abstract=1928870"
      ]
    },
    "references_block_count": 1,
    "references_entries_estimated": 52,
    "heading_count": 20,
    "max_heading_level": 3,
    "partial_document": {
      "is_partial_document": false,
      "reasons": [
        "no_identifier"
      ],
      "toc_dot_lines": 0
    }
  },
  "validation": {
    "dominant_quality": {
      "intext_total": 23,
      "index_coverage": 1.0,
      "intext_citation_coverage": 1.0,
      "preceding_text_coverage": 1.0,
      "footnote_coverage": 1.0,
      "unique_index_count": 22
    },
    "footnotes_quality": {
      "intext_total": 2,
      "index_coverage": 1.0,
      "intext_citation_coverage": 1.0,
      "preceding_text_coverage": 0.0,
      "footnote_coverage": 0.0,
      "unique_index_count": 2,
      "items_total": 0
    },
    "style_validation": {
      "detected_style": "author_year",
      "recommended_style": "author_year",
      "aligned": true,
      "signals": {
        "superscript_hits": 6,
        "superscript_definition_lines": 0,
        "numeric_bracket_hits": 0,
        "numeric_endnote_lines": 5,
        "author_year_hits": 14
      }
    },
    "coverage_validation": {
      "dominant_bib_total": 50.0,
      "dominant_bib_coverage_rate": 0.44,
      "dominant_link_target": "bibliography",
      "dominant_unresolved_flag": "unresolved_reference_links"
    },
    "heading_validation": {
      "heading_count": 20,
      "max_heading_level": 3,
      "level_jump_violations": 0,
      "numbering_parent_violations": 11,
      "has_reference_heading": true,
      "has_subheadings": true
    },
    "metadata_validation": {
      "field_presence": {
        "title": true,
        "authors": true,
        "affiliations": true,
        "emails": true,
        "orcids": false,
        "abstract": true,
        "keywords": false,
        "venue": false,
        "persistent_identifier": false,
        "headings": true,
        "partial_document": false
      },
      "counts": {
        "authors": 1,
        "affiliations": 1,
        "emails": 1,
        "orcids": 0,
        "keywords": 0,
        "doi": 0,
        "issn": 0,
        "isbn": 0,
        "arxiv": 0,
        "pmid": 0,
        "pmcid": 0,
        "urls": 1
      },
      "coverage": {
        "core_coverage": 0.8333333333333334,
        "email_author_link_rate": 1.0
      },
      "partial_document": {
        "is_partial_document": false,
        "reasons": [
          "no_identifier"
        ],
        "toc_dot_lines": 0
      },
      "flags": [
        "meta_missing_persistent_identifier"
      ]
    },
    "flags": [
      "low_bib_coverage",
      "footnotes_bucket_unresolved",
      "heading_numbering_parent_violation",
      "meta_missing_persistent_identifier"
    ]
  },
  "updated_at_utc": "2026-02-14T08:27:34.618871+00:00"
}