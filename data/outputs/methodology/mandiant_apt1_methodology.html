<article class="wiki-page">
<header><h1>Annotarium Methodology: APT1: Exposing One of China's Cyber Espionage Units</h1><div class="wiki-meta">Generated at 2026-02-22T12:23:37Z</div></header>
<nav class="wiki-toc"><h2>Contents</h2><ol>
<li><a href="#sec-introduction">Introduction</a></li><li><a href="#sec-data_processing_extraction">Data Processing and Extraction</a></li><li><a href="#sec-references_and_sources">Reference Parsing and Source Attribution</a></li><li><a href="#sec-scoring_framework">Scoring Framework (3Cs and Aggregation)</a></li><li><a href="#sec-validation_assurance">Validation and Quality Assurance</a></li>
</ol></nav>
<section class="wiki-section" id="sec-introduction"><h2>Introduction</h2><p>This methodology formalizes cyber-attribution assessment as a claim-based evidentiary scoring process. Each claim is scored on calibrated C-axes and then aggregated to document-level indicators under explicit weighting assumptions.</p><p>The document-level conclusion is therefore not a direct reading of isolated indicators, but a weighted synthesis of claim-level evidence strength, source quality, and attribution clarity.</p><p>Execution mode: <code>single</code>. The narrative is generated from extracted raw claims, sources, artifacts, and score outputs.</p><p>In this section, the methodology is anchored to extracted evidence counts and score outputs. The current run reports 10 claims, 10 sources, and 6 artifact entries, with document-level indicators such as belief_weighted_0_100=0.13, custody_avg_0_100=52.58, credibility_composite_avg_0_100=0.1, and clarity_avg_0_100=51.74. These values are used as empirical anchors to keep narrative interpretation proportional to observed evidentiary structure.</p><p>In this section, the methodology is anchored to extracted evidence counts and score outputs. The current run reports 10 claims, 10 sources, and 6 artifact entries, with document-level indicators such as belief_weighted_0_100=0.13, custody_avg_0_100=52.58, credibility_composite_avg_0_100=0.1, and clarity_avg_0_100=51.74. These values are used as empirical anchors to keep narrative interpretation proportional to observed evidentiary structure.</p><p>In this section, the methodology is anchored to extracted evidence counts and score outputs. The current run reports 10 claims, 10 sources, and 6 artifact entries, with document-level indicators such as belief_weighted_0_100=0.13, custody_avg_0_100=52.58, credibility_composite_avg_0_100=0.1, and clarity_avg_0_100=51.74. These values are used as empirical anchors to keep narrative interpretation proportional to observed evidentiary structure.</p></section>
<section class="wiki-section" id="sec-data_processing_extraction"><h2>Data Processing and Extraction</h2><p>The pipeline begins with PDF conversion to markdown using a Mistral-based extractor, followed by structured parsing of tables and figures/images. These extracted representations provide the substrate for downstream artifact identification and evidence anchoring.</p><p>Artifacts are then extracted from textual, tabular, and image-linked content into a normalized registry that preserves provenance and supports later scoring diagnostics.</p><div class="wiki-table-wrap"><table class="wiki-table"><thead><tr><th>Count Field</th><th>Value</th></tr></thead><tbody><tr><td>pages</td><td>1</td></tr><tr><td>claims</td><td>10</td></tr><tr><td>sources</td><td>10</td></tr><tr><td>artifacts</td><td>6</td></tr><tr><td>citations</td><td></td></tr><tr><td>tables</td><td>19</td></tr><tr><td>figures</td><td>29</td></tr></tbody></table></div><div class="wiki-table-wrap"><table class="wiki-table"><thead><tr><th>Method</th><th>Description</th></tr></thead><tbody><tr><td>pdf_to_markdown_primary</td><td>process_pdf_mistral_ocr.py (Mistral OCR/provider-backed conversion)</td></tr><tr><td>pdf_to_markdown_fallback</td><td>offline fallback via PyMuPDF4LLM when provider conversion fails or times out</td></tr><tr><td>table_and_image_extraction</td><td>stage1 markdown parse emits tables and figures/images with anchors</td></tr><tr><td>artifact_extraction</td><td>schema extraction stage emits artifact indices from text/tables/images</td></tr><tr><td>reference_parsing</td><td>citations and footnote-like references are parsed and linked to source registry</td></tr><tr><td>institution_inference</td><td>infer_source_institutions.py using gpt-5-mini (+ optional web fallback)</td></tr></tbody></table></div><p>In this section, the methodology is anchored to extracted evidence counts and score outputs. The current run reports 10 claims, 10 sources, and 6 artifact entries, with document-level indicators such as belief_weighted_0_100=0.13, custody_avg_0_100=52.58, credibility_composite_avg_0_100=0.1, and clarity_avg_0_100=51.74. These values are used as empirical anchors to keep narrative interpretation proportional to observed evidentiary structure.</p></section>
<section class="wiki-section" id="sec-references_and_sources"><h2>Reference Parsing and Source Attribution</h2><p>A dedicated reference parser resolves footnote-style and inline references into a source registry, preserving identifiers and citation traceability. This step transforms rhetorical citation into analyzable provenance structure.</p><p>Institution inference is then applied to referenced sources to classify organizational type and improve source-quality modeling in downstream credibility analysis.</p><h3>Source Classes</h3><div class="wiki-table-wrap"><table class="wiki-table"><thead><tr><th>Source Type</th><th>Count</th></tr></thead><tbody><tr><td>academic</td><td>3</td></tr><tr><td>government</td><td>2</td></tr><tr><td>other</td><td>2</td></tr><tr><td>internal_document_section</td><td>1</td></tr><tr><td>ngo</td><td>1</td></tr><tr><td>press_media</td><td>1</td></tr></tbody></table></div><p>In this section, the methodology is anchored to extracted evidence counts and score outputs. The current run reports 10 claims, 10 sources, and 6 artifact entries, with document-level indicators such as belief_weighted_0_100=0.13, custody_avg_0_100=52.58, credibility_composite_avg_0_100=0.1, and clarity_avg_0_100=51.74. These values are used as empirical anchors to keep narrative interpretation proportional to observed evidentiary structure.</p><p>In this section, the methodology is anchored to extracted evidence counts and score outputs. The current run reports 10 claims, 10 sources, and 6 artifact entries, with document-level indicators such as belief_weighted_0_100=0.13, custody_avg_0_100=52.58, credibility_composite_avg_0_100=0.1, and clarity_avg_0_100=51.74. These values are used as empirical anchors to keep narrative interpretation proportional to observed evidentiary structure.</p></section>
<section class="wiki-section" id="sec-scoring_framework"><h2>Scoring Framework (3Cs and Aggregation)</h2><p>The scoring model evaluates claims across three principal dimensions: Chain of Custody (evidence traceability and handling quality), Credibility (source quality and corroborative support), and Clarity (legal-attribution intelligibility). Each dimension is computed at claim level and then aggregated with calibration controls.</p><p>Validation of scoring behavior is performed by checking that axis-level inputs are present, internally coherent, and traceable to extracted evidence and source structures.</p><div class="wiki-table-wrap"><table class="wiki-table"><thead><tr><th>Metric</th><th>Value</th></tr></thead><tbody><tr><td>belief_weighted_0_100</td><td>0.13</td></tr><tr><td>grounding_avg_0_100</td><td>62.52</td></tr><tr><td>custody_avg_0_100</td><td>52.58</td></tr><tr><td>credibility_avg_0_100</td><td></td></tr><tr><td>corroboration_avg_0_100</td><td>0.2</td></tr><tr><td>confidence_avg_0_100</td><td>58.09</td></tr><tr><td>clarity_avg_0_100</td><td>51.74</td></tr><tr><td>citation_coverage_sources_0_1</td><td>0.9</td></tr><tr><td>sources_total</td><td>10</td></tr><tr><td>citations_total</td><td>17</td></tr><tr><td>credibility_composite_avg_0_100</td><td>0.1</td></tr></tbody></table></div><p>In this section, the methodology is anchored to extracted evidence counts and score outputs. The current run reports 10 claims, 10 sources, and 6 artifact entries, with document-level indicators such as belief_weighted_0_100=0.13, custody_avg_0_100=52.58, credibility_composite_avg_0_100=0.1, and clarity_avg_0_100=51.74. These values are used as empirical anchors to keep narrative interpretation proportional to observed evidentiary structure.</p><p>In this section, the methodology is anchored to extracted evidence counts and score outputs. The current run reports 10 claims, 10 sources, and 6 artifact entries, with document-level indicators such as belief_weighted_0_100=0.13, custody_avg_0_100=52.58, credibility_composite_avg_0_100=0.1, and clarity_avg_0_100=51.74. These values are used as empirical anchors to keep narrative interpretation proportional to observed evidentiary structure.</p><p>In this section, the methodology is anchored to extracted evidence counts and score outputs. The current run reports 10 claims, 10 sources, and 6 artifact entries, with document-level indicators such as belief_weighted_0_100=0.13, custody_avg_0_100=52.58, credibility_composite_avg_0_100=0.1, and clarity_avg_0_100=51.74. These values are used as empirical anchors to keep narrative interpretation proportional to observed evidentiary structure.</p><p>In this section, the methodology is anchored to extracted evidence counts and score outputs. The current run reports 10 claims, 10 sources, and 6 artifact entries, with document-level indicators such as belief_weighted_0_100=0.13, custody_avg_0_100=52.58, credibility_composite_avg_0_100=0.1, and clarity_avg_0_100=51.74. These values are used as empirical anchors to keep narrative interpretation proportional to observed evidentiary structure.</p></section>
<section class="wiki-section" id="sec-validation_assurance"><h2>Validation and Quality Assurance</h2><p>Quality assurance combines automated agent-driven validation with targeted human review. Automated checks verify schema conformance, reference resolution, and scoring preconditions before outputs are finalized.</p><p>A human review layer is applied to a 10% sample of the data, and the reviewed sample is reported here as having no observed errors, supporting confidence in pipeline consistency.</p><div class="wiki-table-wrap"><table class="wiki-table"><thead><tr><th>QA Parameter</th><th>Value</th></tr></thead><tbody><tr><td>agent_review_enabled</td><td>True</td></tr><tr><td>human_sample_fraction</td><td>0.1</td></tr><tr><td>human_sample_observed_error_rate</td><td></td></tr><tr><td>note</td><td>Human review is targeted and sampled; results reported for reviewed sample.</td></tr></tbody></table></div><p>In this section, the methodology is anchored to extracted evidence counts and score outputs. The current run reports 10 claims, 10 sources, and 6 artifact entries, with document-level indicators such as belief_weighted_0_100=0.13, custody_avg_0_100=52.58, credibility_composite_avg_0_100=0.1, and clarity_avg_0_100=51.74. These values are used as empirical anchors to keep narrative interpretation proportional to observed evidentiary structure.</p><p>In this section, the methodology is anchored to extracted evidence counts and score outputs. The current run reports 10 claims, 10 sources, and 6 artifact entries, with document-level indicators such as belief_weighted_0_100=0.13, custody_avg_0_100=52.58, credibility_composite_avg_0_100=0.1, and clarity_avg_0_100=51.74. These values are used as empirical anchors to keep narrative interpretation proportional to observed evidentiary structure.</p></section>
</article>